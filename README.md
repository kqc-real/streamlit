# üìù MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?b‚îú‚îÄmain)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App.
Sie bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse f√ºr verschiedene Fragensets.
Die App ist modular aufgebaut und nutzt eine SQLite-Datenbank zur persistenten Speicherung von Testergebnissen.
Die App verf√ºgt √ºber ein integriertes Feedback-System, das es Nutzern erm√∂glicht, Probleme mit Fragen zu melden, und Admins, dieses Feedback zu verwalten.

---

## üöÄ Schnellstart

**üìñ Installationsanleitungen f√ºr Einsteiger/innen**
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_MAC_ANLEITUNG.md
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_WINDOWS_ANLEITUNG.md
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_VS-CODE_SSH-AUTHENTIFIZIERUNG.md

Diese Schritt-f√ºr-Schritt-Anleitungen erkl√§ren alles von Grund auf:
- Python & Git installieren (Windows & Mac)
- App herunterladen und starten
- H√§ufige Probleme und L√∂sungen
- **Perfekt f√ºr BWL-Studierende ohne IT-Kenntnisse!**

**Admin-Panel lokal testen?**
‚Üí **[üîê Admin-Panel Anleitung f√ºr Kursteilnehmer/innen](ADMIN_PANEL_ANLEITUNG.md)**

Diese Anleitung zeigt dir:
- Wie du als "Albert Einstein" Admin-Rechte erh√§ltst
- Was du im Admin-Panel alles tun kannst (Analytics, Itemanalyse, Feedback)
- Wie Itemanalyse und Distraktor-Analyse funktionieren
- **Perfekt f√ºr Projektmitglieder, die alle Features verstehen wollen!**

---

## üöÄ √úbersicht

Diese App ist ein vollst√§ndiger MC-Test f√ºr Kursinhalte, entwickelt mit Streamlit.
Sie erm√∂glicht anonyme Tests mit Pseudonymen, zuf√§lliger Fragenreihenfolge, Zeitlimit und einem integrierten Feedback-System zur kontinuierlichen Verbesserung der Fragen.
Perfekt f√ºr Bildungsumgebungen, Selbstlernphasen oder zur Pr√ºfungsvorbereitung.

### Hauptfunktionen

| Kategorie      | Funktion                                                                                      |
|----------------|-----------------------------------------------------------------------------------------------|
| Zugang         | Pseudonym-Login (anonym, keine Registrierung)                                                 |
| Fragen         | Zuf√§llige Reihenfolge, Gewichtung je Frage, strikte Trennung nach Fragenset                   |
| Fragenset      | Dynamische Auswahl verschiedener Fragensets (`questions_*.json`)                               |
| Scoring-Modi   | "Nur +Punkte" (falsch = 0) oder "+/- Punkte" (falsch = -Gewichtung)                            |
| Feedback       | Sofortiges Ergebnis mit optionalen, detaillierten Erkl√§rungen zu Theorie und Herleitung       |
| Navigation     | Fragen k√∂nnen markiert und √ºbersprungen werden, mit direkter Navigation √ºber die Seitenleiste |
| Fortschritt    | Fortschritt wird pro Pseudonym und Fragenset in einer SQLite-Datenbank gespeichert            |
| Zeitlimit      | Optionales 60-Minuten-Fenster                                                                 |
| Feedback       | Nutzer k√∂nnen Probleme mit Fragen melden (inhaltlich, technisch etc.)                         |
| Leaderboard    | √ñffentliches Top‚Äë10 (pro Fragenset); vollst√§ndige Ansicht f√ºr Admin                           |
| Analyse & Wartung | Itemanalyse, Distraktor-Analyse, Verwaltung von gemeldetem Feedback                         |
| PDF-Export     | Professioneller Report mit LaTeX-Rendering, Durchschnittsvergleich, Mini-Glossar, Bookmarks   |
| Export         | CSV-Download aller Antworten und SQL-Dump der Datenbank √ºber Admin-Panel                      |
| Admin-Panel    | Passwortgesch√ºtzter Bereich f√ºr Analyse, Feedback-Management, Export und Systemeinstellungen  |

## ‚ùì Fragenset-Schema

Die App l√§dt Fragensets aus JSON-Dateien im `data/questions_*.json`-Format. Jede Frage ben√∂tigt die folgenden Kernfelder:

```json
{
  "question": "Text der Frage",
  "options": ["Option A", "Option B", "Option C", "Option D"],
  "answer": 2,
  "explanation": "Warum Antwort C korrekt ist",
  "weight": 1,
  "topic": "Traversierung: BFS",
  "concept": "BFS visitation order",
  "cognitive_level": "Application"
}
```

- `concept` beschreibt das didaktische Ziel der Frage (z.‚ÄØB. ‚ÄûBFS visitation order" oder ‚ÄûPivot table filtering").
- `cognitive_level` orientiert sich an Taxonomien wie Bloom oder SOLO (z.‚ÄØB. "Knowledge", "Understanding", "Application", "Analysis").
- Beide Felder d√ºrfen optional leer sein, aber wir empfehlen, sie f√ºr jede Frage zu pflegen, damit Exporte und Analytics die Lernziele besser gruppieren k√∂nnen.

F√ºr Contributors und Admins gilt:

1. **Konsistente Werte**: Verwende eine feste Terminologie, damit Dashboards und Exporte aussagekr√§ftige Gruppen bilden.
2. **Kurze, pr√§zise Texte**: Halte `konzept`/`kognitive_stufe` auf Satz- oder Schlagwortl√§nge, keine ausf√ºhrlichen Erkl√§rungen.
3. **Validierung**: F√ºhre `python validate_sets.py` aus, bevor du ein Fragenset committest; das Skript pr√ºft die √ºblichen Felder inklusive `konzept` und `kognitive_stufe`.

Mehr Details (z.‚ÄØB. Mini-Glossar, Extended Explanation) findest du im `data/questions_*.json`-Format und dem `GLOSSARY_SCHEMA.md`.

### Neuere Features (seit v1.2)

- Musterl√∂sung (PDF): Admins k√∂nnen jetzt eine formatierte Musterl√∂sung mit allen korrekten Antworten, ausf√ºhrlichen Erkl√§rungen und angeh√§ngtem Mini-Glossar erzeugen. Die Musterl√∂sung rendert LaTeX-Formeln als hochwertige PNGs und hebt korrekte Optionen hervor.
- Nutzer-Download: Nach Abschluss eines Tests steht den Nutzer/innen ebenfalls eine Musterl√∂sung zum Download zur Verf√ºgung (sinnvollerweise nur zum Lernen). Die UI zeigt eine Kurzinfo, dass die Musterl√∂sung pr√ºfungsrelevant ist und nicht geteilt werden sollte.
- Robustere Formel-Rendering-Pipeline: Formeln werden parallel gerendert (ThreadPool) mit einem konfigurierbaren Gesamt-Timeout; ausgefallene oder zu langsame Renderings werden durch Platzhalter ersetzt, damit ein Export nicht ewig h√§ngt.
- Per-User Export-Cache: Um wiederholte Anfragen schnell zu bedienen, werden erzeugte Musterl√∂sungen tempor√§r im Session-Cache (`st.session_state`) zwischengespeichert und beim erneuten Download sofort ausgeliefert.
- Zeitabsch√§tzung & Probe-Rendering: Bei PDF-Exporten mit Formeln wird optional ein Probe-Rendering durchgef√ºhrt, um die voraussichtliche Gesamtdauer anzugeben.

### Formel-Cache (Disk) & automatische Eviction

Die App cached gerenderte LaTeX-Formel-Bilder als PNG-Dateien auf der lokalen Festplatte, um Netzwerkaufrufe zur Remote-Render-API zu reduzieren und Exporte zu beschleunigen. Damit der Cache auf Hosts mit begrenztem oder ephemerem Speicher (z. B. Streamlit Community Cloud) nicht unkontrolliert w√§chst, gibt es automatische Eviction-Mechanismen.

Konfigurierbare Umgebungsvariablen:

- FORMULA_CACHE_DIR: Pfad zum Cache-Verzeichnis (Standard: ./var/formula_cache)
- FORMULA_CACHE_MAX_FILES: Maximale Anzahl Dateien im Cache (Standard: 200)
- FORMULA_CACHE_MAX_MB: Maximale Gesamtgr√∂√üe des Caches in MiB (Standard: 200)
- FORMULA_CACHE_TTL_DAYS: Lebensdauer von Cache-Dateien in Tagen (Standard: 7)

Verhalten:

- Vor jedem Schreiben einer neuen Formeldatei wird die Eviction-Routine ausgef√ºhrt: Dateien √§lter als TTL werden zuerst entfernt; danach werden die √§ltesten Dateien gel√∂scht, bis sowohl Dateianzahl als auch Gesamtgr√∂√üe innerhalb der Grenzwerte liegen.
- Auf schreibgesch√ºtzten oder nicht verf√ºgbaren Dateisystemen werden Schreibfehler ignoriert und die In-Memory-Fallback-Strategie verwendet. Das verhindert Abst√ºrze auf restriktiven Plattformen.

Empfehlung: Setze konservative Limits f√ºr Cloud-Deploys (z. B. FORMULA_CACHE_MAX_MB=50, FORMULA_CACHE_MAX_FILES=100) und √ºberwache die Nutzung in Logs.


---

## ÔøΩ Security Features

Die MC-Test-App implementiert **Enterprise-Grade Security** √ºber drei aufeinander aufbauende Phasen:

### Phase 1: Quick Wins (v1.1.0)
- ‚ö†Ô∏è **Empty Admin-Key Warnings**: Warnung bei unsicherem Admin-Passwort
- üîÑ **Re-Authentication**: Passwortabfrage vor kritischen Operationen (L√∂schen, Export)

### Phase 2: Server-Side Session Validation (v1.2.0)
- üîê **Cryptographic Tokens**: Sichere Session-Tokens mit `secrets.token_urlsafe(32)`
- üîí **SHA-256 Hashing**: Keine Klartext-Passw√∂rter im Session State
- ‚è±Ô∏è **Session Timeouts**: Automatische Abmeldung nach 2 Stunden Inaktivit√§t
- üßµ **Thread-Safe**: Sichere Concurrent-Access mit Threading-Locks

### Phase 3: Audit-Logging & Rate-Limiting (v1.3.0) ‚≠ê **NEU**
- üìä **SQLite-Based Audit-Logging**: Alle Admin-Aktionen persistent geloggt
  - Login-Versuche (erfolg/fehlgeschlagen)
  - Delete-Operationen (User-Daten, Global)
  - CSV-Exports
  - CRITICAL Actions markiert
- üö´ **Rate-Limiting**: Brute-Force-Schutz
  - 3 fehlgeschlagene Login-Versuche ‚Üí 5 Minuten Sperre
  - Automatisches Reset nach erfolgreichem Login
  - Anzeige der Sperr-Zeit
- üìà **Admin Dashboard**: Neuer "üîí Audit-Log" Tab
  - Statistiken: Gesamt-Aktionen, Success/Fail-Raten
  - Filter: User, Action-Typ, Erfolg-Status, Limit
  - CSV-Export f√ºr forensische Analyse
- üóëÔ∏è **DSGVO-Compliance**: Automatische L√∂schung nach 90 Tagen
- üåç **IP-Tracking**: Optional Client-IP-Logging (wenn verf√ºgbar)

**Security Level:** üõ°Ô∏è **VERY HIGH (Enterprise-Grade)**

**Dokumentation:**
- üìò [SECURITY_PHASE3_SUMMARY.md](SECURITY_PHASE3_SUMMARY.md) - Technische Details
- üìã [CHANGELOG_SECURITY_PHASE3.md](CHANGELOG_SECURITY_PHASE3.md) - Vollst√§ndiger Changelog
- üìÑ [PHASE3_ABSCHLUSS.md](PHASE3_ABSCHLUSS.md) - User-Guide

---

## ÔøΩüìã Voraussetzungen

- **Python:** Version 3.9 oder h√∂her.
- **Abh√§ngigkeiten:** Installiere via `pip install -r requirements.txt`.

---

## üõ†Ô∏è Installation und Start

### Lokaler Start

1.  Klone das Repository.
2.  Installiere die Abh√§ngigkeiten:
    ```bash
    pip install -r requirements.txt
    ```
3.  Starte die App:
    ```bash
    streamlit run app.py
    ```
4.  √ñffne [http://localhost:8501](http://localhost:8501) im Browser.

### Deployment (z.B. Streamlit Cloud)

1.  Verbinde dein GitHub-Repository mit deinem Streamlit-Cloud-Konto.
2.  Deploye die App.
3.  Konfiguriere die Secrets (siehe n√§chster Abschnitt) im Dashboard deiner Streamlit-Cloud-App.

---

## ‚öôÔ∏è Konfiguration

### Umgebungsvariablen / Secrets

Die App wird √ºber Umgebungsvariablen (f√ºr sensible Daten) und eine Konfigurationsdatei (f√ºr nicht-sensible Daten) konfiguriert.

F√ºr die lokale Entwicklung kannst du eine `.env`-Datei erstellen. F√ºr das Deployment auf Streamlit Cloud m√ºssen diese Variablen als "Secrets" im Dashboard der App hinterlegt werden.

-```env
# Beispiel f√ºr .env oder Streamlit Cloud Secrets
MC_TEST_ADMIN_USER="dein_admin_user"
MC_TEST_ADMIN_KEY="dein_geheimes_passwort"
APP_URL="https://ihre-streamlit-app.streamlit.app"
```


- **`MC_TEST_ADMIN_USER`**: Der Benutzername, der f√ºr den Admin-Login erforderlich ist.
- **`MC_TEST_ADMIN_KEY`**: Das Passwort f√ºr den Admin-Login.
- **(Removed) `MC_TEST_MIN_SECONDS_BETWEEN`**: Legacy global answer cooldown removed. Per-question cooldowns are now handled by the UI. If you previously relied on this setting, migrate to the per-question tempo/cooldown logic.
 - **`MC_NEXT_COOLDOWN_NORMALIZATION_FACTOR`**: Optionaler Skalierungsfaktor f√ºr die gesamte Wartezeit, die beim Dr√ºcken von "N√§chste Frage" nach dem Lesen von Erkl√§rungen angewendet wird. Standard: `1.0` (keine √Ñnderung). Werte < 1.0 reduzieren die gesamte Cooldown-Zeit (z. B. `0.5` halbiert `base + extras`). Setze diesen Wert in Streamlit-Secrets oder als Umgebungsvariable.
 - **`APP_URL`**: Die URL der Streamlit-App f√ºr den QR-Code im PDF-Export. (Default: `https://mc-test-amalea.streamlit.app`)
- **`APP_URL`**: Die URL der Streamlit-App f√ºr den QR-Code im PDF-Export. (Default: `https://mc-test-amalea.streamlit.app`)

Zus√§tzliche Secrets / Umgebungsvariablen (kurz erkl√§rt):

- **`MC_TEST_DURATION_MINUTES`**: Optionaler Default f√ºr die Testdauer (in Minuten) wenn nicht im Fragenset-Meta angegeben. (Default: `60`)
- **`MC_USER_QSET_CLEANUP_HOURS`**: Wie viele Stunden tempor√§re, von Nutzern hochgeladene Fragensets als "stale" gelten und automatisch beim Laden der Startseite entfernt werden k√∂nnen. (Default: `24`)
- **`MC_RATE_LIMIT_ATTEMPTS`**: Anzahl erlaubter fehlgeschlagener Login-/Wiederherstellungs-Versuche bevor Rate-Limiting greift. (Default: `3`)
- **`MC_RATE_LIMIT_WINDOW_MINUTES`**: Fenstergr√∂√üe in Minuten f√ºr das Rate-Limit. (Default: `5`)
- **`MC_RECOVERY_MIN_LENGTH`**: Minimale L√§nge f√ºr ein Wiederherstellungs-Geheimwort (Default: `6`).
- **`MC_RECOVERY_ALLOW_SHORT`**: Falls gesetzt auf `1`/`true`, werden k√ºrzere Wiederherstellungs-Geheimw√∂rter erlaubt.

Hinweis: Du kannst diese Werte lokal in einer `.env` Datei setzen (z.B. f√ºr die Entwicklung) oder als Secrets in deiner Deployment-Umgebung (z. B. Streamlit Cloud). Die App liest zuerst Streamlit-Secrets, dann Umgebungsvariablen und schlie√ülich die lokale JSON-Konfiguration `mc_test_config.json`.

### üåê Sprache / Locale

- Die App liest die Sprache nicht aus URL-Query-Parametern (z. B. `?lang=de`).
- Sprache wird ausschlie√ülich √ºber den UI-Sprachselektor gesteuert und in der Session gespeichert.
- M√∂chtest du das Standardverhalten √§ndern, passe den Default in `i18n/__init__.py` (`_DEFAULT_LOCALE`) an.


Kurzes Beispiel: Setzen des Cleanup-Timeouts

Lokale Shell (tempor√§r f√ºr die laufende Shell):

```bash
export MC_USER_QSET_CLEANUP_HOURS=1  # Tempor√§re Fragensets √§lter als 1 Stunde gelten als stale
streamlit run app.py
```

Beispiel: Setzen des Normalisierungsfaktors in der Shell (tempor√§r):

```bash
export MC_NEXT_COOLDOWN_NORMALIZATION_FACTOR=0.5
streamlit run app.py
```

Als Streamlit-Cloud-Secret (im Secrets-Editor):

```yaml
# Im Secrets-Editor der Streamlit-App hinzuf√ºgen
MC_NEXT_COOLDOWN_NORMALIZATION_FACTOR: "0.5"
```

Als Streamlit-Cloud-Secret (YAML / UI):

```yaml
# Im Secrets-Editor der Streamlit-App hinzuf√ºgen
MC_USER_QSET_CLEANUP_HOURS: "1"
```

Hinweis: Die App priorisiert Werte in dieser Reihenfolge: Streamlit-Secrets ‚Üí Umgebungsvariablen ‚Üí `mc_test_config.json`.

---

## üìÅ Projektstruktur

```
.
‚îú‚îÄ‚îÄ .github/                # GitHub Actions Workflows (CI/CD)
‚îú‚îÄ‚îÄ .streamlit/             # Streamlit-Konfiguration (z.B. Themes)
‚îú‚îÄ‚îÄ data/                   # Enth√§lt JSON-Dateien (Fragensets, Pseudonyme)
‚îú‚îÄ‚îÄ db/                     # Speichert die SQLite-Datenbankdatei
‚îú‚îÄ‚îÄ tests/                  # Pytest-Tests f√ºr die Anwendungslogik
‚îú‚îÄ‚îÄ .env.example            # Beispiel f√ºr Umgebungsvariablen
‚îú‚îÄ‚îÄ admin_panel.py          # Logik f√ºr das Admin-Panel
‚îú‚îÄ‚îÄ app.py                  # Haupt-Anwendungsskript
‚îú‚îÄ‚îÄ auth.py                 # Authentifizierung und Session-Management
‚îú‚îÄ‚îÄ components.py           # Wiederverwendbare UI-Komponenten
‚îú‚îÄ‚îÄ config.py               # Laden der Konfiguration und Fragensets
‚îú‚îÄ‚îÄ database.py             # Datenbankinteraktionen (SQLite)
‚îú‚îÄ‚îÄ helpers.py              # Kleine Hilfsfunktionen
‚îú‚îÄ‚îÄ logic.py                # Kernlogik der App (Scoring, etc.)
‚îú‚îÄ‚îÄ main_view.py            # UI-Logik f√ºr die Hauptansichten
‚îú‚îÄ‚îÄ pdf_export.py           # PDF-Report-Generierung mit LaTeX & Mini-Glossar
‚îú‚îÄ‚îÄ requirements.txt        # Python-Abh√§ngigkeiten
‚îú‚îÄ‚îÄ AI_QUESTION_GENERATOR_PLAN.md      # Plan f√ºr KI-basierte Fragenset-Generierung
‚îú‚îÄ‚îÄ DEPLOYMENT_FEASIBILITY_STUDY.md    # Infrastruktur & Kostenanalyse (Streamlit/Cloudflare)
‚îú‚îÄ‚îÄ GLOSSARY_SCHEMA.md                 # Dokumentation f√ºr Mini-Glossar in Fragensets
‚îú‚îÄ‚îÄ VISION_RELEASE_2.0.md              # Strategische Vision & Feature-Roadmap Release 2.0
‚îî‚îÄ‚îÄ README.md                          # Diese Dokumentation
```

---

## üìä Repository-Gr√∂√üe & Statistiken

**Wie gro√ü ist dieses Repository?**

| Metrik                    | Wert                |
|---------------------------|---------------------|
| üì¶ Gesamtgr√∂√üe            | ~29 MB              |
| üìÇ Git-Historie           | ~13 MB              |
| üìÑ Dateien gesamt         | ~175 Dateien        |
| üêç Python-Dateien         | 42 Dateien          |
| üìù Markdown-Dokumentation | 67 Dateien          |
| üóÇÔ∏è JSON-Dateien           | 36 Dateien          |
| üíª Python-Codezeilen      | ~15 900 Zeilen      |
| üìÅ Hauptverzeichnisse     | 13 Verzeichnisse    |

**Hinweis:** Die Statistiken k√∂nnen sich mit der Weiterentwicklung des Projekts √§ndern. Die Werte gelten f√ºr den aktuellen Stand des Repositories.

---

## üß∞ Developer tools (local)

Es gibt kleine Hilfs-Skripte zum Testen und Benchmarking im Ordner `tools/`:

- `tools/test_evict.py` ‚Äî Erzeugt Dummy-Dateien im Cache (`var/formula_cache`) und testet die Eviction-Routine.
- `tools/run_export_test.py` ‚Äî F√ºhrt einen einzelnen Musterl√∂sungs-Export durch und schreibt das PDF nach `exports/`.
- `tools/benchmark_exports.py` ‚Äî F√ºhre N Exporte hintereinander aus (Standard N=5) und schreibe eine `exports/benchmark_summary.txt`.

Beispiele:

```bash
# Eviction smoke test
PYTHONPATH=. python3 tools/test_evict.py

# Single export (shows progress)
PYTHONPATH=. python3 tools/run_export_test.py

# Benchmark with 5 runs
BENCH_EXPORTS_N=5 PYTHONPATH=. python3 tools/benchmark_exports.py
```

---

## üß≠ Hinweise zum Prompting (f√ºr AI / Text‚ÄëGenerierung)

Kleine, aber wichtige Regel f√ºr alle Prompts, die in diese App (oder in Templates) eingespeist werden:

- Verwende echte Leerzeilen / Abs√§tze. Gib niemals die zwei Zeichen Backslash + n (`"\\n"`) als Ersatz f√ºr einen Zeilenumbruch aus.
- Korrekt: eine echte leere Zeile zwischen zwei Abs√§tzen.
- Nicht verwenden: der Literal‚ÄëString `"\\n"` (Backslash + n).

Beispiel (nicht so):

```
Ergebnis:\n\n- Punkt 1\n- Punkt 2
```

Beispiel (richtig):

```
Ergebnis:

- Punkt 1
- Punkt 2
```

Warum das wichtig ist:
- Manche Modelle liefern `"\\n"` anstelle echter Zeilenumbr√ºche ‚Äî das bricht Markdown/HTML‚ÄëRendering und macht die Ausgabe schwer lesbar.
- Eine kurze Nachbearbeitung der Modell‚ÄëAntworten (Sanitizer) ist zus√§tzlich empfehlenswert, siehe `helpers.py`.

Short note (EN): Use real blank lines, not the literal string "\\n". This avoids escaped newline artifacts in Markdown/HTML output.


Hinweis: Die generierten Artefakte landen in `exports/` und werden in `.gitignore` ausgeschlossen, damit sie nicht versehentlich in Git landen.
```

---

## ÔøΩüõ†Ô∏è Administration & Wartung

### Admin-Bereich

- **Zugang:**
    1. W√§hle auf der Startseite das in den Secrets (`MC_TEST_ADMIN_USER`) definierte Admin-Pseudonym aus.
    2. Nach dem Start des Tests erscheint in der Seitenleiste der Bereich "üîê Admin Panel".
    3. Gib dort das Admin-Passwort (`MC_TEST_ADMIN_KEY`) ein, um vollen Zugriff zu erhalten.
- **Funktionen:** Das Panel bietet detaillierte Analysen (Item- & Distraktoranalyse), eine √úbersicht und Verwaltung f√ºr gemeldetes Feedback, Datenexport (CSV, SQL-Dump, **PDF-Export**) und Systemeinstellungen (Scoring-Modus, Zur√ºcksetzen der Testdaten).

### Tests ausf√ºhren

```bash
pip install -r requirements.txt
PYTHONPATH=. pytest
```

---

## üêõ Troubleshooting

-   **App startet nicht:** Stelle sicher, dass alle Abh√§ngigkeiten aus `requirements.txt` installiert sind.

---

## ü§ù Contributing


Beitr√§ge sind willkommen! Forke das Repository, erstelle einen Branch und √∂ffne einen Pull Request.

---
