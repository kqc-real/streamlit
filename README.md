# üìù MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?b‚îú‚îÄmain)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App.
Sie bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse f√ºr verschiedene Fragensets.
Die App ist modular aufgebaut und nutzt eine SQLite-Datenbank zur persistenten Speicherung von Testergebnissen.
Die App verf√ºgt √ºber ein integriertes Feedback-System, das es Nutzern erm√∂glicht, Probleme mit Fragen zu melden, und Admins, dieses Feedback zu verwalten.

---

## üöÄ Schnellstart

**Neu hier? Keine Programmierkenntnisse?**
‚Üí **[üìñ Installationsanleitung f√ºr Einsteiger](INSTALLATION_ANLEITUNG.md)**

Diese Schritt-f√ºr-Schritt-Anleitung erkl√§rt alles von Grund auf:
- Python & Git installieren (Windows & Mac)
- App herunterladen und starten
- H√§ufige Probleme und L√∂sungen
- **Perfekt f√ºr BWL-Studierende ohne IT-Kenntnisse!**

**Admin-Panel lokal testen?**
‚Üí **[üîê Admin-Panel Anleitung f√ºr Kursteilnehmer/innen](ADMIN_PANEL_ANLEITUNG.md)**

Diese Anleitung zeigt dir:
- Wie du als "Albert Einstein" Admin-Rechte erh√§ltst
- Was du im Admin-Panel alles tun kannst (Analytics, Itemanalyse, Feedback)
- Wie Itemanalyse und Distraktor-Analyse funktionieren
- **Perfekt f√ºr Projektmitglieder, die alle Features verstehen wollen!**

---

## üöÄ √úbersicht

Diese App ist ein vollst√§ndiger MC-Test f√ºr Kursinhalte, entwickelt mit Streamlit.
Sie erm√∂glicht anonyme Tests mit Pseudonymen, zuf√§lliger Fragenreihenfolge, Zeitlimit und einem integrierten Feedback-System zur kontinuierlichen Verbesserung der Fragen.
Perfekt f√ºr Bildungsumgebungen, Selbstlernphasen oder zur Pr√ºfungsvorbereitung.

### Hauptfunktionen

| Kategorie      | Funktion                                                                                      |
|----------------|-----------------------------------------------------------------------------------------------|
| Zugang         | Pseudonym-Login (anonym, keine Registrierung)                                                 |
| Fragen         | Zuf√§llige Reihenfolge, Gewichtung je Frage, strikte Trennung nach Fragenset                   |
| Fragenset      | Dynamische Auswahl verschiedener Fragensets (`questions_*.json`)                               |
| Scoring-Modi   | "Nur +Punkte" (falsch = 0) oder "+/- Punkte" (falsch = -Gewichtung)                            |
| Feedback       | Sofortiges Ergebnis mit optionalen, detaillierten Erkl√§rungen zu Theorie und Herleitung       |
| Navigation     | Fragen k√∂nnen markiert und √ºbersprungen werden, mit direkter Navigation √ºber die Seitenleiste |
| Fortschritt    | Fortschritt wird pro Pseudonym und Fragenset in einer SQLite-Datenbank gespeichert            |
| Zeitlimit      | Optionales 60-Minuten-Fenster                                                                 |
| Feedback       | Nutzer k√∂nnen Probleme mit Fragen melden (inhaltlich, technisch etc.)                         |
| Leaderboard    | √ñffentliches Top‚Äë10 (pro Fragenset); vollst√§ndige Ansicht f√ºr Admin                           |
| Analyse & Wartung | Itemanalyse, Distraktor-Analyse, Verwaltung von gemeldetem Feedback                         |
| PDF-Export     | Professioneller Report mit LaTeX-Rendering, Durchschnittsvergleich, Mini-Glossar, Bookmarks   |
| Export         | CSV-Download aller Antworten und SQL-Dump der Datenbank √ºber Admin-Panel                      |
| Admin-Panel    | Passwortgesch√ºtzter Bereich f√ºr Analyse, Feedback-Management, Export und Systemeinstellungen  |

---

## ÔøΩ Security Features

Die MC-Test-App implementiert **Enterprise-Grade Security** √ºber drei aufeinander aufbauende Phasen:

### Phase 1: Quick Wins (v1.1.0)
- ‚ö†Ô∏è **Empty Admin-Key Warnings**: Warnung bei unsicherem Admin-Passwort
- üîÑ **Re-Authentication**: Passwortabfrage vor kritischen Operationen (L√∂schen, Export)

### Phase 2: Server-Side Session Validation (v1.2.0)
- üîê **Cryptographic Tokens**: Sichere Session-Tokens mit `secrets.token_urlsafe(32)`
- üîí **SHA-256 Hashing**: Keine Klartext-Passw√∂rter im Session State
- ‚è±Ô∏è **Session Timeouts**: Automatische Abmeldung nach 2 Stunden Inaktivit√§t
- üßµ **Thread-Safe**: Sichere Concurrent-Access mit Threading-Locks

### Phase 3: Audit-Logging & Rate-Limiting (v1.3.0) ‚≠ê **NEU**
- üìä **SQLite-Based Audit-Logging**: Alle Admin-Aktionen persistent geloggt
  - Login-Versuche (erfolg/fehlgeschlagen)
  - Delete-Operationen (User-Daten, Global)
  - CSV-Exports
  - CRITICAL Actions markiert
- üö´ **Rate-Limiting**: Brute-Force-Schutz
  - 3 fehlgeschlagene Login-Versuche ‚Üí 5 Minuten Sperre
  - Automatisches Reset nach erfolgreichem Login
  - Anzeige der Sperr-Zeit
- üìà **Admin Dashboard**: Neuer "üîí Audit-Log" Tab
  - Statistiken: Gesamt-Aktionen, Success/Fail-Raten
  - Filter: User, Action-Typ, Erfolg-Status, Limit
  - CSV-Export f√ºr forensische Analyse
- üóëÔ∏è **DSGVO-Compliance**: Automatische L√∂schung nach 90 Tagen
- üåç **IP-Tracking**: Optional Client-IP-Logging (wenn verf√ºgbar)

**Security Level:** üõ°Ô∏è **VERY HIGH (Enterprise-Grade)**

**Dokumentation:**
- üìò [SECURITY_PHASE3_SUMMARY.md](SECURITY_PHASE3_SUMMARY.md) - Technische Details
- üìã [CHANGELOG_SECURITY_PHASE3.md](CHANGELOG_SECURITY_PHASE3.md) - Vollst√§ndiger Changelog
- üìÑ [PHASE3_ABSCHLUSS.md](PHASE3_ABSCHLUSS.md) - User-Guide

---

## ÔøΩüìã Voraussetzungen

- **Python:** Version 3.9 oder h√∂her.
- **Abh√§ngigkeiten:** Installiere via `pip install -r requirements.txt`.

---

## üõ†Ô∏è Installation und Start

### Lokaler Start

1.  Klone das Repository.
2.  Installiere die Abh√§ngigkeiten:
    ```bash
    pip install -r requirements.txt
    ```
3.  Starte die App:
    ```bash
    streamlit run app.py
    ```
4.  √ñffne [http://localhost:8501](http://localhost:8501) im Browser.

### Deployment (z.B. Streamlit Cloud)

1.  Verbinde dein GitHub-Repository mit deinem Streamlit-Cloud-Konto.
2.  Deploye die App.
3.  Konfiguriere die Secrets (siehe n√§chster Abschnitt) im Dashboard deiner Streamlit-Cloud-App.

---

## ‚öôÔ∏è Konfiguration

### Umgebungsvariablen / Secrets

Die App wird √ºber Umgebungsvariablen (f√ºr sensible Daten) und eine Konfigurationsdatei (f√ºr nicht-sensible Daten) konfiguriert.

F√ºr die lokale Entwicklung kannst du eine `.env`-Datei erstellen. F√ºr das Deployment auf Streamlit Cloud m√ºssen diese Variablen als "Secrets" im Dashboard der App hinterlegt werden.

```env
# Beispiel f√ºr .env oder Streamlit Cloud Secrets
MC_TEST_ADMIN_USER="dein_admin_user"
MC_TEST_ADMIN_KEY="dein_geheimes_passwort"
MC_TEST_MIN_SECONDS_BETWEEN="2"
APP_URL="https://ihre-streamlit-app.streamlit.app"
```

- **`MC_TEST_ADMIN_USER`**: Der Benutzername, der f√ºr den Admin-Login erforderlich ist.
- **`MC_TEST_ADMIN_KEY`**: Das Passwort f√ºr den Admin-Login.
- **`MC_TEST_MIN_SECONDS_BETWEEN`**: Die Mindestanzahl an Sekunden, die zwischen zwei Antworten vergehen muss. Verhindert Spam. Ein Wert von `0` deaktiviert das Limit. (Default: `3`)
- **`APP_URL`**: Die URL der Streamlit-App f√ºr den QR-Code im PDF-Export. (Default: `https://mc-test-amalea.streamlit.app`)

---

## üìÅ Projektstruktur

```
.
‚îú‚îÄ‚îÄ .github/                # GitHub Actions Workflows (CI/CD)
‚îú‚îÄ‚îÄ .streamlit/             # Streamlit-Konfiguration (z.B. Themes)
‚îú‚îÄ‚îÄ data/                   # Enth√§lt JSON-Dateien (Fragensets, Pseudonyme)
‚îú‚îÄ‚îÄ db/                     # Speichert die SQLite-Datenbankdatei
‚îú‚îÄ‚îÄ tests/                  # Pytest-Tests f√ºr die Anwendungslogik
‚îú‚îÄ‚îÄ .env.example            # Beispiel f√ºr Umgebungsvariablen
‚îú‚îÄ‚îÄ admin_panel.py          # Logik f√ºr das Admin-Panel
‚îú‚îÄ‚îÄ app.py                  # Haupt-Anwendungsskript
‚îú‚îÄ‚îÄ auth.py                 # Authentifizierung und Session-Management
‚îú‚îÄ‚îÄ components.py           # Wiederverwendbare UI-Komponenten
‚îú‚îÄ‚îÄ config.py               # Laden der Konfiguration und Fragensets
‚îú‚îÄ‚îÄ database.py             # Datenbankinteraktionen (SQLite)
‚îú‚îÄ‚îÄ helpers.py              # Kleine Hilfsfunktionen
‚îú‚îÄ‚îÄ logic.py                # Kernlogik der App (Scoring, etc.)
‚îú‚îÄ‚îÄ main_view.py            # UI-Logik f√ºr die Hauptansichten
‚îú‚îÄ‚îÄ pdf_export.py           # PDF-Report-Generierung mit LaTeX & Mini-Glossar
‚îú‚îÄ‚îÄ requirements.txt        # Python-Abh√§ngigkeiten
‚îú‚îÄ‚îÄ AI_QUESTION_GENERATOR_PLAN.md      # Plan f√ºr KI-basierte Fragenset-Generierung
‚îú‚îÄ‚îÄ DEPLOYMENT_FEASIBILITY_STUDY.md    # Infrastruktur & Kostenanalyse (Streamlit/Cloudflare)
‚îú‚îÄ‚îÄ GLOSSARY_SCHEMA.md                 # Dokumentation f√ºr Mini-Glossar in Fragensets
‚îú‚îÄ‚îÄ VISION_RELEASE_2.0.md              # Strategische Vision & Feature-Roadmap Release 2.0
‚îî‚îÄ‚îÄ README.md                          # Diese Dokumentation
```

---

## ÔøΩüõ†Ô∏è Administration & Wartung

### Admin-Bereich

- **Zugang:**
    1. W√§hle auf der Startseite das in den Secrets (`MC_TEST_ADMIN_USER`) definierte Admin-Pseudonym aus.
    2. Nach dem Start des Tests erscheint in der Seitenleiste der Bereich "üîê Admin Panel".
    3. Gib dort das Admin-Passwort (`MC_TEST_ADMIN_KEY`) ein, um vollen Zugriff zu erhalten.
- **Funktionen:** Das Panel bietet detaillierte Analysen (Item- & Distraktoranalyse), eine √úbersicht und Verwaltung f√ºr gemeldetes Feedback, Datenexport (CSV, SQL-Dump, **PDF-Export**) und Systemeinstellungen (Scoring-Modus, Zur√ºcksetzen der Testdaten).

### Tests ausf√ºhren

```bash
pip install -r requirements.txt
PYTHONPATH=. pytest
```

---

## üêõ Troubleshooting

-   **App startet nicht:** Stelle sicher, dass alle Abh√§ngigkeiten aus `requirements.txt` installiert sind.

---

## ü§ù Contributing

Beitr√§ge sind willkommen! Forke das Repository, erstelle einen Branch und √∂ffne einen Pull Request.

---

# ü§ñ Fragensets mit KI erstellen (Optional)

Die App selbst generiert **keine** Fragen automatisch. Der folgende Abschnitt ist eine **Copy & Paste Anleitung** f√ºr die manuelle Nutzung mit einem externen KI-Assistenten (LLM).

## Voraussetzungen

- Zugang zu einem LLM wie **ChatGPT**, **Claude**, **Gemini** oder **GitHub Copilot Chat**
- Optional: PDF-Dokumente als Wissensgrundlage (Skripte, Lehrb√ºcher)

## So funktioniert's

1.  **Kopiere den gesamten Prompt-Text** aus dem n√§chsten Abschnitt
2.  **F√ºge ihn in dein LLM ein** (z.B. ChatGPT Web-Interface, Claude, VS Code Copilot Chat)
3.  **Beantworte die 7 Fragen** des Assistenten Schritt f√ºr Schritt (erst nach deiner Antwort geht es weiter).
4.  **Erhalte eine fertige `questions_*.json`-Datei** zum Download
5.  **Pr√ºfe die JSON-Datei** z.‚ÄØB. mit [jsonlint.com](https://jsonlint.com) oder einem lokalen Linter.
6.  **Speichere die Datei** im `data/`-Ordner deiner App.

Der Prompt enth√§lt alle notwendigen Informationen (JSON-Schema, Formatierungsregeln, didaktische Guidelines), damit der LLM qualitativ hochwertige Fragen f√ºr diese App erstellen kann.

## Prompt (copy & paste)

F√ºhre mich als Experte f√ºr die Erstellung von Multiple-Choice-Fragen in den folgenden sieben Schritten durch die Konfiguration eines neuen Fragensets. Stelle nach jedem Schritt die zugeh√∂rige Frage und warte auf meine Antwort, bevor du mit dem n√§chsten Schritt fortf√§hrst.

---

### **Schritt 1 von 7 ‚Äì Thema festlegen**

Frage mich nach dem **Thema** f√ºr das neue Fragenset. Erw√§hne, dass dies die Grundlage f√ºr den Inhalt und den Dateinamen ist (z.B. `questions_Ihr_Thema.json`). Gib mir Beispiele wie "Data Science Grundlagen", "Software-Architektur" oder "Projektmanagement nach Scrum".

---

### **Schritt 2 von 7 ‚Äì Zielgruppe bestimmen**

Frage mich nach der Zielgruppe f√ºr das Fragenset. Gib mir Beispiele wie "Anf√§nger ohne Vorkenntnisse", "Fortgeschrittene mit Grundwissen" oder "Experten zur Pr√ºfungsvorbereitung".

---

### **Schritt 3 von 7 ‚Äì Umfang & Schwierigkeitsprofil**

Frage mich, wie viele Fragen das Set enthalten soll (z.B. 20, 50) **und** welche Verteilung der Schwierigkeitsgrade gew√ºnscht ist. Verwende die
Gewichtungen der App als Referenz:

- Gewichtung 1 ‚Üí leichte Einstiegs-/Reproduktionsfragen
- Gewichtung 2 ‚Üí anwendungsorientierte Transferfragen
- Gewichtung 3 ‚Üí anspruchsvolle, kombinierte Expertenfragen

Wenn ich keine konkrete Verteilung angebe, schlage ein sinnvolles Verh√§ltnis vor (z.B. 50 % leicht, 35 % mittel, 15 % schwer) und bitte mich um Best√§tigung oder Anpassung.

> üí° **Hinweis:** Plane die Themen so, dass jedes Thema mindestens zwei Fragen enth√§lt
und insgesamt h√∂chstens zehn verschiedene Themen entstehen. Fasse verwandte Inhalte
gegebenenfalls unter einem gemeinsamen Thema zusammen.

---

### **Schritt 4 von 7 ‚Äì Anzahl der Antwortoptionen**

Frage mich nach der Anzahl der Antwortoptionen und pr√§sentiere mir die folgenden drei M√∂glichkeiten zur Auswahl:

  * **A) 4 Optionen:** Ein klassisches Multiple-Choice-Format.
  * **B) 5 Optionen:** Etwas anspruchsvoller, da die Ratewahrscheinlichkeit sinkt.
  * **C) Variabel:** Die Anzahl der Optionen kann pro Frage variieren. Dies bietet die gr√∂√üte Flexibilit√§t, erfordert aber bei der Erstellung mehr Aufmerksamkeit.

---

### **Schritt 5 von 7 ‚Äì Erweiterte Erkl√§rungen (optional)**

Frage mich, ob f√ºr schwierigere Fragen (Gewichtung 2 und 3) zus√§tzlich zur normalen Erkl√§rung auch **erweiterte Erkl√§rungen** (`extended_explanation`) generiert werden sollen. Erkl√§re, dass diese tiefergehenden Hintergrund, Code-Beispiele oder Herleitungen enthalten k√∂nnen. Wenn ich dies verneine, lasse das Feld `extended_explanation` im JSON vollst√§ndig weg.

> Hinweis: Falls `schritte` erstellt werden, formuliere die einzelnen S√§tze ohne Pr√§fixe wie "Schritt 1 ‚Äì" ‚Äì die Reihenfolge ergibt sich aus dem Array.

---

### **Schritt 6 von 7 ‚Äì Mini-Glossar (optional)**

Frage mich, ob f√ºr die Fragen **Mini-Glossar-Eintr√§ge** (`mini_glossary`) generiert werden sollen. Erkl√§re, dass diese im PDF-Export als separate Glossar-Section angezeigt werden und wichtige Fachbegriffe aus den Fragen erkl√§ren. Jede Frage kann 2-4 zentrale Begriffe mit pr√§gnanten Definitionen (1-3 S√§tze) enthalten. Falls verneint, lasse das Feld `mini_glossary` im JSON vollst√§ndig weg.

> Vermeide Querverweise (z.‚ÄØB. ‚ÄûSiehe Frage 12‚Äú) in Glossar-Definitionen; jeder Eintrag soll f√ºr sich verst√§ndlich sein.

---

### **Schritt 7 von 7 ‚Äì Externe Dokumente (optional)**

Frage mich, ob ich externe Dokumente (z.B. Skripte als PDF) als Wissensgrundlage bereitstellen m√∂chte. Erw√§hne, dass dies die Qualit√§t der Fragen verbessern kann. Wenn keine Dokumente verf√ºgbar sind, fahre ohne sie fort.

---

### **Abschluss, Ausgabeformat und Generierung**

Nachdem ich alle sieben Fragen beantwortet habe, erstelle das Fragenset. Das Ergebnis muss ein **einzelnes, valides JSON-Objekt** sein, das genau zwei Top-Level-Schl√ºssel enth√§lt:
Nachdem ich alle sieben Fragen beantwortet habe, fasse meine Antworten zusammen und erstelle dann das Fragenset. Das Ergebnis muss ein **einzelnes, valides JSON-Objekt** sein, das genau zwei Top-Level-Schl√ºssel enth√§lt:
Nachdem ich alle sieben Fragen beantwortet habe, fasse meine Antworten zusammen und erstelle dann das Fragenset. Das Ergebnis muss ein **einzelnes, valides und sauberes JSON-Objekt** sein, das genau zwei Top-Level-Schl√ºssel enth√§lt:

- `meta`: Metadaten zum gesamten Set (Thema, Zielgruppe, Schwierigkeitsprofil, Testzeit usw.).
- `questions`: Eine Liste der einzelnen Fragenobjekte.

Erzeuge optionale Felder (`extended_explanation`, `mini_glossary`) nur, wenn ich sie in den zugeh√∂rigen Schritten ausdr√ºcklich angefordert habe.

> ‚ö†Ô∏è **Ausgabeformat:** Gib ausschlie√ülich das JSON-Objekt zur√ºck ‚Äì keine zus√§tzlichen Kommentare oder erkl√§renden Texte.
>
> Deine Antwort darf NUR das JSON enthalten, sonst nichts.
> Entferne vor der Ausgabe alle internen Marker oder Kommentare (wie `[cite_start]` oder `[cite: ...]`) aus den Textfeldern. Der finale JSON-String muss sauber sein.

Berechne die empfohlene Testzeit pro Fragenset, indem du die tats√§chlich generierten Fragen auswertest:

1. Z√§hle nach Abschluss alle Fragen mit Gewichtung 1, 2 und 3 und schreibe diese Werte in `meta.difficulty_profile`.
2. Nutze als Richtwerte: Gewichtung‚ÄØ1 ‚Üí 0.5 Minuten (30‚ÄØSekunden), Gewichtung‚ÄØ2 ‚Üí 0.75 Minuten (45‚ÄØSekunden), Gewichtung‚ÄØ3 ‚Üí 1.0 Minute (60‚ÄØSekunden). Du darfst diese Werte anpassen, wenn ich im Dialog andere Zeitw√ºnsche √§u√üere.
3. Multipliziere die jeweiligen Anzahlen mit diesen Minutenwerten, addiere optional einen sinnvollen Puffer (`meta.additional_buffer_minutes`, z.B. 5) und runde das Ergebnis auf volle Minuten.
4. Testzeiten ab 10 Minuten werden automatisch auf das n√§chste Vielfache von 5 Minuten gerundet; Werte unter 10 Minuten bleiben unver√§ndert.
5. Hinterlege die verwendeten Minutenfaktoren in `meta.time_per_weight_minutes` (Schl√ºssel `"1"`, `"2"`, `"3"` mit numerischen Werten) und speichere das gerundete Gesamtergebnis als Ganzzahl in `meta.test_duration_minutes`.

Erg√§nze `meta.question_count` mit der finalen Anzahl der Fragen und halte `meta.title` sowie `meta.target_audience` konsistent mit den Angaben aus den Schritten 1 und 2.

#### **JSON-Grundstruktur:**

```json
{
  "meta": {
    "title": "Data Science Grundlagen",
    "target_audience": "Fortgeschrittene mit Grundwissen",
    "question_count": 36,
    "difficulty_profile": {
      "leicht": 18,
      "mittel": 12,
      "schwer": 6
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 6,
    "test_duration_minutes": 30
  },
  "questions": [
    {
      "frage": "1. Vollst√§ndiger Fragetext...",
      "optionen": [
        "Antwortoption A",
        "Antwortoption B",
        "Antwortoption C",
        "Antwortoption D"
      ],
      "loesung": 0,
      "erklaerung": "Eine klare und pr√§gnante Erkl√§rung, warum die L√∂sung korrekt ist.",
      "gewichtung": 2,
      "thema": "Zugeh√∂riges Themengebiet",
      "extended_explanation": {
        "titel": "Titel der erweiterten Erkl√§rung",
        "schritte": [
          "Erl√§utere den fachlichen Kontext in einem pr√§gnanten Satz.",
          "Vertiefe den Sachverhalt oder gib ein kurzes Beispiel."
        ]
      },
      "mini_glossary": {
        "Begriff 1": "Pr√§gnante Definition in 1-3 S√§tzen mit optionalen $LaTeX$-Formeln.",
        "Begriff 2": "Erkl√§rung eines weiteren zentralen Fachbegriffs aus dieser Frage."
      }
    }
  ]
}
```

#### **Meta-Felder (`meta`):**

  * `title`: (string) Klarer Name des Fragensets, passend zum Dateinamen.
  * `target_audience`: (string) Beschreibt die Zielgruppe aus Schritt‚ÄØ2.
  * `question_count`: (integer) Gesamtanzahl der Fragen (muss zu `questions.length` passen).
  * `difficulty_profile`: (object) Tats√§chliche Verteilung der generierten Fragen mit den Schl√ºsseln `leicht`, `mittel`, `schwer`.
  * `time_per_weight_minutes`: (object) Dokumentiert die verwendeten Minuten pro Gewichtung (Schl√ºssel `"1"`, `"2"`, `"3"` mit numerischen Werten).
  * `additional_buffer_minutes`: (number, optional) Optionaler Zeitpuffer, wenn gew√ºnscht oder begr√ºndet.
  * `test_duration_minutes`: (integer) Finale, empfohlene Testdauer (ganze Minuten).

#### **Felder pro Frage (`questions[]`):**

  * `frage`: (string) Vollst√§ndiger Fragetext, beginnend mit laufender Nummer und Punkt (z.B. "1. Was ist ...").
  * `frage`: (string) Vollst√§ndiger Fragetext. Beginne jede Frage mit einer laufenden Nummer und einem Punkt (z.B. "1. Was ist ...", "2. Wie funktioniert ...").
  * `optionen`: (array of strings) Antwortoptionen, alle plausibel formuliert.
  * `loesung`: (integer) Index der korrekten Option (0-basiert).
  * `erklaerung`: (string) Standarderkl√§rung zur L√∂sung.
  * `gewichtung`: (integer) 1 = leicht, 2 = mittel, 3 = schwer.
  * `thema`: (string) Unterthema oder Kapitel.
  * `extended_explanation`: (object, optional) Zus√§tzliche Tiefe f√ºr anspruchsvolle Fragen (entweder `{ "titel": "...", "schritte": [...] }` oder `{ "title": "...", "content": "..." }`).
    * `schritte`: (array of strings) Klar formulierte S√§tze ohne f√ºhrende "Schritt x"-Pr√§fixe; die Reihenfolge ergibt sich aus der Listenposition.
  * `mini_glossary`: (object, optional) 2-4 Fachbegriffe mit Definitionen, falls in Schritt‚ÄØ6 angefordert.
    * Jede Definition muss f√ºr sich stehen; keine Querverweise wie "Siehe Frage 12" verwenden.

#### ‚úÖ Abschluss-Checkliste f√ºr das Fragenset

**F√ºhre vor der finalen Ausgabe eine Selbstpr√ºfung anhand dieser Checkliste durch:**

1. JSON ist syntaktisch g√ºltig und enth√§lt genau die Keys `meta` und `questions`.
2. `meta.question_count` entspricht der L√§nge von `questions` und `meta.difficulty_profile` spiegelt die tats√§chlichen Gewichtungen wider.
2. **Metadaten-Konsistenz:** `meta.question_count` entspricht exakt der L√§nge von `questions`. `meta.difficulty_profile` spiegelt exakt die tats√§chliche Verteilung der Gewichtungen in der `questions`-Liste wider.
3. `meta.test_duration_minutes` ist eine positive Ganzzahl und ergibt sich aus den Minuten-Faktoren (`meta.time_per_weight_minutes`) plus optionalem Puffer.
4. Jede Frage besitzt genau eine richtige Antwort (`loesung` verweist auf einen g√ºltigen Index).
4. Jede Frage besitzt genau eine richtige Antwort (`loesung` verweist auf einen g√ºltigen Index im `optionen`-Array).
5. Optionale Felder (`extended_explanation`, `mini_glossary`) sind nur enthalten, wenn sie beauftragt wurden und nicht leer.
6. Titel, Zielgruppe und Themen sind konsistent und eindeutig formuliert.
6. **Faktentreue:** Alle Erkl√§rungen und Definitionen basieren auf etablierten Fakten, nicht auf Faustregeln oder vagen Interpretationen.
7. Jede Themenangabe kommt mindestens zweimal vor; insgesamt existieren h√∂chstens zehn unterschiedliche Themen.
8. Mini-Glossar-Eintr√§ge enthalten eigenst√§ndige Definitionen ohne Querverweise auf andere Fragen.

#### **Richtlinien f√ºr Mini-Glossar-Eintr√§ge:**

Falls Mini-Glossar-Eintr√§ge gew√ºnscht werden, beachte folgende Best Practices:

1.  **Anzahl:** 2-4 zentrale Begriffe pro Frage (nicht mehr, nicht weniger)
2.  **Relevanz:** Nur Begriffe aufnehmen, die f√ºr das Verst√§ndnis der Frage essentiell sind
3.  **L√§nge:** Definitionen in 1-3 S√§tzen (ca. 50-150 W√∂rter)
4.  **Pr√§zision:** Fachlich korrekte, pr√§gnante Erkl√§rungen ohne Trivialit√§ten
5.  **LaTeX-Support:** Mathematische/physikalische Formeln in `$...$` oder `$$...$$` Notation
6.  **Keine Redundanz:** Keine Wiederholung von Inhalten aus `erklaerung` oder `extended_explanation`
7.  **Alphabetische Reihenfolge:** Begriffe werden automatisch sortiert, keine manuelle Ordnung n√∂tig
8.  **Eigenst√§ndige Definitionen:** Vermeide Querverweise (z.‚ÄØB. "Siehe Frage 20"); jede Definition soll ohne Kontext verst√§ndlich sein

**Beispiele f√ºr gute Glossar-Eintr√§ge:**
```json
"mini_glossary": {
  "Surjektivit√§t": "Eine Funktion $f: A \\to B$ ist surjektiv, wenn jedes Element aus der Zielmenge $B$ mindestens ein Urbild in $A$ hat.",
  "Injektivit√§t": "Eine Funktion ist injektiv, wenn verschiedene Elemente der Definitionsmenge auf verschiedene Elemente der Zielmenge abgebildet werden.",
  "Bijektivit√§t": "Eine Funktion ist bijektiv, wenn sie sowohl injektiv als auch surjektiv ist. Bijektive Funktionen sind umkehrbar."
}
```

#### **Formatierungsregeln f√ºr Textinhalte:**

Beachte beim Erstellen der Fragen zus√§tzlich die folgenden **didaktischen Richtlinien f√ºr gute MC-Fragen**:

1.  **Keine Hinweise in der Frage:** Die Frage darf keine sprachlichen Hinweise enthalten, die auf die richtige Antwort schlie√üen lassen.
2.  **Plausible Distraktoren:** Alle falschen Antwortoptionen (Distraktoren) m√ºssen plausibel und attraktiv sein. Sie sollten typische Missverst√§ndnisse oder h√§ufige Fehler widerspiegeln.
3.  **Einheitliche Antwortl√§nge:** Alle Antwortoptionen sollten eine √§hnliche L√§nge und grammatikalische Struktur haben, um zu vermeiden, dass die l√§ngste oder detaillierteste Antwort automatisch als richtig erkannt wird.
4.  **Keine "l√§ngste Antwort"-Falle:** Die korrekte Antwort darf nicht systematisch die l√§ngste oder detaillierteste Option sein.
5.  **Vermeide Negationen:** Formuliere Fragen positiv (z.B. "Welche Aussage ist korrekt?") anstatt negativ ("Welche Aussage ist NICHT korrekt?").
6.  **Unter keinen Umst√§nden: Verweise, Pr√§fixe oder Positionsannahmen in Antwortoptionen**
    * Antwortoptionen m√ºssen vollst√§ndig eigenst√§ndige Aussagen sein. Unter keinen Umst√§nden d√ºrfen Optionstexte inhaltlich oder sprachlich auf andere Optionen verweisen oder diese adressieren.
    * Beispiele f√ºr strikt verbotene Formulierungen:
      - "Alle oben genannten"
      - "C und D sind korrekt"
      - "Siehe Option B"
    * Ebenfalls verboten: Buchstaben- oder Zahlenpr√§fixe im Optionstext (z. B. "A) ...", "1. ..."). Optionstexte d√ºrfen nicht mit solchen Pr√§fixen beginnen.
    * Die App zeigt Antwortoptionen zuf√§llig an. Deshalb d√ºrfen Optionstexte in keiner Weise Positions- oder Index‚ÄëAnnahmen enthalten (weder explizit noch implizit).
    * Jede Option steht f√ºr sich: Formuliere jede Antwortoption als vollst√§ndige, eigenst√§ndige Aussage ohne Bezug auf andere Optionen. Wenn mehrere Teil‚ÄëAussagen gepr√ºft werden sollen, verwende separate Fragen.
    * Technischer Hinweis: `loesung` ist ein 0-basierter Index in `optionen`. Damit die Zuordnung eindeutig bleibt, sind referenzierende oder indexabh√§ngige Formulierungen nicht zul√§ssig.
    * Warum: Referenzen oder Pr√§fixe verhindern eindeutige, index‚Äëbasierte L√∂sungen, erschweren automatisierte Validierung und sind f√ºr Lernende verwirrend.
7.  **Zuf√§llige Position der L√∂sung:** Die korrekte Antwort sollte zuf√§llig unter den Optionen platziert werden und nicht immer an derselben Position (z.B. immer als dritte Option) stehen.

---

Wende die folgenden Formatierungsregeln f√ºr **alle** Textinhalte an:

  * **Grundregel 0 (WICHTIGSTE REGEL):** Mathematische Inhalte (Formeln, einzelne Variablen wie `$a$`, `$b$`, `$\\mathbb{Z}$`) geh√∂ren **IMMER** in KaTeX-Dollarzeichen (`$...$`) und **NIEMALS** in Backticks (` `). Backticks sind ausschlie√ülich f√ºr Code-Begriffe, Dateinamen oder Funktionsnamen reserviert.
    * **KORREKT:** Die Formel lautet `$d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$`.
    * **FALSCH:** Die Formel lautet `$d(x,y) = \sqrt{\sum_{i=1}^n (x_i-y_i)^2}$`.
    * **FALSCH:** Die Formel lautet `d(x,y) = ...`.

  * **Fachbegriffe & Code:** Technische Begriffe, Dateinamen oder Funktionsnamen werden in Backticks (` `) eingeschlossen.
    * *Beispiel:* `Docker`, `st.write()`, `requirements.txt`

  * **Hervorhebungen:** Wichtige Schl√ºsselw√∂rter im Text werden mit doppelten Sternchen f√ºr **Fettdruck** (`**Text**`) formatiert.

  * **Mathematische Ausdr√ºcke (KaTeX):**
    * Inline-Formeln: `$a^2 + b^2 = c^2$`
    * Abgesetzte Formeln: `$$x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$$`
    * **Wichtig:** Backslashes (`\`) in JSON-Strings m√ºssen escaped werden: `\\`. Beispiel: `"frage": "Was ist $\\binom{n}{k}$?"`

  * **Grundregel 2:** Normaler Text und Satzzeichen geh√∂ren **au√üerhalb** der KaTeX-Dollarzeichen.
    * **FALSCH:** `$M \\cap N = \\emptyset$, also sind die Mengen disjunkt.$`
    * **RICHTIG:** `$M \\cap N = \\emptyset$, also sind die Mengen disjunkt.`

---

Stelle mir nach Abschluss der Generierung die fertige `questions_Ihr_Thema.json`-Datei direkt hier zum Download zur Verf√ºgung.
