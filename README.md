# üìù MC-Test Streamlit App

[![CI](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml/badge.svg?b‚îú‚îÄmain)](https://github.com/kqc-real/streamlit/actions/workflows/ci.yml)

Eine interaktive Multiple-Choice-Lern- und Selbsttest-App.
Sie bietet schnelles Feedback, Fortschrittsverfolgung und aggregierte Ergebnisse f√ºr verschiedene Fragensets.
Die App ist modular aufgebaut und nutzt eine SQLite-Datenbank zur persistenten Speicherung von Testergebnissen.
Die App verf√ºgt √ºber ein integriertes Feedback-System, das es Nutzern erm√∂glicht, Probleme mit Fragen zu melden, und Admins, dieses Feedback zu verwalten.

---

## üöÄ Schnellstart

**üìñ Installationsanleitungen f√ºr Einsteiger/innen**
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_MAC_ANLEITUNG.md
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_WINDOWS_ANLEITUNG.md
- https://github.com/kqc-real/streamlit/blob/main/INSTALLATION_VS-CODE_SSH-AUTHENTIFIZIERUNG.md

Diese Schritt-f√ºr-Schritt-Anleitungen erkl√§ren alles von Grund auf:
- Python & Git installieren (Windows & Mac)
- App herunterladen und starten
- H√§ufige Probleme und L√∂sungen
- **Perfekt f√ºr BWL-Studierende ohne IT-Kenntnisse!**

**Admin-Panel lokal testen?**
‚Üí **[üîê Admin-Panel Anleitung f√ºr Kursteilnehmer/innen](ADMIN_PANEL_ANLEITUNG.md)**

Diese Anleitung zeigt dir:
- Wie du als "Albert Einstein" Admin-Rechte erh√§ltst
- Was du im Admin-Panel alles tun kannst (Analytics, Itemanalyse, Feedback)
- Wie Itemanalyse und Distraktor-Analyse funktionieren
- **Perfekt f√ºr Projektmitglieder, die alle Features verstehen wollen!**

---

## üöÄ √úbersicht

Diese App ist ein vollst√§ndiger MC-Test f√ºr Kursinhalte, entwickelt mit Streamlit.
Sie erm√∂glicht anonyme Tests mit Pseudonymen, zuf√§lliger Fragenreihenfolge, Zeitlimit und einem integrierten Feedback-System zur kontinuierlichen Verbesserung der Fragen.
Perfekt f√ºr Bildungsumgebungen, Selbstlernphasen oder zur Pr√ºfungsvorbereitung.

### Hauptfunktionen

| Kategorie      | Funktion                                                                                      |
|----------------|-----------------------------------------------------------------------------------------------|
| Zugang         | Pseudonym-Login (anonym, keine Registrierung)                                                 |
| Fragen         | Zuf√§llige Reihenfolge, Gewichtung je Frage, strikte Trennung nach Fragenset                   |
| Fragenset      | Dynamische Auswahl verschiedener Fragensets (`questions_*.json`)                               |
| Scoring-Modi   | "Nur +Punkte" (falsch = 0) oder "+/- Punkte" (falsch = -Gewichtung)                            |
| Feedback       | Sofortiges Ergebnis mit optionalen, detaillierten Erkl√§rungen zu Theorie und Herleitung       |
| Navigation     | Fragen k√∂nnen markiert und √ºbersprungen werden, mit direkter Navigation √ºber die Seitenleiste |
| Fortschritt    | Fortschritt wird pro Pseudonym und Fragenset in einer SQLite-Datenbank gespeichert            |
| Zeitlimit      | Optionales 60-Minuten-Fenster                                                                 |
| Feedback       | Nutzer k√∂nnen Probleme mit Fragen melden (inhaltlich, technisch etc.)                         |
| Leaderboard    | √ñffentliches Top‚Äë10 (pro Fragenset); vollst√§ndige Ansicht f√ºr Admin                           |
| Analyse & Wartung | Itemanalyse, Distraktor-Analyse, Verwaltung von gemeldetem Feedback                         |
| PDF-Export     | Professioneller Report mit LaTeX-Rendering, Durchschnittsvergleich, Mini-Glossar, Bookmarks   |
| Export         | CSV-Download aller Antworten und SQL-Dump der Datenbank √ºber Admin-Panel                      |
| Admin-Panel    | Passwortgesch√ºtzter Bereich f√ºr Analyse, Feedback-Management, Export und Systemeinstellungen  |

### Neuere Features (seit v1.2)

- Musterl√∂sung (PDF): Admins k√∂nnen jetzt eine formatierte Musterl√∂sung mit allen korrekten Antworten, ausf√ºhrlichen Erkl√§rungen und angeh√§ngtem Mini-Glossar erzeugen. Die Musterl√∂sung rendert LaTeX-Formeln als hochwertige PNGs und hebt korrekte Optionen hervor.
- Nutzer-Download: Nach Abschluss eines Tests steht den Nutzer/innen ebenfalls eine Musterl√∂sung zum Download zur Verf√ºgung (sinnvollerweise nur zum Lernen). Die UI zeigt eine Kurzinfo, dass die Musterl√∂sung pr√ºfungsrelevant ist und nicht geteilt werden sollte.
- Robustere Formel-Rendering-Pipeline: Formeln werden parallel gerendert (ThreadPool) mit einem konfigurierbaren Gesamt-Timeout; ausgefallene oder zu langsame Renderings werden durch Platzhalter ersetzt, damit ein Export nicht ewig h√§ngt.
- Per-User Export-Cache: Um wiederholte Anfragen schnell zu bedienen, werden erzeugte Musterl√∂sungen tempor√§r im Session-Cache (`st.session_state`) zwischengespeichert und beim erneuten Download sofort ausgeliefert.
- Zeitabsch√§tzung & Probe-Rendering: Bei PDF-Exporten mit Formeln wird optional ein Probe-Rendering durchgef√ºhrt, um die voraussichtliche Gesamtdauer anzugeben.

### Formel-Cache (Disk) & automatische Eviction

Die App cached gerenderte LaTeX-Formel-Bilder als PNG-Dateien auf der lokalen Festplatte, um Netzwerkaufrufe zur Remote-Render-API zu reduzieren und Exporte zu beschleunigen. Damit der Cache auf Hosts mit begrenztem oder ephemerem Speicher (z. B. Streamlit Community Cloud) nicht unkontrolliert w√§chst, gibt es automatische Eviction-Mechanismen.

Konfigurierbare Umgebungsvariablen:

- FORMULA_CACHE_DIR: Pfad zum Cache-Verzeichnis (Standard: ./var/formula_cache)
- FORMULA_CACHE_MAX_FILES: Maximale Anzahl Dateien im Cache (Standard: 200)
- FORMULA_CACHE_MAX_MB: Maximale Gesamtgr√∂√üe des Caches in MiB (Standard: 200)
- FORMULA_CACHE_TTL_DAYS: Lebensdauer von Cache-Dateien in Tagen (Standard: 7)

Verhalten:

- Vor jedem Schreiben einer neuen Formeldatei wird die Eviction-Routine ausgef√ºhrt: Dateien √§lter als TTL werden zuerst entfernt; danach werden die √§ltesten Dateien gel√∂scht, bis sowohl Dateianzahl als auch Gesamtgr√∂√üe innerhalb der Grenzwerte liegen.
- Auf schreibgesch√ºtzten oder nicht verf√ºgbaren Dateisystemen werden Schreibfehler ignoriert und die In-Memory-Fallback-Strategie verwendet. Das verhindert Abst√ºrze auf restriktiven Plattformen.

Empfehlung: Setze konservative Limits f√ºr Cloud-Deploys (z. B. FORMULA_CACHE_MAX_MB=50, FORMULA_CACHE_MAX_FILES=100) und √ºberwache die Nutzung in Logs.


---

## ÔøΩ Security Features

Die MC-Test-App implementiert **Enterprise-Grade Security** √ºber drei aufeinander aufbauende Phasen:

### Phase 1: Quick Wins (v1.1.0)
- ‚ö†Ô∏è **Empty Admin-Key Warnings**: Warnung bei unsicherem Admin-Passwort
- üîÑ **Re-Authentication**: Passwortabfrage vor kritischen Operationen (L√∂schen, Export)

### Phase 2: Server-Side Session Validation (v1.2.0)
- üîê **Cryptographic Tokens**: Sichere Session-Tokens mit `secrets.token_urlsafe(32)`
- üîí **SHA-256 Hashing**: Keine Klartext-Passw√∂rter im Session State
- ‚è±Ô∏è **Session Timeouts**: Automatische Abmeldung nach 2 Stunden Inaktivit√§t
- üßµ **Thread-Safe**: Sichere Concurrent-Access mit Threading-Locks

### Phase 3: Audit-Logging & Rate-Limiting (v1.3.0) ‚≠ê **NEU**
- üìä **SQLite-Based Audit-Logging**: Alle Admin-Aktionen persistent geloggt
  - Login-Versuche (erfolg/fehlgeschlagen)
  - Delete-Operationen (User-Daten, Global)
  - CSV-Exports
  - CRITICAL Actions markiert
- üö´ **Rate-Limiting**: Brute-Force-Schutz
  - 3 fehlgeschlagene Login-Versuche ‚Üí 5 Minuten Sperre
  - Automatisches Reset nach erfolgreichem Login
  - Anzeige der Sperr-Zeit
- üìà **Admin Dashboard**: Neuer "üîí Audit-Log" Tab
  - Statistiken: Gesamt-Aktionen, Success/Fail-Raten
  - Filter: User, Action-Typ, Erfolg-Status, Limit
  - CSV-Export f√ºr forensische Analyse
- üóëÔ∏è **DSGVO-Compliance**: Automatische L√∂schung nach 90 Tagen
- üåç **IP-Tracking**: Optional Client-IP-Logging (wenn verf√ºgbar)

**Security Level:** üõ°Ô∏è **VERY HIGH (Enterprise-Grade)**

**Dokumentation:**
- üìò [SECURITY_PHASE3_SUMMARY.md](SECURITY_PHASE3_SUMMARY.md) - Technische Details
- üìã [CHANGELOG_SECURITY_PHASE3.md](CHANGELOG_SECURITY_PHASE3.md) - Vollst√§ndiger Changelog
- üìÑ [PHASE3_ABSCHLUSS.md](PHASE3_ABSCHLUSS.md) - User-Guide

---

## ÔøΩüìã Voraussetzungen

- **Python:** Version 3.9 oder h√∂her.
- **Abh√§ngigkeiten:** Installiere via `pip install -r requirements.txt`.

---

## üõ†Ô∏è Installation und Start

### Lokaler Start

1.  Klone das Repository.
2.  Installiere die Abh√§ngigkeiten:
    ```bash
    pip install -r requirements.txt
    ```
3.  Starte die App:
    ```bash
    streamlit run app.py
    ```
4.  √ñffne [http://localhost:8501](http://localhost:8501) im Browser.

### Deployment (z.B. Streamlit Cloud)

1.  Verbinde dein GitHub-Repository mit deinem Streamlit-Cloud-Konto.
2.  Deploye die App.
3.  Konfiguriere die Secrets (siehe n√§chster Abschnitt) im Dashboard deiner Streamlit-Cloud-App.

---

## ‚öôÔ∏è Konfiguration

### Umgebungsvariablen / Secrets

Die App wird √ºber Umgebungsvariablen (f√ºr sensible Daten) und eine Konfigurationsdatei (f√ºr nicht-sensible Daten) konfiguriert.

F√ºr die lokale Entwicklung kannst du eine `.env`-Datei erstellen. F√ºr das Deployment auf Streamlit Cloud m√ºssen diese Variablen als "Secrets" im Dashboard der App hinterlegt werden.

```env
# Beispiel f√ºr .env oder Streamlit Cloud Secrets
MC_TEST_ADMIN_USER="dein_admin_user"
MC_TEST_ADMIN_KEY="dein_geheimes_passwort"
MC_TEST_MIN_SECONDS_BETWEEN="2"
APP_URL="https://ihre-streamlit-app.streamlit.app"
```

- **`MC_TEST_ADMIN_USER`**: Der Benutzername, der f√ºr den Admin-Login erforderlich ist.
- **`MC_TEST_ADMIN_KEY`**: Das Passwort f√ºr den Admin-Login.
- **`MC_TEST_MIN_SECONDS_BETWEEN`**: Die Mindestanzahl an Sekunden, die zwischen zwei Antworten vergehen muss. Verhindert Spam. Ein Wert von `0` deaktiviert das Limit. (Default: `3`)
- **`APP_URL`**: Die URL der Streamlit-App f√ºr den QR-Code im PDF-Export. (Default: `https://mc-test-amalea.streamlit.app`)

---

## üìÅ Projektstruktur

```
.
‚îú‚îÄ‚îÄ .github/                # GitHub Actions Workflows (CI/CD)
‚îú‚îÄ‚îÄ .streamlit/             # Streamlit-Konfiguration (z.B. Themes)
‚îú‚îÄ‚îÄ data/                   # Enth√§lt JSON-Dateien (Fragensets, Pseudonyme)
‚îú‚îÄ‚îÄ db/                     # Speichert die SQLite-Datenbankdatei
‚îú‚îÄ‚îÄ tests/                  # Pytest-Tests f√ºr die Anwendungslogik
‚îú‚îÄ‚îÄ .env.example            # Beispiel f√ºr Umgebungsvariablen
‚îú‚îÄ‚îÄ admin_panel.py          # Logik f√ºr das Admin-Panel
‚îú‚îÄ‚îÄ app.py                  # Haupt-Anwendungsskript
‚îú‚îÄ‚îÄ auth.py                 # Authentifizierung und Session-Management
‚îú‚îÄ‚îÄ components.py           # Wiederverwendbare UI-Komponenten
‚îú‚îÄ‚îÄ config.py               # Laden der Konfiguration und Fragensets
‚îú‚îÄ‚îÄ database.py             # Datenbankinteraktionen (SQLite)
‚îú‚îÄ‚îÄ helpers.py              # Kleine Hilfsfunktionen
‚îú‚îÄ‚îÄ logic.py                # Kernlogik der App (Scoring, etc.)
‚îú‚îÄ‚îÄ main_view.py            # UI-Logik f√ºr die Hauptansichten
‚îú‚îÄ‚îÄ pdf_export.py           # PDF-Report-Generierung mit LaTeX & Mini-Glossar
‚îú‚îÄ‚îÄ requirements.txt        # Python-Abh√§ngigkeiten
‚îú‚îÄ‚îÄ AI_QUESTION_GENERATOR_PLAN.md      # Plan f√ºr KI-basierte Fragenset-Generierung
‚îú‚îÄ‚îÄ DEPLOYMENT_FEASIBILITY_STUDY.md    # Infrastruktur & Kostenanalyse (Streamlit/Cloudflare)
‚îú‚îÄ‚îÄ GLOSSARY_SCHEMA.md                 # Dokumentation f√ºr Mini-Glossar in Fragensets
‚îú‚îÄ‚îÄ VISION_RELEASE_2.0.md              # Strategische Vision & Feature-Roadmap Release 2.0
‚îî‚îÄ‚îÄ README.md                          # Diese Dokumentation

## üß∞ Developer tools (local)

Es gibt kleine Hilfs-Skripte zum Testen und Benchmarking im Ordner `tools/`:

- `tools/test_evict.py` ‚Äî Erzeugt Dummy-Dateien im Cache (`var/formula_cache`) und testet die Eviction-Routine.
- `tools/run_export_test.py` ‚Äî F√ºhrt einen einzelnen Musterl√∂sungs-Export durch und schreibt das PDF nach `exports/`.
- `tools/benchmark_exports.py` ‚Äî F√ºhre N Exporte hintereinander aus (Standard N=5) und schreibe eine `exports/benchmark_summary.txt`.

Beispiele:

```bash
# Eviction smoke test
PYTHONPATH=. python3 tools/test_evict.py

# Single export (shows progress)
PYTHONPATH=. python3 tools/run_export_test.py

# Benchmark with 5 runs
BENCH_EXPORTS_N=5 PYTHONPATH=. python3 tools/benchmark_exports.py
```

---

## üß≠ Hinweise zum Prompting (f√ºr AI / Text‚ÄëGenerierung)

Kleine, aber wichtige Regel f√ºr alle Prompts, die in diese App (oder in Templates) eingespeist werden:

- Verwende echte Leerzeilen / Abs√§tze. Gib niemals die zwei Zeichen Backslash + n (`"\\n"`) als Ersatz f√ºr einen Zeilenumbruch aus.
- Korrekt: eine echte leere Zeile zwischen zwei Abs√§tzen.
- Nicht verwenden: der Literal‚ÄëString `"\\n"` (Backslash + n).

Beispiel (nicht so):

```
Ergebnis:\n\n- Punkt 1\n- Punkt 2
```

Beispiel (richtig):

```
Ergebnis:

- Punkt 1
- Punkt 2
```

Warum das wichtig ist:
- Manche Modelle liefern `"\\n"` anstelle echter Zeilenumbr√ºche ‚Äî das bricht Markdown/HTML‚ÄëRendering und macht die Ausgabe schwer lesbar.
- Eine kurze Nachbearbeitung der Modell‚ÄëAntworten (Sanitizer) ist zus√§tzlich empfehlenswert, siehe `helpers.py`.

Short note (EN): Use real blank lines, not the literal string "\\n". This avoids escaped newline artifacts in Markdown/HTML output.


Hinweis: Die generierten Artefakte landen in `exports/` und werden in `.gitignore` ausgeschlossen, damit sie nicht versehentlich in Git landen.
```

---

## ÔøΩüõ†Ô∏è Administration & Wartung

### Admin-Bereich

- **Zugang:**
    1. W√§hle auf der Startseite das in den Secrets (`MC_TEST_ADMIN_USER`) definierte Admin-Pseudonym aus.
    2. Nach dem Start des Tests erscheint in der Seitenleiste der Bereich "üîê Admin Panel".
    3. Gib dort das Admin-Passwort (`MC_TEST_ADMIN_KEY`) ein, um vollen Zugriff zu erhalten.
- **Funktionen:** Das Panel bietet detaillierte Analysen (Item- & Distraktoranalyse), eine √úbersicht und Verwaltung f√ºr gemeldetes Feedback, Datenexport (CSV, SQL-Dump, **PDF-Export**) und Systemeinstellungen (Scoring-Modus, Zur√ºcksetzen der Testdaten).

### Tests ausf√ºhren

```bash
pip install -r requirements.txt
PYTHONPATH=. pytest
```

---

## üêõ Troubleshooting

-   **App startet nicht:** Stelle sicher, dass alle Abh√§ngigkeiten aus `requirements.txt` installiert sind.

---

## ü§ù Contributing

Beitr√§ge sind willkommen! Forke das Repository, erstelle einen Branch und √∂ffne einen Pull Request.

---

# ü§ñ Fragensets mit KI erstellen (Optional)

Die App selbst generiert **keine** Fragen automatisch. Der folgende Abschnitt ist eine **Copy & Paste Anleitung** f√ºr die manuelle Nutzung mit einem externen KI-Assistenten (LLM).

## Voraussetzungen

- Zugang zu einem LLM wie **ChatGPT**, **Claude**, **Gemini** oder **GitHub Copilot Chat**
- Optional: PDF-Dokumente als Wissensgrundlage (Skripte, Lehrb√ºcher)

## So funktioniert's

1.  **Kopiere den gesamten Prompt-Text** aus dem n√§chsten Abschnitt
2.  **F√ºge ihn in dein LLM ein** (z.B. ChatGPT Web-Interface, Claude, VS Code Copilot Chat)
3.  **Beantworte die 7 Fragen** des Assistenten Schritt f√ºr Schritt (erst nach deiner Antwort geht es weiter).
4.  **Erhalte eine fertige `questions_*.json`-Datei** zum Download
5.  **Pr√ºfe die JSON-Datei** z.‚ÄØB. mit [jsonlint.com](https://jsonlint.com) oder einem lokalen Linter.
6.  **Speichere die Datei** im `data/`-Ordner deiner App.

Der Prompt enth√§lt alle notwendigen Informationen (JSON-Schema, Formatierungsregeln, didaktische Guidelines), damit das LLM qualitativ hochwertige Fragen f√ºr diese App erstellen kann.

## Prompt (copy & paste)

**Rolle:** Du bist ein Experte f√ºr die Erstellung von didaktisch hochwertigen Multiple-Choice-Fragen (MCQs).

**Ziel:** Du wirst mich (den Benutzer) interaktiv durch einen 7-stufigen Konfigurationsprozess f√ºhren, um die Anforderungen f√ºr ein neues Fragenset zu definieren.

**Interaktionsregeln (Zwingend einzuhalten):**
1.  Beginne **sofort** mit Schritt 1.
2.  Stelle pro Schritt **nur die eine, zugeh√∂rige Frage** aus der Anleitung.
3.  Warte **zwingend** auf meine Antwort, bevor Du die n√§chste Frage stellst oder mit dem n√§chsten Schritt fortf√§hrst.
4.  Fahre erst fort, wenn alle 7 Schritte nacheinander durchlaufen wurden.
5.  Verwende echte Leerzeilen / Abs√§tze in allen Ausgaben. Gib niemals den Literal‚ÄëString `"\\n"` (Backslash + n) als Ersatz f√ºr Zeilenumbr√ºche aus. Formatiere Abs√§tze mit echten Zeilenumbr√ºchen, damit Markdown/HTML‚ÄëRendering korrekt funktioniert.

**Finale Aufgabe (Nach Schritt 7):**
1.  Nachdem ich die 7. Frage beantwortet habe, fasse meine 7 Konfigurations-Antworten kurz zusammen.
2.  Bitte mich um eine finale Best√§tigung.
3.  **Nach meiner Best√§tigung**, generiere das vollst√§ndige Fragenset gem√§√ü den unten definierten JSON-Struktur-, Inhalts- und Formatierungsregeln.

---

### **Interaktiver Konfigurationsprozess**

#### **Schritt 1 von 7 ‚Äì Thema festlegen**
Stelle die folgende Frage:
"Was ist das **zentrale Thema** f√ºr das neue Fragenset? Dies dient als Grundlage f√ºr den Inhalt und den Dateinamen (z.B. `questions_Ihr_Thema.json`).
*Beispiele: 'Data Science Grundlagen', 'Software-Architektur', 'Projektmanagement nach Scrum'*"

*(Warte auf meine Antwort)*

---

#### **Schritt 2 von 7 ‚Äì Zielgruppe bestimmen**
Stelle die folgende Frage:
"Wer ist die **Zielgruppe** f√ºr dieses Fragenset?
*Beispiele: 'Anf√§nger ohne Vorkenntnisse', 'Fortgeschrittene mit Grundwissen', 'Experten zur Pr√ºfungsvorbereitung'*"

*(Warte auf meine Antwort)*

---

#### **Schritt 3 von 7 ‚Äì Umfang & Schwierigkeitsprofil**
Stelle die folgende Frage:
"Wie viele Fragen soll das Set ungef√§hr enthalten (z.B. 20 oder 50) und welche **Verteilung der Schwierigkeitsgrade** w√ºnschen Sie?

* **Gewichtung 1:** Leichte Reproduktionsfragen
* **Gewichtung 2:** Anwendungsorientierte Transferfragen
* **Gewichtung 3:** Anspruchsvolle Expertenfragen

Geben Sie mir entweder die genaue Anzahl pro Gewichtung an (z.B. 10 leicht, 8 mittel, 2 schwer) oder ein prozentuales Verh√§ltnis (z.B. 50% / 35% / 15%). Wenn Sie keine Angabe machen, schlage ich ein Standardverh√§ltnis vor."

*(Warte auf meine Antwort und schlage ggf. ein Verh√§ltnis vor, falls keine Verteilung genannt wurde)*

---

#### **Schritt 4 von 7 ‚Äì Anzahl der Antwortoptionen**
Stelle die folgende Frage:
"Wie viele **Antwortoptionen** sollen die Fragen haben? Bitte w√§hlen Sie eine der folgenden M√∂glichkeiten:

* **A) 4 Optionen:** Das klassische Multiple-Choice-Format.
* **B) 5 Optionen:** Etwas anspruchsvoller, da die Ratewahrscheinlichkeit sinkt.
* **C) Variabel:** Die Anzahl der Optionen (z.B. 3 bis 5) kann pro Frage variieren. Dies bietet die gr√∂√üte Flexibilit√§t."

*(Warte auf meine Antwort [A, B oder C])*

---

#### **Schritt 5 von 7 ‚Äì Erweiterte Erkl√§rungen (optional)**
Stelle die folgende Frage:
"Sollen f√ºr schwierigere Fragen (Gewichtung 2 und 3) zus√§tzlich zur normalen Erkl√§rung auch **erweiterte Erkl√§rungen** (`extended_explanation`) generiert werden? Diese k√∂nnen tiefergehenden Hintergrund, Code-Beispiele oder Herleitungen enthalten.

*Bitte antworten Sie mit 'Ja' oder 'Nein'.*"

*(Warte auf meine Antwort)*

---

#### **Schritt 6 von 7 ‚Äì Mini-Glossar (optional)**
Stelle die folgende Frage:
"Sollen f√ºr die Fragen **Mini-Glossar-Eintr√§ge** (`mini_glossary`) generiert werden? Diese erkl√§ren 2-4 zentrale Fachbegriffe pro Frage und werden im PDF-Export als separates Glossar angezeigt.

*Bitte antworten Sie mit 'Ja' oder 'Nein'.*"

*(Warte auf meine Antwort)*

---

#### **Schritt 7 von 7 ‚Äì Externe Dokumente (optional)**
Stelle die folgende Frage:
"M√∂chten Sie externe Dokumente (z.B. Vorlesungsskripte als PDF oder Text) als **Wissensgrundlage** bereitstellen? Dies kann die Qualit√§t und Spezifit√§t der Fragen verbessern.

Wenn ja, laden Sie diese bitte hoch oder f√ºgen Sie den Text ein. Wenn nein, fahre ich ohne zus√§tzliche Wissensbasis fort."

*(Warte auf meine Antwort)*

---
---

### **Anweisungen f√ºr die finale Generierung (Nach Schritt 7)**

Nachdem ich alle 7 Fragen beantwortet und die Zusammenfassung best√§tigt habe, erstelle das Fragenset. Das Ergebnis muss ein **einzelnes, valides JSON-Objekt** sein.

#### **‚ö†Ô∏è Striktes Ausgabeformat**

Deine finale Antwort muss **ausschlie√ülich** einen einzelnen Markdown-Codeblock enthalten, der das valide JSON-Objekt umschlie√üt.

```json
{
  "meta": {
    "title": "...",
    "created": "DD.MM.YYYY HH:MM",
    "modified": "DD.MM.YYYY HH:MM",
    "target_audience": "...",
    "question_count": 0,
    "difficulty_profile": {
      "leicht": 0,
      "mittel": 0,
      "schwer": 0
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 0
  },
  "questions": [
    {
      "frage": "1. ...",
      "optionen": [
        "...",
        "..."
      ],
      "loesung": 0,
      "erklaerung": "...",
      "gewichtung": 1,
      "thema": "...",
      "extended_explanation": {
        "titel": "...",
        "schritte": [
          "..."
        ]
      },
      "mini_glossary": {
        "Begriff 1": "Definition 1...",
        "Begriff 2": "Definition 2..."
      }
    }
  ]
}
```

F√ºge **keinerlei** Text, Kommentare oder Erkl√§rungen (wie "Hier ist das JSON:") vor oder nach diesem Codeblock ein. Entferne alle internen Marker oder Kommentare aus den Textfeldern. Der finale JSON-String muss sauber sein.

-----

### **Berechnung der Metadaten (`meta`)**

F√ºlle die `meta`-Sektion basierend auf den generierten Fragen. WICHTIG: Das Feld `meta.created` muss gesetzt sein (Format: `DD.MM.YYYY HH:MM` oder `DD.MM.YYYY`).

1.  **Zeitberechnung:** Berechne die `meta.test_duration_minutes` basierend auf den *tats√§chlich generierten* Fragen.
      * Verwende als Standard-Minutenfaktoren (dokumentiert in `meta.time_per_weight_minutes`): Gewichtung 1 ‚Üí `0.5`, Gewichtung 2 ‚Üí `0.75`, Gewichtung 3 ‚Üí `1.0`.
      * Multipliziere die Anzahl der Fragen pro Gewichtung mit diesen Faktoren.
      * Addiere einen sinnvollen Puffer (z.B. `meta.additional_buffer_minutes: 5`).
      * Runde das Endergebnis auf eine volle Minute.
      * Falls die berechnete Zeit \>= 10 Minuten ist, runde sie auf das n√§chste Vielfache von 5 Minuten auf. (z.B. 17 -\> 20; 9 -\> 9).
2.  **Z√§hlung:** `meta.question_count` muss `questions.length` entsprechen.
3.  **Profil:** `meta.difficulty_profile` (Schl√ºssel `leicht`, `mittel`, `schwer`) muss exakt die Anzahl der Fragen mit `gewichtung: 1`, `gewichtung: 2` und `gewichtung: 3` widerspiegeln.
4.  **Konsistenz:** `meta.title` und `meta.target_audience` m√ºssen den Antworten aus Schritt 1 und 2 entsprechen.

-----

### **JSON-Strukturdefinition**

#### **Meta-Felder (`meta`):**

  * `title`: (string) Klarer Name des Fragensets (aus Schritt 1).
  * `target_audience`: (string) Beschreibung der Zielgruppe (aus Schritt 2).
  * `question_count`: (integer) Gesamtanzahl der Fragen (muss `questions.length` entsprechen).
  * `difficulty_profile`: (object) Tats√§chliche Verteilung der generierten Fragen (Keys: `leicht`, `mittel`, `schwer`).
  * `time_per_weight_minutes`: (object) Dokumentiert die verwendeten Minuten pro Gewichtung (Keys: `"1"`, `"2"`, `"3"`).
  * `additional_buffer_minutes`: (number, optional) Verwendeter Zeitpuffer.
  * `test_duration_minutes`: (integer) Finale, empfohlee Testdauer (ganze Zahl).

#### **Felder pro Frage (`questions[]`):**

  * `frage`: (string) Vollst√§ndiger Fragetext, beginnend mit einer laufenden Nummer und einem Punkt (z.B. `"1. Was ist ..."`, `"2. Wie funktioniert ..."`).
  * `optionen`: (array of strings) Antwortoptionen.
  * `loesung`: (integer) 0-basierter Index der korrekten Option im `optionen`-Array.
  * `erklaerung`: (string) Standarderkl√§rung zur L√∂sung.
  * `gewichtung`: (integer) 1, 2 oder 3.
  * `thema`: (string) Unterthema oder Kapitel (z.B. "Normalisierung", "Agile Methoden").
  * `extended_explanation`: (object, optional) **Nur** generieren, wenn in Schritt 5 mit 'Ja' beantwortet. Muss die Struktur `{ "titel": "...", "schritte": [...] }` haben.
      * `schritte`: (array of strings) S√§tze ohne f√ºhrende "Schritt x"-Pr√§fixe.
  * `mini_glossary`: (object, optional) **Nur** generieren, wenn in Schritt 6 mit 'Ja' beantwortet. Ein Objekt, bei dem Schl√ºssel die Begriffe und Werte die Definitionen sind.

-----

### **‚úÖ Abschluss-Checkliste (Interne Pr√ºfung vor Ausgabe)**

F√ºhre vor der finalen JSON-Ausgabe eine Selbstpr√ºfung durch:

1.  **Validit√§t:** Das JSON ist syntaktisch valide.
2.  **Struktur:** Es enth√§lt exakt die Top-Level-Keys `meta` und `questions`.
3.  **Metadaten-Konsistenz:** `meta.question_count` entspricht `questions.length`. `meta.difficulty_profile` spiegelt exakt die tats√§chlichen `gewichtung`-Werte in der `questions`-Liste wider.
4.  **Zeitberechnung:** `meta.test_duration_minutes` ist eine positive Ganzzahl, die korrekt nach den oben genannten Regeln berechnet wurde.
5.  **L√∂sbarkeit:** Jede Frage hat genau eine korrekte `loesung`, deren Index auf ein valides Element in `optionen` verweist.
6.  **Optionalit√§t:** Optionale Felder (`extended_explanation`, `mini_glossary`) sind nur enthalten, wenn sie in Schritt 5/6 beauftragt wurden und nicht leer sind.
7.  **Faktentreue:** Alle Erkl√§rungen und Definitionen basieren auf etablierten Fakten.
8.  **Themen-Verteilung:** Jede `thema`-Angabe wird f√ºr mindestens zwei Fragen verwendet. Es gibt insgesamt h√∂chstens zehn (10) verschiedene `thema`-Werte (Inhalte ggf. sinnvoll zusammenfassen).
9.  **Glossar-Integrit√§t:** Mini-Glossar-Eintr√§ge enthalten eigenst√§ndige Definitionen ohne Querverweise auf andere Fragen.

-----

### **Didaktische Richtlinien f√ºr MCQ-Inhalte**

Beachte beim Erstellen der Fragen, Optionen und Erkl√§rungen zwingend diese Regeln:

1.  **Plausable Distraktoren:** Alle falschen Antwortoptionen (Distraktoren) m√ºssen plausibel klingen und typische Missverst√§ndnisse widerspiegeln.

2.  **Einheitliche Optionen:** Alle Antwortoptionen einer Frage sollten eine √§hnliche L√§nge und grammatikalische Struktur haben. Vermeide, dass die korrekte Antwort systematisch die l√§ngste/detaillierte ist.

3.  **Keine Negationen:** Formuliere Fragen positiv ("Welche Aussage ist korrekt?") statt doppelt negativ oder verwirrend ("Welche Aussage ist NICHT inkorrekt?").

4.  **Keine Hinweise:** Der Fragetext darf keine sprachlichen Hinweise (z.B. Genus/Numerus) enthalten, die auf die richtige Antwort schlie√üen lassen.

5.  **Zuf√§llige L√∂sung:** Die Position der korrekten Antwort (`loesung`) muss √ºber das Set hinweg variieren.

6.  **‚ö†Ô∏è STRIKTES VERBOT: REFERENZEN IN OPTIONEN**

      * Antwortoptionen m√ºssen vollst√§ndig eigenst√§ndige Aussagen sein.
      * Formulierungen wie "Alle oben genannten", "A und B sind korrekt", "Keine der Antworten" oder "Siehe Option C" sind **strikt verboten**.
      * Jede Option muss isoliert bewertbar sein.

7.  **‚ö†Ô∏è STRIKTES VERBOT: PR√ÑFIXE IN OPTIONEN**

      * Antwortoptionen d√ºrfen **niemals** mit Buchstaben- oder Zahlenpr√§fixen beginnen (z.B. "A) ...", "1. ...").
      * **FALSCH:** `"optionen": ["A) Text...", "B) Text..."]`
      * **KORREKT:** `"optionen": ["Text...", "Text..."]`

-----

### **Richtlinien f√ºr Mini-Glossar-Eintr√§ge (Falls beauftragt)**

1.  **Anzahl:** 2-4 zentrale, relevante Begriffe pro Frage.
2.  **L√§nge:** Definitionen in 1-3 pr√§gnanten S√§tzen.
3.  **Pr√§zision:** Fachlich korrekte, eigenst√§ndige Erkl√§rungen.
4.  **Keine Redundanz:** Keine Wiederholung von Inhalten aus `erklaerung`.
5.  **Eigenst√§ndig:** **Keine** Querverweise (z.B. "Siehe Frage 12").

-----

### **Formatierungsregeln f√ºr ALLE Textinhalte (Zwingend)**

Beachte beim Erstellen der JSON-Strings die folgenden Formatierungsregeln:

1.  **WICHTIGSTE REGEL (Code vs. Mathe):**

      * Backticks (`` ` ``) sind **ausschlie√ülich** f√ºr Code-Begriffe, Dateinamen oder Funktionsnamen reserviert (z.B. `` `st.write()` ``, `` `requirements.txt` ``).
      * KaTeX-Dollarzeichen (`$...$`) sind **ausschlie√ülich** f√ºr mathematische Inhalte (Formeln, einzelne Variablen, Symbole) reserviert (z.B. `$a^2 + b^2 = c^2$`, `$\\mathbb{R}$`).

2.  **JSON LaTeX Escaping (Zwingend):**

      * Da die Ausgabe ein JSON-String ist, m√ºssen **alle** Backslashes (`\`) in KaTeX-Ausdr√ºcken escaped werden. Ein einzelner Backslash ist ung√ºltiges JSON oder wird falsch interpretiert.
      * **KORREKT (im JSON):** `"Definition von $\\mathbb{R}$..."`
      * **KORREKT (im JSON):** `"Die Formel lautet $d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$."`
      * **FALSCH (im JSON):** `"Die Formel lautet $d(x,y) = \sqrt{\sum...}$"` (Syntaxfehler oder falsche Darstellung)
      * **FALSCH (im JSON):** `"Die Formel lautet \`d(x,y) = ...\`"\` (Falsche Formatierung)

3.  **Formatierung:**

      * Wichtige Schl√ºsselw√∂rter im Text werden mit doppelten Sternchen (`**Wort**`) hervorgehoben.
      * Abgesetzte Formeln verwenden `$$...$$` (auch hier `\\` escapen).
      * Satzzeichen geh√∂ren **au√üerhalb** der KaTeX-Dollarzeichen.
          * **KORREKT:** `...sind disjunkt, wenn $M \\cap N = \\emptyset$ gilt.`
          * **FALSCH:** `...sind disjunkt, wenn $M \\cap N = \\emptyset.$`

-----

*(Beginne jetzt mit Schritt 1)*

---

## Hinweise zur Wiederherstellung & AI‚ÄëGenerator‚ÄëMetadaten

### Pseudonym‚ÄëWiederherstellung (optional)

Wenn Nutzer sp√§ter dasselbe Pseudonym wiederverwenden m√∂chten, kann beim Erstellen eines
Pseudonyms ein optionales Wiederherstellungs‚ÄëGeheimwort gesetzt werden. Technische Details und
Sicherheitshinweise:

- Das Geheimwort wird niemals im Klartext gespeichert. Es wird serverseitig mit PBKDF2‚ÄëHMAC‚ÄëSHA256
  (100.000 Iterationen, 16‚ÄëByte Salt) abgeleitet; nur Salt und Hash werden in der Datenbank
  abgelegt.
- Das System kann das Geheimwort nicht wiederherstellen. Wenn das Geheimwort verloren geht,
  kann ein Administrator das Feld zur√ºcksetzen, aber das urspr√ºngliche Geheimwort bleibt verloren.
- Empfehlung an Nutzer: Verwende eine kurze, merkbare Passphrase oder einen Passwortmanager.
  Vermeide einfache Einwort‚ÄëPassw√∂rter.
- Maximale Eingabel√§nge: empfohlen 8‚Äì32 Zeichen. Das UI bietet eine maskierte Eingabe beim
  Pseudonym‚ÄëErstellen und ein Wiederherstellungsfeld (‚ÄûPseudonym + Geheimwort‚Äú) beim Login.

Akzeptanzkriterien f√ºr die Implementierung:

- Nutzer k√∂nnen beim Erstellen eines Pseudonyms optional ein Geheimwort setzen.
- Nutzer k√∂nnen sp√§ter Pseudonym + Geheimwort eingeben und so ihr altes Pseudonym wiederverwenden.
- Die Datenbank speichert nur Salt und Hash; keine Klartext‚ÄëSecrets.

### AI‚ÄëFragenset‚ÄëGenerator: `meta.created` / `meta.modified`

Damit Fragensets konsistent Datumsangaben enthalten (wichtig f√ºr die Anzeige auf der Startseite),
sollte der AI‚ÄëGenerator die Felder `meta.created` und `meta.modified` setzen. Vorgeschlagene Regeln:

- Format: ISO‚ÄëDatum (z. B. `2025-10-24T12:00:00Z`) wird empfohlen; kompakte Formen wie `DD.MM.YY`
  werden ebenfalls akzeptiert und lokal geparst.
- Beispiel (empfohlen, ISO):

```json
{
  "meta": {
    "title": "Grundlagen des maschinellen Lernens",
    "created": "2025-10-24T12:00:00Z",
    "modified": "2025-10-24T12:00:00Z",
    "allowed_min": 30
  },
  "questions": [ /* ... */ ]
}
```

- Minimales Feld: `meta.created` ‚Äî wenn `meta.modified` fehlt, wird `meta.created` als √Ñnderungszeit verwendet.
- `allowed_min` in `meta` kann gesetzt werden, um die Standard‚ÄëTestdauer f√ºr dieses Set zu √ºberschreiben.

Die Frontend‚ÄëLogik priorisiert diese `meta`‚ÄëFelder gegen√ºber Dateisystem‚ÄëTimestamps beim Anzeigen
von Erstellungs‚Äë/√Ñnderungsdaten.

---
