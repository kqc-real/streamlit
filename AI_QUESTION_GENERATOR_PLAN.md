# ü§ñ AI-Fragenset-Generator: Planungsdokument

**Projekt:** Integration eines KI-gest√ºtzten Fragenset-Generators in die MC-Test Streamlit App  
**Datum:** 3. Oktober 2025  
**Version:** 1.0  
**Status:** Planung

---

## üìã Inhaltsverzeichnis

1. [Executive Summary](#executive-summary)
2. [User Stories & Akzeptanzkriterien](#user-stories--akzeptanzkriterien)
3. [Machbarkeitsanalyse](#machbarkeitsanalyse)
4. [Technische Architektur](#technische-architektur)
5. [Sicherheitskonzept](#sicherheitskonzept)
6. [Implementierungsplan](#implementierungsplan)
7. [Risiken & Mitigation](#risiken--mitigation)
8. [Testing-Strategie](#testing-strategie)
9. [Deployment-Strategie](#deployment-strategie)
10. [Wartung & Support](#wartung--support)

---

## üéØ Executive Summary

### Vision
Integration eines self-hosted DeepSeek R1 LLM-Servers zur automatisierten Generierung von Multiple-Choice-Fragensets direkt aus der Streamlit App heraus.

### Problemstellung
**Aktuell:** Manuelle Erstellung von Fragensets ist zeitaufw√§ndig und erfordert externes Copy & Paste mit externen LLM-Tools (ChatGPT, Claude).

**Ziel:** Automatisierte Generierung qualitativ hochwertiger Fragensets direkt im Admin-Panel mit self-hosted LLM (keine API-Kosten, voller Datenschutz).

### Kernvorteile
- ‚úÖ **Kosteneffizienz:** 0‚Ç¨ laufende Kosten (vs. 50-100‚Ç¨/Monat bei Cloud-LLMs)
- ‚úÖ **Datenschutz:** 100% on-premise, DSGVO-konform
- ‚úÖ **Kontrolle:** Unbegrenzte Generierungen, keine Rate Limits
- ‚úÖ **Integration:** Nahtloser Workflow ohne Tool-Wechsel
- ‚úÖ **Offline-F√§higkeit:** Funktioniert ohne externe API-Abh√§ngigkeiten

### Scope
- **In Scope:** Admin-only Feature, Integration mit bestehendem R1-Server, JSON-Validierung, UI im Admin-Panel
- **Out of Scope:** Public Access (Nutzer-facing), Fine-Tuning des LLM, Multi-Sprachen-Support (v1)

---

## üë• User Stories & Akzeptanzkriterien

### Epic 1: Fragenset-Generierung

#### User Story 1.1: Basis-Generierung
```gherkin
Als Administrator
m√∂chte ich ein neues Fragenset √ºber eine UI generieren
damit ich nicht manuell JSON-Dateien erstellen muss.
```

**Akzeptanzkriterien:**
- [ ] **AC1.1.1:** Admin-Panel enth√§lt neuen Tab "ü§ñ Fragenset-Generator"
- [ ] **AC1.1.2:** UI bietet Eingabefelder f√ºr:
  - Thema (Text, Pflichtfeld)
  - Anzahl Fragen (Zahl, 5-100, Default: 20)
  - Antwortoptionen (Dropdown: 4 oder 5)
  - Erweiterte Erkl√§rungen (Checkbox, Optional)
- [ ] **AC1.1.3:** "Generieren"-Button startet Prozess
- [ ] **AC1.1.4:** Loading-Spinner zeigt Fortschritt w√§hrend Generierung
- [ ] **AC1.1.5:** Generierte Fragen werden als JSON-Datei in `data/` gespeichert
- [ ] **AC1.1.6:** Erfolgsmeldung mit Dateiname wird angezeigt
- [ ] **AC1.1.7:** Preview der ersten 3 Fragen wird angezeigt

**Definition of Done:**
- Code implementiert und getestet
- Mindestens 3 erfolgreiche Testgenerierungen
- Dokumentation aktualisiert
- Keine kritischen Bugs

---

#### User Story 1.2: Server-Verbindung
```gherkin
Als Administrator
m√∂chte ich den Status meines R1-Servers sehen
damit ich wei√ü, ob die Generierung funktionieren wird.
```

**Akzeptanzkriterien:**
- [ ] **AC1.2.1:** "Server-Status" Expander zeigt konfigurierte URL
- [ ] **AC1.2.2:** "Verbindung testen" Button pr√ºft Erreichbarkeit
- [ ] **AC1.2.3:** Erfolgreiche Verbindung zeigt gr√ºnen Success-Status
- [ ] **AC1.2.4:** Fehlerhafte Verbindung zeigt roten Error mit Details
- [ ] **AC1.2.5:** Timeout nach 10 Sekunden mit klarer Fehlermeldung

**Definition of Done:**
- Health-Check implementiert
- Error-Handling f√ºr alle Fehlerf√§lle
- User-freundliche Fehlermeldungen

---

#### User Story 1.3: Qualit√§tssicherung
```gherkin
Als Administrator
m√∂chte ich generierte Fragen vor dem Speichern validieren
damit nur korrekte Fragen in der App landen.
```

**Akzeptanzkriterien:**
- [ ] **AC1.3.1:** Validierung pr√ºft Pflichtfelder (frage, optionen, loesung, etc.)
- [ ] **AC1.3.2:** Validierung pr√ºft Datentypen (loesung ist int, optionen ist array)
- [ ] **AC1.3.3:** Validierung pr√ºft Logik (loesung-Index innerhalb optionen-Range)
- [ ] **AC1.3.4:** Bei Validierungsfehlern: Liste aller Fehler anzeigen
- [ ] **AC1.3.5:** Bei Validierungsfehlern: Rohdaten trotzdem als Debug-Info zeigen
- [ ] **AC1.3.6:** Nur validierte Fragen k√∂nnen gespeichert werden

**Definition of Done:**
- Alle Validierungsregeln implementiert
- Unit-Tests f√ºr Validator-Funktion
- Edge-Cases getestet

---

### Epic 2: Konfiguration & Setup

#### User Story 2.1: Server-Konfiguration
```gherkin
Als Administrator
m√∂chte ich meinen R1-Server √ºber Environment Variables konfigurieren
damit ich sensible Daten nicht im Code habe.
```

**Akzeptanzkriterien:**
- [ ] **AC2.1.1:** `R1_SERVER_URL` in Streamlit Secrets konfigurierbar
- [ ] **AC2.1.2:** `R1_API_KEY` (optional) in Streamlit Secrets konfigurierbar
- [ ] **AC2.1.3:** Fallback auf Defaults wenn nicht konfiguriert
- [ ] **AC2.1.4:** Dokumentation in README.md
- [ ] **AC2.1.5:** Beispiel in `.env.example`

**Definition of Done:**
- Secrets-System implementiert
- Lokale und Cloud-Konfiguration dokumentiert
- Sicherheitshinweise in README

---

#### User Story 2.2: Error Recovery
```gherkin
Als Administrator
m√∂chte ich bei Fehlern klare Fehlermeldungen sehen
damit ich das Problem selbst beheben kann.
```

**Akzeptanzkriterien:**
- [ ] **AC2.2.1:** Timeout-Fehler: "Server antwortet nicht innerhalb von 5 Min"
- [ ] **AC2.2.2:** Connection-Fehler: "Server nicht erreichbar unter [URL]"
- [ ] **AC2.2.3:** Auth-Fehler: "API-Key ung√ºltig oder fehlend"
- [ ] **AC2.2.4:** JSON-Parse-Fehler: "LLM-Response konnte nicht geparst werden" + Rohdaten
- [ ] **AC2.2.5:** Validierungs-Fehler: Liste aller fehlenden/falschen Felder
- [ ] **AC2.2.6:** Alle Fehler werden geloggt f√ºr Debugging

**Definition of Done:**
- Error-Handling f√ºr alle API-Call-Schritte
- User-freundliche Fehlermeldungen
- Logging-System implementiert

---

### Epic 3: User Experience

#### User Story 3.1: Download-Option
```gherkin
Als Administrator
m√∂chte ich generierte Fragen als JSON herunterladen k√∂nnen
damit ich sie vor dem Commit in Git noch pr√ºfen kann.
```

**Akzeptanzkriterien:**
- [ ] **AC3.1.1:** Download-Button nach erfolgreicher Generierung
- [ ] **AC3.1.2:** Dateiname enth√§lt Thema: `questions_[Thema].json`
- [ ] **AC3.1.3:** JSON ist formatiert (indent=2)
- [ ] **AC3.1.4:** UTF-8 Encoding (ensure_ascii=False)

**Definition of Done:**
- Download-Funktion implementiert
- Korrekte Dateibenennung
- Encoding getestet

---

#### User Story 3.2: Batch-Generierung (Optional, v2)
```gherkin
Als Administrator
m√∂chte ich mehrere Fragensets hintereinander generieren
damit ich effizient arbeiten kann.
```

**Akzeptanzkriterien:**
- [ ] **AC3.2.1:** "Noch ein Set generieren" Button nach erfolgreicher Generierung
- [ ] **AC3.2.2:** Formular wird zur√ºckgesetzt, Defaults bleiben
- [ ] **AC3.2.3:** Historie der letzten 5 Generierungen wird angezeigt
- [ ] **AC3.2.4:** Jede Generierung hat eigenen Download-Button

**Definition of Done:**
- Session-State Management implementiert
- UI f√ºr Historie erstellt
- Keine Performance-Probleme bei mehreren Sets

---

## üîç Machbarkeitsanalyse

### Technische Machbarkeit: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

#### Voraussetzungen (bereits erf√ºllt)
- ‚úÖ Self-hosted DeepSeek R1 Server l√§uft
- ‚úÖ OpenAI-kompatible API verf√ºgbar
- ‚úÖ Streamlit App existiert mit Admin-Panel
- ‚úÖ JSON-basiertes Fragenset-Format definiert

#### Technische Komponenten

| Komponente | Status | Komplexit√§t | Risiko |
|------------|--------|-------------|--------|
| HTTP Requests (requests lib) | ‚úÖ Standard | Niedrig | Niedrig |
| JSON Parsing & Validierung | ‚úÖ Standard | Niedrig | Niedrig |
| Streamlit UI-Komponenten | ‚úÖ Vorhanden | Niedrig | Niedrig |
| Admin-Authentifizierung | ‚úÖ Vorhanden | - | - |
| File I/O (JSON speichern) | ‚úÖ Standard | Niedrig | Niedrig |
| Error Handling | üî® Zu implementieren | Mittel | Mittel |
| LLM Prompt Engineering | üî® Zu optimieren | Mittel | Mittel |

**Fazit:** Alle technischen Voraussetzungen erf√ºllt, keine Blocker.

---

### Wirtschaftliche Machbarkeit: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

#### Kostenvergleich (pro Monat, 500 Generierungen)

| L√∂sung | Setup-Kosten | Laufende Kosten | Total (Jahr 1) |
|--------|--------------|-----------------|----------------|
| **Self-Hosted R1** | 0‚Ç¨ (HW vorhanden) | ~15‚Ç¨ (Strom) | **180‚Ç¨** ‚úÖ |
| OpenAI GPT-4 | 0‚Ç¨ | ~100‚Ç¨ (API) | **1.200‚Ç¨** |
| Anthropic Claude | 0‚Ç¨ | ~80‚Ç¨ (API) | **960‚Ç¨** |
| Google Gemini | 0‚Ç¨ | ~60‚Ç¨ (API) | **720‚Ç¨** |

**ROI:** Self-Hosted ist **5-7x g√ºnstiger** als Cloud-LLMs.

#### Zus√§tzliche Vorteile
- ‚úÖ Keine unvorhersehbaren API-Kosten bei steigender Nutzung
- ‚úÖ Keine Rate Limits oder Quotas
- ‚úÖ Keine Vendor Lock-in

**Fazit:** Self-Hosted bietet maximale Kosteneffizienz.

---

### Organisatorische Machbarkeit: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)

#### Ressourcen

| Ressource | Verf√ºgbar | Aufwand | Status |
|-----------|-----------|---------|--------|
| **Entwicklung** | Ja (Du/Team) | 8-12h | ‚úÖ |
| **R1-Server** | Ja (l√§uft) | 0h | ‚úÖ |
| **Testing** | Ja | 4-6h | ‚úÖ |
| **Dokumentation** | Ja | 2-3h | ‚úÖ |
| **Deployment** | Ja (Streamlit Cloud) | 1-2h | ‚úÖ |

**Total Aufwand:** 15-23 Stunden f√ºr vollst√§ndige Implementation.

#### Abh√§ngigkeiten
- ‚ö†Ô∏è R1-Server muss stabil laufen (99% Uptime erforderlich)
- ‚ö†Ô∏è Netzwerk-Konnektivit√§t zwischen Streamlit Cloud und R1-Server
- ‚úÖ Keine externen Abh√§ngigkeiten (APIs, Services)

**Fazit:** Umsetzbar mit vorhandenen Ressourcen.

---

### Datenschutz & Compliance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)

#### DSGVO-Konformit√§t

| Aspekt | Cloud-LLM | Self-Hosted R1 | Bewertung |
|--------|-----------|----------------|-----------|
| **Datenverarbeitung** | USA/extern | Deutschland/on-premise | ‚úÖ Vorteil |
| **Drittanbieter** | OpenAI/Anthropic | Keine | ‚úÖ Vorteil |
| **Datenweitergabe** | Ja (Terms of Service) | Nein | ‚úÖ Vorteil |
| **Logs & Tracking** | Beim Anbieter | Lokal kontrolliert | ‚úÖ Vorteil |
| **Recht auf Vergessen** | Unklar | Vollst√§ndig kontrolliert | ‚úÖ Vorteil |

**Fazit:** Self-Hosted ist DSGVO-optimal, keine rechtlichen Bedenken.

---

### Qualitative Machbarkeit: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5)

#### LLM-Qualit√§t (DeepSeek R1 vs. Cloud)

| Kriterium | GPT-4 | Claude 3.5 | DeepSeek R1-7B | DeepSeek R1-70B |
|-----------|-------|------------|----------------|-----------------|
| **Fachliche Korrektheit** | 9/10 | 8.5/10 | 7/10 | 8/10 |
| **Distraktor-Qualit√§t** | 9/10 | 8/10 | 6.5/10 | 7.5/10 |
| **LaTeX-Syntax** | 8/10 | 8/10 | 7/10 | 8/10 |
| **JSON-Format** | 9/10 | 9/10 | 7/10 | 8/10 |
| **Konsistenz** | 9/10 | 8.5/10 | 6.5/10 | 7.5/10 |

**Empfehlung:** 
- ‚úÖ **R1-7B:** Ausreichend f√ºr Standard-Fragen (80% Use-Cases)
- ‚úÖ **R1-70B:** Empfohlen f√ºr komplexe Themen (Mathematik, Physik)
- ‚ö†Ô∏è **Manuelle Review:** Immer empfohlen, unabh√§ngig vom Modell

**Mitigation:** Validierung + Preview vor Speicherung im UI implementieren.

---

### Risikobewertung: ‚≠ê‚≠ê‚≠ê‚ö†Ô∏è‚ö†Ô∏è (3/5)

#### Identifizierte Risiken

| Risiko | Wahrscheinlichkeit | Impact | Mitigation |
|--------|-------------------|--------|------------|
| **R1-Server Ausfall** | Mittel | Hoch | Health-Check + User-freundliche Fehlermeldung |
| **Schlechte Fragenqualit√§t** | Mittel | Mittel | Validierung + Preview + Manuelle Review |
| **Netzwerk-Latenz** | Niedrig | Niedrig | Timeout-Handling + Progress-Indicator |
| **Security-Breach** | Niedrig | Hoch | API-Key Auth + Rate Limiting + HTTPS |
| **JSON-Parse-Fehler** | Mittel | Niedrig | Robustes Parsing + Fallback |

**Kritische Risiken:** Keine (alle mitigierbar).

---

### Gesamtbewertung

| Dimension | Score | Gewichtung | Gewichtet |
|-----------|-------|------------|-----------|
| Technische Machbarkeit | 5/5 | 30% | 1.5 |
| Wirtschaftliche Machbarkeit | 5/5 | 25% | 1.25 |
| Organisatorische Machbarkeit | 4/5 | 15% | 0.6 |
| Datenschutz & Compliance | 5/5 | 15% | 0.75 |
| Qualitative Machbarkeit | 4/5 | 10% | 0.4 |
| Risikobewertung | 3/5 | 5% | 0.15 |
| **TOTAL** | **4.65/5** | **100%** | **4.65** |

**Empfehlung: ‚úÖ GO - Projekt ist hochgradig machbar mit exzellentem Kosten-Nutzen-Verh√§ltnis.**

---

## üèóÔ∏è Technische Architektur

### System-√úbersicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Streamlit Cloud                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ            Streamlit App (Port 8501)              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ         app.py (Main Entry)                  ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Routing                                   ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Session Management                        ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ      admin_panel.py (Admin UI)               ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Authentication                            ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Analytics                                 ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - üÜï Question Generator Tab                 ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  üÜï chatbot.py (LLM Integration)             ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - generate_questions_from_r1()              ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - validate_questions()                      ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - save_questionset()                        ‚îÇ ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTPS Request
                         ‚îÇ (OpenAI-kompatible API)
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Dein R1-Server (On-Premise)                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ       Ollama / llama.cpp / vLLM                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       DeepSeek R1-7B / R1-70B                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Endpoints:                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - POST /v1/chat/completions                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - GET  /health                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - GET  /v1/models                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Sicherheit:                                             ‚îÇ
‚îÇ  - API-Key Authentication (Bearer Token)                ‚îÇ
‚îÇ  - Rate Limiting (10 req/min)                           ‚îÇ
‚îÇ  - HTTPS (Let's Encrypt / Cloudflare)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Komponenten-Architektur

#### 1. Frontend (Streamlit UI)

**Datei:** `admin_panel.py` (Erweiterung)

```python
# Neuer Tab im Admin-Panel
def show_admin_panel():
    tabs = st.tabs([
        "üìä Dashboard", 
        "üìã Feedback", 
        "‚öôÔ∏è Einstellungen",
        "ü§ñ Fragenset-Generator"  # ‚Üê NEU
    ])
    
    with tabs[3]:
        show_question_generator()  # Import aus chatbot.py
```

**UI-Komponenten:**
- Input-Formular (Thema, Anzahl, Optionen, etc.)
- Server-Status-Check (Expander mit Test-Button)
- Generieren-Button mit Loading-Spinner
- Validierungs-Feedback (Success/Error Messages)
- Preview-Section (erste 3 Fragen)
- Download-Button (JSON)

---

#### 2. Backend (LLM Integration)

**Datei:** `chatbot.py` (NEU)

**Hauptfunktionen:**

```python
def generate_questions_from_r1(
    thema: str,
    anzahl: int,
    optionen: int,
    extended: bool = False
) -> Optional[List[Dict]]:
    """
    Generiert Fragen √ºber R1-Server API.
    
    Returns:
        List[Dict]: Validierte Fragen oder None bei Fehler
    """
    # 1. Prompt Construction
    # 2. API Call (POST /v1/chat/completions)
    # 3. Response Parsing
    # 4. JSON Extraction
    # 5. Return
```

```python
def validate_questions(questions: List[Dict]) -> Tuple[bool, List[str]]:
    """
    Validiert generierte Fragen.
    
    Pr√ºfungen:
    - Pflichtfelder vorhanden
    - Datentypen korrekt
    - loesung-Index innerhalb optionen
    - gewichtung in [1, 2, 3]
    
    Returns:
        (is_valid, error_list)
    """
```

```python
def save_questionset(
    questions: List[Dict],
    thema: str
) -> str:
    """
    Speichert Fragenset als JSON-Datei.
    
    Returns:
        str: Dateipfad der gespeicherten Datei
    """
```

---

#### 3. API-Client

**Bibliothek:** `requests` (Standard)

**Konfiguration:**

```python
# Aus Streamlit Secrets laden
R1_SERVER_URL = st.secrets.get("R1_SERVER_URL")
R1_API_KEY = st.secrets.get("R1_API_KEY", "")  # Optional

# Request-Format (OpenAI-kompatibel)
{
    "model": "deepseek-r1",
    "messages": [
        {"role": "system", "content": "..."},
        {"role": "user", "content": "..."}
    ],
    "temperature": 0.7,
    "max_tokens": 8000
}
```

**Timeouts:**
- Connection: 30 Sekunden
- Read: 300 Sekunden (5 Minuten f√ºr lange Generierungen)

---

### Datenfluss

```
1. Admin √∂ffnet "Fragenset-Generator" Tab
   ‚Üì
2. Admin f√ºllt Formular aus (Thema, Anzahl, etc.)
   ‚Üì
3. Admin klickt "Generieren"
   ‚Üì
4. chatbot.py: Prompt wird konstruiert
   ‚Üì
5. chatbot.py: POST Request an R1-Server
   ‚Üì
6. R1-Server: Generiert Fragen (30-120 Sek)
   ‚Üì
7. R1-Server: Response mit JSON
   ‚Üì
8. chatbot.py: JSON Parsing & Validierung
   ‚Üì
9a. Validierung OK:
    - Fragen speichern in data/questions_[Thema].json
    - Success Message anzeigen
    - Preview anzeigen
    - Download-Button anbieten
   ‚Üì
9b. Validierung FEHLER:
    - Error Messages anzeigen
    - Rohdaten als Debug-Info anzeigen
    - Keine Speicherung
```

---

### Sicherheitsarchitektur

#### Layer 1: Netzwerk-Security

```
Internet
    ‚Üì
Cloudflare Tunnel / DynDNS + Firewall
    ‚Üì (nur HTTPS Port 443)
Reverse Proxy (nginx/caddy)
    ‚Üì (API-Key Check)
R1-Server (localhost:8080)
```

#### Layer 2: Application-Security

```python
# R1-Server: API-Key Middleware
def verify_api_key(request):
    auth_header = request.headers.get("Authorization")
    if not auth_header or not auth_header.startswith("Bearer "):
        return 401, "Unauthorized"
    
    token = auth_header.replace("Bearer ", "")
    if token != os.getenv("R1_API_KEY"):
        return 403, "Forbidden"
    
    return None  # OK
```

#### Layer 3: Rate Limiting

```python
# Max. 10 Requests pro Minute pro IP
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")
def chat_completions(request):
    ...
```

---

## üîê Sicherheitskonzept

### Threat Model

#### Potenzielle Bedrohungen

| Bedrohung | Wahrscheinlichkeit | Impact | Priorit√§t |
|-----------|-------------------|--------|-----------|
| **Unbefugter API-Zugriff** | Mittel | Hoch | üî¥ Kritisch |
| **DDoS auf R1-Server** | Niedrig | Hoch | üü° Mittel |
| **Prompt Injection** | Niedrig | Niedrig | üü¢ Niedrig |
| **Data Leakage** | Niedrig | Mittel | üü° Mittel |
| **Man-in-the-Middle** | Niedrig | Hoch | üü° Mittel |

---

### Sicherheitsma√ünahmen

#### 1. Authentication & Authorization

**Streamlit App ‚Üí R1-Server:**

```python
# chatbot.py
headers = {
    "Authorization": f"Bearer {st.secrets['R1_API_KEY']}",
    "Content-Type": "application/json"
}
```

**R1-Server:**

```python
# Umgebungsvariable (niemals im Code!)
R1_API_KEY=<strong_random_key_256bit>

# Beispiel: openssl rand -base64 32
# Output: xK9mP2nQ8rT4vW6yZ1aB3cD5eF7gH9jL
```

#### 2. Network Security

**Option A: Cloudflare Tunnel (Empfohlen)**

```bash
# Auf R1-Server installieren
curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
sudo dpkg -i cloudflared.deb

# Tunnel starten
cloudflared tunnel --url http://localhost:8080

# Ausgabe:
# Your quick Tunnel has been created! Visit it at:
# https://random-word-1234.trycloudflare.com
```

**Vorteile:**
- ‚úÖ Automatisches HTTPS (TLS 1.3)
- ‚úÖ DDoS-Schutz durch Cloudflare
- ‚úÖ Keine Ports √∂ffnen (outbound only)
- ‚úÖ Kostenlos

**Option B: Reverse Proxy + Let's Encrypt**

```nginx
# /etc/nginx/sites-available/r1-server
server {
    listen 443 ssl http2;
    server_name r1.your-domain.com;

    ssl_certificate /etc/letsencrypt/live/r1.your-domain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/r1.your-domain.com/privkey.pem;
    
    # Security Headers
    add_header Strict-Transport-Security "max-age=31536000" always;
    add_header X-Frame-Options "DENY" always;
    add_header X-Content-Type-Options "nosniff" always;
    
    # Rate Limiting
    limit_req zone=api_limit burst=5 nodelay;
    
    location / {
        proxy_pass http://localhost:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}

# Rate Limit Definition
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/m;
```

#### 3. Input Validation & Sanitization

**Streamlit App:**

```python
def validate_user_input(thema: str, anzahl: int) -> Tuple[bool, str]:
    """Validiert User-Input vor API-Call"""
    
    # Thema: Keine SQL-Injection m√∂glich (kein DB-Query)
    # Aber: Filename-Safe machen
    if not thema or len(thema) > 100:
        return False, "Thema muss 1-100 Zeichen lang sein"
    
    if not re.match(r'^[a-zA-Z0-9\s√§√∂√º√Ñ√ñ√ú√ü_-]+$', thema):
        return False, "Thema enth√§lt ung√ºltige Zeichen"
    
    # Anzahl: Limits setzen
    if not 5 <= anzahl <= 100:
        return False, "Anzahl muss zwischen 5 und 100 liegen"
    
    return True, ""
```

**Prompt Injection Prevention:**

```python
# Escape user input in prompt
import json

def build_prompt(thema: str, anzahl: int) -> str:
    # JSON-escape f√ºr sichere Interpolation
    thema_safe = json.dumps(thema)[1:-1]  # Entfernt Quotes
    
    prompt = f"""Du bist ein Experte f√ºr MC-Fragen.
    
Erstelle {anzahl} Fragen zum Thema: {thema_safe}

WICHTIG: Antworte NUR mit validem JSON, keine Erkl√§rungen.
"""
    return prompt
```

#### 4. Rate Limiting (Defense in Depth)

**Layer 1: Streamlit App (Client-Side)**

```python
# Session-basiertes Limit
if "last_generation_time" in st.session_state:
    elapsed = time.time() - st.session_state.last_generation_time
    if elapsed < 60:  # Min. 1 Minute zwischen Generierungen
        st.error(f"‚è±Ô∏è Bitte warte noch {60-elapsed:.0f} Sekunden")
        return

st.session_state.last_generation_time = time.time()
```

**Layer 2: R1-Server (Server-Side)**

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/v1/chat/completions")
@limiter.limit("10/minute")  # Max 10 Requests pro Minute
def chat_completions():
    ...
```

#### 5. Logging & Monitoring

```python
import logging
from datetime import datetime

# Konfiguration
logging.basicConfig(
    filename='/var/log/r1-server/access.log',
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)

# Log jeder API-Call
@app.post("/v1/chat/completions")
def chat_completions(request: Request):
    client_ip = request.client.host
    auth_header = request.headers.get("Authorization", "None")
    
    logging.info(f"API Call | IP: {client_ip} | Auth: {auth_header[:20]}...")
    
    # ... Rest der Logik
```

**Was loggen:**
- ‚úÖ Timestamp
- ‚úÖ Client-IP (f√ºr Abuse-Detection)
- ‚úÖ API-Key Hash (erste 20 Chars)
- ‚úÖ Request-Dauer
- ‚úÖ Response-Status
- ‚ùå Nicht loggen: Themen/Fragen (Privacy!)

#### 6. Secrets Management

**Lokal (.env):**

```bash
# .env (NEVER commit to Git!)
R1_SERVER_URL=https://your-tunnel.trycloudflare.com
R1_API_KEY=xK9mP2nQ8rT4vW6yZ1aB3cD5eF7gH9jL

# .gitignore
.env
.streamlit/secrets.toml
```

**Streamlit Cloud:**

```toml
# Dashboard ‚Üí Settings ‚Üí Secrets
R1_SERVER_URL = "https://your-tunnel.trycloudflare.com"
R1_API_KEY = "xK9mP2nQ8rT4vW6yZ1aB3cD5eF7gH9jL"
```

**Best Practices:**
- ‚úÖ Nutze environment-spezifische Keys (dev vs. prod)
- ‚úÖ Rotiere Keys regelm√§√üig (alle 90 Tage)
- ‚úÖ Nutze starke Keys (256-bit, cryptographically random)
- ‚ùå Niemals Keys im Code hardcoden
- ‚ùå Niemals Keys in Git committen

---

## üìÖ Implementierungsplan

### Phase 1: Foundation (Tag 1-2, 8-10h)

#### Sprint 1.1: R1-Server Setup & Security (4h)

**Ziele:**
- R1-Server ist √∂ffentlich erreichbar
- API-Key Authentication funktioniert
- HTTPS ist aktiviert

**Tasks:**
- [ ] **1.1.1** Cloudflare Tunnel einrichten (oder DynDNS + nginx)
  - Installation
  - Konfiguration
  - Test: curl von extern
- [ ] **1.1.2** API-Key generieren
  ```bash
  openssl rand -base64 32
  ```
- [ ] **1.1.3** API-Key Authentication im R1-Server implementieren
  - Middleware schreiben
  - Test: Mit/ohne Key
- [ ] **1.1.4** Rate Limiting aktivieren (10 req/min)
- [ ] **1.1.5** Health-Check Endpoint implementieren
  ```python
  @app.get("/health")
  def health():
      return {"status": "ok", "model": "deepseek-r1-7b"}
  ```

**Acceptance Criteria:**
- ‚úÖ `curl https://your-url/health` gibt 200 zur√ºck
- ‚úÖ Request ohne API-Key gibt 401 zur√ºck
- ‚úÖ Request mit falschem Key gibt 403 zur√ºck
- ‚úÖ 11. Request innerhalb 1 Min gibt 429 zur√ºck

---

#### Sprint 1.2: Streamlit Integration Basics (4-6h)

**Ziele:**
- `chatbot.py` existiert mit Basis-Funktionalit√§t
- Secrets sind konfiguriert
- Erster erfolgreicher API-Call

**Tasks:**
- [ ] **1.2.1** `chatbot.py` erstellen
  - `generate_questions_from_r1()` implementieren
  - Request-Handling
  - Error-Handling (Connection, Timeout, Auth)
- [ ] **1.2.2** Secrets konfigurieren
  - `.streamlit/secrets.toml` erstellen (lokal)
  - R1_SERVER_URL und R1_API_KEY eintragen
- [ ] **1.2.3** Test-Script schreiben
  ```python
  # test_chatbot.py
  if __name__ == "__main__":
      questions = generate_questions_from_r1(
          thema="Testthema",
          anzahl=5,
          optionen=4
      )
      print(json.dumps(questions, indent=2))
  ```
- [ ] **1.2.4** Ersten erfolgreichen API-Call durchf√ºhren
- [ ] **1.2.5** Response Parsing & JSON Extraction testen

**Acceptance Criteria:**
- ‚úÖ `python test_chatbot.py` generiert 5 Fragen
- ‚úÖ JSON-Format ist korrekt
- ‚úÖ Keine Exceptions bei normalem Betrieb
- ‚úÖ Timeout nach 5 Min wird abgefangen

---

### Phase 2: UI & Validation (Tag 3-4, 6-8h)

#### Sprint 2.1: Admin-Panel UI (3-4h)

**Ziele:**
- Neuer Tab im Admin-Panel
- Vollst√§ndiges Input-Formular
- Server-Status Check

**Tasks:**
- [ ] **2.1.1** `admin_panel.py` erweitern
  - Neuer Tab "ü§ñ Fragenset-Generator" hinzuf√ºgen
- [ ] **2.1.2** `show_question_generator()` Funktion implementieren
  - Server-Status Expander
  - Verbindung-Test Button
  - Input-Formular (Thema, Anzahl, Optionen, Extended)
- [ ] **2.1.3** UI-States implementieren
  - Loading-Spinner w√§hrend Generierung
  - Success/Error Messages
  - Preview-Section
- [ ] **2.1.4** Download-Button f√ºr JSON

**Acceptance Criteria:**
- ‚úÖ Tab erscheint nur f√ºr Admin-User
- ‚úÖ Server-Status zeigt korrekte URL
- ‚úÖ Test-Button funktioniert (gr√ºn/rot)
- ‚úÖ Formular-Validierung (Pflichtfelder, Ranges)

---

#### Sprint 2.2: Validierung & Speicherung (3-4h)

**Ziele:**
- Robuste Validierung aller Felder
- Sichere Speicherung in `data/`
- Fehlerbehandlung

**Tasks:**
- [ ] **2.2.1** `validate_questions()` implementieren
  - Pflichtfelder-Check
  - Datentyp-Check
  - Logik-Check (loesung-Index, gewichtung)
- [ ] **2.2.2** `save_questionset()` implementieren
  - Filename-Sanitization
  - UTF-8 Encoding
  - Atomares Schreiben (temp file + rename)
- [ ] **2.2.3** Error-Messages f√ºr alle Validierungs-F√§lle
- [ ] **2.2.4** Unit-Tests f√ºr Validator schreiben
  ```python
  # tests/test_validator.py
  def test_validate_questions_missing_field():
      questions = [{"frage": "Test"}]  # optionen fehlt
      valid, errors = validate_questions(questions)
      assert not valid
      assert "optionen" in errors[0]
  ```

**Acceptance Criteria:**
- ‚úÖ Alle 10 Validierungs-Test-Cases bestehen
- ‚úÖ Invalide Fragen werden nicht gespeichert
- ‚úÖ Fehlermeldungen sind user-freundlich
- ‚úÖ Rohdaten werden bei Fehler angezeigt

---

### Phase 3: Polish & Testing (Tag 5-6, 4-6h)

#### Sprint 3.1: User Experience (2-3h)

**Ziele:**
- Optimierte UX
- Hilfstexte & Tooltips
- Performance-Optimierungen

**Tasks:**
- [ ] **3.1.1** Hilfstexte hinzuf√ºgen
  - Tooltip: "Was sind erweiterte Erkl√§rungen?"
  - Beispiele f√ºr gute Themen
- [ ] **3.1.2** Progress-Indicator verbessern
  - Gesch√§tzte Dauer anzeigen ("~60-120 Sek")
  - Aktueller Schritt ("Generiere Fragen...")
- [ ] **3.1.3** Preview optimieren
  - Syntax-Highlighting f√ºr JSON
  - Expandable f√ºr alle Fragen (nicht nur 3)
- [ ] **3.1.4** Input-Validation verbessern
  - Real-time Feedback (rote Umrandung bei Fehler)
  - Character-Counter f√ºr Thema-Feld

**Acceptance Criteria:**
- ‚úÖ Kein User-Feedback nach 3 Test-Generierungen
- ‚úÖ Alle UI-Elemente sind self-explanatory
- ‚úÖ Performance: UI bleibt responsive w√§hrend Generierung

---

#### Sprint 3.2: Testing & Bug Fixing (2-3h)

**Ziele:**
- Alle kritischen Pfade getestet
- Edge-Cases abgedeckt
- Keine bekannten Bugs

**Tasks:**
- [ ] **3.2.1** End-to-End Tests
  - Happy-Path: Thema ‚Üí Generieren ‚Üí Speichern ‚Üí Test starten
  - Error-Path: Server offline ‚Üí User-freundliche Meldung
  - Edge-Case: 100 Fragen (l√§ngste Generierung)
- [ ] **3.2.2** Integration-Tests
  ```python
  def test_full_generation_flow():
      # 1. Mock R1-Server Response
      # 2. Generate Questions
      # 3. Validate
      # 4. Save
      # 5. Load from file
      # 6. Assert correctness
  ```
- [ ] **3.2.3** Stress-Test
  - 10 Generierungen hintereinander
  - Concurrent Requests (falls mehrere Admins)
- [ ] **3.2.4** Security-Test
  - Prompt Injection versuchen
  - API-Key Brute-Force (sollte Rate-Limited sein)
  - HTTPS-Verifizierung

**Acceptance Criteria:**
- ‚úÖ Alle automatisierten Tests bestehen (>95% Coverage)
- ‚úÖ Keine Crashes bei 10 aufeinanderfolgenden Generierungen
- ‚úÖ Security-Audit bestanden

---

### Phase 4: Deployment & Documentation (Tag 7, 2-4h)

#### Sprint 4.1: Deployment (1-2h)

**Ziele:**
- App l√§uft auf Streamlit Cloud
- Secrets sind konfiguriert
- Production-Test erfolgreich

**Tasks:**
- [ ] **4.1.1** `requirements.txt` aktualisieren
  ```txt
  requests>=2.31.0  # NEU
  ```
- [ ] **4.1.2** Streamlit Cloud Secrets konfigurieren
  - R1_SERVER_URL eintragen
  - R1_API_KEY eintragen
- [ ] **4.1.3** Git Push & Deploy
  ```bash
  git add .
  git commit -m "feat: AI Question Generator"
  git push origin main
  ```
- [ ] **4.1.4** Production-Test
  - Als Admin einloggen
  - Generator-Tab √∂ffnen
  - Testgenerierung (5 Fragen)
  - Download testen

**Acceptance Criteria:**
- ‚úÖ App deployed ohne Fehler
- ‚úÖ Testgenerierung in Production erfolgreich
- ‚úÖ Generierte Fragen sind qualitativ gut

---

#### Sprint 4.2: Documentation (1-2h)

**Ziele:**
- README aktualisiert
- Internes Runbook erstellt
- User-Guide geschrieben

**Tasks:**
- [ ] **4.2.1** README.md aktualisieren
  - Neues Feature beschreiben
  - R1_SERVER_URL in Secrets-Section
  - Screenshot vom Generator-Tab
- [ ] **4.2.2** Runbook erstellen (`RUNBOOK_AI_GENERATOR.md`)
  - Setup-Anleitung f√ºr R1-Server
  - Troubleshooting-Guide
  - Monitoring & Alerts
- [ ] **4.2.3** User-Guide f√ºr Admins
  - "Wie generiere ich gute Fragen?"
  - Best Practices f√ºr Themen-Wahl
  - Qualit√§tssicherung-Prozess

**Acceptance Criteria:**
- ‚úÖ README enth√§lt alle neuen Konfig-Optionen
- ‚úÖ Runbook ist vollst√§ndig und getestet
- ‚úÖ User-Guide ist verst√§ndlich f√ºr Nicht-Techniker

---

### Zeitplan & Meilensteine

```mermaid
gantt
    title AI Question Generator - Implementierung
    dateFormat YYYY-MM-DD
    
    section Phase 1: Foundation
    R1-Server Setup         :done, p1s1, 2025-10-03, 4h
    Streamlit Integration   :done, p1s2, 2025-10-03, 6h
    
    section Phase 2: UI & Validation
    Admin-Panel UI          :active, p2s1, 2025-10-04, 4h
    Validierung             :p2s2, after p2s1, 4h
    
    section Phase 3: Polish
    User Experience         :p3s1, 2025-10-05, 3h
    Testing                 :p3s2, after p3s1, 3h
    
    section Phase 4: Deploy
    Deployment              :p4s1, 2025-10-06, 2h
    Documentation           :p4s2, after p4s1, 2h
```

**Total:** 28 Stunden (3.5 Personentage bei 8h/Tag)

**Kritischer Pfad:**
1. R1-Server Security ‚Üê Blocker f√ºr alles
2. API Integration ‚Üê Blocker f√ºr UI
3. Validierung ‚Üê Blocker f√ºr Deployment

---

## üß™ Testing-Strategie

### Test-Pyramide

```
                    ‚ñ≤
                   ‚ï± ‚ï≤
                  ‚ï±   ‚ï≤
                 ‚ï± E2E ‚ï≤              1-2 Tests
                ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤             (Manual + Automated)
               ‚ï±         ‚ï≤
              ‚ï±Integration‚ï≤           5-10 Tests
             ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤          (API Calls, File I/O)
            ‚ï±               ‚ï≤
           ‚ï±   Unit Tests    ‚ï≤        20-30 Tests
          ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤       (Validation, Parsing)
         ‚ñº
```

---

### Unit Tests (20-30 Tests)

#### Test Suite 1: Validierung

```python
# tests/test_validation.py
import pytest
from chatbot import validate_questions

class TestQuestionValidation:
    
    def test_valid_question(self):
        """Valide Frage sollte bestehen"""
        questions = [{
            "frage": "1. Was ist Python?",
            "optionen": ["A", "B", "C", "D"],
            "loesung": 0,
            "erklaerung": "...",
            "gewichtung": 1,
            "thema": "Programmierung"
        }]
        valid, errors = validate_questions(questions)
        assert valid
        assert len(errors) == 0
    
    def test_missing_field(self):
        """Fehlende Pflichtfelder sollten erkannt werden"""
        questions = [{"frage": "Test"}]  # Rest fehlt
        valid, errors = validate_questions(questions)
        assert not valid
        assert len(errors) > 0
        assert any("optionen" in e for e in errors)
    
    def test_wrong_datatype(self):
        """Falsche Datentypen sollten erkannt werden"""
        questions = [{
            "frage": "Test",
            "optionen": "Not a list",  # Sollte Array sein
            "loesung": 0,
            "erklaerung": "...",
            "gewichtung": 1,
            "thema": "Test"
        }]
        valid, errors = validate_questions(questions)
        assert not valid
    
    def test_loesung_out_of_range(self):
        """loesung-Index au√üerhalb von optionen sollte fehlschlagen"""
        questions = [{
            "frage": "Test",
            "optionen": ["A", "B"],
            "loesung": 5,  # Index 5 existiert nicht
            "erklaerung": "...",
            "gewichtung": 1,
            "thema": "Test"
        }]
        valid, errors = validate_questions(questions)
        assert not valid
    
    def test_invalid_gewichtung(self):
        """gewichtung au√üerhalb 1-3 sollte fehlschlagen"""
        questions = [{
            "frage": "Test",
            "optionen": ["A", "B"],
            "loesung": 0,
            "erklaerung": "...",
            "gewichtung": 5,  # Nur 1, 2, 3 erlaubt
            "thema": "Test"
        }]
        valid, errors = validate_questions(questions)
        assert not valid
    
    @pytest.mark.parametrize("anzahl", [5, 20, 50, 100])
    def test_multiple_questions(self, anzahl):
        """Validierung mit verschiedenen Anzahlen"""
        questions = [
            {
                "frage": f"{i}. Test",
                "optionen": ["A", "B", "C", "D"],
                "loesung": 0,
                "erklaerung": "...",
                "gewichtung": 1,
                "thema": "Test"
            }
            for i in range(anzahl)
        ]
        valid, errors = validate_questions(questions)
        assert valid
```

#### Test Suite 2: JSON Parsing

```python
# tests/test_parsing.py
from chatbot import extract_json_from_response

class TestJSONParsing:
    
    def test_plain_json(self):
        """Plain JSON sollte geparst werden"""
        response = '[{"frage": "Test"}]'
        result = extract_json_from_response(response)
        assert isinstance(result, list)
        assert result[0]["frage"] == "Test"
    
    def test_json_in_code_block(self):
        """JSON in ```json``` sollte extrahiert werden"""
        response = '''
        Hier sind die Fragen:
        ```json
        [{"frage": "Test"}]
        ```
        '''
        result = extract_json_from_response(response)
        assert isinstance(result, list)
    
    def test_json_with_markdown(self):
        """JSON mit Markdown sollte bereinigt werden"""
        response = '''
        # Fragenset
        
        ```json
        [{"frage": "Test"}]
        ```
        
        Viel Erfolg!
        '''
        result = extract_json_from_response(response)
        assert isinstance(result, list)
    
    def test_invalid_json(self):
        """Invalides JSON sollte None zur√ºckgeben"""
        response = "This is not JSON at all"
        result = extract_json_from_response(response)
        assert result is None
```

#### Test Suite 3: Input Validation

```python
# tests/test_input_validation.py
from chatbot import validate_user_input

class TestInputValidation:
    
    def test_valid_input(self):
        """Valider Input sollte bestehen"""
        valid, msg = validate_user_input("Mathematik", 20)
        assert valid
    
    def test_thema_too_long(self):
        """Zu langes Thema sollte abgelehnt werden"""
        valid, msg = validate_user_input("A" * 101, 20)
        assert not valid
        assert "100 Zeichen" in msg
    
    def test_thema_invalid_chars(self):
        """Ung√ºltige Zeichen sollten abgelehnt werden"""
        valid, msg = validate_user_input("Test<script>", 20)
        assert not valid
    
    def test_anzahl_too_low(self):
        """Anzahl < 5 sollte abgelehnt werden"""
        valid, msg = validate_user_input("Test", 2)
        assert not valid
    
    def test_anzahl_too_high(self):
        """Anzahl > 100 sollte abgelehnt werden"""
        valid, msg = validate_user_input("Test", 150)
        assert not valid
```

---

### Integration Tests (5-10 Tests)

```python
# tests/test_integration.py
import pytest
from unittest.mock import Mock, patch
from chatbot import generate_questions_from_r1

class TestR1Integration:
    
    @patch('requests.post')
    def test_successful_generation(self, mock_post):
        """Erfolgreiche Generierung sollte Fragen zur√ºckgeben"""
        # Mock R1-Server Response
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = {
            "choices": [{
                "message": {
                    "content": '[{"frage": "Test", "optionen": ["A"], "loesung": 0, "erklaerung": "...", "gewichtung": 1, "thema": "Test"}]'
                }
            }]
        }
        
        questions = generate_questions_from_r1("Test", 1, 4)
        assert questions is not None
        assert len(questions) == 1
    
    @patch('requests.post')
    def test_connection_error(self, mock_post):
        """Connection Error sollte None zur√ºckgeben"""
        mock_post.side_effect = requests.exceptions.ConnectionError()
        
        questions = generate_questions_from_r1("Test", 1, 4)
        assert questions is None
    
    @patch('requests.post')
    def test_timeout(self, mock_post):
        """Timeout sollte None zur√ºckgeben"""
        mock_post.side_effect = requests.exceptions.Timeout()
        
        questions = generate_questions_from_r1("Test", 1, 4)
        assert questions is None
    
    @patch('requests.post')
    def test_invalid_api_key(self, mock_post):
        """Falscher API-Key sollte 403 geben"""
        mock_post.return_value.status_code = 403
        
        questions = generate_questions_from_r1("Test", 1, 4)
        assert questions is None
```

---

### End-to-End Tests (1-2 Tests)

```python
# tests/test_e2e.py
import os
import json
import pytest

@pytest.mark.e2e
@pytest.mark.slow
class TestE2E:
    
    def test_full_generation_flow(self):
        """
        Vollst√§ndiger Flow: Input ‚Üí Generate ‚Üí Validate ‚Üí Save ‚Üí Load
        Ben√∂tigt: R1-Server muss laufen!
        """
        # Setup
        thema = "E2E-Test"
        anzahl = 5
        
        # 1. Generate
        from chatbot import generate_questions_from_r1
        questions = generate_questions_from_r1(thema, anzahl, 4)
        
        assert questions is not None, "Generierung fehlgeschlagen"
        assert len(questions) == anzahl, f"Erwartete {anzahl} Fragen, bekam {len(questions)}"
        
        # 2. Validate
        from chatbot import validate_questions
        valid, errors = validate_questions(questions)
        
        assert valid, f"Validierung fehlgeschlagen: {errors}"
        
        # 3. Save
        from chatbot import save_questionset
        filepath = save_questionset(questions, thema)
        
        assert os.path.exists(filepath), "Datei wurde nicht gespeichert"
        
        # 4. Load
        with open(filepath, 'r', encoding='utf-8') as f:
            loaded = json.load(f)
        
        assert len(loaded) == anzahl, "Geladene Fragen stimmen nicht √ºberein"
        assert loaded == questions, "Geladene Fragen sind nicht identisch"
        
        # Cleanup
        os.remove(filepath)
```

**Wichtig:** E2E-Tests nur in CI/CD mit `@pytest.mark.e2e` markieren, nicht bei jedem `pytest` Run.

---

### Test-Execution

```bash
# Unit Tests (schnell, immer laufen lassen)
pytest tests/ -v

# Integration Tests (mit Mocks)
pytest tests/test_integration.py -v

# E2E Tests (nur bei Release, ben√∂tigt R1-Server)
pytest tests/ -m e2e -v --slow

# Coverage Report
pytest tests/ --cov=chatbot --cov-report=html
```

**Ziel: >90% Code Coverage f√ºr `chatbot.py`**

---

## ‚ö†Ô∏è Risiken & Mitigation

### Risiko-Matrix

| ID | Risiko | P | I | Score | Mitigation | Owner |
|----|--------|---|---|-------|------------|-------|
| R1 | R1-Server Ausfall | M | H | üî¥ 8 | Health-Check + Monitoring + Alerting | Ops |
| R2 | Schlechte Fragenqualit√§t | M | M | üü° 5 | Validierung + Preview + Manuelle Review | Admin |
| R3 | Netzwerk-Latenz >5min | L | L | üü¢ 2 | Timeout-Handling + Retry-Logic | Dev |
| R4 | Security-Breach (API-Key Leak) | L | H | üü° 4 | Secrets Management + Rate Limiting | Dev |
| R5 | Prompt Injection | L | L | üü¢ 2 | Input Sanitization + Output Validation | Dev |
| R6 | JSON-Parse-Fehler | M | L | üü¢ 3 | Robustes Parsing + Fallback | Dev |
| R7 | Streamlit Cloud Timeout | L | M | üü¢ 3 | Background Jobs (future) | Dev |
| R8 | Kosten-Explosion (Strom) | L | L | üü¢ 2 | Monitoring + Budget Alerts | Ops |

**Legende:**
- **P** (Probability): L=Low, M=Medium, H=High
- **I** (Impact): L=Low (1-3), M=Medium (4-6), H=High (7-9)
- **Score** = P √ó I

---

### Detaillierte Mitigations

#### R1: R1-Server Ausfall (Score: 8)

**Symptome:**
- Connection Error in Streamlit App
- Timeout beim Generieren
- Health-Check schl√§gt fehl

**Pr√§vention:**
```python
# Monitoring-Script (cron every 5 minutes)
import requests
import smtplib

def check_r1_health():
    try:
        r = requests.get("https://your-r1-server/health", timeout=10)
        if r.status_code != 200:
            send_alert(f"R1-Server unhealthy: {r.status_code}")
    except Exception as e:
        send_alert(f"R1-Server down: {e}")

def send_alert(message):
    # Email, SMS, Slack, etc.
    ...
```

**Recovery:**
- Automatischer Restart (systemd/supervisor)
- Fallback auf Cloud-LLM (optional, Phase 2)
- User-freundliche Error-Message mit ETA

---

#### R2: Schlechte Fragenqualit√§t (Score: 5)

**Symptome:**
- Fachlich falsche Antworten
- Schlechte Distraktoren (alle offensichtlich falsch)
- Inkonsistente Schwierigkeitsgrade

**Pr√§vention:**
```python
# Quality-Score Funktion (future)
def calculate_quality_score(questions):
    score = 0
    
    # Check 1: L√§ngen-Varianz der Optionen
    for q in questions:
        lengths = [len(opt) for opt in q["optionen"]]
        variance = max(lengths) - min(lengths)
        if variance < 20:  # Gut: √§hnliche L√§ngen
            score += 1
    
    # Check 2: Keine "Alle obigen" Option
    for q in questions:
        if any("alle" in opt.lower() for opt in q["optionen"]):
            score -= 2
    
    return score / len(questions)
```

**Manuelle Review-Checkliste:**
- [ ] Frage ist eindeutig formuliert
- [ ] Korrekte Antwort ist fachlich richtig
- [ ] Distraktoren sind plausibel
- [ ] LaTeX-Syntax ist korrekt
- [ ] Erkl√§rung ist verst√§ndlich

---

#### R4: Security-Breach (Score: 4)

**Worst-Case-Szenario:**
- API-Key wird geleakt (z.B. in Git committed)
- Angreifer kann unbegrenzt Fragen generieren
- R1-Server √ºberlastet

**Immediate Response:**
1. **Detect:** Monitoring zeigt ungew√∂hnlich hohe Request-Rate
2. **Isolate:** API-Key rotieren (neuen Key generieren)
3. **Mitigate:** IP des Angreifers blocken (Firewall-Regel)
4. **Recover:** R1-Server neu starten falls √ºberlastet
5. **Learn:** Post-Mortem, verbesserte Secrets-Handling

**Pr√§vention:**
```bash
# Pre-commit Hook: Checke nach Secrets
# .git/hooks/pre-commit
#!/bin/bash
if grep -r "R1_API_KEY.*=" . --exclude-dir=.git; then
    echo "‚ùå ERROR: API-Key im Code gefunden!"
    exit 1
fi
```

---

## üöÄ Deployment-Strategie

### Deployment-Environments

| Environment | Purpose | URL | Branch | Auto-Deploy |
|-------------|---------|-----|--------|-------------|
| **Local** | Development | localhost:8501 | - | Manual |
| **Staging** | Testing | staging.app.streamlit | develop | ‚úÖ Yes |
| **Production** | Live | mc-test.streamlit.app | main | ‚úÖ Yes |

---

### Deployment-Checklist

#### Pre-Deployment

- [ ] **Code Review:** Alle √Ñnderungen reviewed
- [ ] **Tests:** Alle Tests bestehen (Unit + Integration)
- [ ] **Linting:** `pylint` und `black` ohne Fehler
- [ ] **Security Scan:** Keine Secrets im Code
- [ ] **Documentation:** README und CHANGELOG aktualisiert
- [ ] **Backup:** Datenbank-Backup erstellt

#### Deployment

- [ ] **1. Merge to main:**
  ```bash
  git checkout main
  git merge develop
  git push origin main
  ```

- [ ] **2. Configure Secrets in Streamlit Cloud:**
  - Dashboard ‚Üí App ‚Üí Settings ‚Üí Secrets
  - `R1_SERVER_URL` eintragen
  - `R1_API_KEY` eintragen

- [ ] **3. Deploy:**
  - Streamlit Cloud deployt automatisch nach Push
  - Warte auf "App is live" Status (~2-3 Min)

- [ ] **4. Smoke Test:**
  - App √∂ffnen
  - Als Admin einloggen
  - Generator-Tab √∂ffnen
  - Server-Status testen (sollte gr√ºn sein)
  - Testgenerierung mit 5 Fragen
  - Erfolg verifizieren

#### Post-Deployment

- [ ] **Monitoring aktivieren:**
  - R1-Server Health-Check l√§uft
  - Application Logs pr√ºfen (keine Errors)
  
- [ ] **User Communication:**
  - Admin-Team √ºber neues Feature informieren
  - User-Guide verteilen

- [ ] **Hotfix-Branch bereithalten:**
  ```bash
  git checkout -b hotfix/ai-generator-issue main
  # F√ºr schnelle Rollbacks
  ```

---

### Rollback-Plan

**Trigger:** Kritischer Bug in Production (z.B. App crasht, R1-Server down)

**Rollback-Steps (< 5 Minuten):**

1. **Identify:** Welcher Commit verursacht das Problem?
   ```bash
   git log --oneline
   ```

2. **Revert:**
   ```bash
   # Option A: Revert letzten Commit
   git revert HEAD
   git push origin main
   
   # Option B: Hard Reset (nur wenn n√∂tig!)
   git reset --hard <last_good_commit>
   git push --force origin main
   ```

3. **Verify:** App l√§uft wieder?

4. **Communicate:** Team & Users informieren

5. **Post-Mortem:** Was lief schief? Wie verhindern?

---

## üîß Wartung & Support

### Monitoring

#### Metriken

| Metrik | Tool | Schwellwert | Aktion bei √úberschreitung |
|--------|------|-------------|---------------------------|
| R1-Server Uptime | Cron + curl | <99% | Alert + Manual Check |
| API Response Time | Logging | >10 Sek | Optimize Prompt/Model |
| Error Rate | Streamlit Logs | >5% | Investigate + Fix |
| Daily Generations | Custom Counter | >100 | Check for Abuse |
| Disk Space (data/) | `du -sh data/` | >1GB | Cleanup old files |

#### Alerting-Setup

```bash
# /etc/cron.d/r1-monitoring
*/5 * * * * /usr/local/bin/check_r1_health.sh

# /usr/local/bin/check_r1_health.sh
#!/bin/bash
STATUS=$(curl -s -o /dev/null -w "%{http_code}" https://your-r1-server/health)
if [ $STATUS -ne 200 ]; then
    echo "R1-Server down! Status: $STATUS" | mail -s "ALERT: R1-Server" admin@example.com
fi
```

---

### Maintenance-Tasks

#### W√∂chentlich

- [ ] **Logs pr√ºfen:** Fehler-Patterns erkennen
  ```bash
  tail -100 /var/log/r1-server/error.log
  ```
- [ ] **Disk Space:** `data/` Ordner aufr√§umen wenn >500MB
- [ ] **API-Key Rotation:** (alle 90 Tage, Reminder in Kalender)

#### Monatlich

- [ ] **Performance-Review:** Durchschnittliche Generierungszeit
- [ ] **Quality-Review:** 5 zuf√§llige Fragensets manuell pr√ºfen
- [ ] **Security-Audit:** Logs nach verd√§chtigen IPs durchsuchen
- [ ] **Dependency-Updates:** `pip list --outdated`

#### Quartalsweise

- [ ] **Model-Update:** Neue DeepSeek-Version testen
- [ ] **Prompt-Optimization:** A/B-Test verschiedener Prompts
- [ ] **User-Feedback:** Admins nach Verbesserungsw√ºnschen fragen

---

### Troubleshooting-Guide

#### Problem: "Server nicht erreichbar"

**Symptom:** Verbindung-Test im UI schl√§gt fehl

**Debug-Steps:**
1. Ist R1-Server online?
   ```bash
   ssh your-server
   systemctl status r1-server
   ```
2. Ist Firewall offen?
   ```bash
   sudo ufw status
   # Port 8080 sollte ALLOW sein
   ```
3. Ist Cloudflare Tunnel aktiv?
   ```bash
   ps aux | grep cloudflared
   ```
4. Ist URL korrekt in Streamlit Secrets?
   ```python
   import streamlit as st
   print(st.secrets["R1_SERVER_URL"])
   ```

**Fix:**
- R1-Server neu starten: `systemctl restart r1-server`
- Cloudflare Tunnel neu starten: `cloudflared tunnel ...`
- Secrets korrigieren in Streamlit Cloud Dashboard

---

#### Problem: "Timeout nach 5 Minuten"

**Symptom:** Generierung startet, bricht aber ab

**M√∂gliche Ursachen:**
- R1-Server √ºberlastet (CPU/RAM voll)
- Modell zu gro√ü f√ºr Hardware (z.B. R1-70B auf 8GB VRAM)
- Netzwerk-Latenz zu hoch

**Debug-Steps:**
1. R1-Server Ressourcen pr√ºfen:
   ```bash
   htop
   nvidia-smi  # F√ºr GPU
   ```
2. Test mit weniger Fragen (5 statt 50)
3. Test mit kleinerem Modell (R1-1.5B statt R1-70B)

**Fix:**
- Hardware upgraden (mehr RAM/VRAM)
- Kleineres Modell nutzen
- Timeout erh√∂hen (nur tempor√§r!):
  ```python
  timeout=600  # 10 Minuten
  ```

---

#### Problem: "Validierung fehlgeschlagen"

**Symptom:** Fragen wurden generiert, aber nicht gespeichert

**Debug-Steps:**
1. Rohdaten in UI pr√ºfen (sollte angezeigt werden)
2. Fehlende Felder identifizieren
3. JSON-Struktur pr√ºfen

**Typische Fehler:**
- `loesung` ist String statt Integer
- `optionen` hat nur 3 statt 4 Elemente
- `gewichtung` ist 0 oder 4 (nur 1-3 erlaubt)

**Fix:**
- Prompt optimieren (klarer spezifizieren)
- Manuell korrigieren und erneut hochladen
- Model-Parameter anpassen (temperature runter auf 0.5)

---

### Support-Kontakte

| Rolle | Name | Kontakt | Verantwortung |
|-------|------|---------|---------------|
| **Product Owner** | [Your Name] | [Email] | Features, Roadmap |
| **DevOps** | [Your Name] | [Email] | R1-Server, Deployment |
| **Support** | [Your Name] | [Email] | User-Issues, Bugs |

---

## üìà Success Metrics (KPIs)

### Launch-Phase (Woche 1-4)

| Metrik | Ziel | Messung |
|--------|------|---------|
| Erfolgreiche Generierungen | >90% | API Success Rate |
| Durchschnittliche Generierungszeit | <90 Sek | Server Logs |
| Admin-Adoption | >80% nutzen Feature | Usage Analytics |
| Kritische Bugs | 0 | Bug Tracker |

### Growth-Phase (Monat 2-6)

| Metrik | Ziel | Messung |
|--------|------|---------|
| Generierte Fragensets | >50 | File Count in data/ |
| Fragenqualit√§t (Review) | >8/10 | Manual Review Score |
| R1-Server Uptime | >99% | Monitoring |
| User-Zufriedenheit | >4/5 | Admin-Survey |

### Optimization-Phase (Monat 7-12)

| Metrik | Ziel | Messung |
|--------|------|---------|
| Generierungszeit | <60 Sek | Performance Logs |
| Validierungs-Erfolgsrate | >95% | Validation Logs |
| Feature-Requests umgesetzt | >50% | Roadmap Progress |
| ROI (vs. Cloud-LLM) | >80% Ersparnis | Cost Analysis |

---

## üéØ Fazit & Empfehlung

### Zusammenfassung

‚úÖ **Machbarkeit:** 4.65/5 - Hochgradig machbar mit allen Voraussetzungen erf√ºllt

‚úÖ **Kosten-Nutzen:** Self-Hosted ist 5-7x g√ºnstiger als Cloud-LLMs

‚úÖ **Risiken:** Alle identifizierten Risiken haben Mitigations, keine Blocker

‚úÖ **Timeline:** 28 Stunden (3.5 Tage) f√ºr vollst√§ndige Implementation

‚úÖ **ROI:** Ab Tag 1 profitabel (keine laufenden API-Kosten)

### Go/No-Go Decision

**‚úÖ GO:** Projekt wird zur Umsetzung empfohlen.

**Begr√ºndung:**
1. Technisch vollst√§ndig machbar (Score: 5/5)
2. Wirtschaftlich h√∂chst sinnvoll (0‚Ç¨ vs. 50-100‚Ç¨/Monat)
3. Datenschutz-optimal (100% on-premise, DSGVO-konform)
4. Moderate Komplexit√§t (28h Aufwand)
5. Alle Risiken mitigierbar

**Voraussetzung:**
- R1-Server muss stabil laufen (>99% Uptime)
- Sicherheitsma√ünahmen m√ºssen implementiert werden (API-Key, HTTPS, Rate Limiting)
- Manuelle Review-Prozess f√ºr generierte Fragen etablieren

### Next Steps

1. ‚úÖ **Jetzt:** Dieses Dokument reviewen und freigeben
2. **Tag 1-2:** Phase 1 (Foundation) implementieren
3. **Tag 3-4:** Phase 2 (UI & Validation) implementieren
4. **Tag 5-6:** Phase 3 (Testing & Polish) durchf√ºhren
5. **Tag 7:** Phase 4 (Deployment & Docs) abschlie√üen

**Start:** Sobald freigegeben

---

**Erstellt:** 3. Oktober 2025  
**Autor:** GitHub Copilot  
**Version:** 1.0  
**Status:** ‚úÖ Bereit zur Umsetzung

