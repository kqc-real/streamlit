{
  "meta": {
    "title": "Skalarprodukt, Orthogonalität & QR-Zerlegung",
    "created": "14.11.2025 16:55",
    "modified": "14.11.2025 16:55",
    "target_audience": "Anfänger",
    "question_count": 39,
    "difficulty_profile": {
      "leicht": 8,
      "mittel": 20,
      "schwer": 12
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 40
  },
  "questions": [
    {
      "question": "1. Was misst das Skalarprodukt zweier Vektoren im $\\mathbb{R}^n$?",
      "options": [
        "Die Fläche des von ihnen aufgespannten Parallelogramms.",
        "Wie stark zwei Vektoren in dieselbe Richtung zeigen.",
        "Die Anzahl ihrer linearen Abhängigkeiten.",
        "Die Determinante der Matrix aus den beiden Vektoren.",
        "Die Länge des Kreuzprodukts."
      ],
      "answer": 1,
      "explanation": "Das Skalarprodukt ist $\\langle x,y\\rangle=\\sum_i x_i y_i$ und misst die Ausrichtung bzw. den Winkelbezug der Vektoren.",
      "weight": 1,
      "topic": "Skalarprodukt & Norm",
      "mini_glossary": {
        "Skalarprodukt": "Definiert als $\\langle x,y\\rangle=\\sum_i x_i y_i$; Maß für Ausrichtung.",
        "Norm": "Euklidische Länge: $\\|x\\|=\\sqrt{\\langle x,x\\rangle}$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "2. Welche Aussage zur euklidischen Norm ist korrekt?",
      "options": [
        "Sie ist definiert als $\\|x\\|=\\sqrt{\\langle x,x\\rangle}$.",
        "Sie ist gleich $\\sum_i x_i$.",
        "Sie ist immer 1.",
        "Sie ist das Maximum der Komponenten.",
        "Sie ist negativ für kurze Vektoren."
      ],
      "answer": 0,
      "explanation": "Die euklidische Norm wird über das Skalarprodukt definiert: $\\|x\\|=\\sqrt{\\langle x,x\\rangle}$.",
      "weight": 1,
      "topic": "Skalarprodukt & Norm",
      "mini_glossary": {
        "Norm": "Länge eines Vektors, induziert durch das Skalarprodukt.",
        "Euklidischer Raum": "Standardraum mit innerem Produkt und daraus abgeleiteter Geometrie."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "3. Wann sind zwei Vektoren orthogonal?",
      "options": [
        "Wenn sie dieselbe Norm haben.",
        "Wenn sie linear abhängig sind.",
        "Wenn ihr Skalarprodukt Null ist.",
        "Wenn ihre Summe der Nullvektor ist.",
        "Wenn sie parallel sind."
      ],
      "answer": 2,
      "explanation": "Orthogonalität: $x\\perp y$ genau dann, wenn $\\langle x,y\\rangle=0$.",
      "weight": 1,
      "topic": "Orthogonalität & Unterräume",
      "mini_glossary": {
        "Orthogonalität": "Bedingung $\\langle x,y\\rangle=0$; rechtwinklige Ausrichtung.",
        "Orthogonales Komplement": "Alle Vektoren, die zu einem Unterraum orthogonal sind."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "4. Wofür steht bei einer orthogonalen Projektion der Restvektor $r=x-Px$?",
      "options": [
        "Für die Komponente von $x$ im Unterraum.",
        "Für die Rotation von $x$ um 90°.",
        "Für die Komponente von $x$ orthogonal zum Unterraum.",
        "Für die Länge von $x$.",
        "Für die Summe aller Basisvektoren."
      ],
      "answer": 2,
      "explanation": "Bei orthogonaler Projektion gilt $r=x-Px\\perp U$ und $Px\\in U$.",
      "weight": 1,
      "topic": "Projektionen & Projektoren",
      "mini_glossary": {
        "Projektor": "Lineare Abbildung mit $P^2=P$; bei orthogonaler Projektion zusätzlich $P^\\top=P$.",
        "Restvektor": "Differenz $r=x-Px$, orthogonal zum Zielunterraum."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "5. Welche Eigenschaft hat eine Orthonormalbasis (ONB)?",
      "options": [
        "Die Vektoren sind paarweise orthogonal und normiert.",
        "Die Vektoren sind alle Nullvektoren.",
        "Die Vektoren sind beliebig skaliert, aber orthogonal.",
        "Die Vektoren sind linear abhängig.",
        "Die Vektoren haben unterschiedliche Dimensionen."
      ],
      "answer": 0,
      "explanation": "ONB: paarweise orthogonal und jeweils Norm 1; sie spannt den Raum auf.",
      "weight": 1,
      "topic": "Gram–Schmidt",
      "mini_glossary": {
        "Gram–Schmidt": "Verfahren zur Erzeugung einer ONB aus unabhängigen Vektoren.",
        "ONB": "Orthonormalbasis; orthogonale Einheitsvektoren spannen den Raum auf."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "6. Was bedeutet bei einer QR-Zerlegung die Gleichung $Q^\\top Q=I$?",
      "options": [
        "Q ist invertierbar, aber nicht orthonormal.",
        "Die Spalten von Q sind orthonormal.",
        "R ist orthogonal.",
        "A ist symmetrisch.",
        "R ist diagonal."
      ],
      "answer": 1,
      "explanation": "$Q^\\top Q=I$ bedeutet: Spalten von $Q$ sind orthonormal.",
      "weight": 1,
      "topic": "QR-Zerlegung Grundlagen",
      "mini_glossary": {
        "QR-Zerlegung": "Faktorisierung $A=QR$ mit orthonormalem $Q$ und oberem Dreieck $R$.",
        "Dünnes Q": "$Q\\in\\mathbb{R}^{m\\times n}$ mit $Q^\\top Q=I_n$ für $m\\ge n$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "7. Was ist R in der QR-Zerlegung $A=QR$?",
      "options": [
        "Eine untere Dreiecksmatrix.",
        "Eine obere Dreiecksmatrix.",
        "Eine Diagonalmatrix mit Einsen.",
        "Eine Permutationsmatrix.",
        "Eine Nullmatrix."
      ],
      "answer": 1,
      "explanation": "In $A=QR$ ist $R$ eine obere Dreiecksmatrix.",
      "weight": 1,
      "topic": "QR-Zerlegung Grundlagen",
      "mini_glossary": {
        "Obere Dreiecksmatrix": "Matrix mit Nullen unterhalb der Hauptdiagonalen.",
        "Rückwärtseinsetzen": "Lösen eines oberen Dreieckssystems ohne Inversion."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "8. Welche Grundidee folgt bei Least Squares via QR?",
      "options": [
        "Direkt $A^\\top A$ invertieren.",
        "Zuerst $A=QR$ und dann $Rx=Q^\\top b$ lösen.",
        "Die rechte Seite $b$ orthogonalisieren und invertieren.",
        "Mit Vorwärtseinsetzen in $R$ starten.",
        "Zuerst eine SVD und dann $Q$ verwerfen."
      ],
      "answer": 1,
      "explanation": "Stabil: $A=QR$, dann $y=Q^\\top b$ und Rückwärtseinsetzen in $Rx=y$.",
      "weight": 1,
      "topic": "Least Squares via QR",
      "mini_glossary": {
        "Least Squares": "Minimiert $\\|Ax-b\\|^2$; Lösung via QR: $Rx=Q^\\top b$.",
        "Residuum": "Fehlervektor $r=b-Ax$; bei Lösung orthogonal zu span$(A)$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "9. Welche Gleichungen charakterisieren einen orthogonalen Projektor $P$ auf einen Unterraum $U$ korrekt?",
      "options": [
        "$P^2=P$ und $P^\\top=P$.",
        "$P^2=I$ und $P^\\top=-P$.",
        "$P^2=0$ und $P^\\top=P$.",
        "$P^2=P$ und $P^\\top=I$.",
        "$P^2=I$ und $P^\\top=I$."
      ],
      "answer": 0,
      "explanation": "Orthogonaler Projektor ist idempotent und symmetrisch: $P^2=P$, $P^\\top=P$.",
      "weight": 2,
      "topic": "Projektionen & Projektoren",
      "extended_explanation": {
        "titel": "Eigenschaften eines Projektors",
        "schritte": [
          "Idempotenz garantiert: einmal projiziert bleibt projiziert.",
          "Symmetrie entspricht orthogonaler Projektion bzgl. euklidischem Skalarprodukt.",
          "Für ONB-Matrix $U$ gilt $P=UU^\\top$."
        ]
      },
      "mini_glossary": {
        "Projektor": "Lineare Abbildung mit $P^2=P$; bei orthogonaler Projektion zusätzlich $P^\\top=P$.",
        "Optimalität": "$Px$ minimiert $\\|x-z\\|$ über $z\\in U$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "10. Für $x\\neq 0, y\\neq 0$ gilt $\\cos\\angle(x,y)=\\dfrac{\\langle x,y\\rangle}{\\|x\\|\\,\\|y\\|}$. Welche Aussage ist korrekt?",
      "options": [
        "Der Wert ändert sich bei Skalierung von $x$ oder $y$ nicht.",
        "Der Wert verdoppelt sich, wenn $x$ verdoppelt wird.",
        "Der Wert halbiert sich, wenn $y$ halbiert wird.",
        "Der Wert hängt von der Wahl der Basis ab.",
        "Der Wert ist immer negativ."
      ],
      "answer": 0,
      "explanation": "Die Kosinus-Ähnlichkeit ist skaleninvariant.",
      "weight": 2,
      "topic": "Winkel & Cauchy-Schwarz",
      "extended_explanation": {
        "titel": "Skaleninvarianz der Kosinus-Ähnlichkeit",
        "schritte": [
          "Skalierungen heben sich in Zähler und Nenner weg.",
          "Nur die Richtung, nicht die Länge, beeinflusst den Winkel.",
          "Nützlich in IR/ML für gerichtete Vergleiche."
        ]
      },
      "mini_glossary": {
        "Kosinus-Ähnlichkeit": "$\\cos=\\dfrac{\\langle x,y\\rangle}{\\|x\\|\\,\\|y\\|}$; vergleicht Richtungen.",
        "Normierung": "Skalieren auf gleiche Länge, um Richtungen zu vergleichen."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "11. Gegeben ein Einheitsvektor $u$ und ein Vektor $x$. Welche Formel gibt die Projektion von $x$ auf $\\operatorname{span}\\{u\\}$ an?",
      "options": [
        "$\\operatorname{proj}_u(x)=\\langle x,u\\rangle u$.",
        "$\\operatorname{proj}_u(x)=xu$.",
        "$\\operatorname{proj}_u(x)=\\dfrac{x}{\\langle x,u\\rangle}$.",
        "$\\operatorname{proj}_u(x)=u/x$.",
        "$\\operatorname{proj}_u(x)=\\langle u,u\\rangle x$."
      ],
      "answer": 0,
      "explanation": "Auf eine Gerade mit Einheitsvektor $u$: $\\operatorname{proj}_u(x)=\\langle x,u\\rangle u$.",
      "weight": 2,
      "topic": "Projektionen & Projektoren",
      "extended_explanation": {
        "titel": "Projektion auf eine Gerade",
        "schritte": [
          "Für nicht normierte Richtungen zuerst normieren.",
          "Rest $r=x-\\operatorname{proj}_u(x)$ ist orthogonal zu $u$.",
          "Die Formel minimiert $\\|x-z\\|$ über $z\\in \\operatorname{span}\\{u\\}$."
        ]
      },
      "mini_glossary": {
        "Einheitsvektor": "Vektor mit Norm 1.",
        "Restorthogonalität": "$x-\\operatorname{proj}_u(x)\\perp u$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "12. Sei $(u_1,\\dots,u_k)$ eine ONB eines Unterraums $U$. Welche Darstellung der Projektion ist korrekt?",
      "options": [
        "$\\operatorname{proj}_U(x)=\\sum_{i=1}^k \\langle x,u_i\\rangle u_i$.",
        "$\\operatorname{proj}_U(x)=\\sum_{i=1}^k x_i u_i$.",
        "$\\operatorname{proj}_U(x)=U^{-1}x$.",
        "$\\operatorname{proj}_U(x)=Ux$.",
        "$\\operatorname{proj}_U(x)=U^\\top x$."
      ],
      "answer": 0,
      "explanation": "Projektion auf $U$ via ONB-Summenformel.",
      "weight": 2,
      "topic": "Projektionen & Projektoren",
      "extended_explanation": {
        "titel": "Matrixform der Projektion",
        "schritte": [
          "Mit $U=[u_1\\,\\dots\\,u_k]$ gilt $P=UU^\\top$.",
          "$\\operatorname{proj}_U(x)=Px$.",
          "Rest $x-Px$ ist orthogonal zu $U$."
        ]
      },
      "mini_glossary": {
        "Projektormatrix": "Für ONB $U$ gilt $P=UU^\\top$.",
        "Eindeutigkeit": "Die orthogonale Projektion ist eindeutig."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "13. Welche Aussage zum klassischen Gram–Schmidt (CGS) ist fachlich zutreffend?",
      "options": [
        "CGS ist konzeptionell einfach, aber numerisch weniger stabil.",
        "CGS ist numerisch stabiler als Householder.",
        "CGS erzeugt niemals eine ONB.",
        "CGS benötigt eine invertierbare Matrix.",
        "CGS setzt orthogonale Eingangsvektoren voraus."
      ],
      "answer": 0,
      "explanation": "CGS liefert ONB, ist aber weniger stabil als MGS/Householder.",
      "weight": 2,
      "topic": "Gram–Schmidt",
      "extended_explanation": {
        "titel": "CGS vs. MGS/Householder",
        "schritte": [
          "MGS reduziert Orthogonalitätsverluste durch Rundung.",
          "Householder reflektiert an Hyperebenen und ist sehr robust.",
          "Wahl hängt von Matrixstruktur und Numerik ab."
        ]
      },
      "mini_glossary": {
        "MGS": "Modifizierter Gram–Schmidt; numerisch stabiler als CGS.",
        "Householder": "Spiegelungsverfahren zur robusten QR-Konstruktion."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "14. Was beschreibt die Matrix $Q$ in $A=QR$ geometrisch am besten?",
      "options": [
        "Sie skaliert Achsen unabhängig.",
        "Sie ist eine orthogonale Transformation (Rotation/Spiegelung).",
        "Sie ist eine Scherung entlang der Diagonalen.",
        "Sie invertiert $A$ direkt.",
        "Sie ist stets diagonal mit Einsen."
      ],
      "answer": 1,
      "explanation": "$Q$ erhält Längen und Winkel: Rotation/Spiegelung.",
      "weight": 2,
      "topic": "QR-Zerlegung Grundlagen",
      "extended_explanation": {
        "titel": "Geometrie von Q und R",
        "schritte": [
          "Q: orthogonale Abbildung, erhält inneres Produkt.",
          "R: obere Dreiecksmatrix trägt Skalierung/Scherung.",
          "Komposition: erst Q, dann R bildet A ab."
        ]
      },
      "mini_glossary": {
        "Orthogonale Matrix": "Erfüllt $Q^\\top Q=I$; erhält Normen/Winkel.",
        "Scherung": "Lineare Verzerrung, die Winkel verändert."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "15. Worin besteht der Vorteil von Least Squares via QR gegenüber Normalengleichungen $A^\\top A x=A^\\top b$?",
      "options": [
        "Weniger Rechenaufwand in jedem Fall.",
        "Größere numerische Stabilität durch Vermeidung der Bildung von $A^\\top A$.",
        "Eine exakte Lösung für jedes überbestimmte System.",
        "Keine Orthogonalisierung erforderlich.",
        "Es benötigt keine Dreiecksmatrix."
      ],
      "answer": 1,
      "explanation": "Die Bildung von $A^\\top A$ verschlechtert die Kondition; QR vermeidet das.",
      "weight": 2,
      "topic": "Least Squares via QR",
      "extended_explanation": {
        "titel": "Stabilitätsaspekt",
        "schritte": [
          "Konditionszahl quadriert sich bei $A^\\top A$.",
          "QR nutzt orthogonale Transformationen mit $Q^\\top Q=I$.",
          "Rückwärtseinsetzen in $R$ ist stabil und effizient."
        ]
      },
      "mini_glossary": {
        "Kondition": "Empfindlichkeit der Lösung gegenüber Datenstörungen.",
        "Normalengleichungen": "Form $A^\\top A x=A^\\top b$; oft schlechter konditioniert."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "16. Gegeben $x=(1,2,2)^\\top$ und $y=(2,0,1)^\\top$. Welche Größe hat $\\langle x,y\\rangle$?",
      "options": [
        "2",
        "3",
        "4",
        "5",
        "6"
      ],
      "answer": 2,
      "explanation": "$1\\cdot2+2\\cdot0+2\\cdot1=4$.",
      "weight": 2,
      "topic": "Winkel & Cauchy-Schwarz",
      "extended_explanation": {
        "titel": "Bezug zu Norm und Winkel",
        "schritte": [
          "Mit $\\|x\\|=3$, $\\|y\\|=\\sqrt{5}$ folgt $\\cos=\\dfrac{4}{3\\sqrt{5}}$.",
          "Orthogonalität läge bei Skalarprodukt 0 vor.",
          "Cauchy–Schwarz liefert die obere Schranke."
        ]
      },
      "mini_glossary": {
        "Cauchy–Schwarz": "$|\\langle x,y\\rangle|\\le \\|x\\|\\,\\|y\\|$.",
        "Winkel": "Definiert über den Kosinus der normierten Projektion."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "17. Welche Aussage zur Restorthogonalität bei der LS-Lösung ist korrekt?",
      "options": [
        "Der Rest ist parallel zum Spaltenraum von $A$.",
        "Der Rest ist orthogonal zum Spaltenraum von $A$.",
        "Der Rest ist zufällig verteilt.",
        "Der Rest ist stets der Nullvektor.",
        "Der Rest ist proportional zu $b$."
      ],
      "answer": 1,
      "explanation": "Für $x^\\star$ gilt $r=b-Ax^\\star\\perp \\operatorname{span}(A)$.",
      "weight": 2,
      "topic": "Least Squares via QR",
      "extended_explanation": {
        "titel": "Charakterisierung der LS-Lösung",
        "schritte": [
          "Orthogonalitätsbedingung charakterisiert das Optimum.",
          "Geometrisch: kürzester Abstand von $b$ zum Spaltenraum.",
          "QR macht diese Struktur explizit."
        ]
      },
      "mini_glossary": {
        "Spaltenraum": "Von den Spaltenvektoren aufgespannter Unterraum.",
        "Residuum": "Fehlervektor zwischen Beobachtung und Modell."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "18. Wodurch zeichnet sich $R$ in $A=QR$ bei vollem Spaltenrang aus?",
      "options": [
        "Die Diagonalelemente sind alle Null.",
        "Die Diagonalelemente sind nicht Null.",
        "R ist symmetrisch.",
        "R ist orthogonal.",
        "R ist projektiv."
      ],
      "answer": 1,
      "explanation": "Voller Spaltenrang impliziert nicht verschwindende Diagonale von $R$.",
      "weight": 2,
      "topic": "QR-Zerlegung Grundlagen",
      "extended_explanation": {
        "titel": "Implikationen für die Lösung",
        "schritte": [
          "Nichtverschwindende Diagonalelemente sichern eindeutiges Rückwärtseinsetzen.",
          "Singularität würde Rangmangel anzeigen.",
          "Pivotierung kann bei Grenzfällen nötig sein."
        ]
      },
      "mini_glossary": {
        "Voller Spaltenrang": "Spalten sind linear unabhängig.",
        "Rang": "Dimension des Spalten- oder Zeilenraums."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "19. Welche Reihenfolge der Schritte ist korrekt für Least Squares via QR?",
      "options": [
        "Berechne $Q^\\top b$, zerlege $A=QR$, löse $Rx=y$.",
        "Zerlege $A=QR$, berechne $y=Q^\\top b$, löse $Rx=y$.",
        "Invertiere $R$, berechne $x=R^{-1}Q^\\top b$, überprüfe $A=QR$.",
        "Zerlege $A=QR$, löse $Qx=b$, berechne $Rx=y$.",
        "Zerlege $b=QR$, löse $Rx=Q^\\top y$."
      ],
      "answer": 1,
      "explanation": "Standardrezept: $A=QR\\rightarrow y=Q^\\top b\\rightarrow Rx=y$.",
      "weight": 2,
      "topic": "Least Squares via QR",
      "extended_explanation": {
        "titel": "Algorithmische Pipeline",
        "schritte": [
          "Orthogonale Transformation der rechten Seite zu $y=Q^\\top b$.",
          "Lösen des Dreieckssystems $Rx=y$ per Rückwärtseinsetzen.",
          "Keine explizite Inversion von $R$."
        ]
      },
      "mini_glossary": {
        "Rückwärtseinsetzen": "Start bei letzter Gleichung, dann aufwärts.",
        "Orthogonale Transformation": "Erhält innere Produkte."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "20. Was liefert der modifizierte Gram–Schmidt (MGS) gegenüber klassischem GS typischerweise?",
      "options": [
        "Geringere numerische Stabilität.",
        "Bessere Erhaltung der Orthogonalität.",
        "Eine dichtere Matrix $R$.",
        "Eine kleinere Konditionszahl für $A$.",
        "Ein diagonales $R$."
      ],
      "answer": 1,
      "explanation": "MGS reduziert Orthogonalitätsfehler durch Rundung.",
      "weight": 2,
      "topic": "Gram–Schmidt",
      "extended_explanation": {
        "titel": "Warum MGS?",
        "schritte": [
          "Reihenfolge der Projektionen beeinflusst Rundungsfehler.",
          "In Praxis oft nahe Householder-Genauigkeit.",
          "Einfach zu implementieren."
        ]
      },
      "mini_glossary": {
        "Rundungsfehler": "Numerische Abweichungen durch endliche Darstellung.",
        "Orthogonalitätsverlust": "Abnahme der rechtwinkligen Ausrichtung durch Fehler."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "21. Welche Aussage zur Cauchy–Schwarz-Ungleichung ist korrekt?",
      "options": [
        "$|\\langle x,y\\rangle|\\le \\|x\\|\\,\\|y\\|$.",
        "$|\\langle x,y\\rangle|\\ge \\|x\\|\\,\\|y\\|$.",
        "$\\langle x,y\\rangle=\\|x\\|+\\|y\\|$.",
        "$\\langle x,y\\rangle=0$ für alle $x,y$.",
        "$\\|x+y\\|\\ge \\|x\\|+\\|y\\|$."
      ],
      "answer": 0,
      "explanation": "Cauchy–Schwarz liefert die obere Schranke für das Skalarprodukt.",
      "weight": 2,
      "topic": "Winkel & Cauchy-Schwarz",
      "extended_explanation": {
        "titel": "Folgen von Cauchy–Schwarz",
        "schritte": [
          "Begründet die Definition des Winkels via Kosinus.",
          "Mit Dreiecksungleichung Grundlage der euklidischen Geometrie.",
          "Nützlich für Fehlerabschätzungen."
        ]
      },
      "mini_glossary": {
        "Dreiecksungleichung": "$\\|x+y\\|\\le \\|x\\|+\\|y\\|$.",
        "Inneres Produkt": "Allgemeiner Begriff für Skalarprodukt."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "22. Wozu dienen Givens-Rotationen in der QR-Konstruktion?",
      "options": [
        "Zum Nullsetzen einzelner Einträge durch planare Rotationen.",
        "Zum Diagonalisieren symmetrischer Matrizen.",
        "Zur Permutation von Spalten.",
        "Zur Skalierung der Diagonale von R auf Eins.",
        "Zur Invertierung von A."
      ],
      "answer": 0,
      "explanation": "Givens eliminiert gezielt Einträge unterhalb der Diagonale.",
      "weight": 2,
      "topic": "QR-Konstruktion (Householder/Givens)",
      "extended_explanation": {
        "titel": "Givens vs. Householder",
        "schritte": [
          "Givens arbeitet lokal in Ebenen, gut für dünn besetzte Matrizen.",
          "Householder spiegelt ganze Spaltenbereiche.",
          "Beide erzeugen orthogonale Q-Faktoren."
        ]
      },
      "mini_glossary": {
        "Givens-Rotation": "2D-Rotation zur gezielten Elimination eines Eintrags.",
        "Householder-Reflexion": "Spiegelung an Hyperebenen zur Nullsetzung ganzer Unterspalten."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "23. Welche Eigenschaft besitzt $Q$ bei dünnem QR für $m\\ge n$?",
      "options": [
        "$Q\\in\\mathbb{R}^{m\\times m}$ orthogonal.",
        "$Q\\in\\mathbb{R}^{m\\times n}$ mit orthonormalen Spalten.",
        "$Q\\in\\mathbb{R}^{n\\times n}$ diagonal.",
        "$Q\\in\\mathbb{R}^{n\\times m}$ orthonormal.",
        "$Q$ ist stets symmetrisch."
      ],
      "answer": 1,
      "explanation": "Dünnes $Q$ hat $n$ orthonormale Spalten in $\\mathbb{R}^{m}$.",
      "weight": 2,
      "topic": "QR-Zerlegung Grundlagen",
      "extended_explanation": {
        "titel": "Dünnes vs. volles Q",
        "schritte": [
          "Volles $Q\\in\\mathbb{R}^{m\\times m}$, dünnes $Q\\in\\mathbb{R}^{m\\times n}$.",
          "Beides erfüllt $Q^\\top Q=I_n$.",
          "Unterschiede bei Rechen- und Speicheraufwand."
        ]
      },
      "mini_glossary": {
        "Volles Q": "Quadratisch und orthogonal, $m\\times m$.",
        "Dünnes Q": "$m\\times n$ mit $n$ orthonormalen Spalten."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "24. Was gilt für die Projektion $Px$ bei $P=UU^\\top$ mit ONB-Matrix $U$?",
      "options": [
        "$x-Px\\in U$",
        "$Px\\perp U$",
        "$x-Px\\perp U$",
        "$Px$ ist stets der Nullvektor",
        "$Px$ hat größere Norm als $x$"
      ],
      "answer": 2,
      "explanation": "Der Fehler ist orthogonal zum Unterraum; $Px\\in U$.",
      "weight": 2,
      "topic": "Projektionen & Projektoren",
      "extended_explanation": {
        "titel": "Optimalität der Projektion",
        "schritte": [
          "$Px$ minimiert $\\|x-z\\|$ über $z\\in U$.",
          "Symmetrisch-idempotente Struktur von $P$.",
          "Geometrische Deutung als Schatten von $x$ auf $U$."
        ]
      },
      "mini_glossary": {
        "Eindeutigkeit": "Orthogonale Projektion ist eindeutig bestimmt.",
        "Bild und Kern": "Bild($P$) ist $U$, Kern($P$) ist $U^\\perp$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "25. Welche typische Fehlerquelle bei GS/QR sollte vermieden werden?",
      "options": [
        "Vor der Projektion nicht zu normieren.",
        "Rückwärtseinsetzen zu verwenden.",
        "Orthogonalität zu prüfen.",
        "ONB zu konstruieren.",
        "Householder-Reflexionen zu nutzen."
      ],
      "answer": 0,
      "explanation": "Fehlende Normalisierung führt zu falschen Projektionsanteilen.",
      "weight": 2,
      "topic": "Typische Fehler & Numerik",
      "extended_explanation": {
        "titel": "Praxisfehler",
        "schritte": [
          "Nicht $A^\\top A$ invertieren.",
          "Stets Orthogonalität der $u_i$ prüfen.",
          "Kondition beachten; stabilere Verfahren wählen."
        ]
      },
      "mini_glossary": {
        "Normalisierung": "Skalieren auf Norm 1.",
        "Konditionszahl": "Maß für numerische Empfindlichkeit."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "26. Wie lässt sich $R$ bei QR aus $Q$ und $A$ gewinnen?",
      "options": [
        "$R=QA$",
        "$R=Q^\\top A$",
        "$R=AQ$",
        "$R=A^\\top Q$",
        "$R=QQ^\\top A$"
      ],
      "answer": 1,
      "explanation": "Aus $A=QR$ folgt $Q^\\top A=R$.",
      "weight": 2,
      "topic": "QR-Zerlegung Grundlagen",
      "extended_explanation": {
        "titel": "Ableitung von R",
        "schritte": [
          "Linksmultiplikation mit $Q^\\top$ nutzt $Q^\\top Q=I$.",
          "Ergebnis ist obere Dreiecksmatrix.",
          "Nützlich zur Kontrolle der QR-Zerlegung."
        ]
      },
      "mini_glossary": {
        "Orthogonalität von Q": "$Q^\\top Q=I$.",
        "Dreieckssystem": "Lineares System mit Dreiecksstruktur."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "27. Welche Eigenschaft bleibt unter orthogonalen Transformationen $Q$ erhalten?",
      "options": [
        "Euklidische Norm und Winkel.",
        "Determinante bleibt 1.",
        "Rang halbiert sich.",
        "Sparsamkeit (Sparsity) nimmt zu.",
        "Eigenwerte bleiben gleich."
      ],
      "answer": 0,
      "explanation": "Orthogonale Abbildungen erhalten innere Produkte, also Normen und Winkel.",
      "weight": 2,
      "topic": "QR-Konstruktion (Householder/Givens)",
      "extended_explanation": {
        "titel": "Erhaltungsgrößen",
        "schritte": [
          "$\\langle Qx,Qy\\rangle=\\langle x,y\\rangle$.",
          "Folglich bleibt $\\|Qx\\|=\\|x\\|$ erhalten.",
          "Die Determinante von Q ist $\\pm 1$."
        ]
      },
      "mini_glossary": {
        "Orthogonale Abbildung": "Erhält Längen und Winkel.",
        "Isometrie": "Abbildung, die Abstände erhält."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "28. Welche Aussage zur Kosinus-Ähnlichkeit in IR/ML trifft zu?",
      "options": [
        "Sie ist sensitiv gegenüber globaler Skalierung aller Vektoren.",
        "Sie misst Ähnlichkeit unabhängig von der Norm der Vektoren.",
        "Sie erfordert orthogonale Basisvektoren.",
        "Sie entspricht der Manhattan-Distanz.",
        "Sie ist nur für $\\{0,1\\}$-Vektoren definiert."
      ],
      "answer": 1,
      "explanation": "Kosinus-Ähnlichkeit hängt nur von der Richtung ab.",
      "weight": 2,
      "topic": "Anwendungen & Kosinus-Ähnlichkeit",
      "extended_explanation": {
        "titel": "Einsatz in der Praxis",
        "schritte": [
          "TF–IDF-Vektoren werden häufig mit Kosinus verglichen.",
          "Normierung verhindert Längeneinfluss.",
          "Robustheit gegenüber globaler Gewichtung."
        ]
      },
      "mini_glossary": {
        "TF–IDF": "Gewichtung von Termen zur Vektorisierung von Dokumenten.",
        "Feature-Vektor": "Numerische Repräsentation eines Objekts."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "29. Sei $A\\in\\mathbb{R}^{m\\times n}$ mit $m>n$ und vollem Spaltenrang. Welche Aussage ist korrekt?",
      "options": [
        "Es existiert keine QR-Zerlegung.",
        "Dünnes $Q$ hat $n$ orthonormale Spalten.",
        "R ist singulär.",
        "Die LS-Lösung existiert nicht.",
        "Nur SVD ist möglich."
      ],
      "answer": 1,
      "explanation": "Für vollen Spaltenrang existiert dünnes QR mit $Q\\in\\mathbb{R}^{m\\times n}$.",
      "weight": 2,
      "topic": "QR-Zerlegung Grundlagen",
      "extended_explanation": {
        "titel": "Ranganforderungen",
        "schritte": [
          "Voller Spaltenrang sichert eindeutige LS-Lösung.",
          "R besitzt nichtverschwindende Diagonale.",
          "Rückwärtseinsetzen ist anwendbar."
        ]
      },
      "mini_glossary": {
        "Voller Rang": "Maximale Anzahl linear unabhängiger Spalten.",
        "Eindeutigkeit": "LS-Lösung ist eindeutig bei vollem Spaltenrang."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "30. Welche Reihenfolge beschreibt das klassische Gram–Schmidt-Verfahren für Spalten $v_j$ korrekt?",
      "options": [
        "$u_1=\\dfrac{v_1}{\\|v_1\\|}$, dann $w_j=v_j-\\sum_{i<j}\\langle v_j,u_i\\rangle u_i$, $u_j=\\dfrac{w_j}{\\|w_j\\|}$.",
        "$w_j=v_j+\\sum_{i<j}\\langle v_j,u_i\\rangle u_i$, $u_j=\\dfrac{v_j}{\\|v_j\\|}$.",
        "$u_j=v_j$, $w_j=\\dfrac{v_j}{\\|v_j\\|}$.",
        "$u_j=\\sum_i v_i$, $w_j=u_j$.",
        "$u_1=v_1$, $u_j=v_j$."
      ],
      "answer": 0,
      "explanation": "Erst Orthogonalisieren zu $w_j$, dann Normieren zu $u_j$.",
      "weight": 2,
      "topic": "Gram–Schmidt",
      "extended_explanation": {
        "titel": "Rezept GS",
        "schritte": [
          "Projektionen schrittweise abziehen.",
          "Numerische Stabilität durch MGS verbessern.",
          "Normierungen vermeiden Skalierungsfehler."
        ]
      },
      "mini_glossary": {
        "Projektion": "Anteil eines Vektors in einer Richtung/Unterraum.",
        "Normierung": "Teilen durch die Länge, um Einheitsvektor zu erhalten."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "31. Gegeben $u=\\tfrac{1}{\\sqrt{5}}(1,2)^\\top$ und $x=(3,1)^\\top$. Welche Aussage trifft zu?",
      "options": [
        "$\\langle x,u\\rangle=1$ und $\\operatorname{proj}_u(x)=(1,2)^\\top$.",
        "$\\langle x,u\\rangle=\\sqrt{5}$ und $\\operatorname{proj}_u(x)=(1,2)^\\top$.",
        "$\\langle x,u\\rangle=\\sqrt{5}$ und $\\operatorname{proj}_u(x)=\\tfrac{1}{\\sqrt{5}}(1,2)^\\top$.",
        "$\\langle x,u\\rangle=5$ und $\\operatorname{proj}_u(x)=(1,2)^\\top$.",
        "$\\langle x,u\\rangle=\\tfrac{1}{\\sqrt{5}}$ und $\\operatorname{proj}_u(x)=(2,-1)^\\top$."
      ],
      "answer": 1,
      "explanation": "$\\langle x,u\\rangle=\\tfrac{1}{\\sqrt{5}}(3\\cdot1+1\\cdot2)=\\sqrt{5}$; die Projektion ist Skalar mal $u$ und ergibt $(1,2)^\\top$.",
      "weight": 3,
      "topic": "Skalarprodukt & Norm",
      "extended_explanation": {
        "titel": "Durchrechnen der Projektion",
        "schritte": [
          "Einheitsvektor prüfen: $\\|u\\|=1$.",
          "Skalarprodukt ausrechnen und mit $u$ multiplizieren.",
          "Rest $r=x-\\operatorname{proj}_u(x)=(2,-1)^\\top\\perp u$."
        ]
      },
      "mini_glossary": {
        "Projektion auf Gerade": "$\\operatorname{proj}_u(x)=\\langle x,u\\rangle u$ für Einheitsvektor $u$.",
        "Orthogonalitätsbedingung": "Restvektor steht senkrecht zur Zielrichtung."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "32. Für $A\\in\\mathbb{R}^{m\\times n}$ mit $m>n$ und dünnem QR, $A=QR$, sei $b\\in\\mathbb{R}^m$ und $y=Q^\\top b$. Welche Bedingung garantiert eine eindeutige LS-Lösung?",
      "options": [
        "Rang$(A)=n$ und Diagonale von $R$ ohne Nullen.",
        "Rang$(A)=m$ und $Q$ diagonal.",
        "$R$ symmetrisch positiv definit.",
        "$Q$ antisymmetrisch.",
        "$A$ orthogonal."
      ],
      "answer": 0,
      "explanation": "Voller Spaltenrang $n$ impliziert invertierbares $R$ (eindeutiges Rückwärtseinsetzen).",
      "weight": 3,
      "topic": "Least Squares via QR",
      "extended_explanation": {
        "titel": "Eindeutigkeit der Lösung",
        "schritte": [
          "Nichtverschwindende Diagonalelemente in $R$.",
          "Dann besitzt $Rx=y$ eine eindeutige Lösung.",
          "Rangmangel führt zu Nicht-Eindeutigkeit."
        ]
      },
      "mini_glossary": {
        "Rangbedingung": "Voller Spaltenrang $n$ sichert Eindeutigkeit.",
        "Rückwärtseinsetzen": "Stabiles Lösen von $Rx=y$."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "33. Welche Aussage zur numerischen Stabilität ist korrekt?",
      "options": [
        "Die Bildung von $A^\\top A$ verbessert die Kondition.",
        "Householder-QR ist in der Regel stabiler als klassisches GS.",
        "Givens ist ungeeignet für dünn besetzte Matrizen.",
        "QR verändert die Residuenorthogonalität.",
        "Rückwärtseinsetzen ist instabiler als Invertieren."
      ],
      "answer": 1,
      "explanation": "Householder nutzt orthogonale Spiegelungen und ist sehr robust.",
      "weight": 3,
      "topic": "QR-Konstruktion (Householder/Givens)",
      "extended_explanation": {
        "titel": "Stabilitätsvergleich",
        "schritte": [
          "$A^\\top A$ verschlechtert die Kondition.",
          "Givens ist vorteilhaft bei Sparse-Strukturen.",
          "Inversion meiden; Dreieckssysteme direkt lösen."
        ]
      },
      "mini_glossary": {
        "Kondition": "Numerische Empfindlichkeit eines Problems.",
        "Orthogonale Transformation": "Erhält Normen und verbessert Stabilität."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "34. Sei $U=[u_1\\,u_2]$ mit ONB-Spalten in $\\mathbb{R}^3$ und $x\\in\\mathbb{R}^3$. Welche Aussage ist korrekt?",
      "options": [
        "$x-Px$ liegt in $U$.",
        "$Px$ ist orthogonal zu $U$.",
        "$x-Px$ ist orthogonal zu $U$.",
        "$Px$ ist orthogonal zu $x$.",
        "$Px=x$ nur für $x\\perp U$."
      ],
      "answer": 2,
      "explanation": "Der Restvektor ist orthogonal zum Unterraum; $Px\\in U$.",
      "weight": 3,
      "topic": "Projektionen & Projektoren",
      "extended_explanation": {
        "titel": "Struktur von $P=UU^\\top$",
        "schritte": [
          "Symmetrisch und idempotent.",
          "Orthogonale Zerlegung $x=Px+(x-Px)$.",
          "Optimalitätseigenschaft der Projektion."
        ]
      },
      "mini_glossary": {
        "Bild($P$)": "Gleich dem Unterraum $U$.",
        "Kern($P$)": "Gleich $U^\\perp$."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "35. Bei QR über Householder wird mit Reflexionen gearbeitet. Welcher Effekt wird damit erzielt?",
      "options": [
        "Nullsetzen einer ganzen Unterspalte unterhalb der Diagonale in einem Schritt.",
        "Permutation der Spalten in gewünschter Reihenfolge.",
        "Diagonalisation von $A$ in einem Schritt.",
        "Berechnung der SVD direkt.",
        "Reduktion von $R$ zu einer unteren Dreiecksmatrix."
      ],
      "answer": 0,
      "explanation": "Eine Householder-Spiegelung eliminiert alle Einträge einer Spalte unterhalb der Diagonale.",
      "weight": 3,
      "topic": "QR-Konstruktion (Householder/Givens)",
      "extended_explanation": {
        "titel": "Householder-Mechanik",
        "schritte": [
          "Reflexion an Hyperebene richtet Vektor an Koordinatenachse aus.",
          "Erzeugt orthogonales $Q$.",
          "Wenige, aber globale Operationen."
        ]
      },
      "mini_glossary": {
        "Hyperebene": "Allgemeinisierte Ebene in höherer Dimension.",
        "Reflexion": "Spiegelung eines Vektors an einer Hyperebene."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "36. In der Praxis soll $R$ nicht invertiert werden. Welches Vorgehen ist korrekt?",
      "options": [
        "Löse $Rx=y$ per Rückwärtseinsetzen.",
        "Berechne $x=R^{-1}y$ explizit und multipliziere.",
        "Quadratiere $R$ und invertiere dann.",
        "Ersetze $R$ durch $Q$ und invertiere $Q$.",
        "Forme $R$ zu einer symmetrischen Matrix um."
      ],
      "answer": 0,
      "explanation": "Dreieckssysteme löst man numerisch stabil per Rückwärtseinsetzen.",
      "weight": 3,
      "topic": "Typische Fehler & Numerik",
      "extended_explanation": {
        "titel": "Warum nicht invertieren?",
        "schritte": [
          "Explizite Inversion verstärkt Rundungsfehler.",
          "Rückwärtseinsetzen nutzt die Dreiecksstruktur.",
          "Komplexität und Stabilität sind besser."
        ]
      },
      "mini_glossary": {
        "Explizite Inversion": "Direktes Berechnen der Inversen; zu vermeiden.",
        "Numerische Stabilität": "Robustheit gegen kleine Datenänderungen."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "37. Welche Bedingung garantiert, dass Gram–Schmidt eine ONB liefert?",
      "options": [
        "Die Eingangsvektoren sind linear unabhängig.",
        "Die Eingangsvektoren sind orthogonal.",
        "Die Matrix ist symmetrisch.",
        "Die Vektoren haben Norm 1.",
        "Die Dimension ist $m<n$."
      ],
      "answer": 0,
      "explanation": "GS setzt lineare Unabhängigkeit voraus; daraus wird eine ONB konstruiert.",
      "weight": 3,
      "topic": "Gram–Schmidt",
      "extended_explanation": {
        "titel": "Voraussetzungen für GS",
        "schritte": [
          "Abhängigkeiten führen zu $\\|w_j\\|=0$.",
          "MGS/Householder als Alternative bei Numerikproblemen.",
          "Normierung nach jeder Stufe ist notwendig."
        ]
      },
      "mini_glossary": {
        "Lineare Unabhängigkeit": "Kein Vektor ist Linearkombination der anderen.",
        "Orthogonalisierung": "Herstellen rechtwinkliger Richtungen."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "38. Welche Aussage zur Dreiecksungleichung ist korrekt und relevant für Projektionen?",
      "options": [
        "$\\|x+y\\|\\ge \\|x\\|+\\|y\\|$.",
        "$\\|x+y\\|\\le \\|x\\|+\\|y\\|$.",
        "$\\|x-y\\|=\\|x\\|\\,\\|y\\|$.",
        "$\\|x\\|=\\sum_i x_i$.",
        "$\\|x\\|=0$ nur für $x\\neq 0$."
      ],
      "answer": 1,
      "explanation": "Dreiecksungleichung: $\\|x+y\\|\\le \\|x\\|+\\|y\\|$.",
      "weight": 3,
      "topic": "Winkel & Cauchy-Schwarz",
      "extended_explanation": {
        "titel": "Relevanz für Fehlerabschätzungen",
        "schritte": [
          "Grundlage vieler Abschätzungen in der Analyse.",
          "Zusammen mit Cauchy–Schwarz konsistente Geometrie.",
          "Stabilität von Projektionen und Näherungen."
        ]
      },
      "mini_glossary": {
        "Minkowski-Ungleichung": "Allgemeine Form der Dreiecksungleichung für Normen.",
        "Euklidische Norm": "Aus dem Skalarprodukt induzierte Norm."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "39. Warum ist die QR-Zerlegung für schlecht konditionierte Spalten vorteilhaft?",
      "options": [
        "Weil $Q$ die Kondition verschlechtert.",
        "Weil $A^\\top A$ numerisch stabiler ist.",
        "Weil orthogonale Transformationen numerisch gut konditioniert sind.",
        "Weil $R$ zufällig ist.",
        "Weil Projektionen vermieden werden."
      ],
      "answer": 2,
      "explanation": "Orthogonale Transformationen ($Q$) erhalten Normen/Winkel und sind numerisch stabil.",
      "weight": 3,
      "topic": "Typische Fehler & Numerik",
      "extended_explanation": {
        "titel": "Numerische Robustheit",
        "schritte": [
          "Bildung von $A^\\top A$ führt oft zu Genauigkeitsverlusten.",
          "QR vermeidet das Quadrieren der Konditionszahl.",
          "Bewährt in LS-Problemen."
        ]
      },
      "mini_glossary": {
        "Schlechte Kondition": "Kleine Datenänderungen erzeugen große Lösungsschwankungen.",
        "Orthogonales Q": "Erhält Normen und verbessert die Stabilität."
      },
      "cognitive_level": "Analyse"
    }
  ]
}