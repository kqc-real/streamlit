{
  "meta": {
    "title": "Big Data",
    "created": "09.01.2026 00:00",
    "target_audience": "Informatikstudierende",
    "question_count": 20,
    "difficulty_profile": {
      "easy": 2,
      "medium": 12,
      "hard": 6
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 21
  },
  "questions": [
    {
      "question": "1. Welche Aussage beschreibt „Big Data“ am treffendsten?",
      "options": [
        "Daten, die wegen Umfang und Vielfalt nur mit skalierbarer Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen geringer Vielfalt nur mit manueller Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen Umfang und Vielfalt nur mit lokaler Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen geringer Vielfalt nur mit skalierbarer Verarbeitung sinnvoll nutzbar sind."
      ],
      "answer": 0,
      "explanation": "Big Data meint Daten, deren Umfang und Vielfalt klassische Einzelrechner-Ansätze überfordern. Entscheidend ist die Notwendigkeit skalierbarer Verarbeitung, um die Daten sinnvoll auszuwerten. Die anderen Optionen verdrehen entweder Vielfalt oder Skalierung.",
      "weight": 1,
      "topic": "Big Data",
      "concept": "Skalierbarkeit",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Big Data",
          "definition": "Daten- und Verarbeitungskontext, in dem Umfang und Vielfalt so groß sind, dass skalierbare Systeme nötig werden."
        },
        {
          "term": "Umfang",
          "definition": "Menge an Daten, die gespeichert und verarbeitet werden muss (z. B. sehr große Datenvolumina)."
        },
        {
          "term": "Vielfalt",
          "definition": "Heterogenität der Datenformen, z. B. strukturierte Tabellen, Logs, Texte oder Ereignisdaten."
        },
        {
          "term": "Skalierbare Verarbeitung",
          "definition": "Verarbeitung, die durch Verteilung auf mehrere Knoten wächst, statt nur auf einem einzelnen Rechner zu laufen."
        }
      ]
    },
    {
      "question": "2. Welche Aussage unterscheidet Data Lake und Data Warehouse am korrektesten?",
      "options": [
        "Ein Data Lake nutzt oft Schema-on-read, ein Data Warehouse nutzt typischerweise Schema-on-write.",
        "Ein Data Lake nutzt oft Schema-on-write, ein Data Warehouse nutzt typischerweise Schema-on-read.",
        "Ein Data Lake benötigt immer Schema-on-write, ein Data Warehouse benötigt immer Schema-on-read.",
        "Ein Data Lake und ein Data Warehouse nutzen typischerweise beide Schema-on-read."
      ],
      "answer": 0,
      "explanation": "Data Lakes speichern Rohdaten flexibel und interpretieren das Schema häufig erst beim Lesen (Schema-on-read). Data Warehouses modellieren Daten vorab für Analysen (Schema-on-write). Die übrigen Optionen vertauschen oder verabsolutieren die Konzepte.",
      "weight": 1,
      "topic": "Storage",
      "concept": "Schema-Strategie",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Data Lake",
          "definition": "Zentraler Speicher für Roh- und Halbstrukturierte Daten, oft mit späterer Interpretation je nach Use Case."
        },
        {
          "term": "Data Warehouse",
          "definition": "Für Analysen modellierter Datenbestand mit kuratierten, konsistenten Strukturen."
        },
        {
          "term": "Schema-on-read",
          "definition": "Das Schema wird beim Auslesen angewendet; Speicherung bleibt flexibel, Auswertung definiert die Struktur."
        },
        {
          "term": "Schema-on-write",
          "definition": "Das Schema wird vor dem Schreiben festgelegt; Daten werden beim Laden transformiert und validiert."
        }
      ]
    },
    {
      "question": "3. Du speicherst halbstrukturierte Logdaten für spätere Analysen. Welche Ablage ist am geeignetsten?",
      "options": [
        "Logs im Object Storage, gespeichert als Parquet, mit Schema-on-read für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Parquet, mit Schema-on-write für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Klartext, mit Schema-on-read für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Klartext, mit Schema-on-write für flexible Auswertung."
      ],
      "answer": 0,
      "explanation": "Parquet ist für analytische Abfragen effizient und Object Storage ist skalierbar. Für Logs ist Schema-on-read oft passend, weil Felder sich ändern können. Die anderen Optionen sind entweder ineffizient (Klartext) oder widersprüchlich (Schema-on-write als „flexibel“).",
      "weight": 2,
      "topic": "Storage",
      "concept": "Analytische Ablage",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum diese Ablage für Logs funktioniert",
        "steps": [
          "Wähle Object Storage, wenn du skalierbar und kosteneffizient speichern willst.",
          "Nutze Parquet, um Analysen durch spaltenorientierte Speicherung effizient zu machen.",
          "Setze Schema-on-read ein, wenn Logfelder sich im Zeitverlauf ändern.",
          "Definiere Auswertungs-Schemata in der Analyse, nicht beim Schreiben."
        ],
        "content": "Logs ändern sich häufig, daher passt Schema-on-read gut zum Log-Charakter. Parquet reduziert I/O und beschleunigt Analysen, besonders bei selektiven Abfragen. Object Storage bietet Skalierung ohne komplexe Clusterverwaltung für reine Speicherung."
      },
      "mini_glossary": [
        {
          "term": "Logs",
          "definition": "Ereignis- und Statusdaten aus Systemen/Anwendungen, oft halbstrukturiert und zeitlich geordnet."
        },
        {
          "term": "Object Storage",
          "definition": "Skalierbarer Speicher für Objekte/Dateien, typischerweise günstig und für große Datenmengen geeignet."
        },
        {
          "term": "Parquet",
          "definition": "Spaltenorientiertes Dateiformat, optimiert für analytische Abfragen und Kompression."
        },
        {
          "term": "Schema-on-read",
          "definition": "Schema wird erst beim Lesen angewendet; erleichtert flexible Nutzung sich ändernder Daten."
        }
      ]
    },
    {
      "question": "4. Du willst in Apache Kafka die Reihenfolge von Ereignissen pro Nutzer erhalten. Welche Konfiguration ist am passendsten?",
      "options": [
        "Apache Kafka so nutzen, dass der Key pro Nutzer gesetzt wird, damit alle Events pro Nutzer in einer Partition landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer zufällig gesetzt wird, damit alle Events pro Nutzer in einer Partition landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer gesetzt wird, damit alle Events pro Nutzer in mehreren Partitionen landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer zufällig gesetzt wird, damit alle Events pro Nutzer in mehreren Partitionen landen und Ordering erhalten bleibt."
      ],
      "answer": 0,
      "explanation": "In Kafka gilt Ordering zuverlässig innerhalb einer Partition. Mit einem stabilen Key pro Nutzer landen dessen Events konsistent in derselben Partition. Zufällige Keys oder mehrere Partitionen brechen die Ordnung pro Nutzer.",
      "weight": 2,
      "topic": "Kafka",
      "concept": "Partitionierung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Ordering pro Nutzer in Kafka",
        "steps": [
          "Nutze Kafka-Partitionen als Ordering-Grenze.",
          "Setze den Key stabil auf die Nutzer-ID, damit Routing deterministisch bleibt.",
          "Vermeide zufällige Keys, weil sie Events über Partitionen verteilen.",
          "Plane die Partition-Anzahl so, dass sie Parallelität erlaubt, ohne Ordering-Anforderungen zu verletzen."
        ],
        "content": "Kafka garantiert die Reihenfolge pro Partition, nicht über Partitionen hinweg. Deshalb ist ein stabiler Key die typische Lösung, wenn Ordering pro Entität (z. B. Nutzer) nötig ist. So bleiben Events pro Nutzer geordnet, während andere Nutzer parallel verarbeitet werden können."
      },
      "mini_glossary": [
        {
          "term": "Apache Kafka",
          "definition": "Verteiltes Log-System für Event-Streaming, bei dem Topics in Partitionen aufgeteilt werden."
        },
        {
          "term": "Partition",
          "definition": "Unterteilung eines Kafka-Topics; Ordering ist innerhalb einer Partition garantiert."
        },
        {
          "term": "Key",
          "definition": "Wert, der das Routing eines Events zu einer Partition bestimmt (typisch per Hashing)."
        },
        {
          "term": "Ordering",
          "definition": "Garantierte Reihenfolge von Events innerhalb einer Partition beim Lesen durch Konsumenten."
        }
      ]
    },
    {
      "question": "5. Du führst wiederholte Transformationen auf demselben Datensatz aus. Welche Verarbeitung ist am geeignetsten?",
      "options": [
        "Apache Spark nutzen, weil In-Memory-Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "Apache Spark nutzen, weil Disk-basierte Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "MapReduce nutzen, weil In-Memory-Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "MapReduce nutzen, weil Disk-basierte Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt."
      ],
      "answer": 0,
      "explanation": "Iterative Workloads profitieren stark von In-Memory-Verarbeitung. Spark ist dafür ausgelegt, Zwischenzustände im Speicher zu halten und Iteration zu beschleunigen. MapReduce ist typischerweise stärker disk-orientiert und daher für Iteration oft langsamer.",
      "weight": 2,
      "topic": "Processing",
      "concept": "Iterative Workloads",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Spark vs. MapReduce bei Iteration",
        "steps": [
          "Erkenne Iteration: derselbe Datenbestand wird mehrfach verarbeitet.",
          "Bevorzuge In-Memory-Verarbeitung, um wiederholte I/O-Kosten zu reduzieren.",
          "Wähle Spark, wenn du Transformationen als Pipeline wiederholt ausführen willst.",
          "Nutze MapReduce eher für streng batch-orientierte, weniger iterative Jobs."
        ],
        "content": "Bei iterativen Algorithmen ist das Wiederverwenden von Zwischenresultaten zentral. Spark kann diese im Speicher halten und vermeidet unnötiges Schreiben/Lesen zwischen Iterationsschritten. MapReduce-Workflows sind häufig stärker an diskbasierte Phasen gekoppelt und verlieren dadurch Zeit bei vielen Iterationen."
      },
      "mini_glossary": [
        {
          "term": "Apache Spark",
          "definition": "Verteiltes Verarbeitungssystem, das Transformationen über Daten resilient und oft speicherbasiert ausführt."
        },
        {
          "term": "MapReduce",
          "definition": "Batch-Verarbeitungsmodell mit Map- und Reduce-Phasen, häufig mit starkem Fokus auf diskbasierte Zwischenergebnisse."
        },
        {
          "term": "In-Memory-Verarbeitung",
          "definition": "Verarbeitung, bei der Daten/Zwischenergebnisse im Arbeitsspeicher gehalten werden, um I/O zu reduzieren."
        },
        {
          "term": "Iteration",
          "definition": "Wiederholte Anwendung von Verarbeitungsschritten auf denselben oder abgeleiteten Datenbestand."
        }
      ]
    },
    {
      "question": "6. Ein Join in einem verteilten Job ist sehr langsam, weil wenige Schlüssel extrem häufig auftreten. Welche Maßnahme ist am passendsten?",
      "options": [
        "Join-Skew adressieren, indem du Salting einsetzt oder einen Broadcast Join nutzt, um Daten-Skew zu reduzieren.",
        "Join-Skew adressieren, indem du Salting einsetzt oder einen Broadcast Join nutzt, um Daten-Skew zu erhöhen.",
        "Join-Skew adressieren, indem du die Anzahl der Joins reduzierst, um Daten-Skew zu reduzieren.",
        "Join-Skew adressieren, indem du die Anzahl der Joins reduzierst, um Daten-Skew zu erhöhen."
      ],
      "answer": 0,
      "explanation": "Wenn wenige Schlüssel dominieren, entsteht Daten-Skew und einzelne Tasks werden Hotspots. Salting verteilt schwere Schlüssel auf mehrere Partitionen, Broadcast Join vermeidet Shuffle für kleine Tabellen. Die anderen Optionen ändern das Problem nicht oder verschlechtern es.",
      "weight": 2,
      "topic": "Processing",
      "concept": "Skew-Handling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Skew bei Joins beheben",
        "steps": [
          "Erkenne Daten-Skew: einzelne Schlüssel erzeugen unverhältnismäßig viel Arbeit.",
          "Nutze Broadcast Join, wenn eine Join-Seite klein genug ist.",
          "Nutze Salting, um große Skew-Schlüssel auf mehrere Partitionen zu verteilen.",
          "Überprüfe danach die Task-Laufzeiten, um Hotspots zu verifizieren."
        ],
        "content": "Skew erzeugt ungleichmäßige Lastverteilung, was die Parallelität des Clusters aushebelt. Broadcast Join hilft, indem ein kleiner Datensatz repliziert wird und teure Shuffles vermieden werden. Salting hilft, wenn beide Seiten groß sind, indem es die Last pro Schlüssel künstlich verteilt."
      },
      "mini_glossary": [
        {
          "term": "Join",
          "definition": "Operation, die Datensätze anhand gemeinsamer Schlüssel zusammenführt."
        },
        {
          "term": "Daten-Skew",
          "definition": "Ungleichverteilung von Daten/Schlüsseln, die zu Hotspots und langen Task-Laufzeiten führt."
        },
        {
          "term": "Broadcast Join",
          "definition": "Join-Strategie, bei der ein kleiner Datensatz an alle Worker verteilt wird, um Shuffle zu vermeiden."
        },
        {
          "term": "Salting",
          "definition": "Technik, die Schlüssel künstlich erweitert, um Skew-Schlüssel auf mehrere Partitionen zu verteilen."
        }
      ]
    },
    {
      "question": "7. Du modellierst Time-Series-Daten aus Sensoren mit sehr hohem Schreibdurchsatz. Welche Datenmodellierung ist am geeignetsten?",
      "options": [
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Sensor und TTL für automatische Datenablaufregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Sensor und TTL für automatische Datenaufbauregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Messwert und TTL für automatische Datenablaufregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Messwert und TTL für automatische Datenaufbauregeln."
      ],
      "answer": 0,
      "explanation": "Für Time-Series sind Schreiblast und sequentielle Zugriffe pro Sensor typisch. Ein Partition Key pro Sensor hält zusammengehörige Daten logisch zusammen, TTL sorgt für automatisches Löschen alter Daten. Partition Key pro Messwert ist meist unpraktisch und zerlegt die Zugriffe.",
      "weight": 2,
      "topic": "NoSQL",
      "concept": "Time-Series-Modell",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum Cassandra gut zu Time-Series passt",
        "steps": [
          "Ordne Time-Series-Daten nach Sensor als natürliche Partitionseinheit.",
          "Wähle den Partition Key so, dass typische Abfragen effizient bleiben.",
          "Nutze TTL, um Retention automatisch umzusetzen.",
          "Vermeide zu feingranulare Partition Keys, die Abfragen fragmentieren."
        ],
        "content": "Time-Series-Workloads bestehen oft aus vielen Writes und Abfragen pro Entität (Sensor). Cassandra kann hohe Schreibdurchsätze liefern, wenn der Partition Key die Zugriffsmuster unterstützt. TTL vereinfacht Retention, weil Daten automatisch auslaufen, ohne zusätzliche Löschjobs."
      },
      "mini_glossary": [
        {
          "term": "Time-Series",
          "definition": "Zeitlich geordnete Messwerte, typischerweise mit Zeitstempel und Entität (z. B. Sensor)."
        },
        {
          "term": "Cassandra",
          "definition": "Verteilte Wide-Column-NoSQL-Datenbank, optimiert für hohe Schreiblasten und horizontale Skalierung."
        },
        {
          "term": "Partition Key",
          "definition": "Schlüssel, der bestimmt, wie Daten auf Knoten verteilt und gruppiert werden."
        },
        {
          "term": "TTL",
          "definition": "Time-to-live; Mechanismus, der Daten nach einer festgelegten Zeit automatisch entfernt."
        }
      ]
    },
    {
      "question": "8. Du willst analytische Abfragen beschleunigen, die nur wenige Spalten einer großen Tabelle benötigen. Welche Wahl ist am geeignetsten?",
      "options": [
        "Parquet als Spaltenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "Parquet als Zeilenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "JSON als Spaltenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "JSON als Zeilenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen."
      ],
      "answer": 0,
      "explanation": "Spaltenformate wie Parquet lesen nur die benötigten Spalten und profitieren von Kompression. Predicate Pushdown reduziert zusätzlich die gelesenen Daten. JSON ist für Analysen meist weniger effizient und kein echtes Spaltenformat.",
      "weight": 2,
      "topic": "Storage",
      "concept": "Columnar Analytics",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum Spaltenformate Analysen beschleunigen",
        "steps": [
          "Identifiziere selektive Abfragen, die nur wenige Spalten benötigen.",
          "Nutze Parquet als Spaltenformat, um unnötiges Lesen zu vermeiden.",
          "Aktiviere Predicate Pushdown, um Filter früh anzuwenden.",
          "Nutze Kompression, um I/O zu reduzieren."
        ],
        "content": "Analytische Workloads profitieren von spaltenorientierter Speicherung, weil sie typischerweise wenige Spalten über viele Zeilen aggregieren. Parquet unterstützt effizientes Lesen, Filterung und Kompression. Dadurch sinkt die gelesene Datenmenge und Abfragen laufen schneller."
      },
      "mini_glossary": [
        {
          "term": "Parquet",
          "definition": "Spaltenorientiertes Dateiformat, das für analytische Workloads optimiert ist."
        },
        {
          "term": "Spaltenformat",
          "definition": "Speicherlayout, bei dem Werte spaltenweise gespeichert werden, um selektives Lesen zu ermöglichen."
        },
        {
          "term": "Predicate Pushdown",
          "definition": "Optimierung, bei der Filterbedingungen so früh wie möglich angewendet werden, um weniger Daten zu lesen."
        },
        {
          "term": "Kompression",
          "definition": "Verdichtung von Daten zur Reduktion von Speicherplatz und I/O bei Lesen/Schreiben."
        }
      ]
    },
    {
      "question": "9. Du willst in einem Streaming-Job doppelte Verarbeitung vermeiden, auch bei Neustarts. Welche Strategie ist am passendsten?",
      "options": [
        "Exactly-once anstreben, indem du Offsets zuverlässig verwaltest und Transaktionen mit Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zufällig verwaltest und Transaktionen mit Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zuverlässig verwaltest und Transaktionen ohne Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zufällig verwaltest und Transaktionen ohne Idempotenz kombinierst."
      ],
      "answer": 0,
      "explanation": "Exactly-once erfordert robuste Offset-Verwaltung und atomische Verarbeitung/Commit-Logik. Transaktionen helfen, Konsum und Ausgabe konsistent zu koppeln, Idempotenz reduziert die Wirkung von Wiederholungen. Zufällige Offsets oder fehlende Idempotenz machen Duplikate wahrscheinlicher.",
      "weight": 2,
      "topic": "Streaming",
      "concept": "Exactly-once",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Wie Exactly-once praktisch erreicht wird",
        "steps": [
          "Verwalte Offsets deterministisch, damit Wiederanläufe an der richtigen Stelle fortsetzen.",
          "Nutze Transaktionen, um Lesen und Schreiben atomar zu koppeln.",
          "Nutze Idempotenz, um Wiederholungen ohne doppelte Wirkung zu machen.",
          "Teste Recovery-Szenarien, um Duplikate bei Neustarts zu prüfen."
        ],
        "content": "Exactly-once ist weniger ein einzelner Schalter als ein Zusammenspiel aus Offset-Management, atomischen Updates und idempotenten Schreiboperationen. Transaktionen sorgen für Konsistenz zwischen Eingangsfortschritt und Ausgangszustand. Idempotenz reduziert die Auswirkungen von Retrys, die in verteilten Systemen normal sind."
      },
      "mini_glossary": [
        {
          "term": "Exactly-once",
          "definition": "Verarbeitungsziel, bei dem jedes Event genau einmal wirksam wird, auch bei Fehlern und Neustarts."
        },
        {
          "term": "Offset",
          "definition": "Positionsmarker in einem Event-Log/Stream, der den konsumierten Fortschritt repräsentiert."
        },
        {
          "term": "Transaktionen",
          "definition": "Mechanismus für atomische, konsistente Änderungen, sodass Teilergebnisse nicht sichtbar werden."
        },
        {
          "term": "Idempotenz",
          "definition": "Eigenschaft, dass eine Operation bei Wiederholung denselben Effekt hat wie bei einmaliger Ausführung."
        }
      ]
    },
    {
      "question": "10. Du nutzt Windowing in Stream Processing und bekommst verspätete Events. Welche Einstellung ist am passendsten?",
      "options": [
        "Event Time mit Watermarking nutzen, damit Late Events kontrolliert in Window-Berechnungen einfließen können.",
        "Event Time mit Watermarking nutzen, damit Late Events kontrolliert aus Window-Berechnungen ausgeschlossen werden.",
        "Processing Time mit Watermarking nutzen, damit Late Events kontrolliert in Window-Berechnungen einfließen können.",
        "Processing Time mit Watermarking nutzen, damit Late Events kontrolliert aus Window-Berechnungen ausgeschlossen werden."
      ],
      "answer": 0,
      "explanation": "Bei verspäteten Events ist Event Time die relevante Zeitbasis. Watermarks markieren den Fortschritt und erlauben definierte Toleranzen für Late Events in Windows. Processing Time passt oft nicht zur inhaltlichen Ereigniszeit und führt zu verzerrten Aggregationen.",
      "weight": 2,
      "topic": "Streaming",
      "concept": "Windowing",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Late Events korrekt behandeln",
        "steps": [
          "Nutze Event Time, wenn die Ereigniszeit fachlich relevant ist.",
          "Setze Watermarks, um erwartete Verzögerungen abzubilden.",
          "Konfiguriere Windowing so, dass Late Events bis zur Watermark berücksichtigt werden.",
          "Definiere klar, was nach Ablauf der Watermark passieren soll (z. B. Drop oder Korrekturpfad)."
        ],
        "content": "Late Events entstehen durch Netzwerk- oder Systemverzögerungen. Event Time stellt sicher, dass Aggregationen nach der tatsächlichen Ereigniszeit erfolgen. Watermarks geben dem System eine kontrollierte Grenze, bis wann späte Events noch in Windows eingehen dürfen."
      },
      "mini_glossary": [
        {
          "term": "Event Time",
          "definition": "Zeitstempel des Ereignisses selbst, nicht der Verarbeitungszeit im System."
        },
        {
          "term": "Watermark",
          "definition": "Schätzwert/Marker, bis zu welchem Event-Time-Zeitpunkt Events als „weitgehend vollständig“ gelten."
        },
        {
          "term": "Late Events",
          "definition": "Events, die nach dem erwarteten Zeitpunkt eintreffen, z. B. durch Verzögerungen in der Pipeline."
        },
        {
          "term": "Window",
          "definition": "Zeitliches Aggregationsfenster, in dem Events gesammelt und berechnet werden."
        }
      ]
    },
    {
      "question": "11. Du speicherst Nutzerdaten für Analysen und willst DSGVO-konform arbeiten. Welche Aussage ist am passendsten?",
      "options": [
        "Pseudonymisierung reduziert den Personenbezug, während Anonymisierung den Personenbezug entfernt, wenn sie tatsächlich irreversibel ist.",
        "Pseudonymisierung entfernt den Personenbezug, während Anonymisierung den Personenbezug reduziert, wenn sie tatsächlich irreversibel ist.",
        "Pseudonymisierung reduziert den Personenbezug, während Anonymisierung den Personenbezug entfernt, wenn sie tatsächlich reversibel ist.",
        "Pseudonymisierung entfernt den Personenbezug, während Anonymisierung den Personenbezug reduziert, wenn sie tatsächlich reversibel ist."
      ],
      "answer": 0,
      "explanation": "Pseudonymisierung ersetzt Identifikatoren, lässt aber prinzipiell eine Re-Identifikation zu und bleibt daher oft personenbezogen. Anonymisierung entfernt den Personenbezug nur dann, wenn sie wirklich irreversibel ist. Die anderen Optionen vertauschen oder verwässern diese Abgrenzung.",
      "weight": 2,
      "topic": "Governance",
      "concept": "Datenschutz",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Pseudonymisierung vs. Anonymisierung",
        "steps": [
          "Bestimme, ob ein Personenbezug noch hergestellt werden kann.",
          "Nutze Pseudonymisierung, wenn du Identifikatoren ersetzen willst, aber ggf. Rückbezug brauchst.",
          "Nutze Anonymisierung nur, wenn Re-Identifikation praktisch ausgeschlossen ist.",
          "Dokumentiere Verarbeitung und Schutzmaßnahmen im DSGVO-Kontext."
        ],
        "content": "DSGVO-Konformität hängt stark am Personenbezug. Pseudonymisierung ist ein Sicherheits- und Minimierungsmechanismus, beseitigt aber den Personenbezug nicht automatisch. Anonymisierung ist nur dann wirklich anonym, wenn eine Re-Identifikation nicht mehr möglich ist."
      },
      "mini_glossary": [
        {
          "term": "DSGVO",
          "definition": "Datenschutz-Grundverordnung; EU-Regelwerk zur Verarbeitung personenbezogener Daten."
        },
        {
          "term": "Personenbezug",
          "definition": "Bezug einer Information zu einer identifizierten oder identifizierbaren Person."
        },
        {
          "term": "Pseudonymisierung",
          "definition": "Ersetzung direkter Identifikatoren durch Pseudonyme, sodass eine Zuordnung nur mit Zusatzinformation möglich ist."
        },
        {
          "term": "Anonymisierung",
          "definition": "Entfernung des Personenbezugs, sodass eine Re-Identifikation nicht mehr möglich ist."
        }
      ]
    },
    {
      "question": "12. Du willst in einer Pipeline früh Datenfehler erkennen. Welche Maßnahme ist am passendsten?",
      "options": [
        "Data Quality durch Schema-Validierung und Constraints absichern, damit fehlerhafte Daten früh blockiert oder markiert werden.",
        "Data Quality durch Schema-Validierung und Constraints absichern, damit fehlerhafte Daten spät blockiert oder markiert werden.",
        "Data Quality durch manuelle Sichtprüfung absichern, damit fehlerhafte Daten früh blockiert oder markiert werden.",
        "Data Quality durch manuelle Sichtprüfung absichern, damit fehlerhafte Daten spät blockiert oder markiert werden."
      ],
      "answer": 0,
      "explanation": "Automatisierte Schema-Validierung und Constraints sind skalierbar und erkennen Fehler früh. Das verhindert, dass schlechte Daten downstream Analysen verfälschen. Manuelle Sichtprüfung ist bei Big Data nicht zuverlässig skalierbar.",
      "weight": 2,
      "topic": "Data Quality",
      "concept": "Validierung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Frühe Data-Quality-Gates",
        "steps": [
          "Definiere ein Schema, das Typen und Felder eindeutig beschreibt.",
          "Formuliere Constraints, z. B. Wertebereiche oder Nicht-Null-Regeln.",
          "Validiere eingehende Daten automatisch beim Ingest oder vor kritischen Transformationen.",
          "Leite fehlerhafte Daten in Quarantäne oder markiere sie für Korrektur."
        ],
        "content": "Data Quality ist in Big-Data-Pipelines ein Systemthema, kein manueller Prozess. Schema-Validierung erkennt strukturelle Abweichungen, Constraints erkennen inhaltliche Verletzungen. Beides ermöglicht frühzeitige, reproduzierbare Qualitätssicherung."
      },
      "mini_glossary": [
        {
          "term": "Data Quality",
          "definition": "Maß dafür, ob Daten korrekt, vollständig, konsistent und für den Zweck geeignet sind."
        },
        {
          "term": "Schema",
          "definition": "Strukturbeschreibung von Daten, z. B. Felder, Datentypen und erlaubte Formen."
        },
        {
          "term": "Validierung",
          "definition": "Automatisiertes Prüfen, ob Daten einem Schema und Regeln entsprechen."
        },
        {
          "term": "Constraint",
          "definition": "Regel/Restriktion, z. B. Wertebereich, Eindeutigkeit oder Nicht-Null-Anforderung."
        }
      ]
    },
    {
      "question": "13. Du willst in Elasticsearch häufig Aggregationen über große Datenmengen ausführen. Welche Konfiguration ist am geeignetsten?",
      "options": [
        "Elasticsearch so modellieren, dass das Mapping Aggregation-geeignete Felder definiert und der Inverted Index effiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-geeignete Felder definiert und der Inverted Index ineffiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-ungeeignete Felder definiert und der Inverted Index effiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-ungeeignete Felder definiert und der Inverted Index ineffiziente Abfragen unterstützt."
      ],
      "answer": 0,
      "explanation": "Für Aggregationen müssen Felder passend gemappt sein, damit sie effizient aggregierbar sind. Der Inverted Index unterstützt schnelle Suche und Filter, was Aggregationen vorbereitet. Falsches Mapping bremst oder verhindert sinnvolle Aggregationen.",
      "weight": 2,
      "topic": "Search",
      "concept": "Aggregation Modeling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Aggregationen in Elasticsearch effizient machen",
        "steps": [
          "Definiere im Mapping, welche Felder für Aggregation gedacht sind.",
          "Nutze den Inverted Index für schnelle Filterung vor der Aggregation.",
          "Vermeide ungeeignete Feldtypen für Aggregationen.",
          "Teste Aggregationen mit realistischen Query-Patterns und Datenvolumen."
        ],
        "content": "Elasticsearch lebt von korrekter Feldmodellierung. Wenn das Mapping Aggregationsfelder sauber definiert, können Aggregationen effizient ausgeführt werden, während der Inverted Index Filter und Suche beschleunigt. Unpassende Feldtypen führen zu teuren Workarounds oder unbrauchbaren Ergebnissen."
      },
      "mini_glossary": [
        {
          "term": "Elasticsearch",
          "definition": "Such- und Analyse-Engine, die Daten indexiert und Abfragen sowie Aggregationen schnell ausführt."
        },
        {
          "term": "Mapping",
          "definition": "Feld- und Typdefinition in Elasticsearch, die bestimmt, wie Daten indexiert und genutzt werden."
        },
        {
          "term": "Inverted Index",
          "definition": "Indexstruktur, die von Begriffen auf Dokumente verweist und schnelle Suche/Filter ermöglicht."
        },
        {
          "term": "Aggregation",
          "definition": "Zusammenfassung/Statistik über Daten, z. B. Counts, Summen oder Verteilungen nach Gruppen."
        }
      ]
    },
    {
      "question": "14. Du brauchst hohe Haltbarkeit in Object Storage und willst Kosten senken. Welche Wahl ist am passendsten?",
      "options": [
        "Erasure Coding statt vollständiger Replikation nutzen, um Haltbarkeit mit weniger Speicher-Overhead zu erreichen.",
        "Erasure Coding statt vollständiger Replikation nutzen, um Haltbarkeit mit mehr Speicher-Overhead zu erreichen.",
        "Vollständige Replikation statt Erasure Coding nutzen, um Haltbarkeit mit weniger Speicher-Overhead zu erreichen.",
        "Vollständige Replikation statt Erasure Coding nutzen, um Haltbarkeit mit mehr Speicher-Overhead zu erreichen."
      ],
      "answer": 0,
      "explanation": "Erasure Coding verteilt Redundanz effizienter als vollständige Replikation und senkt typischerweise den Speicher-Overhead bei hoher Haltbarkeit. Replikation ist einfacher, aber meist speicherintensiver. Die anderen Optionen verdrehen diese Zusammenhänge.",
      "weight": 2,
      "topic": "Storage",
      "concept": "Durability vs Cost",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Redundanzstrategien für Haltbarkeit",
        "steps": [
          "Definiere die geforderte Haltbarkeit für dein Object Storage.",
          "Vergleiche Replikation und Erasure Coding hinsichtlich Speicher-Overhead.",
          "Wähle Erasure Coding, wenn du Haltbarkeit bei geringerem Overhead brauchst.",
          "Berücksichtige Wiederherstellungs- und Latenz-Tradeoffs je nach System."
        ],
        "content": "Erasure Coding erreicht Haltbarkeit durch kodierte Redundanz über mehrere Fragmente und reduziert damit häufig den Speicherbedarf gegenüber Mehrfachkopien. Replikation ist konzeptionell simpel, benötigt aber meist mehr Speicher. In vielen Object-Storage-Systemen wird Erasure Coding daher für kosteneffiziente Haltbarkeit eingesetzt."
      },
      "mini_glossary": [
        {
          "term": "Object Storage",
          "definition": "Skalierbarer Speicher für Objekte/Dateien, häufig mit eingebauter Redundanz für Haltbarkeit."
        },
        {
          "term": "Haltbarkeit",
          "definition": "Wahrscheinlichkeit, dass Daten über Zeit nicht verloren gehen, typischerweise durch Redundanzmechanismen."
        },
        {
          "term": "Replikation",
          "definition": "Speichern mehrerer vollständiger Kopien derselben Daten auf unterschiedlichen Knoten/Standorten."
        },
        {
          "term": "Erasure Coding",
          "definition": "Kodierung, bei der Daten in Fragmente plus Redundanzfragmente zerlegt werden, um Verluste zu tolerieren."
        }
      ]
    },
    {
      "question": "15. Du betreibst nur Stream Processing und willst die Komplexität minimieren. Welche Architektur ist am passendsten?",
      "options": [
        "Kappa Architecture statt Lambda Architecture wählen, weil Stream Processing den Datenpfad vereinheitlicht und Batch entfällt.",
        "Kappa Architecture statt Lambda Architecture wählen, weil Stream Processing den Datenpfad aufspaltet und Batch entfällt.",
        "Lambda Architecture statt Kappa Architecture wählen, weil Stream Processing den Datenpfad vereinheitlicht und Batch entfällt.",
        "Lambda Architecture statt Kappa Architecture wählen, weil Stream Processing den Datenpfad aufspaltet und Batch entfällt."
      ],
      "answer": 0,
      "explanation": "Kappa setzt auf einen einheitlichen Stream-Pfad und vermeidet getrennte Batch- und Speed-Layer. Lambda führt typischerweise einen Batch- und einen Stream-Pfad parallel, was Komplexität erhöht. Wenn Batch nicht benötigt wird, passt Kappa oft besser.",
      "weight": 3,
      "topic": "Architecture",
      "concept": "Lambda vs Kappa",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Wann Kappa einfacher ist als Lambda",
        "steps": [
          "Prüfe, ob du einen separaten Batch-Pfad wirklich brauchst.",
          "Bewerte die Komplexität von zwei parallelen Datenpfaden in Lambda.",
          "Wähle Kappa, wenn Stream Processing alle Anforderungen abdeckt.",
          "Stelle sicher, dass Reprocessing und Backfills über den Stream-Pfad sauber möglich sind."
        ],
        "content": "Lambda Architecture trennt Batch und Stream, um sowohl Genauigkeit als auch niedrige Latenz abzudecken, zahlt aber mit zwei Pfaden und doppelter Logik. Kappa Architecture reduziert diese Komplexität, indem sie Stream Processing zum zentralen Pfad macht. Das ist besonders attraktiv, wenn Batch-Anforderungen gering sind oder anders gelöst werden können."
      },
      "mini_glossary": [
        {
          "term": "Lambda Architecture",
          "definition": "Architektur mit getrenntem Batch- und Stream-Pfad, die Ergebnisse zusammenführt."
        },
        {
          "term": "Kappa Architecture",
          "definition": "Architektur mit einem einheitlichen Stream-Pfad, in dem auch Reprocessing über den Stream erfolgt."
        },
        {
          "term": "Stream Processing",
          "definition": "Kontinuierliche Verarbeitung von Events, typischerweise mit geringer Latenz."
        },
        {
          "term": "Batch",
          "definition": "Periodische Verarbeitung großer Datenmengen in diskreten Jobs mit höherer Latenz."
        }
      ]
    },
    {
      "question": "16. Ein System erlebt eine Partition. Welche Aussage ist im Sinne des CAP-Theorems am passendsten?",
      "options": [
        "Bei Partitionstoleranz muss ein System zwischen Konsistenz und Verfügbarkeit abwägen, weil nicht beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System zwischen Konsistenz und Verfügbarkeit abwägen, weil beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System Konsistenz und Verfügbarkeit nicht abwägen, weil nicht beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System Konsistenz und Verfügbarkeit nicht abwägen, weil beides gleichzeitig garantiert werden kann."
      ],
      "answer": 0,
      "explanation": "CAP beschreibt den Tradeoff zwischen Konsistenz und Verfügbarkeit unter Partitionen. Wenn Partitionstoleranz nötig ist, kann das System bei einer Partition nicht gleichzeitig starke Konsistenz und volle Verfügbarkeit garantieren. Die falschen Optionen widersprechen diesem Grundsatz.",
      "weight": 3,
      "topic": "Distributed Systems",
      "concept": "CAP Tradeoff",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "CAP-Theorem unter Partitionen",
        "content": "Unter Netzwerkpartitionen sind vollständige Garantien schwer. Systeme müssen entscheiden, ob sie lieber konsistente Antworten liefern (und dann ggf. Requests ablehnen) oder lieber verfügbar bleiben (und dann ggf. inkonsistente Zustände zulassen). Diese Entscheidung sollte aus den Anforderungen und Fehlertoleranzen abgeleitet werden.",
        "steps": [
          "Nimm Partitionstoleranz als realistische Anforderung in verteilten Systemen an.",
          "Entscheide, ob Konsistenz oder Verfügbarkeit im Partition-Fall wichtiger ist.",
          "Passe Datenmodell und Client-Verhalten an die gewählte Priorität an.",
          "Dokumentiere die Semantik, damit Nutzer die Tradeoffs verstehen."
        ]
      },
      "mini_glossary": [
        {
          "term": "CAP-Theorem",
          "definition": "Aussage, dass bei Partitionen Konsistenz und Verfügbarkeit nicht gleichzeitig vollständig garantiert werden können."
        },
        {
          "term": "Konsistenz",
          "definition": "Eigenschaft, dass alle Clients einen logisch einheitlichen Zustand sehen, z. B. nach einem Write."
        },
        {
          "term": "Verfügbarkeit",
          "definition": "Eigenschaft, dass das System auf Requests antwortet, auch bei Teilfehlern."
        },
        {
          "term": "Partitionstoleranz",
          "definition": "Eigenschaft, dass das System trotz Netzwerkpartitionen weiter funktioniert (mit Tradeoffs)."
        }
      ]
    },
    {
      "question": "17. Du willst Data Lake und Warehouse vereinen, inklusive Transaktionssicherheit. Welche Aussage ist am passendsten?",
      "options": [
        "Ein Lakehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Analysen zusammenzubringen.",
        "Ein Lakehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Schreibzugriffe zu trennen.",
        "Ein Warehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Analysen zusammenzubringen.",
        "Ein Warehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Schreibzugriffe zu trennen."
      ],
      "answer": 0,
      "explanation": "Lakehouse ist ein Konzept, das Data-Lake-Flexibilität mit Warehouse-ähnlicher Zuverlässigkeit verbindet. ACID-Eigenschaften und ein Metadatenkatalog unterstützen Governance und konsistente Analysen. Die anderen Optionen verwechseln Begriffe oder Ziele.",
      "weight": 3,
      "topic": "Governance",
      "concept": "Lakehouse",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Warum Lakehouse ein Brückenkonzept ist",
        "steps": [
          "Nutze Data Lake als flexible Ablage für Roh- und Kurationsdaten.",
          "Füge ACID-Eigenschaften hinzu, um konsistente Tabellen-Operationen zu ermöglichen.",
          "Setze einen Metadatenkatalog ein, um Governance und Auffindbarkeit zu verbessern.",
          "Bewerte, ob Warehouse-Anforderungen (z. B. SLAs) durch das Lakehouse erfüllt werden."
        ],
        "content": "Ein reiner Data Lake ist flexibel, aber kann ohne zusätzliche Mechanismen in Governance und Konsistenz schwächeln. Lakehouse adressiert genau diese Lücke durch ACID-ähnliche Semantik und Katalogisierung. So können Daten kuratiert, nachvollziehbar und analytisch nutzbar werden, ohne die Flexibilität vollständig aufzugeben."
      },
      "mini_glossary": [
        {
          "term": "Lakehouse",
          "definition": "Architekturidee, die Data-Lake-Speicherung mit Warehouse-ähnlichen Eigenschaften für Analysen und Governance verbindet."
        },
        {
          "term": "Data Lake",
          "definition": "Flexibler Speicher für Roh- und Halbstrukturierte Daten, häufig mit Schema-on-read."
        },
        {
          "term": "ACID",
          "definition": "Transaktionseigenschaften (Atomarität, Konsistenz, Isolation, Dauerhaftigkeit) für zuverlässige Datenoperationen."
        },
        {
          "term": "Metadatenkatalog",
          "definition": "Verzeichnis, das Datenbestände beschreibt, auffindbar macht und Governance unterstützt."
        }
      ]
    },
    {
      "question": "18. Ein Modell verschlechtert sich im Betrieb, obwohl der Code unverändert ist. Welche Analyse ist am passendsten?",
      "options": [
        "Monitoring auf Data Drift und Concept Drift einführen, um Veränderungen der Eingabedaten und der Zielbeziehung zu erkennen.",
        "Monitoring auf Data Drift und Concept Drift einführen, um Veränderungen der Eingabedaten und der Zielbeziehung zu verhindern.",
        "Monitoring auf Data Drift ohne Concept Drift einführen, um Veränderungen der Eingabedaten und der Zielbeziehung zu erkennen.",
        "Monitoring auf Data Drift ohne Concept Drift einführen, um Veränderungen der Eingabedaten und der Zielbeziehung zu verhindern."
      ],
      "answer": 0,
      "explanation": "Modelle können schlechter werden, wenn sich die Datenverteilung (Data Drift) oder die Beziehung zwischen Features und Ziel (Concept Drift) ändert. Monitoring macht diese Veränderungen sichtbar und unterstützt Gegenmaßnahmen. Drift zu „verhindern“ ist meist nicht realistisch, sondern man erkennt und reagiert.",
      "weight": 3,
      "topic": "ML Operations",
      "concept": "Drift",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Drift als Ursache für Modellabfall",
        "steps": [
          "Überwache Data Drift, um Verteilungsänderungen der Eingabedaten zu erkennen.",
          "Überwache Concept Drift, um Änderungen der Zielbeziehung zu erkennen.",
          "Nutze Monitoring, um Schwellenwerte und Alarme zu definieren.",
          "Plane Re-Training oder Anpassungen, wenn Drift die Performance beeinflusst."
        ],
        "content": "Im Betrieb ändern sich Umgebungen: Nutzerverhalten, Sensoren oder Prozesse. Data Drift zeigt, dass sich Inputs verschieben, Concept Drift zeigt, dass die „Bedeutung“ der Inputs für das Ziel kippt. Monitoring liefert die Diagnosebasis, um Retraining, Feature-Anpassung oder Modellwechsel begründet auszulösen."
      },
      "mini_glossary": [
        {
          "term": "Data Drift",
          "definition": "Änderung der Verteilung von Eingabedaten im Vergleich zur Trainingsphase."
        },
        {
          "term": "Concept Drift",
          "definition": "Änderung der Beziehung zwischen Eingabedaten und Zielvariable über Zeit."
        },
        {
          "term": "Monitoring",
          "definition": "Kontinuierliche Messung und Überwachung von Daten- und Modellmetriken im Betrieb."
        },
        {
          "term": "Feature",
          "definition": "Eingangsmerkmal/Variable, die als Input für ein Modell genutzt wird."
        }
      ]
    },
    {
      "question": "19. Du verteilst Daten auf Shards. Welche Aussage ist am passendsten, wenn du Hotspots vermeiden willst und gleichmäßige Last anstrebst?",
      "options": [
        "Hash-Sharding statt Range-Sharding wählen, weil Hash-Sharding Hotspot-Risiken reduziert und gleichmäßige Verteilung fördert.",
        "Hash-Sharding statt Range-Sharding wählen, weil Hash-Sharding Hotspot-Risiken erhöht und gleichmäßige Verteilung fördert.",
        "Range-Sharding statt Hash-Sharding wählen, weil Range-Sharding Hotspot-Risiken reduziert und gleichmäßige Verteilung fördert.",
        "Range-Sharding statt Hash-Sharding wählen, weil Range-Sharding Hotspot-Risiken erhöht und gleichmäßige Verteilung fördert."
      ],
      "answer": 0,
      "explanation": "Hash-Sharding verteilt Keys typischerweise gleichmäßiger und reduziert Hotspots bei sequentiellen oder konzentrierten Key-Bereichen. Range-Sharding ist gut für Range-Queries, kann aber Hotspots erzeugen, wenn bestimmte Ranges sehr aktiv sind. Die richtige Wahl hängt von Query-Pattern ab, aber für Hotspot-Vermeidung ist Hash oft vorteilhaft.",
      "weight": 3,
      "topic": "Distributed Systems",
      "concept": "Sharding Tradeoff",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Sharding: Verteilung vs. Range-Queries",
        "steps": [
          "Bewerte, ob Hotspots durch ungleichmäßige Key-Verteilung entstehen.",
          "Nutze Hash-Sharding, wenn gleichmäßige Verteilung Priorität hat.",
          "Nutze Range-Sharding, wenn Range-Queries zentral sind und du Hotspots kontrollieren kannst.",
          "Teste Verteilung und Last mit realistischen Zugriffsmustern."
        ],
        "content": "Hash-Sharding verteilt Daten anhand eines Hashes und erzeugt typischerweise gleichmäßige Shard-Auslastung. Range-Sharding gruppiert zusammenhängende Keys, was Range-Queries erleichtert, aber Hotspots begünstigen kann. Deshalb ist die Entscheidung ein Tradeoff zwischen Zugriffsmustern und Lastverteilung."
      },
      "mini_glossary": [
        {
          "term": "Sharding",
          "definition": "Horizontale Partitionierung, bei der Daten auf mehrere Shards/Partitionen verteilt werden."
        },
        {
          "term": "Hash-Sharding",
          "definition": "Sharding, bei dem eine Hashfunktion den Ziel-Shard bestimmt, oft mit gleichmäßiger Verteilung."
        },
        {
          "term": "Range-Sharding",
          "definition": "Sharding, bei dem zusammenhängende Key-Bereiche einem Shard zugeordnet werden."
        },
        {
          "term": "Hotspot",
          "definition": "Überlastung eines Shards/Knotens durch ungleichmäßige Daten- oder Zugriffskonzentration."
        }
      ]
    },
    {
      "question": "20. Du willst veröffentlichte Statistik-Ergebnisse schützen, ohne sie unbrauchbar zu machen. Welche Aussage ist am passendsten?",
      "options": [
        "Differential Privacy nutzt Rauschen und ein Privacy Budget, um Utility zu erhalten und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen und ein Privacy Budget, um Utility zu entfernen und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen ohne Privacy Budget, um Utility zu erhalten und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen ohne Privacy Budget, um Utility zu entfernen und individuelle Beiträge zu schützen."
      ],
      "answer": 0,
      "explanation": "Differential Privacy fügt kontrolliertes Rauschen hinzu und steuert den Schutz über ein Privacy Budget. Ziel ist ein sinnvolles Gleichgewicht zwischen Utility und Datenschutz. Ohne Budget fehlt eine klare Steuerung der Schutzstärke, und „Utility entfernen“ ist nicht das Ziel.",
      "weight": 3,
      "topic": "Privacy",
      "concept": "Differential Privacy",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Utility und Datenschutz balancieren",
        "steps": [
          "Definiere, welche Statistik-Ergebnisse du veröffentlichen willst.",
          "Füge Rauschen so hinzu, dass einzelne Beiträge nicht zuverlässig ableitbar sind.",
          "Steuere die Schutzstärke über ein Privacy Budget.",
          "Überprüfe Utility, damit Ergebnisse weiterhin interpretierbar bleiben."
        ],
        "content": "Differential Privacy ist ein formaler Ansatz, der den Einfluss einzelner Datensätze auf veröffentlichte Statistiken begrenzt. Rauschen sorgt für Schutz, das Privacy Budget steuert die Stärke und akkumuliert über Abfragen. Der Kern ist das Abwägen von Utility und Schutz, nicht das vollständige Unbrauchbarmachen der Ergebnisse."
      },
      "mini_glossary": [
        {
          "term": "Differential Privacy",
          "definition": "Datenschutzkonzept, das den Einfluss einzelner Datensätze auf veröffentlichte Ergebnisse mathematisch begrenzt."
        },
        {
          "term": "Rauschen",
          "definition": "Gezielte Zufallsstörung von Ergebnissen, um Rückschlüsse auf einzelne Beiträge zu erschweren."
        },
        {
          "term": "Privacy Budget",
          "definition": "Parameter, der die Gesamtmenge an „Datenschutzverbrauch“ über Abfragen beschreibt und die Schutzstärke steuert."
        },
        {
          "term": "Utility",
          "definition": "Nützlichkeit/Aussagekraft der veröffentlichten Ergebnisse trotz Datenschutzmechanismen."
        }
      ]
    }
  ]
}