{
  "meta": {
    "schema_version": "1.1",
    "title": "Deep Learning Fundamentals",
    "created": "02.01.2026 12:00",
    "target_audience": "Computer Science students",
    "question_count": 20,
    "difficulty_profile": {
      "easy": 6,
      "medium": 8,
      "hard": 6
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 18
  },
  "questions": [
    {
      "question_id": "Q001",
      "question": "1. What is the primary difference between traditional machine learning and deep learning?",
      "options": [
        "Deep learning requires more computational resources",
        "Deep learning can automatically learn hierarchical features from data",
        "Deep learning works only with structured data",
        "Deep learning eliminates the need for data preprocessing"
      ],
      "answer": 1,
      "explanation": "Deep learning differs from traditional machine learning by automatically learning hierarchical features from raw data through multiple layers of neural networks, rather than requiring manual feature engineering.",
      "weight": 1,
      "topic": "Neural Network Fundamentals",
      "concept": "Feature Learning Hierarchy",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Feature Learning",
          "definition": "The process by which deep learning models automatically extract and learn relevant features from raw input data"
        },
        {
          "term": "Hierarchical Features",
          "definition": "Features that are learned at different levels of abstraction, from simple edges to complex object recognition"
        }
      ]
    },
    {
      "question_id": "Q002",
      "question": "2. Which activation function is commonly used in the output layer for binary classification tasks in deep learning?",
      "options": [
        "ReLU",
        "Sigmoid",
        "Tanh",
        "Softmax"
      ],
      "answer": 1,
      "explanation": "The sigmoid activation function outputs values between 0 and 1, making it suitable for binary classification where the output represents probability.",
      "weight": 1,
      "topic": "Neural Network Fundamentals",
      "concept": "Activation Functions",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Activation Function",
          "definition": "A mathematical function applied to the output of each neuron that introduces non-linearity into the network"
        },
        {
          "term": "Sigmoid",
          "definition": "An S-shaped activation function that maps input values to a range between 0 and 1"
        }
      ]
    },
    {
      "question_id": "Q003",
      "question": "3. What does CNN stand for in the context of deep learning?",
      "options": [
        "Central Neural Network",
        "Convolutional Neural Network",
        "Computational Neural Network",
        "Connected Neural Network"
      ],
      "answer": 1,
      "explanation": "CNN stands for Convolutional Neural Network, a specialized type of neural network designed for processing grid-like data such as images.",
      "weight": 1,
      "topic": "Network Architectures",
      "concept": "CNN Architecture",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Convolutional Neural Network",
          "definition": "A deep learning architecture specifically designed for processing grid-structured data like images"
        },
        {
          "term": "Grid-like Data",
          "definition": "Data that has a spatial structure, such as pixels in an image arranged in a 2D grid"
        }
      ]
    },
    {
      "question_id": "Q004",
      "question": "4. Which optimization algorithm is most commonly used for training deep neural networks?",
      "options": [
        "Gradient Descent",
        "Adam",
        "Newton's Method",
        "Genetic Algorithms"
      ],
      "answer": 1,
      "explanation": "Adam (Adaptive Moment Estimation) is the most widely used optimization algorithm for training deep neural networks due to its adaptive learning rates and momentum.",
      "weight": 1,
      "topic": "Training and Optimization",
      "concept": "Optimization Methods",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Adam Optimizer",
          "definition": "An adaptive optimization algorithm that combines the benefits of AdaGrad and RMSProp"
        },
        {
          "term": "Adaptive Learning Rate",
          "definition": "A learning rate that adjusts itself during training based on the historical gradients"
        }
      ]
    },
    {
      "question_id": "Q005",
      "question": "5. What is the purpose of dropout in deep neural networks?",
      "options": [
        "To reduce training time",
        "To prevent overfitting",
        "To increase model complexity",
        "To speed up inference"
      ],
      "answer": 1,
      "explanation": "Dropout randomly deactivates neurons during training to prevent overfitting by forcing the network to learn redundant representations.",
      "weight": 1,
      "topic": "Training and Optimization",
      "concept": "Overfitting Prevention",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Dropout",
          "definition": "A regularization technique that randomly drops neurons during training to prevent overfitting"
        },
        {
          "term": "Overfitting",
          "definition": "When a model learns the training data too well and performs poorly on unseen data"
        }
      ]
    },
    {
      "question_id": "Q006",
      "question": "6. What does RNN stand for and what makes it different from feedforward neural networks?",
      "options": [
        "Recurrent Neural Network; it has feedback connections",
        "Recursive Neural Network; it processes hierarchical data",
        "Regional Neural Network; it focuses on local features",
        "Random Neural Network; it uses stochastic connections"
      ],
      "answer": 0,
      "explanation": "RNN stands for Recurrent Neural Network and differs from feedforward networks by having feedback connections that allow it to maintain memory of previous inputs.",
      "weight": 1,
      "topic": "Sequence Processing",
      "concept": "Temporal Dependencies",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Recurrent Neural Network",
          "definition": "A type of neural network designed to process sequential data by maintaining an internal state"
        },
        {
          "term": "Feedback Connections",
          "definition": "Connections that allow information to flow backwards in time within the network"
        }
      ]
    },
    {
      "question_id": "Q007",
      "question": "7. You are training a neural network and notice that the validation loss is increasing while training loss decreases. What regularization technique would you apply to address this issue?",
      "options": [
        "Increase the learning rate",
        "Add dropout layers",
        "Remove batch normalization",
        "Increase model complexity"
      ],
      "answer": 1,
      "explanation": "Adding dropout layers would help prevent overfitting, which is indicated by decreasing training loss but increasing validation loss.",
      "weight": 2,
      "topic": "Training and Optimization",
      "concept": "Overfitting Diagnosis",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Diagnosing and Treating Overfitting",
        "steps": [
          "Monitor both training and validation loss during training",
          "Identify overfitting when validation loss increases while training loss decreases",
          "Apply regularization techniques like dropout or L2 regularization",
          "Consider early stopping or reducing model complexity"
        ],
        "content": "Overfitting occurs when a model learns the training data too well but fails to generalize to unseen data. The gap between training and validation performance is a clear indicator."
      },
      "mini_glossary": [
        {
          "term": "Validation Loss",
          "definition": "The loss calculated on a separate validation dataset not used for training"
        },
        {
          "term": "Overfitting",
          "definition": "When a model performs well on training data but poorly on unseen data"
        }
      ]
    },
    {
      "question_id": "Q008",
      "question": "8. In a convolutional neural network, what happens during the pooling operation?",
      "options": [
        "Feature maps are combined through matrix multiplication",
        "Spatial dimensions are reduced while preserving important features",
        "New channels are added to the feature maps",
        "The network learns new convolutional filters"
      ],
      "answer": 1,
      "explanation": "Pooling operations reduce the spatial dimensions of feature maps by selecting the maximum or average value from small regions, helping to make the network more robust to small translations.",
      "weight": 2,
      "topic": "Network Architectures",
      "concept": "Spatial Reduction",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Understanding Pooling in CNNs",
        "steps": [
          "Feature maps from convolution contain spatial information",
          "Pooling divides each feature map into small regions",
          "Max pooling selects the highest value, average pooling computes the mean",
          "Result is a smaller feature map that retains important features"
        ],
        "content": "Pooling helps reduce computational complexity and provides translation invariance by focusing on the most important features in local regions."
      },
      "mini_glossary": [
        {
          "term": "Pooling",
          "definition": "A downsampling operation that reduces the spatial size of feature maps"
        },
        {
          "term": "Max Pooling",
          "definition": "A pooling operation that selects the maximum value from each region"
        }
      ]
    },
    {
      "question_id": "Q009",
      "question": "9. You have a dataset with 10,000 images for training a CNN. How would you handle the vanishing gradient problem during training?",
      "options": [
        "Use only sigmoid activation functions",
        "Implement batch normalization",
        "Apply skip connections (residual learning)",
        "Reduce the learning rate to very small values"
      ],
      "answer": 2,
      "explanation": "Skip connections in residual networks allow gradients to flow directly through the network, helping to mitigate the vanishing gradient problem in deep networks.",
      "weight": 2,
      "topic": "Network Architectures",
      "concept": "Gradient Flow",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Solving the Vanishing Gradient Problem",
        "steps": [
          "Understand that vanishing gradients occur in deep networks due to repeated multiplication",
          "Implement skip connections that allow direct gradient flow",
          "Use activation functions like ReLU that don't saturate",
          "Apply batch normalization to stabilize training"
        ],
        "content": "The vanishing gradient problem makes it difficult to train deep networks and were a major challenge before techniques like ReLU and residual connections."
      },
      "mini_glossary": [
        {
          "term": "Vanishing Gradient",
          "definition": "The problem where gradients become extremely small in deep networks, preventing effective learning"
        },
        {
          "term": "Skip Connections",
          "definition": "Direct connections that bypass one or more layers, allowing gradients to flow more easily"
        }
      ]
    },
    {
      "question_id": "Q010",
      "question": "10. When implementing transfer learning for image classification, which layers of a pre-trained CNN would you typically fine-tune?",
      "options": [
        "Only the input layer",
        "Only the fully connected layers at the end",
        "All layers equally",
        "Only the convolutional layers in the middle"
      ],
      "answer": 1,
      "explanation": "In transfer learning, you typically freeze the early convolutional layers (which learn general features) and fine-tune only the later fully connected layers for the specific task.",
      "weight": 2,
      "topic": "Transfer Learning",
      "concept": "Layer Fine-tuning",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Transfer Learning Strategy",
        "steps": [
          "Load a pre-trained model trained on a large dataset",
          "Freeze the early layers that learn general features",
          "Replace and train the final fully connected layers",
          "Optionally fine-tune some of the later convolutional layers"
        ],
        "content": "Transfer learning leverages knowledge from pre-trained models, significantly reducing training time and data requirements for new tasks."
      },
      "mini_glossary": [
        {
          "term": "Transfer Learning",
          "definition": "Using a pre-trained model as a starting point for a different but related task"
        },
        {
          "term": "Fine-tuning",
          "definition": "The process of slightly adjusting a pre-trained model's weights for a new task"
        }
      ]
    },
    {
      "question_id": "Q011",
      "question": "11. In sequence-to-sequence models, what is the purpose of the attention mechanism?",
      "options": [
        "To reduce model size",
        "To focus on relevant parts of the input sequence",
        "To speed up training",
        "To prevent overfitting"
      ],
      "answer": 1,
      "explanation": "Attention mechanisms allow the model to focus on relevant parts of the input sequence when generating each output element, improving performance on long sequences.",
      "weight": 2,
      "topic": "Sequence Processing",
      "concept": "Attention Mechanism",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "How Attention Works in Seq2Seq",
        "steps": [
          "For each output position, compute attention scores with all input positions",
          "Convert scores to probabilities using softmax",
          "Weight the input representations by these probabilities",
          "Use the weighted sum as context for generating the output"
        ],
        "content": "Attention solves the bottleneck problem of fixed-length context vectors in basic seq2seq models by allowing dynamic focus on relevant input parts."
      },
      "mini_glossary": [
        {
          "term": "Attention Mechanism",
          "definition": "A technique that allows models to focus on relevant parts of the input when generating output"
        },
        {
          "term": "Sequence-to-Sequence",
          "definition": "Models that transform one sequence into another, like machine translation"
        }
      ]
    },
    {
      "question_id": "Q012",
      "question": "12. You are designing a neural network for time series prediction. Which architecture would be most appropriate?",
      "options": [
        "Convolutional Neural Network",
        "Recurrent Neural Network",
        "Feedforward Neural Network",
        "Autoencoder"
      ],
      "answer": 1,
      "explanation": "RNNs are designed to handle sequential data and maintain temporal dependencies, making them ideal for time series prediction tasks.",
      "weight": 2,
      "topic": "Sequence Processing",
      "concept": "Sequential Modeling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Choosing Architecture for Time Series",
        "steps": [
          "Identify that time series data has temporal dependencies",
          "Consider RNN/LSTM/GRU for maintaining memory of past values",
          "CNNs can work for pattern recognition but lack temporal memory",
          "Feedforward networks can't handle sequences without preprocessing"
        ],
        "content": "The choice of architecture depends on the nature of temporal dependencies in your data and the prediction horizon."
      },
      "mini_glossary": [
        {
          "term": "Time Series",
          "definition": "Data points collected at regular time intervals"
        },
        {
          "term": "Temporal Dependencies",
          "definition": "Relationships between data points that depend on their time ordering"
        }
      ]
    },
    {
      "question_id": "Q013",
      "question": "13. What is the vanishing gradient problem and how does it affect training of deep networks?",
      "options": [
        "Gradients become too large, causing unstable training",
        "Gradients become too small, preventing weight updates",
        "The loss function becomes non-convex",
        "The network becomes too complex to optimize"
      ],
      "answer": 1,
      "explanation": "The vanishing gradient problem occurs when gradients become extremely small during backpropagation in deep networks, preventing effective learning in early layers.",
      "weight": 2,
      "topic": "Training and Optimization",
      "concept": "Gradient Issues",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Understanding Vanishing Gradients",
        "steps": [
          "During backpropagation, gradients are multiplied through many layers",
          "Activation functions like sigmoid have derivatives less than 1",
          "Repeated multiplication of small numbers leads to vanishing gradients",
          "Early layers receive tiny gradients and learn very slowly"
        ],
        "content": "Vanishing gradients make it difficult to train deep networks and were a major challenge before techniques like ReLU and residual connections."
      },
      "mini_glossary": [
        {
          "term": "Backpropagation",
          "definition": "The algorithm used to compute gradients and update weights in neural networks"
        },
        {
          "term": "Vanishing Gradient",
          "definition": "The problem where gradients become extremely small, preventing learning in deep layers"
        }
      ]
    },
    {
      "question_id": "Q014",
      "question": "14. In the context of object detection, what is the difference between Faster R-CNN and YOLO?",
      "options": [
        "Faster R-CNN is real-time, YOLO is not",
        "YOLO processes the entire image at once, Faster R-CNN uses region proposals",
        "Faster R-CNN works only with small images",
        "YOLO requires more training data"
      ],
      "answer": 1,
      "explanation": "YOLO processes the entire image in a single pass to predict bounding boxes and classes simultaneously, while Faster R-CNN uses a two-stage approach with region proposal networks.",
      "weight": 2,
      "topic": "Computer Vision",
      "concept": "Detection Architectures",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Comparing Detection Architectures",
        "steps": [
          "Faster R-CNN: Two-stage approach - propose regions, then classify",
          "YOLO: Single-stage approach - predict directly from grid cells",
          "Faster R-CNN typically more accurate but slower",
          "YOLO is faster and real-time capable but may have lower accuracy"
        ],
        "content": "The choice between architectures depends on the trade-off between speed and accuracy requirements."
      },
      "mini_glossary": [
        {
          "term": "Object Detection",
          "definition": "The task of identifying and locating objects within an image"
        },
        {
          "term": "Region Proposals",
          "definition": "Candidate regions in an image that might contain objects"
        }
      ]
    },
    {
      "question_id": "Q015",
      "question": "15. Analyze why batch normalization improves training stability in deep networks.",
      "options": [
        "It reduces the learning rate requirements",
        "It normalizes layer inputs to have zero mean and unit variance",
        "It increases the model capacity",
        "It reduces the number of parameters"
      ],
      "answer": 1,
      "explanation": "Batch normalization normalizes the inputs to each layer to have zero mean and unit variance, reducing internal covariate shift and making training more stable.",
      "weight": 3,
      "topic": "Training and Optimization",
      "concept": "Normalization Techniques",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "How Batch Normalization Works",
        "steps": [
          "Compute mean and variance across the batch for each feature",
          "Normalize the features to zero mean and unit variance",
          "Scale and shift using learned parameters γ and β",
          "Apply the transformation to stabilize training"
        ],
        "content": "Batch normalization reduces internal covariate shift, allowing higher learning rates and more stable training by keeping layer inputs in a consistent range."
      },
      "mini_glossary": [
        {
          "term": "Batch Normalization",
          "definition": "A technique that normalizes layer inputs across a mini-batch to improve training stability"
        },
        {
          "term": "Internal Covariate Shift",
          "definition": "The change in the distribution of layer inputs during training"
        }
      ]
    },
    {
      "question_id": "Q016",
      "question": "16. Evaluate the trade-offs between using a larger batch size versus a smaller batch size in deep learning training.",
      "options": [
        "Larger batches always train faster and achieve better accuracy",
        "Smaller batches provide better generalization but may be unstable",
        "Batch size has no significant impact on training",
        "Larger batches always lead to overfitting"
      ],
      "answer": 1,
      "explanation": "Smaller batches provide better generalization by introducing noise that acts as regularization, but may be less stable. Larger batches train faster but may lead to poorer generalization.",
      "weight": 3,
      "topic": "Training and Optimization",
      "concept": "Batch Size Optimization",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Batch Size Trade-offs Analysis",
        "steps": [
          "Larger batches: Faster training, more stable gradients, but poorer generalization",
          "Smaller batches: Slower training, noisier gradients, but better generalization",
          "Consider memory constraints and dataset size",
          "Often use larger batches during training, smaller for fine-tuning"
        ],
        "content": "The optimal batch size depends on the specific task, dataset, and computational resources available."
      },
      "mini_glossary": [
        {
          "term": "Batch Size",
          "definition": "The number of training examples processed together in one forward/backward pass"
        },
        {
          "term": "Generalization",
          "definition": "How well a trained model performs on unseen data"
        }
      ]
    },
    {
      "question_id": "Q017",
      "question": "17. Compare and contrast generative adversarial networks (GANs) with variational autoencoders (VAEs) for generative modeling.",
      "options": [
        "GANs are supervised, VAEs are unsupervised",
        "GANs learn implicitly through adversarial training, VAEs learn explicitly through reconstruction",
        "VAEs are better for image generation, GANs for text generation",
        "GANs require paired data, VAEs work with unpaired data"
      ],
      "answer": 1,
      "explanation": "GANs learn data distributions implicitly through adversarial training between generator and discriminator, while VAEs learn explicitly by reconstructing inputs through a bottleneck latent space.",
      "weight": 3,
      "topic": "Generative Models",
      "concept": "Learning Paradigms",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "GANs vs VAEs Comparison",
        "steps": [
          "GANs: Generator creates fake data, discriminator distinguishes real from fake",
          "VAEs: Encoder compresses to latent space, decoder reconstructs from latent vectors",
          "GANs often produce sharper images but can be unstable",
          "VAEs provide smoother latent spaces but may produce blurrier outputs"
        ],
        "content": "Both approaches have different strengths: GANs excel at generating high-quality samples, while VAEs provide better latent space properties for interpolation and manipulation."
      },
      "mini_glossary": [
        {
          "term": "Generative Adversarial Network",
          "definition": "A framework where two neural networks compete: generator creates fake data, discriminator detects fakes"
        },
        {
          "term": "Variational Autoencoder",
          "definition": "A generative model that learns to encode data into a latent space and decode it back"
        }
      ]
    },
    {
      "question_id": "Q018",
      "question": "18. Analyze how self-attention mechanisms in transformers differ from traditional RNN approaches for sequence processing.",
      "options": [
        "Self-attention can only process fixed-length sequences",
        "Self-attention allows parallel processing and better long-range dependencies",
        "RNNs are better at capturing local dependencies",
        "Transformers require sequential processing like RNNs"
      ],
      "answer": 1,
      "explanation": "Self-attention processes all positions simultaneously, allowing parallel computation and better modeling of long-range dependencies compared to RNNs which process sequentially.",
      "weight": 3,
      "topic": "Sequence Processing",
      "concept": "Parallel Processing",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Transformers vs RNNs",
        "steps": [
          "RNNs process sequences step-by-step, creating bottlenecks",
          "Self-attention computes all pairwise interactions simultaneously",
          "This allows parallel processing and better long-range modeling",
          "Transformers can capture dependencies regardless of distance"
        ],
        "content": "The shift from RNNs to transformers represents a fundamental change in how we process sequential data, enabling much more efficient and effective modeling."
      },
      "mini_glossary": [
        {
          "term": "Self-Attention",
          "definition": "A mechanism that computes relationships between all positions in a sequence simultaneously"
        },
        {
          "term": "Transformer",
          "definition": "A neural network architecture based entirely on attention mechanisms"
        }
      ]
    },
    {
      "question_id": "Q019",
      "question": "19. Evaluate the impact of different loss functions on training dynamics in deep learning models.",
      "options": [
        "All loss functions converge at the same rate",
        "MSE loss is always better than cross-entropy for classification",
        "The choice of loss function affects gradient magnitudes and convergence",
        "Loss functions only affect the final accuracy, not training dynamics"
      ],
      "answer": 2,
      "explanation": "Different loss functions produce different gradient magnitudes and landscapes, affecting convergence speed, stability, and the types of errors the model makes during training.",
      "weight": 3,
      "topic": "Training and Optimization",
      "concept": "Optimization Dynamics",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Loss Function Impact Analysis",
        "steps": [
          "Cross-entropy provides stronger gradients for confident wrong predictions",
          "MSE can be more sensitive to outliers in regression tasks",
          "Focal loss helps with class imbalance by down-weighting easy examples",
          "Robust loss functions can improve training stability"
        ],
        "content": "The choice of loss function is crucial as it directly influences the gradient flow and learning dynamics throughout training."
      },
      "mini_glossary": [
        {
          "term": "Loss Function",
          "definition": "A mathematical function that measures how well a model's predictions match the true values"
        },
        {
          "term": "Gradient Magnitude",
          "definition": "The size of the gradient vector, which determines the step size during optimization"
        }
      ]
    },
    {
      "question_id": "Q020",
      "question": "20. Design a strategy for handling class imbalance in deep learning classification tasks.",
      "options": [
        "Always use the same batch size for all classes",
        "Implement weighted loss functions and data augmentation",
        "Remove minority class samples to balance the dataset",
        "Use only the majority class for training"
      ],
      "answer": 1,
      "explanation": "Class imbalance can be addressed through weighted loss functions that give higher importance to minority classes, data augmentation to increase minority samples, and sampling strategies.",
      "weight": 3,
      "topic": "Training and Optimization",
      "concept": "Class Distribution",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Comprehensive Imbalance Handling Strategy",
        "steps": [
          "Assess the degree of class imbalance in your dataset",
          "Use weighted loss functions to penalize minority class errors more",
          "Apply data augmentation specifically to minority classes",
          "Consider oversampling, undersampling, or synthetic sample generation",
          "Use appropriate evaluation metrics beyond accuracy"
        ],
        "content": "Handling class imbalance requires a multi-faceted approach combining data-level techniques, algorithm-level modifications, and proper evaluation strategies."
      },
      "mini_glossary": [
        {
          "term": "Class Imbalance",
          "definition": "When some classes in a dataset have significantly fewer samples than others"
        },
        {
          "term": "Weighted Loss",
          "definition": "A loss function where different classes contribute differently to the total loss"
        }
      ]
    }
  ]
}