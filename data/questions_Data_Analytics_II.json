{
  "meta": {
    "title": "Data Analytics",
    "target_audience": "Fortgeschrittene",
    "question_count": 30,
    "difficulty_profile": {
      "leicht": 5,
      "mittel": 15,
      "schwer": 10
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 30
  },
  "questions": [
    {
      "question": "1. Welche Aussage beschreibt **Bias-Variance-Trade-off** im Kontext von $ML$ am treffendsten?",
      "options": [
        "Hoher Bias und hohe Varianz führen stets zu guter Generalisierung.",
        "Niedriger Bias und niedrige Varianz sind bei realen Daten leicht zu erreichen.",
        "Ein Modell mit hohem Bias ist tendenziell **unterfittet**, eines mit hoher Varianz tendenziell **überfittet**.",
        "Bias und Varianz sind unabhängig voneinander und beeinflussen die Generalisierung nicht.",
        "Hohe Varianz ist in der Praxis immer besser als hoher Bias."
      ],
      "answer": 2,
      "explanation": "**Bias** misst die systematische Abweichung (Unteranpassung), **Varianz** die Empfindlichkeit gegenüber Datenfluktuation (Überanpassung). Gute Generalisierung erfordert ein ausgewogenes Verhältnis.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung"
    },
    {
      "question": "2. Welche Maßnahme reduziert **Data Leakage** in einem klassischen Analytics-Workflow am zuverlässigsten?",
      "options": [
        "Die gesamte Vorverarbeitung auf dem vollständigen Datensatz durchführen.",
        "Die Vorverarbeitung strikt innerhalb von Cross-Validation-Folds fitten und anwenden.",
        "Vorverarbeitung nur auf dem Testset fitten und auf das Train-Set anwenden.",
        "Keine Vorverarbeitung durchführen, um keine Informationen zu leaken.",
        "Feature-Engineering ausschließlich nach der Modellbewertung durchführen."
      ],
      "answer": 1,
      "explanation": "**Leckagen** entstehen, wenn Informationen aus dem Testset in den Trainingsprozess einfließen. Daher müssen Transformationen wie `StandardScaler` innerhalb der Folds gelernt (fit) und angewendet (transform) werden.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Leckagen durch saubere Pipeline-Scopes vermeiden",
        "schritte": [
          "**Fit** von Transformationen (z. B. `StandardScaler`) nur auf Train-Folds, **Transform** auf Train- und Val-Fold separat.",
          "Nutzung von `Pipeline`/`ColumnTransformer` stellt konsistente Reihenfolge und Scope sicher.",
          "Finales Refitting ausschließlich auf dem kompletten Trainingssplit; Testset bleibt unangetastet."
        ]
      },
      "mini_glossary": {
        "Cross-Validation": "Aufteilung der Trainingsdaten in mehrere Folds; jeder Fold dient einmal als Validierung. Reduziert Varianz der Schätzung.",
        "Data Leakage": "Unbeabsichtigter Informationsfluss von Test- auf Trainingsprozess (z. B. durch gemeinsames Fitten von Scaler), führt zu zu optimistischen Ergebnissen."
      }
    },
    {
      "question": "3. Wofür wird **`StandardScaler`** in `scikit-learn` primär eingesetzt?",
      "options": [
        "Zum One-Hot-Encoding kategorialer Variablen.",
        "Zur Zentrierung auf Mittelwert 0 und Skalierung auf Standardabweichung 1.",
        "Zur Reduktion der Dimensionalität auf zwei Hauptkomponenten.",
        "Zur Erzeugung künstlicher Datenpunkte für Data Augmentation.",
        "Zur Binarisierung kontinuierlicher Ziele."
      ],
      "answer": 1,
      "explanation": "`StandardScaler` transformiert numerische Features auf Mittelwert 0 und Standardabweichung 1, was distanz- und gradientenbasierte Verfahren stabiler macht.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA"
    },
    {
      "question": "4. Welche Kennzahl ist für **binäre Klassifikation** am robustesten bei **unausgeglichenen Klassen**?",
      "options": [
        "`Accuracy`",
        "`ROC-AUC`",
        "`MSE`",
        "`Explained Variance`",
        "`R^2`"
      ],
      "answer": 1,
      "explanation": "Bei stark unbalancierten Klassen kann `Accuracy` täuschen. `ROC-AUC` misst die Ranking-Qualität über Schwellen und ist robuster gegenüber Imbalance.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Warum `ROC-AUC` bei Imbalance oft sinnvoller ist",
        "schritte": [
          "`ROC-AUC` berücksichtigt die Sensitivität und 1−Spezifität über alle Schwellen.",
          "Damit wird die **Ranking-Fähigkeit** bewertet statt eine feste Schwelle.",
          "Für extrem unausgeglichene Probleme ist `PR-AUC` oft zusätzlich aussagekräftig."
        ]
      },
      "mini_glossary": {
        "ROC-AUC": "Fläche unter der Receiver-Operating-Characteristic-Kurve; misst die Trennschärfe unabhängig von einer Schwelle.",
        "PR-AUC": "Fläche unter Precision-Recall-Kurve; besonders informativ bei starker Klassenimbalance."
      }
    },
    {
      "question": "5. Welche Vorteil bringt die Verwendung von **`Pipeline`** in `scikit-learn`?",
      "options": [
        "Modelle werden automatisch parallelisiert.",
        "Alle Schritte (Preprocessing bis Modell) sind reproduzierbar und leackage-sicher integrierbar.",
        "Hyperparameter müssen nicht mehr abgestimmt werden.",
        "Cross-Validation ist nicht mehr nötig.",
        "Feature-Selection wird obsolet."
      ],
      "answer": 1,
      "explanation": "`Pipeline` bündelt Vorverarbeitung und Modell zu einem Ablauf, der konsistent trainiert und validiert wird—wichtig gegen **Data Leakage** und für Reproduzierbarkeit.",
      "weight": 1,
      "topic": "MLOps & Reproduzierbarkeit"
    },
    {
      "question": "6. Welche Technik ist **keine** typische Komponente der **Exploratory Data Analysis (EDA)**?",
      "options": [
        "Univariate und bivariate Visualisierungen (z. B. Histogramme, Scatterplots).",
        "Berechnung von Lage- und Streuungsmaßen.",
        "Hyperparameter-Tuning eines Modells.",
        "Identifikation von Ausreißern.",
        "Korrelationsanalyse numerischer Variablen."
      ],
      "answer": 2,
      "explanation": "**EDA** dient Verständnis und Datenqualitätsprüfung; Hyperparameter-Tuning gehört zur Modellierungsphase, nicht zur reinen Exploration.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Grenze zwischen Exploration und Modellierung",
        "schritte": [
          "EDA beantwortet „Was liegt vor?“: Struktur, Qualität, Verteilungen, Zusammenhänge.",
          "Tuning optimiert Modellhyperparameter und setzt ein trainierbares Setup voraus.",
          "Saubere Trennung verhindert voreilige Schlussfolgerungen und Leakage."
        ]
      },
      "mini_glossary": {
        "EDA": "Explorative Analysephase zur Hypothesengenerierung und Datenprüfung.",
        "Hyperparameter-Tuning": "Systematische Suche (z. B. `GridSearchCV`) nach nicht-gelernten Steuerparametern eines Modells."
      }
    },
    {
      "question": "7. Welche Maßnahme verbessert die **Robustheit gegenüber Ausreißern** in linearen Modellen am ehesten?",
      "options": [
        "Verwendung von `HuberRegressor` statt `LinearRegression`.",
        "Stärkere $L_2$-Regularisierung (`Ridge`).",
        "Vergrößerung der Lernrate.",
        "Standardisierung der Zielvariablen.",
        "Verzicht auf Bias-Term."
      ],
      "answer": 0,
      "explanation": "Der `HuberRegressor` nutzt einen robusten Verlust, der Ausreißer weniger stark gewichtet als das quadratische Fehlermaß der klassischen `LinearRegression`.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Robuster Verlust statt quadratischem Fehler",
        "schritte": [
          "Quadratischer Loss bestraft große Residuen stark → Ausreißer dominieren.",
          "Huber-Loss wechselt ab einem Schwellwert in eine lineare Penalisierung.",
          "Ergebnis: stabilere Schätzungen und geringere Sensitivität."
        ]
      },
      "mini_glossary": {
        "Huber-Loss": "Stückweise quadratischer/linearer Verlust; kombiniert Robustheit und Effizienz.",
        "Ridge ($L_2$)": "Quadratische Regularisierung; reduziert Varianz, aber nicht speziell Ausreißerempfindlichkeit."
      }
    },
    {
      "question": "8. Welche Aussage trifft **am ehesten** auf **`PCA`** zu?",
      "options": [
        "`PCA` maximiert die Klassengenauigkeit.",
        "`PCA` ist eine überwachte Methode zur Feature-Selektion.",
        "`PCA` projiziert auf orthogonale Richtungen maximaler Varianz.",
        "`PCA` ist nur für binäre Ziele geeignet.",
        "`PCA` ersetzt immer Feature-Engineering."
      ],
      "answer": 2,
      "explanation": "`PCA` ist **unüberwacht** und findet orthogonale Komponenten, die maximale Varianz erklären; sie dient der **Dimensionsreduktion** und Visualisierung.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Varianzorientierte Projektion",
        "schritte": [
          "Kovarianzmatrix zerlegen und Eigenvektoren als Achsen verwenden.",
          "Höchste Eigenwerte → Komponenten mit größter erklärter Varianz.",
          "Reduktion kann Rauschen verringern und Modelle stabilisieren."
        ]
      },
      "mini_glossary": {
        "Erklärte Varianz": "Anteil der Gesamtvarianz, den eine Komponente abdeckt.",
        "Eigenvektor/-wert": "Richtungen und zugehörige Varianzstärken der Kovarianzmatrix."
      }
    },
    {
      "question": "9. Warum ist **Feature-Scaling** für `KNN` besonders wichtig?",
      "options": [
        "Weil `KNN` keine Distanzmaße verwendet.",
        "Weil `KNN` auf Distanzvergleichen basiert und Skalenunterschiede verzerren.",
        "Weil `KNN` sonst keine kategorialen Variablen verarbeitet.",
        "Weil `KNN` nur mit normalverteilten Merkmalen funktioniert.",
        "Weil `KNN` sonst die Nachbarn zufällig wählt."
      ],
      "answer": 1,
      "explanation": "`KNN` nutzt Distanzen; Features mit größeren Skalen dominieren sonst die Nachbarschaftssuche, weshalb **Standardisierung/Normalisierung** nötig ist.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA"
    },
    {
      "question": "10. Welche Praxis ist bei **Class Imbalance** für die **Schwellenwahl** sinnvoll?",
      "options": [
        "Immer bei Schwelle 0.5 bleiben.",
        "Schwellenwahl per Youden-Index oder Kostenmatrix anhand `ROC`/`PR`-Kurven.",
        "Schwelle so verschieben, dass `Accuracy` maximal wird.",
        "Schwellen zufällig sampeln und mitteln.",
        "Schwellenwahl ignorieren; sie beeinflusst Metriken nicht."
      ],
      "answer": 1,
      "explanation": "Bei unausgeglichenen Klassen optimiert eine feste Schwelle selten die gewünschten **Kosten/Nutzen**. Kurvenbasierte Verfahren erlauben zielabhängige Auswahl.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Schwellen daten- und zielabhängig bestimmen",
        "schritte": [
          "`ROC`/`PR`-Kurven über alle Schwellen erzeugen.",
          "Youden-Index oder kostenbasierte Optimierung auf Geschäftsziele abstimmen.",
          "Schwelle regelmäßig rekalibrieren, wenn Daten-Drift vorliegt."
        ]
      },
      "mini_glossary": {
        "Youden-Index": "Maximiert Sensitivität + Spezifität − 1; liefert eine Schwellenempfehlung.",
        "Daten-Drift": "Änderung der Verteilung über die Zeit; erfordert Nachkalibrierung."
      }
    },
    {
      "question": "11. Welche Methode ist **keine** gängige Strategie zur **Feature-Selektion**?",
      "options": [
        "Filtermethoden (z. B. Korrelation, Mutual Information).",
        "Wrappermethoden (z. B. rekursives Weglassen).",
        "Embedded-Methoden (z. B. `Lasso`).",
        "Zufälliges Entfernen von Features ohne Bewertung.",
        "Stabilitätsselektion über resampelte Daten."
      ],
      "answer": 3,
      "explanation": "**Randomes Entfernen** ohne Bewertung ist keine systematische Methode; etablierte Ansätze nutzen statistische Kriterien oder modellinduzierte Gewichte.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Selektion erfordert Kriterium oder Modellkopplung",
        "schritte": [
          "Filter stützen sich auf datengetriebene Maße (z. B. Korrelation).",
          "Wrapper bewerten Feature-Sets über wiederholtes Train/Val.",
          "Embedded nutzen Regularisierung/Gewichte im Modell (z. B. `L1`)."
        ]
      },
      "mini_glossary": {
        "Lasso (`L1`)": "Sparsame Regularisierung; kann Koeffizienten exakt auf 0 setzen.",
        "Stabilitätsselektion": "Wählt Features, die über Resamples hinweg konsistent wichtig sind."
      }
    },
    {
      "question": "12. Welche Aussage trifft auf **`StratifiedKFold`** am besten zu?",
      "options": [
        "Teilt Daten zufällig ohne Rücksicht auf die Zielverteilung.",
        "Erhält die Klassenverteilung in jedem Fold näherungsweise bei.",
        "Ist nur für Regression geeignet.",
        "Verwendet deterministisch immer dieselben Splits ohne Seed.",
        "Erfordert, dass alle Features bereits skaliert sind."
      ],
      "answer": 1,
      "explanation": "`StratifiedKFold` sorgt dafür, dass die **Klassenverteilung** in jedem Fold ähnlich der Gesamtheit ist—wichtig bei Imbalance.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung"
    },
    {
      "question": "13. Welche Aussage zur **Kalibrierung** von Klassifikationsmodellen ist korrekt?",
      "options": [
        "Kalibrierung verändert die Rangfolge der Scores nicht.",
        "Kalibrierung maximiert zwangsläufig `Accuracy`.",
        "Kalibrierung ist bei logistischen Modellen nicht nötig.",
        "Kalibrierung ist identisch mit Regularisierung.",
        "Kalibrierung ersetzt die Schwellenwahl."
      ],
      "answer": 0,
      "explanation": "Methoden wie **Platt Scaling** oder **Isotonic Regression** justieren **Wahrscheinlichkeiten**, nicht das Ranking; `ROC-AUC` bleibt oft unverändert, `Brier`/`LogLoss` verbessern sich.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Wahrscheinlichkeiten auf reale Häufigkeiten abbilden",
        "schritte": [
          "Auf Val-Daten wird ein Kalibrierungsmodell auf Scores gefittet.",
          "Output sind besser kalibrierte Wahrscheinlichkeiten (z. B. 0.7 ≈ 70 %).",
          "Nützlich für kostenbasierte Entscheidungen und Risikoabschätzungen."
        ]
      },
      "mini_glossary": {
        "Platt Scaling": "Sigmoid-Modell auf Scores; mappt zu Wahrscheinlichkeiten.",
        "Isotone Regression": "Monotone, nichtlineare Anpassung der Score→Proba-Abbildung."
      }
    },
    {
      "question": "14. Welche Option beschreibt **`TimeSeriesSplit`** richtig?",
      "options": [
        "Verwendet zufällige Splits ohne Zeitordnung.",
        "Erweitert den Trainingszeitraum schrittweise und validiert auf nachfolgenden Zeitfenstern.",
        "Ist nur für stationäre Reihen nutzbar.",
        "Mischt die Beobachtungen vor jedem Split.",
        "Benötigt immer Tagesdaten."
      ],
      "answer": 1,
      "explanation": "`TimeSeriesSplit` respektiert **Zeitkausalität**: Train auf Vergangenheit, Val auf Zukunft—verhindert **Look-ahead Leakage**.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Kausal korrekte Validierung für Zeitreihen",
        "schritte": [
          "Splits wachsen kumulativ im Training und schieben das Val-Fenster vor.",
          "Keine Durchmischung über die Zeit; Ordnung bleibt erhalten.",
          "Geeignet für Prognosen, Drift-Erkennung und Modellwahl."
        ]
      },
      "mini_glossary": {
        "Look-ahead Leakage": "Zukünftige Informationen beeinflussen Training; führt zu zu guten Schätzungen.",
        "Drift": "Systematische Verschiebung der Datenverteilung über die Zeit."
      }
    },
    {
      "question": "15. Was ist der **Hauptzweck** von **`MLflow Tracking`** im Analytics-Stack?",
      "options": [
        "Training von neuronalen Netzen auf GPU.",
        "Nachverfolgung von Parametern, Metriken, Artefakten und Modellen je Experiment.",
        "Automatisches Generieren perfekter Hyperparameter.",
        "Berechnung von Shapley-Werten.",
        "Deployment in Kubernetes ohne weitere Tools."
      ],
      "answer": 1,
      "explanation": "`MLflow Tracking` protokolliert **Parameter, Metriken, Artefakte und Modelldateien** und erleichtert Vergleich und Reproduzierbarkeit.",
      "weight": 2,
      "topic": "MLOps & Reproduzierbarkeit",
      "extended_explanation": {
        "titel": "Warum strukturierte Experiment-Historie entscheidend ist",
        "schritte": [
          "Konsistente Runs mit identischen Seeds und Umgebungen protokollieren.",
          "Artefakte (Plots, Modelle) zentral ablegen und vergleichen.",
          "Reproduzierbare Entscheidungen durch nachvollziehbare Historie ermöglichen."
        ]
      },
      "mini_glossary": {
        "Artefakt": "Begleitobjekt eines Runs (z. B. Figure, Confusion Matrix, Modell).",
        "Run": "Einzelner Trainings-/Evaluationsdurchlauf mit Logeinträgen in `MLflow`."
      }
    },
    {
      "question": "16. Welche Aussage beschreibt **Regularisierung** korrekt?",
      "options": [
        "`L1` erhöht Varianz, `L2` erhöht Bias.",
        "`L1` kann Koeffizienten exakt nullen, `L2` schrumpft kontinuierlich.",
        "`L2` führt stets zu sparsamen, exakt Null-Koeffizienten.",
        "Regularisierung ist nur bei Deep Learning sinnvoll.",
        "Regularisierung ersetzt Datenqualität."
      ],
      "answer": 1,
      "explanation": "`L1` (Lasso) induziert **Sparsamkeit** durch Nullsetzen, `L2` (Ridge) glättet und reduziert Varianz ohne harte Nullung.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Bias–Varianz gezielt beeinflussen",
        "schritte": [
          "`L1` fördert Feature-Selektion durch koeffiziente Nullung.",
          "`L2` verteilt Gewichte und stabilisiert Vorhersagen.",
          "Kombination (`Elastic Net`) vereint Vorteile beider Welten."
        ]
      },
      "mini_glossary": {
        "Elastic Net": "Kombiniert `L1` und `L2`; kontrolliert Sparsamkeit und Glättung.",
        "Sparsamkeit": "Viele Koeffizienten exakt 0; vereinfacht das Modell."
      }
    },
    {
      "question": "17. Welche **Docker**-Praxis unterstützt **Reproduzierbarkeit** in Analytics-Projekten am stärksten?",
      "options": [
        "Installationen direkt auf dem Host ohne Versionsangaben.",
        "Nutzung eines versionierten `Dockerfile` plus `requirements.txt`/`environment.yml`.",
        "Nur ein Readme mit Setup-Hinweisen verwenden.",
        "Abhängigkeiten zur Laufzeit zufällig aktualisieren.",
        "Container ohne feste Basisimages starten."
      ],
      "answer": 1,
      "explanation": "Ein **fixiertes** `Dockerfile` mit **versionierten** Abhängigkeiten erzeugt portable, reproduzierbare Umgebungen über Systeme hinweg.",
      "weight": 2,
      "topic": "MLOps & Reproduzierbarkeit",
      "extended_explanation": {
        "titel": "Umgebung einfrieren – Ergebnisse stabil halten",
        "schritte": [
          "Basisimage und Paketversionen explizit deklarieren.",
          "Build-Caching durch frühes Kopieren der Anforderungen nutzen.",
          "Identische Images über Dev/CI/Prod ausrollen."
        ]
      },
      "mini_glossary": {
        "Image": "Schreibgeschützte Vorlage eines Containers mit allen Abhängigkeiten.",
        "Container": "Laufende Instanz eines Images mit isolierter Umgebung."
      }
    },
    {
      "question": "18. Welche Aussage beschreibt **`RandomForest`** gegenüber einem einzelnen **Decision Tree** am besten?",
      "options": [
        "`RandomForest` erhöht systematisch Overfitting.",
        "`RandomForest` reduziert Varianz durch Bagging vieler Bäume.",
        "`RandomForest` braucht zwingend weniger Daten.",
        "`RandomForest` ist deterministisch ohne Zufallseinfluss.",
        "`RandomForest` eignet sich nicht für Klassifikation."
      ],
      "answer": 1,
      "explanation": "Durch **Bagging** (Bootstrapping + Merkmalssampling) mittelt der Wald über viele Bäume und reduziert so **Varianz** und Overfitting-Tendenzen einzelner Bäume.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Varianzreduktion durch Diversität",
        "schritte": [
          "Bootstraps und zufällige Feature-Subsets erzeugen diverse Bäume.",
          "Aggregation (Mehrheit/Mittel) glättet Einzelschätzungen.",
          "Resultat: stabilere Performance und höhere Robustheit."
        ]
      },
      "mini_glossary": {
        "Bagging": "Parallel trainierte Modelle auf Bootstraps; Aggregation der Vorhersagen.",
        "Feature-Sampling": "Zufällige Auswahl von Merkmalen pro Split zur Entkorrelation."
      }
    },
    {
      "question": "19. Welche Metrik eignet sich **besonders** zur Bewertung einer **Regression**?",
      "options": [
        "`F1-Score`",
        "`MSE`",
        "`Recall`",
        "`ROC-AUC`",
        "`Accuracy`"
      ],
      "answer": 1,
      "explanation": "In Regressionsaufgaben sind Fehlermaße wie **`MSE`** oder `MAE` üblich; Klassifikationsmetriken wie `F1`, `Accuracy`, `ROC-AUC` sind ungeeignet.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung"
    },
    {
      "question": "20. Welche Maßnahme ist zur **Erklärung** von Modellentscheidungen in tabellarischen Daten **am praktikabelsten**?",
      "options": [
        "`LIME` oder `SHAP` zur lokalen/ globalen Feature-Beitragsanalyse.",
        "Nur die Koeffizienten eines Ridge-Modells betrachten.",
        "Gewichte eines `RandomForest` ignorieren, da sie uninterpretierbar sind.",
        "Stets nur Korrelationsmatrix zeigen.",
        "Feature-Skalierung abschalten, um Interpretierbarkeit zu erhöhen."
      ],
      "answer": 0,
      "explanation": "**`LIME`/`SHAP`** liefern **lokale** und **globale** Beiträge von Features—geeignet für komplexe Modelle und Audit-Zwecke.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Lokale Erklärungen für individuelle Vorhersagen",
        "schritte": [
          "`LIME` approximiert lokal ein einfaches Modell um eine Vorhersage.",
          "`SHAP` nutzt Shapley-Werte aus der Spieltheorie für konsistente Beiträge.",
          "Kombination erlaubt Debugging, Kommunikation und Governance."
        ]
      },
      "mini_glossary": {
        "SHAP": "Shapley Additive Explanations; additiv zerlegbare Feature-Beiträge.",
        "LIME": "Local Interpretable Model-agnostic Explanations; lokale Linearisierung."
      }
    },
    {
      "question": "21. Welche Praxis ist bei **SQL-Analysen** für Kennzahlenbildung korrekt?",
      "options": [
        "Kennzahlen direkt aus Roh-Events ohne Aggregation berichten.",
        "Window-Functions verwenden, um rollierende Metriken zu berechnen.",
        "Nur `GROUP BY` reicht immer für Kohortenanalysen.",
        "`JOIN`s vermeiden, um Performance zu sichern.",
        "Zeitstempel in Text konvertieren, um Zonenprobleme zu umgehen."
      ],
      "answer": 1,
      "explanation": "**Window-Functions** (`OVER`-Klausel) erlauben **rollierende**, **kumulative** und rangbasierte Auswertungen ohne Informationsverlust durch Aggregation.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Zeitbezogene Metriken korrekt bilden",
        "schritte": [
          "Mit `PARTITION BY` und `ORDER BY` Kohorten/Zeitläufe definieren.",
          "Funktionen wie `SUM() OVER` oder `AVG() OVER` für rollierende Werte nutzen.",
          "So bleiben Zeilengranularität und Metrikbildung vereinbar."
        ]
      },
      "mini_glossary": {
        "Window Function": "Aggregation über ein gleitendes Fenster statt Gruppenverdichtung.",
        "`OVER`-Klausel": "Definiert Partition und Ordnung für Window-Berechnungen."
      }
    },
    {
      "question": "22. Welche Option trifft auf **A/B-Tests** methodisch zu?",
      "options": [
        "Die Stichprobengröße kann nach ersten Ergebnissen beliebig angepasst werden.",
        "Vorab festgelegte **Power** und **Signifikanzniveau** bestimmen die benötigte Stichprobengröße.",
        "Der p-Wert gibt die Wahrscheinlichkeit der Nullhypothese an.",
        "Mehrfaches Zwischenstopp-Look erhöht nicht die Fehlerwahrscheinlichkeit.",
        "Kein Randomisieren nötig, wenn Gruppen ähnlich wirken."
      ],
      "answer": 1,
      "explanation": "Testdesign verlangt **a priori** definierte **$\\u03b1$** (Signifikanz) und **Power** (1−$\\u03b2$). Daraus ergibt sich die notwendige **Sample Size**.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Sauberes Testdesign vor Datensammlung",
        "schritte": [
          "Effektgröße, $α$ und Power festlegen → Stichprobengröße berechnen.",
          "Randomisieren und vorab Analyseplan fixieren (Pre-Registration).",
          "Zwischenanalysen nur mit Alpha-Spending/Group-Sequential-Design."
        ]
      },
      "mini_glossary": {
        "Power": "Wahrscheinlichkeit, einen echten Effekt zu entdecken (1−$β$).",
        "Signifikanzniveau ($α$)": "Fehlerrate 1. Art; üblicherweise 0.05."
      }
    },
    {
      "question": "23. Welche Maßnahme verbessert **Reproduzierbarkeit** zusätzlich zu `Docker`?",
      "options": [
        "Seeds und Daten-Splits zufällig halten.",
        "`requirements.txt` ohne Versions-Pins verwenden.",
        "**Random-Seeds** setzen und **Daten-Splits** deterministisch speichern.",
        "Artefakte nicht versionieren.",
        "Notebook-Zellen beliebig neu ausführen."
      ],
      "answer": 2,
      "explanation": "**Deterministische Seeds** und persistierte **Splits** sichern identische Bedingungen für Folgeruns—wichtig für Vergleichbarkeit.",
      "weight": 1,
      "topic": "MLOps & Reproduzierbarkeit"
    },
    {
      "question": "24. Wodurch unterscheidet sich **`GridSearchCV`** von **`RandomizedSearchCV`**?",
      "options": [
        "`GridSearchCV` wählt Parameter zufällig, `RandomizedSearchCV` durchsucht lückenlos.",
        "`GridSearchCV` durchsucht ein Gitter vollständig, `RandomizedSearchCV` sampelt zufällige Kombinationen.",
        "Beide liefern immer identische Ergebnisse.",
        "`RandomizedSearchCV` benötigt keine Cross-Validation.",
        "`GridSearchCV` ist grundsätzlich schneller."
      ],
      "answer": 1,
      "explanation": "**Grid** prüft alle Gitterpunkte; **Randomized** sampelt und kann schneller gute Regionen finden—insbesondere bei großen Suchräumen.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Suchstrategien im Parameterraum",
        "schritte": [
          "Grid: Exhaustiv, aber teuer bei vielen Dimensionen.",
          "Randomized: Stochastisch, erkundet Breite effizienter.",
          "Praxis: Randomized → Feingrid um gute Regionen."
        ]
      },
      "mini_glossary": {
        "Suchraum": "Kartesisches Produkt aller Hyperparameterbereiche.",
        "Validierungsstrategie": "Art der Datenpartitionierung während der Suche (z. B. K-Fold)."
      }
    },
    {
      "question": "25. Welche Strategie ist **zweckmäßig**, um **Ausreißer** **vor** dem Modelltraining zu behandeln?",
      "options": [
        "Immer alle Ausreißer entfernen.",
        "Winsorisierung oder robuste Transformationen prüfen und deren Effekt validieren.",
        "Ausreißer ignorieren; Modelle lernen das automatisch.",
        "Zielvariable clippen, Features unverändert lassen.",
        "Nur logarithmische Transformationen anwenden."
      ],
      "answer": 1,
      "explanation": "**Ausreißer-Handling** ist kontextabhängig. Robuste Alternativen (Winsorize, Huber/Quantile-Transformer) sollten **validiert** statt dogmatisch angewandt werden.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Kontext und Validierung statt Dogma",
        "schritte": [
          "Ausreißer detektieren (z. B. IQR/robuste Z-Scores).",
          "Optionen wie Winsorisierung/robuste Scaler testen.",
          "Wirksamkeit per Cross-Validation und Domänenmetriken prüfen."
        ]
      },
      "mini_glossary": {
        "Winsorisierung": "Beschneidet Extremwerte auf Perzentilschwellen, statt sie zu verwerfen.",
        "IQR": "Interquartilsabstand $Q3-Q1$; robustes Streuungsmaß."
      }
    },
    {
      "question": "26. Welche Eigenschaft zeichnet **`LogisticRegression`** in `scikit-learn` aus?",
      "options": [
        "Sie liefert kalibrierte Wahrscheinlichkeiten ohne weitere Maßnahmen.",
        "Sie kann mit `liblinear` oder `lbfgs` optimiert werden.",
        "Sie ist ein Regressionsmodell für kontinuierliche Ziele.",
        "Sie benötigt keine Regularisierung.",
        "Sie kann keine multiklassigen Probleme lösen."
      ],
      "answer": 1,
      "explanation": "Die Implementierung unterstützt verschiedene **Solver** wie `liblinear`/`lbfgs`; Regularisierung ist standardmäßig aktiv (`C`).",
      "weight": 1,
      "topic": "Modelle & Algorithmen"
    },
    {
      "question": "27. Welche Visualisierung eignet sich **direkt** zur Diagnose von **Klassifikationsfehlern**?",
      "options": [
        "Elbow-Plot der Clusteranzahl.",
        "Confusion-Matrix mit Normalisierung.",
        "Scree-Plot der PCA-Varianzanteile.",
        "Lag-Plot für Zeitreihen.",
        "QQ-Plot der Residuen."
      ],
      "answer": 1,
      "explanation": "Eine **Confusion-Matrix** zeigt Fehlklassifikationen je Klasse, optional normalisiert—nützlich für zielgerichtete Fehleranalysen.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Fehlerstruktur sichtbar machen",
        "schritte": [
          "Matrix je Klasse: True/False Positives/Negatives.",
          "Normalisierung macht Imbalance vergleichbar.",
          "Ableiten gezielter Maßnahmen (Daten, Schwelle, Kosten)."
        ]
      },
      "mini_glossary": {
        "False Positive": "Fälschlich als positiv klassifiziert.",
        "False Negative": "Fälschlich als negativ klassifiziert."
      }
    },
    {
      "question": "28. Welche Aussage zu **`Train/Test Split`** ist **richtig**?",
      "options": [
        "Testset darf für Hyperparameter-Suche verwendet werden.",
        "Train/Validation/Test trennen Rollen: Tuning auf Val, finale Schätzung auf Test.",
        "Testset muss immer größer als Trainset sein.",
        "Validierungsset ist unnötig bei Cross-Validation.",
        "Stratifizierung ist nur bei Regression relevant."
      ],
      "answer": 1,
      "explanation": "**Rollen-Trennung**: Tuning erfolgt ohne Testwissen (Val/CV), die finale Leistung wird **einmalig** auf dem **Testset** geschätzt.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Saubere Rollen für belastbare Schätzungen",
        "schritte": [
          "Entwicklung/Tuning ohne Testkontakt.",
          "Test nur für die Schlussmessung reservieren.",
          "Reproduzierbare Splits dokumentieren und versionieren."
        ]
      },
      "mini_glossary": {
        "Holdout": "Feste Aufteilung in Train/Val/Test.",
        "Leckage": "Informationsübertrag von Test in Training/Tuning."
      }
    },
    {
      "question": "29. Welche Maßnahme verbessert die **Modellüberwachung** nach Deployment?",
      "options": [
        "Verzicht auf Logging zur Performance-Steigerung.",
        "Drift-Detektion, Re-Calibration und periodisches Re-Training nach SLAs.",
        "Ausschließlich manuelle Stichprobenprüfung ohne Metriken.",
        "Nur Hardware-Monitoring (CPU/RAM) betrachten.",
        "Feature-Statistiken nie speichern, um Speicherplatz zu sparen."
      ],
      "answer": 1,
      "explanation": "**MLOps** verlangt Monitoring von **Daten-/Kontextdrift**, **Metriken** und **Re-Training**-Regeln, um Leistung stabil zu halten.",
      "weight": 3,
      "topic": "MLOps & Reproduzierbarkeit",
      "extended_explanation": {
        "titel": "Kontinuierliche Qualitätssicherung im Betrieb",
        "schritte": [
          "Eingangsdaten und Predictions statistisch überwachen.",
          "Schwellen/SLAs definieren und Alerts etablieren.",
          "Re-Training/Neu-Kalibrierung bei Verletzungen anstoßen."
        ]
      },
      "mini_glossary": {
        "SLA": "Service Level Agreement; Zielwerte für Qualität/Verfügbarkeit.",
        "Drift-Detektion": "Statistische Verfahren zum Erkennen von Verteilungsänderungen."
      }
    },
    {
      "question": "30. Welche Praxis ist bei **Feature-Encoding** für **Baumverfahren** sinnvoll?",
      "options": [
        "Kategoriale High-Cardinality-Features immer one-hot-encoden.",
        "Zyklische Features (z. B. Monat) stets als Integer lassen.",
        "Target-Encoding nur strikt CV-intern anwenden, um Leakage zu vermeiden.",
        "Numerische Features stets normalisieren.",
        "Label-Encoding ist immer besser als One-Hot."
      ],
      "answer": 2,
      "explanation": "**Target-Encoding** kann starken Leakage verursachen; daher muss es **fold-intern** fit/transformiert werden. Bäume benötigen kein Scaling, aber Encoding muss leakage-sicher sein.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Hochkardinale Kategorien ohne Leckagen encoden",
        "schritte": [
          "Target-Statistiken nur aus Trainingsfold berechnen.",
          "Auf Val/Test dieselben Regeln ohne Zielinformation anwenden.",
          "Regulierung (Smoothing) gegen Overfitting der Kategorien nutzen."
        ]
      },
      "mini_glossary": {
        "Target-Encoding": "Ersetzt Kategorie durch aggregierte Zielstatistik (z. B. Mittelwert).",
        "Smoothing": "Mischung aus Kategorie- und Global-Statistik zur Stabilisierung."
      }
    }
  ]
}
