{
  "meta": {
    "title": "Mathematik für Machine Learning",
    "created": "07.12.2025 17:05",
    "target_audience": "Informatikstudierende (Grundkenntnisse LinAlg)",
    "question_count": 30,
    "difficulty_profile": {
      "easy": 8,
      "medium": 14,
      "hard": 8
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 3,
    "test_duration_minutes": 26,
    "language": "de"
  },
  "questions": [
    {
      "question": "1. Gegeben ist eine Matrix $A \\in \\mathbb{R}^{m \\times n}$ und eine Matrix $B \\in \\mathbb{R}^{n \\times k}$. Welche Dimensionen hat das Produkt $C = A \\cdot B$?",
      "options": [
        "$m \\times k$",
        "$n \\times n$",
        "$k \\times m$",
        "$m \\times n$"
      ],
      "answer": 0,
      "explanation": "Bei der Matrixmultiplikation muss die Spaltenanzahl der ersten Matrix ($n$) mit der Zeilenanzahl der zweiten Matrix ($n$) übereinstimmen. Das Ergebnis erbt die Zeilen von $A$ ($m$) und die Spalten von $B$ ($k$).",
      "weight": 1,
      "topic": "Lineare Algebra",
      "concept": "Matrix-Multiplikation",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Dimension",
          "definition": "Größe einer Matrix (Zeilen x Spalten)."
        }
      ]
    },
    {
      "question": "2. Was beschreibt der Gradient $\\nabla f(x)$ einer skalaren Funktion $f: \\mathbb{R}^n \\to \\mathbb{R}$ an der Stelle $x$?",
      "options": [
        "Die Richtung des steilsten Abstiegs.",
        "Die Richtung des steilsten Anstiegs.",
        "Den Wert des globalen Minimums.",
        "Die Krümmung der Funktion."
      ],
      "answer": 1,
      "explanation": "Der Gradient ist ein Vektor, der in Richtung der größten Steigung der Funktion zeigt. Für Minimierungsprobleme (wie Gradient Descent) geht man daher in die *negative* Gradientenrichtung.",
      "weight": 1,
      "topic": "Analysis",
      "concept": "Gradient",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Gradient",
          "definition": "Vektor der partiellen Ableitungen erster Ordnung."
        }
      ]
    },
    {
      "question": "3. Welche Eigenschaft muss eine symmetrische Matrix $A$ haben, damit die quadratische Form $x^T A x$ streng konvex ist (d.h. ein eindeutiges Minimum bei 0 hat)?",
      "options": [
        "Sie muss orthogonal sein.",
        "Sie muss positiv definit sein (alle Eigenwerte $> 0$).",
        "Sie muss singulär sein.",
        "Sie muss invertierbar sein."
      ],
      "answer": 1,
      "explanation": "Eine Matrix ist positiv definit, wenn für alle $x \\neq 0$ gilt: $x^T A x > 0$. Geometrisch bedeutet dies, dass die Funktion in alle Richtungen nach oben gekrümmt ist (wie eine Schüssel).",
      "weight": 2,
      "topic": "Lineare Algebra",
      "concept": "Positive Definitheit",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Konvexität und Matrizen",
        "steps": [
          "Quadratische Form: $f(x) = x^T A x$.",
          "Eigenwerte: Bestimmen die Krümmung entlang der Eigenvektoren.",
          "Positiv Definit: Alle Eigenwerte positiv -> Krümmung überall positiv -> Konvex."
        ],
        "content": "Dies ist entscheidend für die Analyse von Kostenfunktionen und deren Minima."
      },
      "mini_glossary": [
        {
          "term": "Eigenwert",
          "definition": "Skalar $\\lambda$, für den $Av = \\lambda v$ gilt."
        }
      ]
    },
    {
      "question": "4. Sie wollen die L2-Norm (Euklidische Norm) eines Vektors berechnen. Welcher NumPy-Befehl fehlt?\n\n```python\n1: import numpy as np\n2: v = np.array([3, 4])\n3: norm = np.linalg.__________(v)\n4: # Ergebnis sollte 5.0 sein\n```",
      "options": [
        "abs",
        "norm",
        "dist",
        "length"
      ],
      "answer": 1,
      "explanation": "`np.linalg.norm(x)` berechnet standardmäßig die L2-Norm (Wurzel aus der Summe der Quadrate). $\\sqrt{3^2 + 4^2} = \\sqrt{9+16} = 5$.",
      "weight": 1,
      "topic": "Lineare Algebra (Code)",
      "concept": "Vektor-Normen",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "L2-Norm",
          "definition": "Euklidische Länge eines Vektors."
        }
      ]
    },
    {
      "question": "5. Gegeben ist $f(x, y) = x^2 + 3y$. Was ist der partielle Ableitungsvektor (Gradient) $\\nabla f$?",
      "options": [
        "$\\begin{pmatrix} 2x \\\\ 3 \\end{pmatrix}$",
        "$\\begin{pmatrix} 2x \\\\ 3y \\end{pmatrix}$",
        "$\\begin{pmatrix} x \\\\ 3 \\end{pmatrix}$",
        "$2x + 3$"
      ],
      "answer": 0,
      "explanation": "Die partielle Ableitung nach $x$ ist $2x$ ($y$ wird als Konstante betrachtet, fällt weg). Die partielle Ableitung nach $y$ ist $3$ ($x^2$ fällt weg). Zusammen ergibt das den Vektor $(2x, 3)^T$.",
      "weight": 2,
      "topic": "Analysis",
      "concept": "Partielle Ableitung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Berechnung partieller Ableitungen",
        "steps": [
          "Nach x ableiten: $x^2 \\to 2x$, $3y \\to 0$.",
          "Nach y ableiten: $x^2 \\to 0$, $3y \\to 3$.",
          "Zusammenfügen: Vektor aus den Ergebnissen."
        ],
        "content": "Im Machine Learning ist dies der Grundbaustein für Backpropagation."
      },
      "mini_glossary": [
        {
          "term": "Partielle Ableitung",
          "definition": "Ableitung einer Funktion mehrerer Variablen nach einer einzigen Variablen."
        }
      ]
    },
    {
      "question": "6. Wofür wird die Singulärwertzerlegung (SVD) im Machine Learning häufig verwendet?",
      "options": [
        "Um die Ableitung einer Funktion zu finden.",
        "Zur Dimensionsreduktion (z.B. in PCA oder Latent Semantic Analysis).",
        "Um Overfitting durch mehr Parameter zu erzwingen.",
        "Zur Erhöhung der Datenkomplexität."
      ],
      "answer": 1,
      "explanation": "SVD zerlegt eine Matrix in Komponenten, die nach Wichtigkeit (Singulärwerten) sortiert sind. Durch Weglassen der kleinen Singulärwerte kann man Daten komprimieren oder Rauschen entfernen (Low-Rank Approximation).",
      "weight": 2,
      "topic": "Lineare Algebra",
      "concept": "SVD / PCA",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "SVD Anwendung",
        "steps": [
          "Zerlegung: $A = U \\Sigma V^T$.",
          "$\\Sigma$: Diagonalmatrix mit Singulärwerten (Wichtigkeit).",
          "Reduktion: Behalte nur die größten $k$ Werte in $\\Sigma$.",
          "Resultat: Matrix bester Annäherung mit niedrigerem Rang."
        ],
        "content": "Dies ist der mathematische Kern der Principal Component Analysis (PCA)."
      },
      "mini_glossary": [
        {
          "term": "PCA",
          "definition": "Hauptkomponentenanalyse; Verfahren zur Dimensionsreduktion."
        }
      ]
    },
    {
      "question": "7. Was besagt der Satz von Bayes $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$?",
      "options": [
        "Er erlaubt die Berechnung der Wahrscheinlichkeit von A, gegeben B, basierend auf der Likelihood und dem Prior.",
        "Er besagt, dass A und B unabhängig sind.",
        "Er definiert den Erwartungswert einer Verteilung.",
        "Er dient zur Berechnung der Matrix-Inverse."
      ],
      "answer": 0,
      "explanation": "Der Satz von Bayes ist fundamental für 'Bayesian Inference'. Er aktualisiert die Wahrscheinlichkeit einer Hypothese $A$ (Posterior), nachdem Beweise $B$ beobachtet wurden.",
      "weight": 1,
      "topic": "Wahrscheinlichkeitstheorie",
      "concept": "Satz von Bayes",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Posterior",
          "definition": "Wahrscheinlichkeit nach Berücksichtigung neuer Daten ($P(A|B)$)."
        },
        {
          "term": "Prior",
          "definition": "Vorwissen/A-priori-Wahrscheinlichkeit ($P(A)$)."
        }
      ]
    },
    {
      "question": "8. Sie haben zwei Vektoren $a, b \\in \\mathbb{R}^n$. Wenn das Skalarprodukt $a^T b = 0$ ist, was bedeutet das geometrisch?",
      "options": [
        "Die Vektoren sind parallel.",
        "Die Vektoren sind orthogonal (stehen senkrecht aufeinander).",
        "Die Vektoren sind identisch.",
        "Die Länge beider Vektoren ist 0."
      ],
      "answer": 1,
      "explanation": "Das Skalarprodukt ist definiert als $||a|| \\cdot ||b|| \\cdot \\cos(\\theta)$. Wenn es 0 ist, muss $\\cos(\\theta) = 0$ sein (sofern $a,b \\neq 0$), was einem Winkel von $90^\\circ$ entspricht.",
      "weight": 2,
      "topic": "Lineare Algebra",
      "concept": "Orthogonalität",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Skalarprodukt und Winkel",
        "steps": [
          "Formel: $a \\cdot b = \\sum a_i b_i$.",
          "Geometrie: $a \\cdot b = |a| |b| \\cos(\\alpha)$.",
          "Nullstelle: $\\cos(90^\\circ) = 0$.",
          "Schlussfolgerung: Orthogonalität."
        ],
        "content": "Orthogonalität ist wichtig für unkorrelierte Features und Basiswechsel."
      },
      "mini_glossary": [
        {
          "term": "Skalarprodukt",
          "definition": "Inneres Produkt zweier Vektoren, ergibt eine Zahl."
        }
      ]
    },
    {
      "question": "9. Berechnen Sie das Matrix-Vektor-Produkt:\n$$ \\begin{pmatrix} 1 & 2 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $$",
      "options": [
        "$\\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$",
        "$\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$",
        "$\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$",
        "$\\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix}$"
      ],
      "answer": 0,
      "explanation": "Zeile 1 mal Vektor: $1\\cdot2 + 2\\cdot1 = 4$. Zeile 2 mal Vektor: $0\\cdot2 + 3\\cdot1 = 3$. Ergebnis: $(4, 3)^T$.",
      "weight": 2,
      "topic": "Lineare Algebra",
      "concept": "Matrix-Rechnung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Rechenweg",
        "steps": [
          "1. Komponente: Zeile 1 ($1, 2$) dot Spalte ($2, 1$) -> $1*2 + 2*1 = 4$.",
          "2. Komponente: Zeile 2 ($0, 3$) dot Spalte ($2, 1$) -> $0*2 + 3*1 = 3$."
        ],
        "content": "Dies ist die Basisoperation in jedem Neural Network Layer ($W \\cdot x$)."
      },
      "mini_glossary": [
        {
          "term": "Dot Product",
          "definition": "Skalarprodukt von Zeilenvektor und Spaltenvektor."
        }
      ]
    },
    {
      "question": "10. In der Optimierung (z.B. Training neuronaler Netze) suchen wir $x$, sodass $\\nabla f(x) = 0$. Die Hesse-Matrix $H_f(x)$ ist an dieser Stelle positiv definit. Was liegt vor?",
      "options": [
        "Ein lokales Maximum.",
        "Ein Sattelpunkt.",
        "Ein lokales Minimum.",
        "Ein Wendepunkt ohne Extremum."
      ],
      "answer": 2,
      "explanation": "Wenn der Gradient 0 ist (kritischer Punkt) und die Hesse-Matrix (zweite Ableitung) positiv definit ist (positive Krümmung), handelt es sich um ein lokales Minimum.",
      "weight": 3,
      "topic": "Optimierung",
      "concept": "Hesse-Matrix & Extrema",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Kriterien zweiter Ordnung",
        "steps": [
          "Notwendig: $\\nabla f(x) = 0$.",
          "Hinreichend: Prüfe Eigenwerte von $H_f(x)$.",
          "Alle Eigenwerte > 0 (Positiv Definit): Minimum.",
          "Alle Eigenwerte < 0 (Negativ Definit): Maximum.",
          "Gemischt: Sattelpunkt."
        ],
        "content": "In hochdimensionalen Räumen (Deep Learning) sind Sattelpunkte ein größeres Problem als lokale Minima."
      },
      "mini_glossary": [
        {
          "term": "Hesse-Matrix",
          "definition": "Matrix der zweiten partiellen Ableitungen."
        }
      ]
    },
    {
      "question": "11. Welcher Python-Code berechnet den Erwartungswert eines Arrays `x` mit Wahrscheinlichkeiten `p`?",
      "options": [
        "`np.sum(x * p)`",
        "`np.mean(x)`",
        "`np.sum(x) / len(x)`",
        "`np.dot(x, x)`"
      ],
      "answer": 0,
      "explanation": "Der Erwartungswert $E[X]$ ist definiert als $\\sum x_i \\cdot P(x_i)$. Dies entspricht dem Skalarprodukt der Werte und ihrer Wahrscheinlichkeiten. `np.mean(x)` wäre nur korrekt, wenn alle `p` gleich wären (uniform).",
      "weight": 2,
      "topic": "Statistik (Code)",
      "concept": "Erwartungswert",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Gewichteter Mittelwert",
        "steps": [
          "Formel: $E[X] = \\sum x_i p_i$.",
          "Code: `x * p` multipliziert elementweise.",
          "Code: `np.sum(...)` summiert die Ergebnisse.",
          "Bedingung: `sum(p)` muss 1 sein."
        ],
        "content": "Der Erwartungswert ist das theoretische Mittel einer Zufallsvariablen."
      },
      "mini_glossary": [
        {
          "term": "Erwartungswert",
          "definition": "Mittelwert einer Zufallsvariablen gewichtet nach Wahrscheinlichkeit."
        }
      ]
    },
    {
      "question": "12. Was ist der Rang (Rank) einer Matrix?",
      "options": [
        "Die Anzahl der Elemente in der Matrix.",
        "Die Dimension der Matrix.",
        "Die maximale Anzahl linear unabhängiger Zeilen- oder Spaltenvektoren.",
        "Die Summe der Diagonalelemente."
      ],
      "answer": 2,
      "explanation": "Der Rang gibt an, wie viele Dimensionen der durch die Matrix aufgespannte Raum tatsächlich hat. Eine $3 \\times 3$ Matrix mit Rang 2 projiziert den Raum auf eine Ebene.",
      "weight": 1,
      "topic": "Lineare Algebra",
      "concept": "Matrix-Rang",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Lineare Unabhängigkeit",
          "definition": "Ein Vektor lässt sich nicht als Linearkombination der anderen darstellen."
        }
      ]
    },
    {
      "question": "13. Wenn zwei Zufallsvariablen $X$ und $Y$ unabhängig sind, was gilt für ihre Kovarianz $Cov(X, Y)$?",
      "options": [
        "Sie ist 1.",
        "Sie ist 0.",
        "Sie ist unendlich.",
        "Sie ist gleich der Varianz von X."
      ],
      "answer": 1,
      "explanation": "Unabhängigkeit impliziert Unkorreliertheit. Wenn kein linearer Zusammenhang besteht, ist die Kovarianz 0. (Achtung: Kovarianz 0 impliziert nicht zwingend Unabhängigkeit, außer bei Normalverteilungen).",
      "weight": 1,
      "topic": "Statistik",
      "concept": "Kovarianz",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Kovarianz",
          "definition": "Maß für den gemeinsamen linearen Zusammenhang zweier Variablen."
        }
      ]
    },
    {
      "question": "14. Warum verwenden wir im Machine Learning oft die Log-Likelihood $\\ln(L(\\theta))$ statt der Likelihood $L(\\theta)$ zur Maximierung?",
      "options": [
        "Weil der Logarithmus monotone Produkte in Summen umwandelt, was numerisch stabiler ist und die Ableitung vereinfacht.",
        "Weil der Logarithmus das Vorzeichen ändert und wir immer minimieren wollen.",
        "Weil die Likelihood-Funktion sonst negativ wäre.",
        "Weil Computer keine Multiplikationen können."
      ],
      "answer": 0,
      "explanation": "Likelihoods sind oft Produkte vieler kleiner Wahrscheinlichkeiten ($P(x_1) \\cdot P(x_2) \\dots$). Das führt zu numerischem Underflow. Der Logarithmus macht daraus eine Summe $\\sum \\ln P(x_i)$, und da $\\ln$ streng monoton steigend ist, bleibt die Position des Maximums gleich.",
      "weight": 3,
      "topic": "Optimierung / Statistik",
      "concept": "Log-Likelihood",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Log-Trick",
        "steps": [
          "Problem: $L = \\prod p_i$. Wenn $p_i < 1$, geht $L \\to 0$ (Underflow).",
          "Lösung: $\\ln(L) = \\ln(\\prod p_i) = \\sum \\ln(p_i)$.",
          "Eigenschaft: $\\text{argmax}_x f(x) = \\text{argmax}_x \\ln(f(x))$ (Monotonie).",
          "Vorteil: Summen sind einfacher abzuleiten als Produkte."
        ],
        "content": "Standardpraxis bei Maximum Likelihood Estimation (MLE) und Cross-Entropy Loss."
      },
      "mini_glossary": [
        {
          "term": "Underflow",
          "definition": "Zahl ist zu klein, um im Computer präzise dargestellt zu werden, wird zu 0."
        }
      ]
    },
    {
      "question": "15. Welche NumPy-Funktion fehlt, um zwei Matrizen `A` und `B` korrekt zu multiplizieren (Matrixprodukt)?\n\n```python\n1: A = np.eye(3)\n2: B = np.ones((3, 1))\n3: C = A.__________(B)\n```",
      "options": [
        "multiply",
        "dot",
        "times",
        "prod"
      ],
      "answer": 1,
      "explanation": "`A.dot(B)` oder `np.dot(A, B)` (oder der `@` Operator) führt die Matrixmultiplikation aus. `multiply` würde eine elementweise Multiplikation durchführen.",
      "weight": 2,
      "topic": "Lineare Algebra (Code)",
      "concept": "NumPy Syntax",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Elementweise vs. Matrix-Multiplikation",
        "steps": [
          "Elementweise (`*` oder `multiply`): $C_{ij} = A_{ij} \\cdot B_{ij}$. Benötigt gleiche Shape.",
          "Matrix (`@` oder `dot`): Zeile mal Spalte. Benötigt kompatible Shapes ($(n,k), (k,m)$)."
        ],
        "content": "Der häufigste Fehler in Python-ML-Code ist die Verwechslung dieser beiden Operationen."
      },
      "mini_glossary": [
        {
          "term": "Broadcasting",
          "definition": "NumPy-Mechanismus, um Arrays unterschiedlicher Größe elementweise zu verknüpfen."
        }
      ]
    },
    {
      "question": "16. Was ist die Spur (Trace) einer quadratischen Matrix?",
      "options": [
        "Das Produkt der Eigenwerte.",
        "Die Summe der Diagonalelemente.",
        "Die Determinante.",
        "Der größte Eintrag der Matrix."
      ],
      "answer": 1,
      "explanation": "Die Spur ist definiert als die Summe der Elemente auf der Hauptdiagonalen: $Tr(A) = \\sum A_{ii}$. Interessanterweise ist sie auch gleich der Summe der Eigenwerte.",
      "weight": 1,
      "topic": "Lineare Algebra",
      "concept": "Matrix-Spur",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Trace",
          "definition": "Summe der Diagonalelemente."
        }
      ]
    },
    {
      "question": "17. Eine Funktion $f: \\mathbb{R}^n \\to \\mathbb{R}$ heißt konvex, wenn für alle $x, y$ und $\\lambda \\in [0, 1]$ gilt:",
      "options": [
        "$f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)$",
        "$f(\\lambda x + (1-\\lambda)y) \\geq \\lambda f(x) + (1-\\lambda)f(y)$",
        "$f(x+y) = f(x) + f(y)$",
        "$\\nabla f(x) = 0$"
      ],
      "answer": 0,
      "explanation": "Geometrisch bedeutet dies: Die Verbindungsstrecke (Sekante) zwischen zwei beliebigen Punkten auf dem Funktionsgraphen liegt immer *oberhalb* oder auf dem Graphen selbst. Dies garantiert, dass jedes lokale Minimum auch ein globales Minimum ist.",
      "weight": 2,
      "topic": "Optimierung",
      "concept": "Konvexität",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Konvexitäts-Ungleichung",
        "steps": [
          "Linke Seite: Funktionswert an einer Stelle zwischen x und y.",
          "Rechte Seite: Gewichteter Durchschnitt der Funktionswerte von x und y (Gerade).",
          "Bedingung: Funktion liegt 'unter' der Geraden.",
          "Wichtigkeit: Konvexe Probleme sind effizient lösbar."
        ],
        "content": "Lineare Regression und SVMs basieren auf konvexen Optimierungsproblemen."
      },
      "mini_glossary": [
        {
          "term": "Globales Minimum",
          "definition": "Tiefster Punkt der gesamten Funktion, nicht nur einer Umgebung."
        }
      ]
    },
    {
      "question": "18. Für welche Matrixoperation wird die Determinante benötigt, um die Existenz zu prüfen?",
      "options": [
        "Matrixaddition",
        "Transposition",
        "Inversion",
        "Multiplikation"
      ],
      "answer": 2,
      "explanation": "Eine quadratische Matrix ist genau dann invertierbar, wenn ihre Determinante ungleich 0 ist. Wenn $\\det(A) = 0$, ist die Matrix singulär.",
      "weight": 1,
      "topic": "Lineare Algebra",
      "concept": "Determinante",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Singulär",
          "definition": "Nicht invertierbare Matrix ($\\det = 0$)."
        }
      ]
    },
    {
      "question": "19. Welcher Begriff beschreibt den Prozess des Findens der optimalen Parameter $\\theta$, die die Wahrscheinlichkeit der beobachteten Daten maximieren?",
      "options": [
        "Gradient Descent",
        "Maximum Likelihood Estimation (MLE)",
        "Bayesian Inference",
        "Principal Component Analysis"
      ],
      "answer": 1,
      "explanation": "MLE ist das statistische Prinzip, Parameter so zu wählen, dass die Daten unter dem Modell am wahrscheinlichsten erscheinen. Gradient Descent ist oft der *Algorithmus*, um dieses Maximum numerisch zu finden.",
      "weight": 2,
      "topic": "Statistik",
      "concept": "MLE",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "MLE Prinzip",
        "steps": [
          "Gegeben: Daten $X$.",
          "Modell: Wahrscheinlichkeit $P(X|\\theta)$.",
          "Ziel: Finde $\\theta$, das $P(X|\\theta)$ maximiert.",
          "Methode: Leite Likelihood nach $\\theta$ ab und setze 0."
        ],
        "content": "Die Methode der kleinsten Quadrate (Least Squares) ist ein Spezialfall von MLE unter Annahme normalverteilter Fehler."
      },
      "mini_glossary": [
        {
          "term": "Likelihood",
          "definition": "Wahrscheinlichkeit der Daten als Funktion der Parameter."
        }
      ]
    },
    {
      "question": "20. Welche Aussage über Eigenvektoren $v$ und Eigenwerte $\\lambda$ einer Matrix $A$ ist korrekt?",
      "options": [
        "$Av = \\lambda v$",
        "$Av = v + \\lambda$",
        "$A\\lambda = v$",
        "$v^T A = \\lambda$"
      ],
      "answer": 0,
      "explanation": "Ein Eigenvektor ist ein Vektor, dessen Richtung sich durch die Anwendung der linearen Abbildung $A$ nicht ändert, sondern nur skaliert wird. Der Skalierungsfaktor ist der Eigenwert $\\lambda$.",
      "weight": 1,
      "topic": "Lineare Algebra",
      "concept": "Eigenwert-Gleichung",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Lineare Abbildung",
          "definition": "Funktion zwischen Vektorräumen, die Addition und Skalarmultiplikation erhält."
        }
      ]
    },
    {
      "question": "21. Sie nutzen die Kettenregel, um den Gradienten in einem tiefen neuronalen Netz zu berechnen. Wenn $f(x) = g(h(x))$, was ist $f'(x)$?",
      "options": [
        "$g'(h(x)) \\cdot h'(x)$",
        "$g'(x) \\cdot h'(x)$",
        "$g'(h'(x))$",
        "$g(x) + h(x)$"
      ],
      "answer": 0,
      "explanation": "Die Kettenregel besagt: Äußere Ableitung (an der Stelle der inneren Funktion) mal innere Ableitung. Dies ist das fundamentale Prinzip von Backpropagation.",
      "weight": 2,
      "topic": "Analysis",
      "concept": "Kettenregel",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Backpropagation",
        "steps": [
          "Forward Pass: Berechne $z = h(x)$ und $y = g(z)$.",
          "Backward Pass: Wir wollen Änderung von y bezüglich x.",
          "Kettenregel: $\\frac{dy}{dx} = \\frac{dy}{dz} \\cdot \\frac{dz}{dx}$.",
          "Layer für Layer: Multiplikation der lokalen Gradienten."
        ],
        "content": "Ohne die Kettenregel könnten wir keine tiefen Netze trainieren."
      },
      "mini_glossary": [
        {
          "term": "Backpropagation",
          "definition": "Algorithmus zur effizienten Berechnung von Gradienten in Graphen/Netzen."
        }
      ]
    },
    {
      "question": "22. Welche Norm führt bei der Regularisierung (Lasso) dazu, dass viele Parameter exakt 0 werden (Sparsity)?",
      "options": [
        "L2-Norm ($||w||_2$)",
        "L1-Norm ($||w||_1$)",
        "L-Unendlich Norm",
        "Frobenius-Norm"
      ],
      "answer": 1,
      "explanation": "Die L1-Norm (Summe der Beträge) hat spitze Ecken im Parameterraum (Geometrie eines Oktaeders/Raute). Bei der Optimierung trifft man die Level-Sets der Kostenfunktion oft genau auf den Achsen, wo Parameter 0 sind.",
      "weight": 3,
      "topic": "Optimierung / ML",
      "concept": "Regularisierung & Normen",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "L1 vs L2 Geometrie",
        "steps": [
          "Constraint: Beschränkung der Parametergröße.",
          "L2 (Ridge): Kreisform. Berührungspunkt meist nicht auf Achse -> kleine Werte, aber nicht 0.",
          "L1 (Lasso): Rautenform. Ecken auf den Achsen. Berührungspunkt oft Ecke -> exakte 0.",
          "Nutzen: Feature Selection."
        ],
        "content": "L1 erzeugt 'Sparsity' (dünnbesetzte Vektoren), L2 verhindert nur extrem große Werte."
      },
      "mini_glossary": [
        {
          "term": "Sparsity",
          "definition": "Eigenschaft, dass viele Einträge eines Vektors null sind."
        }
      ]
    },
    {
      "question": "23. Was passiert mit der Determinante einer Matrix, wenn man zwei Zeilen vertauscht?",
      "options": [
        "Sie bleibt gleich.",
        "Sie ändert das Vorzeichen.",
        "Sie wird 0.",
        "Sie wird invertiert ($1/det$)."
      ],
      "answer": 1,
      "explanation": "Die Determinante ist eine alternierende Form. Das Vertauschen zweier Zeilen (oder Spalten) multipliziert den Wert mit -1.",
      "weight": 2,
      "topic": "Lineare Algebra",
      "concept": "Determinanten-Eigenschaften",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Determinante und Orientierung",
        "steps": [
          "Geometrie: Determinante misst Volumenänderung und Orientierung.",
          "Zeilentausch: Spiegelt das Koordinatensystem (z.B. x und y Achse tauschen).",
          "Effekt: Orientierung dreht sich um (Rechte-Hand-Regel wird Linke-Hand-Regel) -> Vorzeichenwechsel."
        ],
        "content": "Dies ist relevant beim Verständnis von Permutationsmatrizen."
      },
      "mini_glossary": [
        {
          "term": "Permutation",
          "definition": "Vertauschung von Elementen/Reihenfolgen."
        }
      ]
    },
    {
      "question": "24. Ein Vektor $v$ wird als Eigenvektor einer Matrix $A$ bezeichnet, wenn $v$ nicht der Nullvektor ist. Warum schließen wir den Nullvektor aus?",
      "options": [
        "Weil $A \\cdot 0 = \\lambda \\cdot 0$ für jedes beliebige $\\lambda$ wahr ist und der Eigenwert somit nicht eindeutig definiert wäre.",
        "Weil man durch 0 nicht teilen darf.",
        "Weil die Länge 0 Probleme bei der Normierung macht.",
        "Das ist nur Konvention ohne tieferen Grund."
      ],
      "answer": 0,
      "explanation": "Die Gleichung $A0 = 0$ gilt immer, egal welches $\\lambda$ man wählt. Der Nullvektor enthält keine Information über die Richtungs-Eigenschaften der Matrix.",
      "weight": 3,
      "topic": "Lineare Algebra",
      "concept": "Eigenvektor-Definition",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Triviale Lösung",
        "steps": [
          "Gleichung: $Av = \\lambda v$.",
          "Setze v=0: $A0 = 0$ und $\\lambda 0 = 0$.",
          "Ergebnis: $0=0$ ist immer wahr.",
          "Problem: Wir suchen spezifische Richtungen, 0 hat keine Richtung."
        ],
        "content": "Mathematische Definitionen schließen triviale Fälle oft aus, um Eindeutigkeit zu wahren."
      },
      "mini_glossary": [
        {
          "term": "Trivial",
          "definition": "Offensichtlich, einfach, oder ohne Informationsgehalt (wie die Lösung x=0)."
        }
      ]
    },
    {
      "question": "25. Wie berechnet man in NumPy die Inverse einer Matrix `A` (sofern sie existiert)?",
      "options": [
        "`np.inv(A)`",
        "`A ** -1`",
        "`np.linalg.inv(A)`",
        "`1 / A`"
      ],
      "answer": 2,
      "explanation": "Die Funktion liegt im Submodul `linalg`. `1/A` oder `A**-1` würde versuchen, die Operation elementweise durchzuführen, was mathematisch falsch ist für die Matrix-Inverse.",
      "weight": 1,
      "topic": "Lineare Algebra (Code)",
      "concept": "Inverse Matrix",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Inverse Matrix",
          "definition": "Matrix $A^{-1}$, für die gilt $A A^{-1} = I$."
        }
      ]
    },
    {
      "question": "26. Die Taylor-Reihe wird genutzt, um Funktionen anzunähern. Wie lautet die Approximation erster Ordnung (Linearisierung) einer Funktion $f(x)$ an der Stelle $a$?",
      "options": [
        "$f(a) + f'(a)(x-a)$",
        "$f(a) + f'(a)$",
        "$f(a) + \\frac{1}{2}f''(a)(x-a)^2$",
        "$f'(a)(x-a)$"
      ],
      "answer": 0,
      "explanation": "Dies ist die Gleichung der Tangente an der Stelle $a$. Sie besteht aus dem Stützwert $f(a)$ und der Steigung $f'(a)$ multipliziert mit dem Abstand $(x-a)$.",
      "weight": 2,
      "topic": "Analysis",
      "concept": "Taylor-Reihe",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Lokale Approximation",
        "steps": [
          "Idee: Ersetze komplexe Kurve durch Gerade.",
          "Startpunkt: $f(a)$.",
          "Richtung: $f'(a)$.",
          "Schrittweite: $(x-a)$.",
          "Gradient Descent: Basiert auf genau dieser Approximation erster Ordnung."
        ],
        "content": "Newton-Verfahren nutzt zusätzlich die Approximation zweiter Ordnung (Hesse-Matrix)."
      },
      "mini_glossary": [
        {
          "term": "Linearisierung",
          "definition": "Annäherung einer nicht-linearen Funktion durch eine lineare Funktion."
        }
      ]
    },
    {
      "question": "27. Sie haben eine Matrix $X$ (Daten) der Größe $N \\times D$. Um die Kovarianzmatrix der Features zu berechnen (nach Zentrierung), welche Operation führen Sie aus?",
      "options": [
        "$\\frac{1}{N-1} X^T X$",
        "$\\frac{1}{N-1} X X^T$",
        "$X + X^T$",
        "$\\det(X)$"
      ],
      "answer": 0,
      "explanation": "$X^T X$ berechnet die Skalarprodukte zwischen den Spalten (Features). Das Ergebnis ist eine $D \\times D$ Matrix. $X X^T$ wäre $N \\times N$ (Ähnlichkeit zwischen Samples).",
      "weight": 3,
      "topic": "Statistik / LinAlg",
      "concept": "Kovarianzmatrix Berechnung",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Datenmatrix Geometrie",
        "steps": [
          "X: Zeilen=Samples, Spalten=Features.",
          "Zentriert: Mittelwert jeder Spalte ist 0.",
          "Kovarianz: $E[(x-\\mu)(y-\\mu)]$.",
          "Matrix-Form: Summe über alle Samples entspricht Skalarprodukt der Spaltenvektoren -> $X^T X$."
        ],
        "content": "Dies ist der erste Schritt der PCA."
      },
      "mini_glossary": [
        {
          "term": "Sample",
          "definition": "Ein Datenpunkt/Beobachtung (Zeile)."
        },
        {
          "term": "Feature",
          "definition": "Merkmal/Eigenschaft (Spalte)."
        }
      ]
    },
    {
      "question": "28. Was bedeutet es, wenn ein Optimierungsproblem 'constrained' (nebenbedingungsbehaftet) ist? Welches mathematische Werkzeug hilft hier?",
      "options": [
        "Man nutzt Lagrange-Multiplikatoren, um die Nebenbedingungen in die Zielfunktion zu integrieren.",
        "Man ignoriert die Nebenbedingungen und prüft am Ende.",
        "Man setzt den Gradienten auf unendlich.",
        "Man kann das Problem nicht lösen."
      ],
      "answer": 0,
      "explanation": "Lagrange-Multiplikatoren erweitern die Zielfunktion $L(x, \\lambda) = f(x) - \\lambda g(x)$. Dies erlaubt das Finden von Extrema unter der Bedingung $g(x)=0$.",
      "weight": 2,
      "topic": "Optimierung",
      "concept": "Lagrange-Multiplikatoren",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Lagrange Funktion",
        "steps": [
          "Original: Minimiere $f(x)$ unter $g(x)=0$.",
          "Idee: Gradienten von $f$ und $g$ müssen parallel sein.",
          "Lagrange: $\\nabla f = \\lambda \\nabla g$.",
          "Lösung: Suche stationäre Punkte von $L(x, \\lambda)$."
        ],
        "content": "Grundlage für Support Vector Machines (SVM) und viele Regularisierungsverfahren."
      },
      "mini_glossary": [
        {
          "term": "Constraint",
          "definition": "Einschränkung/Nebenbedingung, die die Lösung erfüllen muss."
        }
      ]
    },
    {
      "question": "29. Ein neuronaler Layer führt die Operation $y = \\sigma(Wx + b)$ aus. Wenn $x$ Dimension $d$ hat und der Layer $k$ Neuronen besitzt, welche Dimension hat der Bias-Vektor $b$?",
      "options": [
        "$d$",
        "$k$",
        "$d \\times k$",
        "$1$"
      ],
      "answer": 1,
      "explanation": "Jedes der $k$ Neuronen hat seinen eigenen Bias-Wert (Schwellenwert). Der Output $y$ hat Dimension $k$, daher muss auch $b$ Dimension $k$ haben, damit die Vektoraddition $Wx + b$ funktioniert.",
      "weight": 2,
      "topic": "Lineare Algebra / DL",
      "concept": "Dimensionen in NN",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Layer Dimensionen",
        "steps": [
          "Input x: Vektor der Länge d.",
          "Gewichte W: Matrix $k \\times d$.",
          "Produkt Wx: Vektor der Länge k.",
          "Bias b: Muss zum Addieren Länge k haben."
        ],
        "content": "Broadcasting in Python erlaubt manchmal b als Skalar, aber mathematisch ist es ein Vektor."
      },
      "mini_glossary": [
        {
          "term": "Bias",
          "definition": "Verschiebungsparameter (Intercept) in einem linearen Modell."
        }
      ]
    },
    {
      "question": "30. Warum ist die Sigmoid-Funktion $\\sigma(x) = \\frac{1}{1+e^{-x}}$ für sehr große oder sehr kleine x problematisch für das Training (Gradient Descent)?",
      "options": [
        "Sie ist dort nicht definiert.",
        "Ihre Ableitung (Gradient) geht gegen 0 (Vanishing Gradient), wodurch das Lernen stoppt.",
        "Sie wird unendlich groß.",
        "Sie wird negativ."
      ],
      "answer": 1,
      "explanation": "Die Sigmoid-Funktion flacht für große Beträge von $x$ ab (Sättigung). Die Tangente wird waagerecht, die Ableitung somit nahe 0. Da Gradient Descent die Ableitung zum Update nutzt, ändern sich die Gewichte kaum noch.",
      "weight": 3,
      "topic": "Analysis / DL",
      "concept": "Vanishing Gradient",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Sättigung von Aktivierungsfunktionen",
        "steps": [
          "Sigmoid Ableitung: $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$.",
          "Grenzwerte: Für $x \\to \\pm \\infty$ geht $\\sigma'$ gegen 0.",
          "Kettenregel: Viele kleine Zahlen multipliziert ergeben fast 0.",
          "Folge: Tiefe Netze lernen nicht."
        ],
        "content": "Dies führte zur Popularität von ReLU (Rectified Linear Unit), deren Ableitung 1 ist (für x>0)."
      },
      "mini_glossary": [
        {
          "term": "Sättigung",
          "definition": "Bereich einer Funktion, in dem sie sich kaum noch ändert."
        }
      ]
    }
  ]
}
