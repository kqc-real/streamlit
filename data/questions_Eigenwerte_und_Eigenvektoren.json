{
  "meta": {
    "title": "Eigenwerte & Eigenvektoren – Charakteristisches Polynom",
    "created": "30.10.2025 20:15",
    "modified": "30.10.2025 20:15",
    "target_audience": "Anfänger ohne Vorkenntnisse",
    "question_count": 30,
    "difficulty_profile": {
      "leicht": 12,
      "mittel": 15,
      "schwer": 3
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 25,
    "language": "de"
  },
  "questions": [
    {
      "question": "1. Was drückt die Gleichung $Av=\\lambda v$ begrifflich über $v$ aus?",
      "options": [
        "Die Richtung von $v$ bleibt unter $A$ erhalten und es erfolgt nur eine Skalierung.",
        "Die Richtung von $v$ wird immer orthogonal gedreht.",
        "Der Vektor $v$ wird um einen festen Betrag verschoben.",
        "Der Vektor $v$ wird unabhängig von $\\lambda$ auf 0 abgebildet."
      ],
      "answer": 0,
      "explanation": "Ein Eigenvektor ändert unter der linearen Abbildung nur seine Länge (und ggf. Vorzeichen), nicht seine Richtung.",
      "weight": 1,
      "topic": "Grundbegriffe & Definitionen",
      "mini_glossary": {
        "Eigenwert": "Skalar $\\lambda$, der die Streckung/Spiegelung eines Eigenvektors beschreibt.",
        "Eigenvektor": "Von 0 verschiedener Vektor, der durch $A$ nur skaliert wird."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "2. Wozu dient das charakteristische Polynom $p_A(\\lambda)=\\det(A-\\lambda I)$?",
      "options": [
        "Seine Nullstellen sind genau die Eigenwerte von $A$.",
        "Es liefert die Spaltennormen von $A$.",
        "Es bestimmt die Rangzahl von $A$ ohne Bezug zu $\\lambda$.",
        "Es ist nur für diagonale Matrizen definiert."
      ],
      "answer": 0,
      "explanation": "Eigenwerte sind die Nullstellen des charakteristischen Polynoms.",
      "weight": 1,
      "topic": "Charakteristisches Polynom",
      "mini_glossary": {
        "Determinante": "Skalar, der u.a. Nichtinvertierbarkeit anzeigt.",
        "Einheitsmatrix $I$": "Matrix mit Einsen auf der Diagonalen, sonst Nullen."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "3. Wie stehen algebraische und geometrische Vielfachheit eines Eigenwertes zueinander?",
      "options": [
        "Die geometrische Vielfachheit ist nie größer als die algebraische.",
        "Die algebraische Vielfachheit ist nie größer als die geometrische.",
        "Beide sind immer gleich.",
        "Zwischen beiden gibt es keinen Zusammenhang."
      ],
      "answer": 0,
      "explanation": "Allgemein gilt: geometrische Vielfachheit $\\le$ algebraische Vielfachheit.",
      "weight": 1,
      "topic": "Vielfachheiten & Eigenräume",
      "mini_glossary": {
        "Algebraische Vielfachheit": "Nullstellenvielfachheit eines Eigenwerts im charakteristischen Polynom.",
        "Geometrische Vielfachheit": "Dimension des Eigenraums $\\ker(A-\\lambda I)$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "4. Wann ist eine Matrix $A$ diagonalisierbar?",
      "options": [
        "Genau dann, wenn es eine Basis aus Eigenvektoren gibt.",
        "Genau dann, wenn alle Matrixeinträge ganzzahlig sind.",
        "Genau dann, wenn $\\det(A)=1$ gilt.",
        "Genau dann, wenn $A$ dreieckig ist."
      ],
      "answer": 0,
      "explanation": "Diagonalisierbarkeit bedeutet Existenz einer Eigenbasis.",
      "weight": 1,
      "topic": "Diagonalisierbarkeit & Ähnlichkeit",
      "mini_glossary": {
        "Eigenbasis": "Basis, die vollständig aus Eigenvektoren besteht.",
        "Ähnlichkeit": "Beziehung $A' = S^{-1}AS$; erhält Eigenwerte."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "5. Welche Aussage zum Spektralsatz für reell-symmetrische Matrizen ist korrekt?",
      "options": [
        "Reell-symmetrische Matrizen sind orthogonal diagonalisierbar.",
        "Reell-symmetrische Matrizen besitzen nur komplexe Eigenwerte.",
        "Reell-symmetrische Matrizen sind nie diagonalisierbar.",
        "Reell-symmetrische Matrizen haben stets Determinante 1."
      ],
      "answer": 0,
      "explanation": "Der Spektralsatz garantiert reelle Eigenwerte und eine orthogonale Eigenbasis.",
      "weight": 1,
      "topic": "Reell-symmetrisch & Spektralsatz",
      "mini_glossary": {
        "Orthogonale Diagonalisierung": "Darstellung $A=QDQ^{\\top}$ mit orthogonalem $Q$.",
        "Spektralsatz": "Fundamentalsatz zum Eigenwertverhalten symmetrischer Matrizen."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "6. Welche Aussage über Dreiecks- und Diagonalmatrizen trifft zu?",
      "options": [
        "Ihre Eigenwerte sind die Diagonaleinträge (mit Vielfachheiten).",
        "Ihre Eigenwerte sind die Spaltensummen.",
        "Sie besitzen genau einen Eigenwert.",
        "Sie sind nur dann diagonal, wenn alle Nebendiagonalen 1 sind."
      ],
      "answer": 0,
      "explanation": "Für Dreiecks- und Diagonalmatrizen liest man die Eigenwerte direkt von der Hauptdiagonale ab.",
      "weight": 1,
      "topic": "Dreiecks- und Diagonalmatrizen",
      "mini_glossary": {
        "Hauptdiagonale": "Einträge $a_{11},\\dots,a_{nn}$.",
        "Vielfachheit": "Wiederholungsanzahl eines Eigenwerts."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "7. Welche Größe stimmt mit der Summe der Eigenwerte überein?",
      "options": [
        "Die Spur von $A$.",
        "Die Determinante von $A$.",
        "Die Rangzahl von $A$.",
        "Die Frobeniusnorm von $A$."
      ],
      "answer": 0,
      "explanation": "Spur($A$) = Summe der Eigenwerte (algebraische Vielfachheiten gezählt).",
      "weight": 1,
      "topic": "Plausibilitätschecks (Spur/Det)",
      "mini_glossary": {
        "Spur": "Summe der Diagonaleinträge.",
        "Eigenwertsumme": "Summe der Eigenwerte unter Berücksichtigung der Vielfachheiten."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "8. Welche Aussage zur Jordan-Normalform (JNF) ist korrekt?",
      "options": [
        "Nichtdiagonalisierbarkeit zeigt sich durch Jordan-Blöcke größer als $1\\times1$.",
        "Sie existiert nur für symmetrische Matrizen.",
        "Sie ändert die Eigenwerte der Matrix.",
        "Sie ist identisch mit der QR-Zerlegung."
      ],
      "answer": 0,
      "explanation": "Jordan-Blöcke fassen generalisierte Eigenvektoren zusammen und erklären Nichtdiagonalisierbarkeit.",
      "weight": 1,
      "topic": "Jordan-Basiswissen",
      "mini_glossary": {
        "Jordan-Block": "Block mit gleichem Eigenwert auf der Diagonalen und ggf. Einsen darüber.",
        "Generalisierter Eigenvektor": "Vektor mit $(A-\\lambda I)^k v=0$ für ein $k>1$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "9. Welche Größe bleibt bei einer Ähnlichkeitstransformation $A' = S^{-1}AS$ erhalten?",
      "options": [
        "Das Spektrum (die Menge der Eigenwerte).",
        "Jeder einzelne Matrixeintrag.",
        "Die Spaltennormen.",
        "Die Lage der Nebendiagonalen-Einsen."
      ],
      "answer": 0,
      "explanation": "Eigenwerte sind Ähnlichkeitsinvarianten.",
      "weight": 1,
      "topic": "Basiswechsel & Invarianten",
      "mini_glossary": {
        "Ähnlichkeitsinvariante": "Größe, die unter $S^{-1}AS$ unverändert bleibt.",
        "Spektrum": "Multimenge der Eigenwerte einer Matrix."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "10. Welche Aussage zur numerischen Eigenwertberechnung ist korrekt?",
      "options": [
        "Iterative Verfahren wie der QR-Algorithmus sind Standard zur Eigenwertapproximation.",
        "Nur exakte symbolische Methoden sind praktikabel.",
        "Gauß-Elimination liefert direkt Eigenwerte ohne Anpassung.",
        "Eigenwerte lassen sich nicht numerisch bestimmen."
      ],
      "answer": 0,
      "explanation": "Der QR-Algorithmus ist das klassische robuste Verfahren zur numerischen Bestimmung von Eigenwerten.",
      "weight": 1,
      "topic": "Numerik & Stabilität",
      "mini_glossary": {
        "QR-Algorithmus": "Iteratives Verfahren auf Basis der QR-Zerlegung.",
        "Stabilität": "Empfindlichkeit eines Verfahrens gegenüber Rundungsfehlern."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "11. Welches Strukturmerkmal besitzt $p_A(\\lambda)$ für eine $n\\times n$-Matrix?",
      "options": [
        "Der führende Koeffizient ist $(-1)^n$.",
        "Das Polynom hat stets Grad $n-1$.",
        "Das konstante Glied ist immer 0.",
        "Die Koeffizienten sind unabhängig von $A$."
      ],
      "answer": 0,
      "explanation": "Aus $\\det(A-\\lambda I)$ folgt der führende Koeffizient $(-1)^n$ und Grad $n$.",
      "weight": 1,
      "topic": "Charakteristisches Polynom",
      "mini_glossary": {
        "Führender Koeffizient": "Koeffizient der höchsten Potenz.",
        "Grad": "Höchste Potenz von $\\lambda$ mit nichtverschwindendem Koeffizienten."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "12. Welche Aussage zum Eigenraum $E_{\\lambda}$ ist korrekt?",
      "options": [
        "Er ist der Kern von $(A-\\lambda I)$ und damit ein Unterraum.",
        "Er ist die Menge aller normierten Vektoren.",
        "Er ist nie eindimensional.",
        "Er enthält keine Nullvektoren."
      ],
      "answer": 0,
      "explanation": "Per Definition $E_{\\lambda}=\\ker(A-\\lambda I)$; Unterräume enthalten stets den Nullvektor.",
      "weight": 1,
      "topic": "Grundbegriffe & Definitionen",
      "mini_glossary": {
        "Kern": "Menge aller Vektoren, die auf 0 abgebildet werden.",
        "Unterraum": "Nichtleere Menge, abgeschlossen unter Linearkombinationen."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "13. Welche Aussage beschreibt den Zusammenhang zwischen $p_A$ und $p_{S^{-1}AS}$?",
      "options": [
        "Beide Polynome sind identisch.",
        "Sie unterscheiden sich immer in allen Koeffizienten.",
        "Sie haben nur dieselben Nullstellen, aber unterschiedliche Grade.",
        "Sie stimmen nur im führenden Koeffizienten überein."
      ],
      "answer": 0,
      "explanation": "Ähnliche Matrizen besitzen dasselbe charakteristische Polynom.",
      "weight": 2,
      "topic": "Vielfachheiten & Eigenräume",
      "extended_explanation": {
        "titel": "Ähnlichkeit und charakteristisches Polynom",
        "schritte": [
          "Für $A' = S^{-1}AS$ gilt $A'-\\lambda I=S^{-1}(A-\\lambda I)S$.",
          "Die Determinante ist multiplikativ und $\\det(S)\\det(S^{-1})=1$.",
          "Daher $p_{A'}(\\lambda)=p_A(\\lambda)$."
        ]
      },
      "mini_glossary": {
        "Ähnlichkeit": "Transformation $S^{-1}AS$ mit invertierbarem $S$.",
        "Polynominvarianz": "Unveränderlichkeit von $p_A$ unter Ähnlichkeit."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "14. Woran scheitert Diagonalisierbarkeit typischerweise?",
      "options": [
        "Mindestens ein Eigenwert hat geometrische Vielfachheit kleiner als seine algebraische.",
        "Die Determinante ist ungleich Null.",
        "Die Spur ist ungleich Null.",
        "Die Matrix ist orthogonal."
      ],
      "answer": 0,
      "explanation": "Fehlen genügend linear unabhängige Eigenvektoren, existiert keine Eigenbasis.",
      "weight": 2,
      "topic": "Diagonalisierbarkeit & Ähnlichkeit",
      "extended_explanation": {
        "titel": "Vielfachheiten als Kriterium",
        "schritte": [
          "Algebraische Vielfachheit zählt Nullstellen im Polynom.",
          "Geometrische Vielfachheit ist die Eigenraumdimension.",
          "Diagonalisierbar nur, wenn Summen der geometrischen Vielfachheiten $n$ ergeben."
        ]
      },
      "mini_glossary": {
        "Lineare Unabhängigkeit": "Kein Vektor ist Linearkombination der anderen.",
        "Eigenbasis": "Basis aus Eigenvektoren."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "15. Welche Aussage über Eigenvektoren reell-symmetrischer Matrizen ist korrekt?",
      "options": [
        "Eigenvektoren zu verschiedenen Eigenwerten sind orthogonal.",
        "Eigenvektoren zu verschiedenen Eigenwerten sind linear abhängig.",
        "Eigenvektoren können nicht normiert werden.",
        "Eigenvektoren existieren nur im Komplexen."
      ],
      "answer": 0,
      "explanation": "Für reell-symmetrische Matrizen sind Eigenräume zu verschiedenen Eigenwerten orthogonal.",
      "weight": 2,
      "topic": "Reell-symmetrisch & Spektralsatz",
      "extended_explanation": {
        "titel": "Orthogonalität aus Symmetrie",
        "schritte": [
          "Symmetrie impliziert Selbstadjungiertheit bezüglich des Standardskalarprodukts.",
          "Eigenräume verschiedener Eigenwerte sind orthogonal.",
          "Daraus folgt die orthogonale Diagonalisierbarkeit."
        ]
      },
      "mini_glossary": {
        "Orthogonalität": "Skalarprodukt ist 0.",
        "Selbstadjungiert": "Gleichheit mit der Transponierten im Reellen."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "16. Welche Eigenschaft von Dreiecksmatrizen ist für Eigenwerte entscheidend?",
      "options": [
        "Die Eigenwerte stehen auf der Hauptdiagonale, unabhängig von den Nebendiagonalen.",
        "Die Eigenwerte sind die Zeilensummen.",
        "Die Eigenwerte sind die Spaltensummen.",
        "Die Eigenwerte sind stets alle gleich."
      ],
      "answer": 0,
      "explanation": "Bei Dreiecksmatrizen ergeben sich Eigenwerte direkt aus den Diagonaleinträgen.",
      "weight": 2,
      "topic": "Dreiecks- und Diagonalmatrizen",
      "extended_explanation": {
        "titel": "Ablesen statt Rechnen",
        "schritte": [
          "Das charakteristische Polynom faktorisiert zu Produkt von $(a_{ii}-\\lambda)$.",
          "Nebendiagonaleinträge ändern die Eigenwerte nicht.",
          "Vielfachheiten ergeben sich aus Wiederholungen auf der Diagonale."
        ]
      },
      "mini_glossary": {
        "Nebendiagonale": "Diagonale von links unten nach rechts oben.",
        "Faktorisierung": "Zerlegung in Linearfaktoren."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "17. Welche Beziehung ist ein schneller Plausibilitätscheck für gefundene Eigenwerte?",
      "options": [
        "Summe der Eigenwerte = Spur, Produkt der Eigenwerte = Determinante.",
        "Summe der Eigenwerte = Determinante, Produkt = Spur.",
        "Spur = Rang, Produkt = Norm.",
        "Produkt = Rang, Summe = Norm."
      ],
      "answer": 0,
      "explanation": "Diese Identitäten gelten allgemein (Vielfachheiten gezählt).",
      "weight": 2,
      "topic": "Plausibilitätschecks (Spur/Det)",
      "extended_explanation": {
        "titel": "Koeffizientenvergleich von $p_A$",
        "schritte": [
          "Aus Viète-Beziehungen für $p_A$ folgen Summen- und Produktrelationen.",
          "Spur und Determinante sind effektiv berechenbar.",
          "Abgleich deckt viele Fehler auf."
        ]
      },
      "mini_glossary": {
        "Viète-Beziehungen": "Zusammenhang zwischen Wurzelsummen/-produkten und Polynomkoeffizienten.",
        "Determinante": "Skalar, gleich Produkt der Eigenwerte."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "18. Welche Aussage charakterisiert Jordan-Blöcke in Bezug auf $p_A$ richtig?",
      "options": [
        "Jordan-Blöcke verändern nicht die Eigenwerte, wohl aber die geometrischen Vielfachheiten.",
        "Jordan-Blöcke ändern die Eigenwerte zu neuen Werten.",
        "Jordan-Blöcke existieren nur bei diagonalen Matrizen.",
        "Jordan-Blöcke machen $p_A$ nicht definierbar."
      ],
      "answer": 0,
      "explanation": "Die Eigenwerte bleiben identisch; die Blockgröße spiegelt die algebraische Vielfachheit wider, die Eigenraumdimension die geometrische.",
      "weight": 2,
      "topic": "Jordan-Basiswissen",
      "extended_explanation": {
        "titel": "Eigenwerte vs. Blockstruktur",
        "schritte": [
          "Algebraische Vielfachheit = Summe der Blockgrößen je Eigenwert.",
          "Geometrische Vielfachheit = Anzahl der Blöcke je Eigenwert.",
          "Damit erklärt Jordan-Struktur Nichtdiagonalisierbarkeit."
        ]
      },
      "mini_glossary": {
        "Blockgröße": "Dimension eines Jordan-Blocks.",
        "Geometrische Vielfachheit": "Zahl der Blöcke für einen Eigenwert."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "19. Welche Größe ändert sich bei einem reinen Basiswechsel der Koordinaten nicht?",
      "options": [
        "Das charakteristische Polynom.",
        "Der einzelne Wert jedes Matrixeintrags.",
        "Die Norm jedes Vektors.",
        "Die Einträge der Nebendiagonale."
      ],
      "answer": 0,
      "explanation": "Ähnlichkeitsinvarianz bewahrt $p_A$ und damit das Spektrum.",
      "weight": 2,
      "topic": "Basiswechsel & Invarianten",
      "extended_explanation": {
        "titel": "Invarianten identifizieren",
        "schritte": [
          "Koordinatenwechsel entsprechen Ähnlichkeitstransformationen.",
          "Determinante und Spur sind ebenfalls invariant.",
          "Einzelne Matrixeinträge sind koordinatenabhängig."
        ]
      },
      "mini_glossary": {
        "Invariante": "Größe, die unter Transformationen erhalten bleibt.",
        "Koordinatenwechsel": "Darstellung derselben Abbildung in anderer Basis."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "20. Welche numerische Herausforderung besteht bei der expliziten Bildung von $p_A$ für große Matrizen?",
      "options": [
        "Die Bildung ist numerisch instabil und ineffizient; moderne Verfahren vermeiden das Polynom.",
        "Die Bildung ist immer der stabilste Weg.",
        "Die Bildung ist notwendig, um iterative Verfahren zu starten.",
        "Die Bildung liefert bessere Kondition als QR-Verfahren."
      ],
      "answer": 0,
      "explanation": "Direkte Polynomkoeffizienten sind empfindlich; stabile Iterationen (z.B. QR) arbeiten matrixnah.",
      "weight": 2,
      "topic": "Numerik & Stabilität",
      "extended_explanation": {
        "titel": "Warum nicht explizit $p_A$?",
        "schritte": [
          "Koeffizienten von $p_A$ sind schlecht konditioniert.",
          "Rundungsfehler können die Wurzeln stark verfälschen.",
          "Matrixnahe Iterationen umgehen diese Probleme."
        ]
      },
      "mini_glossary": {
        "Kondition": "Empfindlichkeit der Lösung gegenüber Datenstörungen.",
        "Rundungsfehler": "Fehler durch endliche Maschinenarithmetik."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "21. Welche Strukturinformation enthält das konstante Glied von $p_A(\\lambda)$ (bis auf Vorzeichen)?",
      "options": [
        "Es ist die Determinante von $A$.",
        "Es ist die Spur von $A$.",
        "Es ist der Rang von $A$.",
        "Es ist die Frobeniusnorm von $A$."
      ],
      "answer": 0,
      "explanation": "Das konstante Glied von $\\det(A-\\lambda I)$ ist (bis auf Vorzeichen) $\\det(A)$.",
      "weight": 2,
      "topic": "Charakteristisches Polynom",
      "extended_explanation": {
        "titel": "Viète-Bezug der Koeffizienten",
        "schritte": [
          "Die Koeffizienten von $p_A$ sind elementarsymmetrische Funktionen der Eigenwerte.",
          "Das konstante Glied ergibt das Produkt der Eigenwerte.",
          "Damit ist es bis auf Vorzeichen die Determinante."
        ]
      },
      "mini_glossary": {
        "Elementarsymmetrisch": "Symmetrische Funktionen wie Summen/Produkte über Wurzeln.",
        "Konstantes Glied": "Koeffizient bei $\\lambda^0$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "22. Welche Aussage zur Dimension der direkten Summe der Eigenräume ist korrekt?",
      "options": [
        "Sie ist höchstens $n$ und gleich $n$ genau dann, wenn $A$ diagonalisierbar ist.",
        "Sie ist immer kleiner als $n$.",
        "Sie ist unabhängig von der Diagonalisierbarkeit.",
        "Sie ist genau die algebraische Vielfachheit eines festen Eigenwerts."
      ],
      "answer": 0,
      "explanation": "Ergibt die Summe der Eigenraumdimensionen $n$, existiert eine Eigenbasis.",
      "weight": 2,
      "topic": "Vielfachheiten & Eigenräume",
      "extended_explanation": {
        "titel": "Eigenräume und Direktheit",
        "schritte": [
          "Eigenräume verschiedener Eigenwerte schneiden sich nur im Nullvektor.",
          "Ihre direkte Summe hat Dimension gleich der Summe der Dimensionen.",
          "Gleich $n$ impliziert Eigenbasis und Diagonalisierbarkeit."
        ]
      },
      "mini_glossary": {
        "Direkte Summe": "Zerlegung ohne nichttrivialen Schnitt.",
        "Eigenbasis": "Basis aus Eigenvektoren."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "23. Welche Bedingung an $P$ in $A=PDP^{-1}$ ist notwendig?",
      "options": [
        "Die Spalten von $P$ müssen linear unabhängig sein.",
        "Die Zeilen von $P$ müssen orthonormal sein.",
        "Die Diagonale von $P$ muss 1 sein.",
        "Die Matrix $P$ muss symmetrisch sein."
      ],
      "answer": 0,
      "explanation": "Linear unabhängige Spalten garantieren die Invertierbarkeit von $P$.",
      "weight": 2,
      "topic": "Diagonalisierbarkeit & Ähnlichkeit",
      "extended_explanation": {
        "titel": "Rolle der Übergangsmatrix",
        "schritte": [
          "Invertierbarkeit ist Voraussetzung für $P^{-1}$.",
          "Spalten von $P$ sind Eigenvektoren bei Diagonalisierung.",
          "Lineare Unabhängigkeit der Eigenvektoren ist entscheidend."
        ]
      },
      "mini_glossary": {
        "Invertierbarkeit": "Existenz der inversen Matrix.",
        "Übergangsmatrix": "Matrix des Basiswechsels."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "24. Welche Zusatzstruktur besitzen reell-symmetrische Matrizen in Bezug auf Eigenräume gleicher Eigenwerte?",
      "options": [
        "Es existiert eine orthonormale Basis in jedem Eigenraum.",
        "Es existieren keine orthogonalen Eigenvektoren.",
        "Eigenräume sind nie mehrdimensional.",
        "Orthonormalisierung zerstört Eigenvektoreigenschaften."
      ],
      "answer": 0,
      "explanation": "Jeder Eigenraum lässt sich orthonormalisieren, ohne die Eigenvektoreigenschaft zu verlieren.",
      "weight": 2,
      "topic": "Reell-symmetrisch & Spektralsatz",
      "extended_explanation": {
        "titel": "Orthonormalbasen in Eigenräumen",
        "schritte": [
          "Gram-Schmidt kann innerhalb eines Eigenraums angewendet werden.",
          "Eigenvektoreigenschaft bleibt erhalten.",
          "Dies erleichtert die orthogonale Diagonalisierung."
        ]
      },
      "mini_glossary": {
        "Orthonormalbasis": "Orthogonale Vektoren der Norm 1.",
        "Gram–Schmidt": "Verfahren zur Orthonormalisierung."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "25. Welche Aussage zur Invarianz der Spur ist korrekt?",
      "options": [
        "Die Spur ist invariant unter Ähnlichkeitstransformationen.",
        "Die Spur ändert sich bei jedem Basiswechsel.",
        "Die Spur ist nur für diagonale Matrizen definiert.",
        "Die Spur ist das Produkt der Eigenwerte."
      ],
      "answer": 0,
      "explanation": "Spur($A$) = Spur($S^{-1}AS$); sie stimmt mit der Eigenwertsumme überein.",
      "weight": 2,
      "topic": "Basiswechsel & Invarianten",
      "extended_explanation": {
        "titel": "Spur als Ähnlichkeitsinvariante",
        "schritte": [
          "Spur ist lineare, zyklisch invarianten Funktion.",
          "Unter $S^{-1}AS$ bleibt sie unverändert.",
          "Dies reflektiert die Invarianz der Eigenwertsumme."
        ]
      },
      "mini_glossary": {
        "Zyklische Invarianz": "Eigenschaft $\\operatorname{tr}(XY)=\\operatorname{tr}(YX)$.",
        "Eigenwertsumme": "Summe der Eigenwerte mit Vielfachheiten."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "26. Warum ist die direkte Bestimmung aller Wurzeln von $p_A$ numerisch heikel?",
      "options": [
        "Polynomwurzelsuche ist schlecht konditioniert; kleine Koeffizientenfehler führen zu großen Wurzelfehlern.",
        "Polynomwurzelsuche ist stets exakter als Matrizeniteration.",
        "Polynomwurzelsuche ist für jedes $n$ in konstanter Zeit möglich.",
        "Polynomwurzelsuche benötigt keine Fließkommaarithmetik."
      ],
      "answer": 0,
      "explanation": "Die Wurzelkarten reagieren empfindlich auf Koeffizientenperturbationen; deshalb arbeitet man matrixnah.",
      "weight": 2,
      "topic": "Numerik & Stabilität",
      "extended_explanation": {
        "titel": "Konditionsproblem der Wurzelsuche",
        "schritte": [
          "Kleine Änderungen der Koeffizienten verschieben die Wurzeln stark.",
          "Hohe Grade verstärken das Problem.",
          "Matrixnahe Verfahren umgehen die Koeffizientenebene."
        ]
      },
      "mini_glossary": {
        "Perturbation": "Kleine Störung der Eingangsdaten.",
        "Wurzelkarte": "Abbildung von Koeffizienten auf Polynomwurzeln."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "27. Welche Identität verbindet $A$ und $p_A$ allgemein (ohne Rechnung)?",
      "options": [
        "Cayley–Hamilton: $p_A(A)=0$.",
        "Gram–Schmidt: $p_A(A)=I$.",
        "Sylvester: $p_A(A)$ ist immer invertierbar.",
        "Schur: $p_A(A)$ ist orthogonal."
      ],
      "answer": 0,
      "explanation": "Der Satz von Cayley–Hamilton besagt, dass jede Matrix ihr eigenes charakteristisches Polynom erfüllt.",
      "weight": 2,
      "topic": "Plausibilitätschecks (Spur/Det)",
      "extended_explanation": {
        "titel": "Cayley–Hamilton konsequent gelesen",
        "schritte": [
          "Setze die Matrix formal in ihr Polynom ein.",
          "Das Ergebnis ist die Nullmatrix.",
          "Nützlich für Beweise und theoretische Abschätzungen."
        ]
      },
      "mini_glossary": {
        "Cayley–Hamilton": "Fundamentalsatz: $p_A(A)=0$.",
        "Nullmatrix": "Matrix mit nur Nulleinträgen."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "28. Welche Aussage zur Jordan-Form liefert das präziseste Kriterium für Nichtdiagonalisierbarkeit?",
      "options": [
        "Existieren zu einem Eigenwert Jordan-Blöcke größer als $1\\times1$, ist $A$ nicht diagonalisierbar.",
        "Existiert ein Null-Eigenwert, ist $A$ nie diagonalisierbar.",
        "Ist $\\det(A)\\neq0$, ist $A$ nie diagonalisierbar.",
        "Hat $A$ reelle Einträge, ist $A$ immer diagonalisierbar."
      ],
      "answer": 0,
      "explanation": "Blöcke größer als $1\\times1$ zeigen fehlende Eigenvektoren an.",
      "weight": 3,
      "topic": "Jordan-Basiswissen",
      "extended_explanation": {
        "titel": "Blockstruktur als Diagnose",
        "schritte": [
          "Größe der Blöcke reflektiert algebraische Vielfachheiten.",
          "Anzahl der Blöcke reflektiert geometrische Vielfachheiten.",
          "Größere Blöcke bedeuten unvollständige Eigenbasis."
        ]
      },
      "mini_glossary": {
        "Diagonalblock": "Block mit nur einem Eintrag auf der Diagonale.",
        "Nichtdiagonalisierbar": "Keine Eigenbasis vorhanden."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "29. Welche Aussage über das charakteristische Polynom ist korrekt und nichttrivial?",
      "options": [
        "Es ist invariant unter Ähnlichkeit und bestimmt die Eigenwerte einschließlich Vielfachheiten.",
        "Es ist invariant unter beliebigen Zeilenvertauschungen ohne Vorzeichenänderung.",
        "Es hängt nur von den Spaltensummen ab.",
        "Es bestimmt stets auch die geometrischen Vielfachheiten eindeutig."
      ],
      "answer": 0,
      "explanation": "Das Polynom kodiert die Nullstellen samt algebraischer Vielfachheiten, nicht jedoch die geometrischen.",
      "weight": 3,
      "topic": "Charakteristisches Polynom",
      "extended_explanation": {
        "titel": "Information und Grenzen von $p_A$",
        "schritte": [
          "Nullstellen liefern Spektrum und algebraische Vielfachheiten.",
          "Geometrische Vielfachheiten erfordern Eigenraumdimensionen.",
          "Ähnlichkeit bewahrt $p_A$ vollständig."
        ]
      },
      "mini_glossary": {
        "Spektrum": "Multimenge der Eigenwerte.",
        "Eigenraumdimension": "Geometrische Vielfachheit eines Eigenwerts."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "30. Welche Bedingung fasst Diagonalisierbarkeit in einem Satz zusammen?",
      "options": [
        "Eine Matrix ist genau dann diagonalisierbar, wenn die Summe der Dimensionen aller Eigenräume gleich $n$ ist.",
        "Eine Matrix ist genau dann diagonalisierbar, wenn $\\det(A)=0$ ist.",
        "Eine Matrix ist genau dann diagonalisierbar, wenn alle Einträge positiv sind.",
        "Eine Matrix ist genau dann diagonalisierbar, wenn sie normalisiert ist."
      ],
      "answer": 0,
      "explanation": "Ergibt die direkte Summe der Eigenräume die volle Dimension, existiert eine Eigenbasis.",
      "weight": 3,
      "topic": "Diagonalisierbarkeit & Ähnlichkeit",
      "extended_explanation": {
        "titel": "Eigenräume summieren sich zur Basis",
        "schritte": [
          "Eigenräume verschiedener Eigenwerte sind direkt summierbar.",
          "Gleichheit der Dimensionssumme mit $n$ liefert eine Eigenbasis.",
          "Dies ist äquivalent zur Existenz von $P$ mit $A=PDP^{-1}$."
        ]
      },
      "mini_glossary": {
        "Direkte Summe": "Zerlegung mit trivialem Schnitt.",
        "Eigenbasis": "Basis aus Eigenvektoren über den ganzen Raum."
      },
      "cognitive_level": "Analyse"
    }
  ]
}
