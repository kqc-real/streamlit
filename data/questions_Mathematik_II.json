{
  "meta": {
    "title": "Mathematik II (Probeklausur)",
    "created": "26.10.2025 00:00",
    "modified": "26.10.2025 13:00",
    "target_audience": "Fortgeschrittene Studierende",
    "question_count": 40,
    "difficulty_profile": {
      "leicht": 12,
      "mittel": 20,
      "schwer": 8
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 35,
    "language": "de"
  },
  "questions": [
    {
      "question": "1. Welche Aussage ist eine Folgerung aus den Vektorraumeigenschaften (und selbst kein Axiom)?",
      "options": [
        "$0 \\cdot v = 0$ gilt für alle Vektoren v.",
        "$\\alpha \\cdot (\\beta \\cdot v) = (\\alpha\\beta) \\cdot v$ für alle Skalare und Vektoren.",
        "$1 \\cdot v = v$ für alle Vektoren v.",
        "$\\alpha \\cdot (u+v) = \\alpha \\cdot u + \\alpha \\cdot v$ für alle u,v.",
        "$u+(-u)=0$ für alle u."
      ],
      "answer": 0,
      "explanation": "Die Gleichung $0 \\cdot v = 0$ folgt aus den Axiomen, ist aber selbst kein Axiom. Die anderen Aussagen sind Standardaxiome eines Vektorraums.",
      "weight": 1,
      "topic": "LE 11: Vektorräume",
      "mini_glossary": {
        "Vektorraumaxiome": "Axiome über $V$ und Körper $K$: $(V,+)$ ist abelsche Gruppe; Distributivität $\\alpha(u+v)=\\alpha u+\\alpha v$, $(\\alpha+\\beta)v=\\alpha v+\\beta v$; Assoziativität $\\alpha(\\beta v)=(\\alpha\\beta)v$; Einheitsgesetz $1\\cdot v=v$. Daraus folgt u.a. $0\\cdot v=0$ und $(-1)\\cdot v=-v$.",
        "Nullvektor": "Eindeutiges Element $0\\in V$ mit $v+0=v$ für alle $v\\in V$. Eindeutigkeit: Gilt $0'$ ebenfalls neutral, so $0=0'+0=0'$.",
        "Skalarmultiplikation": "Abbildung $K\\times V\\to V$, $(\\alpha,v)\\mapsto \\alpha v$, die mit der Körpermultiplikation kompatibel ist und Linearität in beiden Argumenten gewährleistet."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "2. Welche der folgenden Mengen ist ein Untervektorraum von $\\mathbb{R}^2$?",
      "options": [
        "$U=\\{(x,y)\\in\\mathbb{R}^2\\mid x=3y\\}$",
        "$U=\\{(x,y)\\in\\mathbb{R}^2\\mid x+y=1\\}$",
        "$U=\\{(x,y)\\in\\mathbb{R}^2\\mid y=x^2\\}$",
        "$U=\\{(x,y)\\in\\mathbb{R}^2\\mid x\\ge 0, y\\ge 0\\}$",
        "$U=\\{(x,y)\\in\\mathbb{R}^2\\mid x\\in\\mathbb{Z}, y\\in\\mathbb{Z}\\}$"
      ],
      "answer": 0,
      "explanation": "Die Menge $x=3y$ ist eine durch den Ursprung verlaufende Gerade und damit abgeschlossen unter Addition und Skalarmultiplikation.",
      "weight": 1,
      "topic": "LE 11: Vektorräume",
      "mini_glossary": {
        "Untervektorraum": "Nichtleere Teilmenge $U\\subseteq V$, die unter $+$ und Skalarmultiplikation abgeschlossen ist und den Nullvektor enthält; äquivalent: $\\alpha u+\\beta v\\in U$ für alle $u,v\\in U$, $\\alpha,\\beta\\in K$.",
        "Abgeschlossenheit": "Für alle $u,v\\in U$, $\\alpha\\in K$ gilt $u+v\\in U$ und $\\alpha u\\in U$. Verletzungen (z.B. Nebenbedingungen wie $x+y=1$) zerstören die Unterraumstruktur.",
        "Ursprung": "Der Punkt $(0,0)$; jede Unterraumgerade muss ihn enthalten, da $0\\cdot v=0\\in U$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "3. Warum ist die Menge $P_2^*$ (Polynome genau vom Grad 2) kein Vektorraum, $P_2$ (Grad \\leq 2) aber schon?",
      "options": [
        "Weil $P_2^*$ den Nullvektor (das Nullpolynom) nicht enthält.",
        "Weil $P_2^*$ nicht kommutativ addiert werden kann.",
        "Weil $P_2^*$ keine Skalarmultiplikation zulässt.",
        "Weil $P_2$ nicht abgeschlossen unter Addition ist.",
        "Weil $P_2$ die Distributivität verletzt."
      ],
      "answer": 0,
      "explanation": "Ein Vektorraum muss den Nullvektor enthalten. In $P_2^*$ ist das Nullpolynom nicht erlaubt; damit scheitern die Axiome. $P_2$ enthält das Nullpolynom und ist abgeschlossen.",
      "weight": 2,
      "topic": "LE 11: Vektorräume",
      "extended_explanation": {
        "titel": "Vektorraumkriterien auf Polynommengen",
        "schritte": [
          "Ein Vektorraum muss den **Nullvektor** enthalten; in Polynommengen ist dies das **Nullpolynom**.",
          "Für jedes $p\\in P_2^*$ gilt $0\\cdot p=0$; da $0\\notin P_2^*$, ist die Skalarmultiplikation nicht abgeschlossen.",
          "Die **Additionsabgeschlossenheit** scheitert ebenfalls: Die Summe zweier quadratischer Polynome kann vom Grad $\\le 1$ sein.",
          "$P_2=\\{p\\mid \\deg p\\le 2\\}$ enthält das Nullpolynom und ist unter Addition und Skalarmultiplikation abgeschlossen.",
          "Damit erfüllt $P_2$ alle Vektorraumaxiome, $P_2^*$ verletzt sie."
        ]
      },
      "mini_glossary": {
        "Polynomgrad": "Für $p(x)=\\sum_{i=0}^n a_i x^i$ mit $a_n\\ne 0$ ist $\\deg p=n$. Für das Nullpolynom ist $\\deg 0$ undefiniert oder $-\\infty$ (konventionsabhängig).",
        "Nullpolynom": "Polynom mit allen Koeffizienten $0$; wirkt als Nullvektor: $p+0=p$ und $\\alpha\\cdot 0=0$.",
        "Abgeschlossenheit (Polynome)": "Wenn $\\deg p,\\deg q\\le 2$, dann $\\deg(p+q)\\le 2$ und $\\deg(\\alpha p)\\le 2$; bei $P_2^*$ ist $0\\notin P_2^*$, daher kein Unterraum."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "4. Welche Aussage beschreibt hinreichende und notwendige Bedingungen dafür, dass $U\\subseteq V$ ein Untervektorraum ist?",
      "options": [
        "$0\\in U$, $u,v\\in U \\Rightarrow u+v\\in U$, $\\alpha\\in\\mathbb{R}, u\\in U \\Rightarrow \\alpha u\\in U$",
        "$U\\neq\\emptyset$, $u\\in U \\Rightarrow -u\\in U$, $\\alpha\\in\\mathbb{R} \\Rightarrow \\alpha\\in U$",
        "$1\\in U$, $u,v\\in U \\Rightarrow u+v\\in U$, $\\alpha\\in\\mathbb{R}, u\\in U \\Rightarrow \\alpha+u\\in U$",
        "$0\\in U$, $u,v\\in U \\Rightarrow u-v\\in U$, $\\alpha\\in\\mathbb{R}, u\\in U \\Rightarrow \\alpha+u\\in U$",
        "$U\\neq\\emptyset$, $u\\in U \\Rightarrow 1\\cdot u=u$, $u,v\\in U \\Rightarrow u+v\\in V$"
      ],
      "answer": 0,
      "explanation": "Die drei Standardkriterien sind: Nullvektor enthalten, Additiv abgeschlossen, skalar abgeschlossen.",
      "weight": 2,
      "topic": "LE 11: Vektorräume",
      "extended_explanation": {
        "titel": "Unterraumkriterien kompakt",
        "schritte": [
          "Notwendig und hinreichend: $0\\in U$, $u,v\\in U\\Rightarrow u+v\\in U$, $\\alpha\\in\\mathbb{R},u\\in U\\Rightarrow \\alpha u\\in U$.",
          "Aus $\\alpha=-1$ folgt $u\\in U\\Rightarrow -u\\in U$, also ist $U$ **additiv abgeschlossen** inkl. Existenz additiver Inverser.",
          "Die Kriterien implizieren, dass die **Axiome** der Vektoraddition und -skalierung auf $U$ geerbt werden.",
          "Alternative Kurzform: $U\\ne\\emptyset$ und $\\alpha u+\\beta v\\in U$ für alle $u,v\\in U$, $\\alpha,\\beta\\in\\mathbb{R}$.",
          "Beide Formen sind äquivalent und werden je nach Kontext verwendet."
        ]
      },
      "mini_glossary": {
        "Unterraumkriterien": "Test: $U\\ne\\emptyset$ und $\\alpha u+\\beta v\\in U$ für alle $u,v\\in U$, $\\alpha,\\beta\\in K$. Daraus folgen Nullvektor und additive Inversen automatisch.",
        "Skalar": "Element des Körpers $K$; definiert die externe Multiplikation auf $V$. In $\\mathbb{R}^n$ sind Skalare reelle Zahlen.",
        "Additionsabschluss": "Für $u,v\\in U$ gilt $u+v\\in U$. Zusammen mit Skalarabschluss bildet $(U,+)$ wieder eine abelsche Gruppe."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "5. Welches Axiom stellt die Konsistenz der Skalarmultiplikation mit der Körpermultiplikation sicher?",
      "options": [
        "Assoziativgesetz der Skalarmultiplikation: $\\alpha\\cdot(\\beta\\cdot v)=(\\alpha\\beta)\\cdot v$",
        "Kommutativgesetz der Addition: $u+v=v+u$",
        "Neutrales Element der Skalarmultiplikation: $1\\cdot v=v$",
        "Distributivgesetz (Skalare): $(\\alpha+\\beta)\\cdot v=\\alpha\\cdot v+\\beta\\cdot v$",
        "Distributivgesetz (Vektoren): $\\alpha\\cdot(u+v)=\\alpha\\cdot u+\\alpha\\cdot v$"
      ],
      "answer": 0,
      "explanation": "Das Assoziativgesetz der Skalarmultiplikation verbindet die Multiplikation im Körper mit der Skalarmultiplikation auf dem Vektorraum.",
      "weight": 3,
      "topic": "LE 11: Vektorräume",
      "extended_explanation": {
        "titel": "Assoziativität als Brücke zwischen Körper und Vektorraum",
        "schritte": [
          "In einem Vektorraum über einem Körper $K$ muss gelten: $\\alpha(\\beta v)=(\\alpha\\beta)v$ für alle $\\alpha,\\beta\\in K$, $v\\in V$.",
          "Die rechte Seite berechnet zuerst im **Körper** $(\\alpha\\beta)$, die linke zuerst in der **Vektorstruktur**; Gleichheit koppelt beide Ebenen.",
          "Zusammen mit den Distributivgesetzen und $1\\cdot v=v$ entsteht die **Linearität** in beiden Argumenten.",
          "Ohne diese Assoziativität wäre die Notation $\\alpha\\beta v$ mehrdeutig und die Theorie inkonsistent.",
          "In Modulen über Ringen ist analog die Modulaxiomatik formuliert; die Körperstruktur macht zusätzlich **Skalarmultiplikation eindeutig invertierbar** für $\\alpha\\ne 0$."
        ]
      },
      "mini_glossary": {
        "Körper": "Algebraische Struktur $(K,+,\\cdot)$ mit kommutativer Multiplikation, Einheiten $1\\ne 0$ und Existenz multiplikativer Inverser für $\\alpha\\ne 0$. Beispiel: $\\mathbb{R},\\mathbb{C}$.",
        "Assoziativität": "Eigenschaft $a\\cdot(b\\cdot c)=(a\\cdot b)\\cdot c$. Erlaubt Umklammern ohne Wertänderung.",
        "Linearität": "Erhaltung von Addition und Skalarmultiplikation: $T(\\alpha u+\\beta v)=\\alpha T(u)+\\beta T(v)$."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "6. Wie groß ist die Dimension des Vektorraums $P_3$ der Polynome vom Grad höchstens 3?",
      "options": [
        "4",
        "3",
        "5",
        "2",
        "1"
      ],
      "answer": 0,
      "explanation": "Eine Basis ist beispielsweise $(1, x, x^2, x^3)$, also vier Basisvektoren.",
      "weight": 1,
      "topic": "LE 12: Lineare Unabhängigkeit, Basis, Dimension",
      "mini_glossary": {
        "Dimension": "Anzahl der Basisvektoren eines Vektorraums. In endlicher Dimension ist sie invariant: Alle Basen haben dieselbe Kardinalität.",
        "Basis": "Menge linear unabhängiger Vektoren, die den Raum vollständig erzeugt. Jede Darstellung ist eindeutig.",
        "Polynomraum": "Vektorraum $P_n=\\{p\\mid \\deg p\\le n\\}$ mit kanonischer Basis $(1,x,\\dots,x^n)$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "7. Die Vektoren $v_1=(1,0)$ und $v_2=(2,0)$ in $\\mathbb{R}^2$ sind ...",
      "options": [
        "linear abhängig und spannen nicht den $\\mathbb{R}^2$ auf.",
        "linear unabhängig und bilden eine Basis.",
        "linear unabhängig, spannen aber den $\\mathbb{R}^2$ nicht auf.",
        "orthogonal und bilden daher eine Basis.",
        "linear abhängig und spannen den $\\mathbb{R}^2$ auf."
      ],
      "answer": 0,
      "explanation": "Beide liegen auf derselben Geraden (x-Achse), sind daher abhängig und erzeugen nicht die ganze Ebene.",
      "weight": 1,
      "topic": "LE 12: Lineare Unabhängigkeit, Basis, Dimension",
      "mini_glossary": {
        "Lineare Abhängigkeit": "Es existieren nicht alle $0$-Koeffizienten $\\alpha_i$, so dass $\\sum\\alpha_i v_i=0$. Hier: $(2,0)=2\\cdot(1,0)$.",
        "Erzeugendensystem": "Menge, deren Linearkombinationen den gesamten Raum ergeben. Zwei kollineare Vektoren erzeugen nur eine Gerade.",
        "Basis": "Minimal erzeugende, linear unabhängige Menge. In $\\mathbb{R}^2$ werden zwei nicht kollineare Vektoren benötigt."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "8. Wie lautet der Koordinatenvektor von $v=(3,5)$ bezüglich der Standardbasis $E=\\{(1,0),(0,1)\\}$ in $\\mathbb{R}^2$?",
      "options": [
        "$(3,5)$",
        "$(5,3)$",
        "$(1,0)$",
        "$(0,1)$",
        "$(8,8)$"
      ],
      "answer": 0,
      "explanation": "In der Standardbasis sind die Koordinaten identisch mit den Komponenten des Vektors.",
      "weight": 2,
      "topic": "LE 12: Lineare Unabhängigkeit, Basis, Dimension",
      "extended_explanation": {
        "titel": "Koordinaten in der Standardbasis",
        "schritte": [
          "Eine Basis $E=(e_1,e_2)$ liefert die eindeutige Darstellung $v=\\alpha_1 e_1+\\alpha_2 e_2$.",
          "Für die Standardbasis $e_1=(1,0)$, $e_2=(0,1)$ gilt $v=(3,5)=3e_1+5e_2$.",
          "Die **Koordinatenabbildung** $[\\cdot]_E: \\mathbb{R}^2\\to\\mathbb{R}^2$ ist ein Isomorphismus mit $[v]_E=(\\alpha_1,\\alpha_2)$.",
          "Basiswechsel zu $F$ geschieht mit einer invertierbaren Matrix $T$: $[v]_F=T^{-1}[v]_E$.",
          "Linearität der Koordinatenabbildung folgt aus der Linearität der Darstellung."
        ]
      },
      "mini_glossary": {
        "Standardbasis": "Kanonische Basis $((1,0),(0,1))$ in $\\mathbb{R}^2$. Allgemein: Einheitsvektoren $e_i$ in $\\mathbb{R}^n$.",
        "Koordinatenvektor": "Die eindeutigen Koeffizienten $[v]_E=(\\alpha_i)$ in der Darstellung $v=\\sum \\alpha_i e_i$.",
        "Basiswechsel": "Für Basen $E,F$ gilt $[v]_F=T^{-1}[v]_E$ mit der Übergangsmatrix $T$ (invertierbar)."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "9. Was ist unter einer Linearkombination zu verstehen?",
      "options": [
        "Ein Vektor, der durch Skalieren und Addieren einer Menge gegebener Vektoren entsteht.",
        "Das Ergebnis des Skalarprodukts zweier Vektoren.",
        "Der Vektor, der zu allen Vektoren der Menge orthogonal ist.",
        "Die Menge aller Vektoren, die eine Basis bilden.",
        "Das Ergebnis der Multiplikation zweier Matrizen."
      ],
      "answer": 0,
      "explanation": "Linearkombinationen sind Summen $\\sum_i \\alpha_i v_i$ mit Skalaren $\\alpha_i$ und Vektoren $v_i$.",
      "weight": 2,
      "topic": "LE 12: Lineare Unabhängigkeit, Basis, Dimension",
      "extended_explanation": {
        "titel": "Linearkombinationen formal und anschaulich",
        "schritte": [
          "Formal: Für $v_i\\in V$ und $\\alpha_i\\in \\mathbb{R}$ heißt $\\sum_{i=1}^k \\alpha_i v_i$ **Linearkombination**.",
          "Die Menge aller Linearkombinationen bildet den **Span**: $\\operatorname{span}\\{v_1,\\dots,v_k\\}$.",
          "Gilt $w\\in \\operatorname{span}\\{v_1,\\dots,v_k\\}$, so existieren Koeffizienten $\\alpha_i$ mit $w=\\sum_i \\alpha_i v_i$.",
          "Linear **unabhängig** heißt, dass nur die triviale Kombination $\\sum_i \\alpha_i v_i=0$ mit allen $\\alpha_i=0$ existiert.",
          "Eine **Basis** ist eine minimal erzeugende, linear unabhängige Menge; jede Darstellung ist dann eindeutig."
        ]
      },
      "mini_glossary": {
        "Span": "Der erzeugte Unterraum $\\operatorname{span}(S)=\\{\\sum\\alpha_i v_i\\mid v_i\\in S,\\alpha_i\\in K\\}$. Minimalste Erzeuger sind Basen.",
        "Koeffizienten": "Skalare Gewichte in $\\sum \\alpha_i v_i$. Ihre Eindeutigkeit hängt von Linearunabhängigkeit ab.",
        "Linearität": "Additivität und Homogenität; zentrale Eigenschaft linearer Abbildungen und Räume."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "10. Welche Dimension hat das Produkt $A\\,B\\,C$, wenn $A\\in\\mathbb{R}^{3\\times 4}$, $B\\in\\mathbb{R}^{4\\times 5}$ und $C\\in\\mathbb{R}^{5\\times 2}$?",
      "options": [
        "$3\\times 2$",
        "$4\\times 2$",
        "$3\\times 5$",
        "$5\\times 4$",
        "Das Produkt ist nicht definiert."
      ],
      "answer": 0,
      "explanation": "Die inneren Dimensionen passen: $(3\\times4)(4\\times5)(5\\times2)\\Rightarrow 3\\times 2$.",
      "weight": 1,
      "topic": "LE 13: Matrizen",
      "mini_glossary": {
        "Matrixdimension": "Eine Matrix in $\\mathbb{R}^{m\\times n}$ hat $m$ Zeilen, $n$ Spalten. Das Produkt $AB$ ist definiert, wenn die inneren Maße übereinstimmen.",
        "Kompatibilität": "Für $A\\in\\mathbb{R}^{m\\times n}$ und $B\\in\\mathbb{R}^{n\\times p}$ ergibt sich $AB\\in\\mathbb{R}^{m\\times p}$.",
        "Assoziativität": "Für definierte Produkte gilt $(AB)C=A(BC)$; die Klammerung beeinflusst Komplexität, nicht das Resultat."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "11. Was ist die Einheitsmatrix $I_n$?",
      "options": [
        "Eine Diagonalmatrix mit Einsen auf der Hauptdiagonalen.",
        "Eine Matrix, die nur aus Einsen besteht.",
        "Das neutrale Element der Matrixaddition.",
        "Eine Matrix mit Determinante stets 1.",
        "Eine Matrix mit Nullen auf der Hauptdiagonalen."
      ],
      "answer": 0,
      "explanation": "$I_n$ hat Einsen auf der Hauptdiagonalen und wirkt als neutrales Element der Matrixmultiplikation.",
      "weight": 2,
      "topic": "LE 13: Matrizen",
      "extended_explanation": {
        "titel": "Rolle der Einheitsmatrix",
        "schritte": [
          "Definition: $I_n=(\\delta_{ij})_{i,j=1}^n$ mit Kronecker-Delta $\\delta_{ij}$.",
          "Für passende Dimensionen gilt **Links- und Rechtsneutrales**: $I_n A=A$ und $A I_m=A$.",
          "Eigenwerte von $I_n$ sind alle gleich $1$; jeder Vektor ist Eigenvektor.",
          "Die Inverse von $I_n$ ist $I_n$ selbst; $I_n$ ist positiv definit und orthogonal.",
          "In Ausdrücken wie $A-\\lambda I_n$ verschiebt $I_n$ das Spektrum um $\\lambda$."
        ]
      },
      "mini_glossary": {
        "Hauptdiagonale": "Einträge $(i,i)$; bei $I_n$ alle gleich $1$. Off-Diagonalen sind $0$.",
        "Neutrales Element": "Für Multiplikation: $I A=A= A I$. Für Addition existiert das Nullmatrix-Element.",
        "Diagonalmatrix": "Nur Diagonaleinträge können ungleich 0 sein; erleichtert Inversion und Potenzieren."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "12. Welche Eigenschaft gilt stets für eine symmetrische Matrix $A$?",
      "options": [
        "$A = A^\\top$",
        "$A = -A^\\top$",
        "$A = A^{-1}$",
        "$\\det(A)=1$",
        "$A = A^2$"
      ],
      "answer": 0,
      "explanation": "Per Definition ist $A$ symmetrisch genau dann, wenn $A=A^\\top$.",
      "weight": 2,
      "topic": "LE 13: Matrizen",
      "extended_explanation": {
        "titel": "Symmetrie und Konsequenzen",
        "schritte": [
          "Definition: $A$ symmetrisch $\\Leftrightarrow A=A^\\top$ über $\\mathbb{R}$.",
          "Spektralsatz: Es existiert eine orthogonale Matrix $Q$ mit $Q^\\top A Q=\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$.",
          "Alle Eigenwerte reeller symmetrischer Matrizen sind **reell**.",
          "Eigenvektoren zu verschiedenen Eigenwerten sind **orthogonal**.",
          "Quadratische Formen $x^\\top A x$ werden durch die Diagonalisierung analysierbar."
        ]
      },
      "mini_glossary": {
        "Transponierte": "Matrix $A^\\top$ mit vertauschten Indizes: $(A^\\top)_{ij}=A_{ji}$. Spiegelt an der Hauptdiagonalen.",
        "Spektralsatz": "Für reelle symmetrische $A$ existiert eine orthogonale Diagonalisierung; garantiert reelle Eigenwerte.",
        "Eigenwert": "Skalar $\\lambda$ mit $A v=\\lambda v$; bestimmt Streckung/Stauchung entlang $v\\ne 0$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "13. Die Abbildung $y=A\\,x$ ist im Allgemeinen eine ...",
      "options": [
        "lineare Abbildung von $x$ nach $y$.",
        "nichtlineare Abbildung von $x$ nach $y$.",
        "orthogonale Projektion von $x$ auf $y$.",
        "Determinantenberechnung von $A$.",
        "Skalarprodukt von $A$ und $x$."
      ],
      "answer": 0,
      "explanation": "Matrix-Vektor-Multiplikation ist per Definition linear in $x$.",
      "weight": 2,
      "topic": "LE 13: Matrizen",
      "extended_explanation": {
        "titel": "Linearität von $x \\mapsto A x$",
        "schritte": [
          "Für $x,y\\in\\mathbb{R}^n$, $\\alpha,\\beta\\in\\mathbb{R}$ gilt $A(\\alpha x+\\beta y)=\\alpha A x+\\beta A y$.",
          "Die Abbildung ist vollständig durch die Bilder der Basisvektoren $e_i$ bestimmt: $A e_i$ sind die **Spalten** von $A$.",
          "Der Bildraum ist $\\operatorname{im}(A)=\\{A x\\mid x\\in\\mathbb{R}^n\\}$, der Kern $\\ker(A)=\\{x\\mid A x=0\\}$. ",
          "Rang-Nullität: $\\dim\\ker(A)+\\operatorname{rang}(A)=n$.",
          "Lineare Abbildungen sind genau diejenigen, die den Ursprung fixieren und Geraden durch den Ursprung in Geraden abbilden."
        ]
      },
      "mini_glossary": {
        "Lineare Abbildung": "Abbildung $T:V\\to W$ mit $T(\\alpha u+\\beta v)=\\alpha T(u)+\\beta T(v)$. Wird durch Wirkung auf eine Basis eindeutig festgelegt.",
        "Bildraum": "$\\operatorname{im}(A)=\\{A x\\}$; Dimension ist der Rang von $A$.",
        "Kern": "$\\ker(A)=\\{x\\mid A x=0\\}$; Dimension ist die Nullität; zusammen: Rang-Nullität."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "14. Welche Kombination ist stets eine wohldefinierte Operation zwischen Matrix und Vektor (bei passenden Dimensionen)?",
      "options": [
        "$A\\cdot v$ (Matrix-Vektor-Produkt)",
        "$A+v$ (Matrix plus Vektor)",
        "$\\det(v)$ (Determinante eines Vektors)",
        "$v\\cdot A$ (Vektor rechts mal Matrix, Spaltenvektor)",
        "$A:v$ (Matrix durch Vektor)"
      ],
      "answer": 0,
      "explanation": "Das Matrix-Vektor-Produkt ist standardisiert; die übrigen Ausdrücke sind im Allgemeinen nicht definiert.",
      "weight": 3,
      "topic": "LE 13: Matrizen",
      "extended_explanation": {
        "titel": "Zulässige Operationen präzisieren",
        "schritte": [
          "Konvention: Vektoren sind **Spaltenvektoren** in $\\mathbb{R}^n$, Matrizen wirken links.",
          "Das Produkt $A v$ ist definiert für $A\\in\\mathbb{R}^{m\\times n}$ und $v\\in\\mathbb{R}^n$; Ergebnis in $\\mathbb{R}^m$.",
          "Die Summe $A+v$ ist nicht definiert, da unterschiedliche Dimensionstypen vorliegen.",
          "Eine Determinante ist nur für **quadratische Matrizen** definiert, nicht für Vektoren.",
          "Das Produkt $v A$ ist nur als **Zeilenvektor** $v^\\top A$ sinnvoll; mit Spaltenkonvention ist es nicht wohldefiniert."
        ]
      },
      "mini_glossary": {
        "Determinante": "Skalarfunktion $\\det: \\mathbb{R}^{n\\times n}\\to\\mathbb{R}$ mit $\\det(AB)=\\det(A)\\det(B)$ und $\\det(I)=1$. Nur für quadratische Matrizen definiert.",
        "Spaltenvektor": "Vektor als $n\\times 1$-Matrix; Transponieren liefert den Zeilenvektor $1\\times n$.",
        "Wohldefiniertheit": "Operation ist eindeutig und kontextfrei bestimmt; erfordert passende Strukturen und Maße."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "15. Welche Operation gehört zu den elementaren Zeilenumformungen beim Gauß-Algorithmus?",
      "options": [
        "Multiplikation einer Zeile mit $c\\neq 0$",
        "Addition eines konstanten Spaltenvektors zu einer Zeile",
        "Transponieren einer einzelnen Zeile",
        "Multiplikation einer Zeile mit $0$",
        "Anhängen einer zusätzlichen Spalte"
      ],
      "answer": 0,
      "explanation": "Die drei elementaren Operationen sind Zeilentausch, Skalierung mit $c\\neq 0$ und Addition eines Vielfachen einer Zeile zu einer anderen.",
      "weight": 1,
      "topic": "LE 14: Lineare Gleichungssysteme (LGS)",
      "mini_glossary": {
        "Gauß-Algorithmus": "Eliminationsverfahren zur Transformation eines LGS in (reduzierte) Zeilenstufenform; erhält die Lösungsmenge invariant.",
        "Zeilenumformung": "Operationen: Zeilentausch, $c\\ne 0$-Skalierung, Addition eines Vielfachen einer anderen Zeile. Alle sind durch invertierbare linke Multiplikation modellierbar.",
        "Zeilenstufenform": "Form, in der Pivots stufenweise nach rechts wandern; in reduzierter Form stehen Pivots auf 1 und darüber/unter ihnen Nullen."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "16. Gilt $\\operatorname{rang}(A)=\\operatorname{rang}(A|b)=n$ bei $n$ Unbekannten, was folgt für $A x=b$?",
      "options": [
        "Es gibt genau eine Lösung.",
        "Es gibt keine Lösung.",
        "Es gibt unendlich viele Lösungen.",
        "Das System ist homogen.",
        "Das System ist nicht lösbar."
      ],
      "answer": 0,
      "explanation": "Voller Rang und gleiche Ränge der erweiterten Matrix implizieren Eindeutigkeit.",
      "weight": 2,
      "topic": "LE 14: Lineare Gleichungssysteme (LGS)",
      "extended_explanation": {
        "titel": "Rangkriterium für Eindeutigkeit",
        "schritte": [
          "Rouché–Capelli: $A x=b$ ist lösbar $\\Leftrightarrow \\operatorname{rang}(A)=\\operatorname{rang}(A|b)$.",
          "Ist zusätzlich $\\operatorname{rang}(A)=n$, existieren **keine freien Variablen**.",
          "Die reduzierte Zeilenstufenform besitzt in jeder Spalte einen Pivot; die Lösung ist eindeutig.",
          "Äquivalent: $A$ ist invertierbar und $x=A^{-1}b$.",
          "Geometrisch: Die Abbildung $x\\mapsto A x$ ist bijektiv."
        ]
      },
      "mini_glossary": {
        "Rang": "Maximale Anzahl linear unabhängiger Zeilen/Spalten; gleich der Dimension des Bildraums.",
        "Erweiterte Matrix": "Konkatenation $(A|b)$; vergleicht Gleichungsstruktur mit rechter Seite für Konsistenz.",
        "Eindeutige Lösung": "Exakt ein $x$ erfüllt $A x=b$; folgt bei vollem Rang ohne Widerspruchszeilen."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "17. Eine Zeile $[0\\ 0\\ 0\\ |\\ 5]$ in Zeilenstufenform bedeutet ...",
      "options": [
        "Das System ist widersprüchlich und hat keine Lösung.",
        "Die eindeutige Lösung ist $x=0, y=0, z=5$.",
        "Das System hat unendlich viele Lösungen.",
        "Nur die Nulllösung ist möglich.",
        "Die Zeile kann folgenlos entfernt werden."
      ],
      "answer": 0,
      "explanation": "Die Gleichung $0=5$ ist ein Widerspruch; das System ist unlösbar.",
      "weight": 2,
      "topic": "LE 14: Lineare Gleichungssysteme (LGS)",
      "extended_explanation": {
        "titel": "Widerspruchszeile erkennen",
        "schritte": [
          "Die Zeile stellt die Gleichung $0=5$ dar; dies ist **falscher** Satz.",
          "Nach Rouché–Capelli muss für Lösbarkeit $\\operatorname{rang}(A)=\\operatorname{rang}(A|b)$ gelten.",
          "Eine Widerspruchszeile erhöht den Rang der erweiterten Matrix, nicht aber den von $A$.",
          "Folge: $\\operatorname{rang}(A)<\\operatorname{rang}(A|b)$ und es existiert **keine Lösung**.",
          "Numerisch erkennt man solche Zeilen nach Elimination durch eine 0-Zeile links und Nichtnull rechts."
        ]
      },
      "mini_glossary": {
        "Zeilenstufenform": "Trianguläre Struktur mit Pivots. Eine reine Nullzeile links zeigt Redundanz; Nichtnull rechts erzeugt Widerspruch.",
        "Widerspruchszeile": "Gleichung $0=c$ mit $c\\ne 0$; macht das LGS inkonsistent.",
        "Lösungsmenge": "Leere Menge bei Inkonsistenz; sonst affine Untermenge des $\\mathbb{R}^n$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "18. Ein LGS in Zeilenstufenform mit 3 Unbekannten besitzt 2 Pivot-Elemente und keine Widerspruchszeile. Was folgt?",
      "options": [
        "Unendlich viele Lösungen mit einem freien Parameter.",
        "Genau eine Lösung.",
        "Keine Lösung.",
        "Unendlich viele Lösungen mit zwei freien Parametern.",
        "Das System ist nicht lösbar."
      ],
      "answer": 0,
      "explanation": "Mit 3 Variablen und 2 Pivoten bleibt eine freie Variable; dadurch gibt es unendlich viele Lösungen.",
      "weight": 2,
      "topic": "LE 14: Lineare Gleichungssysteme (LGS)",
      "extended_explanation": {
        "titel": "Pivotanzahl und Freiheitsgrade",
        "schritte": [
          "Anzahl freier Variablen ist $n-\\operatorname{rang}(A)$; hier $3-2=1$.",
          "Ohne Widerspruchszeile ist das System **konsistent**.",
          "Die Lösungsmenge ist eine affine Gerade im $\\mathbb{R}^3$.",
          "Parameterdarstellung entsteht durch Rückwärtseinsetzen in der reduzierten Stufenform.",
          "Geometrisch: Schnitt von zwei Ebenen ist eine Gerade."
        ]
      },
      "mini_glossary": {
        "Pivot": "Linkester Nichtnull-Eintrag einer Zeile in Stufenform; markiert gebundene Variable.",
        "Freie Variable": "Nicht durch Pivots festgelegt; parametrisiert unendlich viele Lösungen.",
        "Parametrisierung": "Darstellung der Lösungsmenge durch Parameter, z.B. $x(t)=x_0+t v$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "19. Was ist im Kontext des Gauß-Algorithmus ein Pivot-Element?",
      "options": [
        "Das erste von Null verschiedene Element in einer Zeile der Stufenform.",
        "Ein Diagonalelement, das stets 0 sein muss.",
        "Das Element in der letzten Spalte der erweiterten Matrix.",
        "Ein beliebiges Element, das zum Zeilentausch verwendet wird.",
        "Das erste Element der ersten Zeile."
      ],
      "answer": 0,
      "explanation": "Ein Pivot ist der führende Eintrag einer Zeile in Stufenform und bestimmt die Eliminationsschritte.",
      "weight": 3,
      "topic": "LE 14: Lineare Gleichungssysteme (LGS)",
      "extended_explanation": {
        "titel": "Pivotkonzept in der Eliminationspraxis",
        "schritte": [
          "In der (reduzierten) Zeilenstufenform ist ein Pivot die linkeste Nichtnull in einer Zeile.",
          "Pivotpositionen definieren die **gebundenen Variablen**; andere sind frei.",
          "Durch Zeilenoperationen werden unterhalb eines Pivots Nullen erzeugt; in reduzierter Form auch oberhalb.",
          "Teil der **LU-Zerlegung**: Pivots bilden die Diagonale von $U$; Zeilentausch entspricht Permutation $P$.",
          "Pivotwahl beeinflusst numerische Stabilität; **partielles Pivotieren** wählt betragsgrößte Einträge."
        ]
      },
      "mini_glossary": {
        "Elimination": "Sequenz von Zeilenoperationen zur Nullung von Einträgen unter (und ggf. über) Pivotpositionen.",
        "Gebundene Variable": "Durch Pivot-Spalte festgelegt; kein Freiheitsgrad.",
        "Diagonaldominanz": "Eigenschaft $|a_{ii}|\\ge \\sum_{j\\ne i}|a_{ij}|$; verbessert Stabilität, ist aber nicht zwingend."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "20. Was beschreibt ein Eigenwert $\\lambda$ einer Matrix $A$?",
      "options": [
        "Den Skalierungsfaktor, um den ein zugehöriger Eigenvektor unter $A v$ gestreckt oder gestaucht wird.",
        "Den Winkel, um den ein Eigenvektor unter $A v$ gedreht wird.",
        "Die Anzahl linear unabhängiger Zeilen der Matrix.",
        "Die Determinante von $A$, die stets gleich $\\lambda$ ist.",
        "Einen Vektor, der zu allen Spalten von $A$ orthogonal ist."
      ],
      "answer": 0,
      "explanation": "Per Definition gilt $A v = \\lambda v$ für einen Eigenvektor $v\\ne 0$; $\\lambda$ ist der Skalierungsfaktor.",
      "weight": 1,
      "topic": "LE 15: Eigenwerte und Eigenvektoren",
      "mini_glossary": {
        "Eigenwert": "Nullstelle des charakteristischen Polynoms $\\det(A-\\lambda I)=0$; skaliert zugehörige Eigenvektoren.",
        "Eigenvektor": "Nichtnullvektor $v$ mit $A v=\\lambda v$; bestimmt Richtungen invarianten Verhaltens.",
        "Spektrum": "Menge aller Eigenwerte $\\sigma(A)$; beeinflusst Stabilität, Potenzen und Dynamik von $A$."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "21. Wann ist eine quadratische Matrix $A$ invertierbar?",
      "options": [
        "Genau dann, wenn $0$ kein Eigenwert von $A$ ist.",
        "Genau dann, wenn $1$ Eigenwert von $A$ ist.",
        "Genau dann, wenn alle Eigenwerte positiv sind.",
        "Genau dann, wenn alle Eigenwerte reell sind.",
        "Genau dann, wenn $A$ symmetrisch ist."
      ],
      "answer": 0,
      "explanation": "Invertierbarkeit entspricht $\\det(A)\\ne 0$; das ist äquivalent dazu, dass 0 kein Eigenwert ist.",
      "weight": 2,
      "topic": "LE 15: Eigenwerte und Eigenvektoren",
      "extended_explanation": {
        "titel": "Invertierbarkeit und Spektrum",
        "schritte": [
          "Äquivalente Kriterien: $A$ invertierbar $\\Leftrightarrow \\det(A)\\ne 0 \\Leftrightarrow \\ker(A)=\\{0\\} \\Leftrightarrow 0$ ist kein Eigenwert.",
          "Ist $0$ Eigenwert, existiert $v\\ne 0$ mit $A v=0$, also nicht-injektiv und damit nicht invertierbar.",
          "Charakteristisches Polynom $p_A(\\lambda)=\\det(A-\\lambda I)$ hat $\\lambda=0$ als Nullstelle $\\Leftrightarrow \\det(A)=0$.",
          "Für reguläre $A$ existiert die eindeutige Inverse $A^{-1}$ mit $A^{-1}A=I$.",
          "Spektral betrachtet liegt dann $0\\notin \\sigma(A)$, dem **Spektrum**."
        ]
      },
      "mini_glossary": {
        "Determinante": "Multiplikatives Skalar, null genau bei Singulärität; geometrisch: orientiertes Volumenverhältnis.",
        "Singulär": "Nicht invertierbar; es existiert $x\\ne 0$ mit $A x=0$; Spalten sind linear abhängig.",
        "Spektrum": "Eigenwertmenge; $0\\notin\\sigma(A)$ ist äquivalent zu Invertierbarkeit."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "22. Was ist das charakteristische Polynom einer $n\\times n$-Matrix $A$?",
      "options": [
        "Ein Polynom $p(\\lambda)=\\det(A-\\lambda I_n)$ vom Grad $n$, dessen Nullstellen die Eigenwerte sind.",
        "Ein Polynom in $x$ vom Grad $n$, definiert durch $A x=0$.",
        "Ein Polynom, dessen Koeffizienten die Elemente von $A$ sind.",
        "Ein Polynom vom Grad $n-1$, dessen Nullstellen die Eigenvektoren sind.",
        "Ein Polynom $\\det(A-\\lambda v)$ in einem Vektor $v$."
      ],
      "answer": 0,
      "explanation": "Standarddefinition: $p_A(\\lambda)=\\det(A-\\lambda I_n)$; die Nullstellen sind die Eigenwerte.",
      "weight": 2,
      "topic": "LE 15: Eigenwerte und Eigenvektoren",
      "extended_explanation": {
        "titel": "Vom Polynom zur Spektralanalyse",
        "schritte": [
          "Grad-$n$-Polynom: $p_A(\\lambda)=\\det(A-\\lambda I)$ besitzt genau $n$ Nullstellen mit **Vielfachheiten** (Fundamentalsatz der Algebra).",
          "Nullstelle $\\lambda_0$ bedeutet $\\det(A-\\lambda_0 I)=0$, also nichttriviale Lösung von $(A-\\lambda_0 I)v=0$.",
          "Algebraische Vielfachheit ist die Nullstellenordnung; geometrische Vielfachheit ist $\\dim\\ker(A-\\lambda_0 I)$. ",
          "Es gilt stets: geometrische Vielfachheit $\\le$ algebraische Vielfachheit.",
          "Die Summe der algebraischen Vielfachheiten ist $n$."
        ]
      },
      "mini_glossary": {
        "Charakteristisches Polynom": "Definiert als $\\det(A-\\lambda I)$; Koeffizienten sind bis auf Vorzeichen elementarsymmetrische Funktionen der Eigenwerte.",
        "Nullstelle": "Wert $\\lambda$ mit $p_A(\\lambda)=0$; erzeugt singuläres $A-\\lambda I$ und damit einen Eigenvektor.",
        "Eigenwertgleichung": "$(A-\\lambda I)v=0$; nichttriviale Lösungen existieren genau für Eigenwerte."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "23. Wofür wird die Eigenwertzerlegung (z.B. PCA) in der Datenanalyse primär genutzt?",
      "options": [
        "Zur Dimensionsreduktion durch Bestimmung von Hauptkomponenten (Richtungen größter Varianz).",
        "Zur Lösung beliebiger linearer Gleichungssysteme.",
        "Zur Verschlüsselung von Daten durch Basiswechsel.",
        "Zur Bestimmung des kürzesten Pfads zwischen Datenpunkten.",
        "Zur Berechnung der Determinante einer Datenmatrix."
      ],
      "answer": 0,
      "explanation": "PCA identifiziert Hauptachsen der Varianz und reduziert die Dimension bei minimalem Informationsverlust.",
      "weight": 2,
      "topic": "LE 15: Eigenwerte und Eigenvektoren",
      "extended_explanation": {
        "titel": "PCA in präzisen Schritten",
        "schritte": [
          "Zentriere die Datenmatrix $X\\in\\mathbb{R}^{m\\times n}$ spaltenweise.",
          "Berechne die Kovarianz $C=\\tfrac{1}{m-1} X^\\top X$.",
          "Führe die Spektralzerlegung $C=Q \\Lambda Q^\\top$ durch mit orthogonalem $Q$ und nichtnegativem diagonalem $\\Lambda$.",
          "Die Spalten von $Q$ sind **Hauptkomponenten**, die Diagonale von $\\Lambda$ enthält die Varianzen je Komponente.",
          "Projektion auf die ersten $k$ Komponenten liefert $Z=X Q_k$ mit maximaler erklärter Varianz."
        ]
      },
      "mini_glossary": {
        "PCA": "Hauptkomponentenanalyse: Eigenzerlegung der Kovarianz zur Projektion auf Richtungen maximaler Varianz.",
        "Kovarianzmatrix": "$C=\\tfrac{1}{m-1}(X-\\bar X)^\\top(X-\\bar X)$; symmetrisch und positiv semidefinit.",
        "Varianz": "Erwarteter quadratischer Abstand vom Mittelwert; in PCA proportional zu den Eigenwerten von $C$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "24. Welche der folgenden Eigenschaften ist definierend für ein euklidisches Skalarprodukt?",
      "options": [
        "Positive Definitheit: $\\langle v,v\\rangle \\ge 0$ und $\\langle v,v\\rangle =0 \\Leftrightarrow v=0$.",
        "Assoziativität: $\\langle u,\\langle v,w\\rangle\\rangle=\\langle\\langle u,v\\rangle,w\\rangle$.",
        "Antisymmetrie: $\\langle v,w\\rangle=-\\langle w,v\\rangle$.",
        "Quadratische Homogenität: $\\langle 2v,w\\rangle=4\\langle v,w\\rangle$.",
        "Idempotenz: $\\langle v,v\\rangle=\\langle v,w\\rangle$ für alle w."
      ],
      "answer": 0,
      "explanation": "Positive Definitheit ist (neben Linearität und Symmetrie) definierend; die anderen Aussagen sind falsch formuliert.",
      "weight": 1,
      "topic": "LE 16: Skalarprodukt und Orthogonalität",
      "mini_glossary": {
        "Skalarprodukt": "Bilinear (in $\\mathbb{R}^n$), symmetrisch und positiv definit: $\\langle v,v\\rangle>0$ für $v\\ne 0$. Induziert Norm und Winkel.",
        "Norm": "$\\|v\\|=\\sqrt{\\langle v,v\\rangle}$; erfüllt Dreiecksungleichung und Homogenität.",
        "Orthogonalität": "$\\langle v,w\\rangle=0$ definiert rechten Winkel; orthogonale Projektionen minimieren quadratische Fehler."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "25. Was beschreibt eine Orthonormalbasis (ONB)?",
      "options": [
        "Eine Basis mit paarweise orthogonalen Vektoren und Norm $1$.",
        "Eine Basis mit Vektoren beliebiger Normen, aber paarweise orthogonal.",
        "Eine Basis, deren Vektoren die Norm $1$ haben, aber nicht orthogonal sind.",
        "Eine Menge linear abhängiger Vektoren mit Norm $1$.",
        "Eine Basis aus Vektoren mit ganzzahligen Komponenten."
      ],
      "answer": 0,
      "explanation": "ONB bedeutet orthogonal und normiert: $\\langle e_i,e_j\\rangle=\\delta_{ij}$ und $\\|e_i\\|=1$.",
      "weight": 2,
      "topic": "LE 16: Skalarprodukt und Orthogonalität",
      "extended_explanation": {
        "titel": "Warum ONB praktisch ist",
        "schritte": [
          "Für ONB $(e_i)$ gilt $x=\\sum_i \\langle x,e_i\\rangle e_i$; die **Koordinaten** sind innere Produkte.",
          "Parseval: $\\|x\\|^2=\\sum_i |\\langle x,e_i\\rangle|^2$; Energieerhaltung in ONB.",
          "Projektionen auf Unterräume $U=\\operatorname{span}\\{e_1,\\dots,e_k\\}$ sind $P_U x=\\sum_{i=1}^k \\langle x,e_i\\rangle e_i$.",
          "Numerisch sind ONBs stabil, da $Q$ orthogonal $\\Rightarrow Q^{-1}=Q^\\top$.",
          "Viele Zerlegungen (QR, SVD) nutzen ONB-Strukturen explizit."
        ]
      },
      "mini_glossary": {
        "Kronecker-Delta": "$\\delta_{ij}=1$ für $i=j$, sonst $0$; formalisiert Orthonormalität in Matrixform $Q^\\top Q=I$.",
        "Projektion": "Lineare Abbildung $P$ mit $P^2=P$ und $\\|x-Px\\|$ minimal; orthogonal, wenn zusätzlich $P^\\top=P$.",
        "QR-Zerlegung": "Faktorisierung $A=QR$ mit orthogonalem $Q$ und oberdreieckigem $R$; entsteht z.B. via Gram–Schmidt."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "26. Wie lautet die Formel für den Winkelkosinus zwischen $v$ und $w$?",
      "options": [
        "$\\cos(\\theta)=\\dfrac{\\langle v,w\\rangle}{\\|v\\|\\,\\|w\\|}$",
        "$\\cos(\\theta)=\\dfrac{\\langle v,w\\rangle}{\\|v\\|^2\\,\\|w\\|}$",
        "$\\cos(\\theta)=\\langle v,w\\rangle\\,\\|v\\|\\,\\|w\\|$",
        "$\\cos(\\theta)=\\dfrac{\\|v\\|\\,\\|w\\|}{\\langle v,w\\rangle}$",
        "$\\cos(\\theta)=\\dfrac{\\langle v,v\\rangle}{\\langle w,w\\rangle}$"
      ],
      "answer": 0,
      "explanation": "Normiert man die Vektoren, ergibt sich der Winkel über $\\cos(\\theta)=\\langle v,w\\rangle/(\\|v\\|\\,\\|w\\|)$.",
      "weight": 2,
      "topic": "LE 16: Skalarprodukt und Orthogonalität",
      "extended_explanation": {
        "titel": "Vom Skalarprodukt zum Winkel",
        "schritte": [
          "Definiere die Norm durch $\\|v\\|=\\sqrt{\\langle v,v\\rangle}$.",
          "Aus der Polaridentität bzw. dem Kosinussatz folgt $\\langle v,w\\rangle=\\|v\\|\\,\\|w\\|\\cos\\theta$.",
          "Äquivalent: $\\cos\\theta=\\dfrac{\\langle v,w\\rangle}{\\|v\\|\\,\\|w\\|}$ für $v,w\\ne 0$.",
          "Cauchy–Schwarz garantiert $|\\langle v,w\\rangle|\\le \\|v\\|\\,\\|w\\|$, daher $\\cos\\theta\\in[-1,1]$.",
          "Orthogonalität entspricht $\\langle v,w\\rangle=0$ und somit $\\theta=\\tfrac{\\pi}{2}$."
        ]
      },
      "mini_glossary": {
        "Norm": "Aus Skalarprodukt abgeleitet: $\\|v\\|=\\sqrt{\\langle v,v\\rangle}$; induziert Metrik $d(x,y)=\\|x-y\\|$.",
        "Normalisierung": "Ersetzt $v$ durch $\\hat v=\\dfrac{v}{\\|v\\|}$; setzt die Länge auf 1.",
        "Winkel": "Definiert über $\\cos\\theta=\\dfrac{\\langle v,w\\rangle}{\\|v\\|\\,\\|w\\|}$; unabhängig von der Länge der Vektoren."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "27. Wie lautet die orthogonale Projektion von $v$ auf $u\\ne 0$?",
      "options": [
        "$\\displaystyle \\operatorname{proj}_u(v)=\\frac{\\langle v,u\\rangle}{\\langle u,u\\rangle}\\,u$",
        "$\\displaystyle \\operatorname{proj}_u(v)=\\frac{\\langle u,u\\rangle}{\\langle v,u\\rangle}\\,v$",
        "$\\displaystyle \\operatorname{proj}_u(v)=\\langle v,u\\rangle\\,u$",
        "$\\displaystyle \\operatorname{proj}_u(v)=\\frac{\\langle v,v\\rangle}{\\langle u,u\\rangle}\\,u$",
        "$\\displaystyle \\operatorname{proj}_u(v)=\\frac{\\langle u,v\\rangle}{\\|v\\|}\\,u$"
      ],
      "answer": 0,
      "explanation": "Die Projektion entsteht durch Skalierung von $u$ mit dem Komponentenanteil $\\langle v,u\\rangle/\\langle u,u\\rangle$.",
      "weight": 3,
      "topic": "LE 16: Skalarprodukt und Orthogonalität",
      "extended_explanation": {
        "titel": "Herleitung der Projektionsformel",
        "schritte": [
          "Zerlege $v$ als $v=\\alpha u+w$ mit $w\\perp u$; dann ist $\\operatorname{proj}_u(v)=\\alpha u$.",
          "Orthogonalität liefert $0=\\langle v-\\alpha u, u\\rangle=\\langle v,u\\rangle-\\alpha\\langle u,u\\rangle$.",
          "Somit $\\alpha=\\dfrac{\\langle v,u\\rangle}{\\langle u,u\\rangle}$ und $\\operatorname{proj}_u(v)=\\dfrac{\\langle v,u\\rangle}{\\langle u,u\\rangle}u$.",
          "Der Projektor $P=\\dfrac{u u^\\top}{\\langle u,u\\rangle}$ ist **idempotent**: $P^2=P$ und **selbstadjungiert**: $P^\\top=P$.",
          "Der Fehler $v-\\operatorname{proj}_u(v)$ ist orthogonal zu $u$ und minimiert die Distanz im Sinne der kleinsten Quadrate."
        ]
      },
      "mini_glossary": {
        "Projektion": "Lineare Abbildung auf einen Unterraum, die jedes $v$ auf seine parallele Komponente abbildet und den Fehler orthogonal macht.",
        "Orthogonal": "Rechtwinklig im Sinne $\\langle v,w\\rangle=0$; impliziert Pythagoras $\\|v\\|^2=\\|p\\|^2+\\|v-p\\|^2$.",
        "Komponente": "Zerlegung $v=p+(v-p)$ mit $p$ parallel zu $u$; $p=\\operatorname{proj}_u(v)$."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "28. Das Gram–Schmidt-Verfahren wird angewendet auf ...",
      "options": [
        "eine Menge linear unabhängiger Vektoren, um eine ONB zu erzeugen.",
        "eine Menge linear abhängiger Vektoren, um Eigenwerte zu finden.",
        "eine einzelne Matrix, um ihre Determinante zu berechnen.",
        "zwei Vektoren, um deren Skalarprodukt zu bestimmen.",
        "bereits orthogonale Vektoren, um den Rang zu erhöhen."
      ],
      "answer": 0,
      "explanation": "Gram–Schmidt orthogonalisiert eine linear unabhängige Menge und normiert sie zur ONB.",
      "weight": 2,
      "topic": "LE 16: Skalarprodukt und Orthogonalität",
      "extended_explanation": {
        "titel": "Kernidee von Gram–Schmidt",
        "schritte": [
          "Starte mit linear unabhängigen $v_1,\\dots,v_k$ und setze $u_1=v_1$.",
          "Für $j\\ge 2$: $u_j=v_j-\\sum_{i=1}^{j-1} \\dfrac{\\langle v_j,u_i\\rangle}{\\langle u_i,u_i\\rangle} u_i$; dadurch werden $u_j$ orthogonal zu allen $u_i$, $i<j$.",
          "Normiere: $e_j=\\dfrac{u_j}{\\|u_j\\|}$ ergibt eine ONB.",
          "In Matrixform liefert Gram–Schmidt eine **QR-Zerlegung** $A=QR$ mit orthogonalem $Q$ und oberdreieckigem $R$.",
          "Die modifizierte Variante verbessert die numerische Stabilität durch sukzessive Orthogonalisierung."
        ]
      },
      "mini_glossary": {
        "Orthogonalisierung": "Verfahren, aus unabhängigen Vektoren orthogonale zu konstruieren, typischerweise durch Projektionen.",
        "ONB": "Orthonormalbasis; orthogonal und normiert, $Q^\\top Q=I$.",
        "Unterraum": "Durch eine Vektormenge erzeugte Menge aller Linearkombinationen; Gram–Schmidt liefert eine ONB dieses Unterraums."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "29. Wie groß ist die Summe aller Knotengrade in einem ungerichteten Graphen mit $k$ Kanten?",
      "options": [
        "$2k$",
        "$k$",
        "$n\\cdot k$",
        "$n-1$",
        "$n$"
      ],
      "answer": 0,
      "explanation": "Jede Kante trägt zu genau zwei Knotengraden bei; die Summe ist daher $2k$ (Handshaking-Lemma).",
      "weight": 1,
      "topic": "LE 17: Graphentheorie",
      "mini_glossary": {
        "Grad": "Anzahl der inzidenten Kanten eines Knotens; Schleifen zählen in ungerichteten Graphen typischerweise als 2.",
        "Handshaking-Lemma": "In ungerichteten Graphen gilt $\\sum_{v\\in V} \\deg(v)=2|E|$; folgt durch doppelte Zählung.",
        "Ungerichteter Graph": "Kanten sind ungeordnete Paare; es existiert keine Richtung an Kanten."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "30. Welche Repräsentation ist für dichte Graphen typischerweise speicherangemessen?",
      "options": [
        "Adjazenzmatrix mit Speicheraufwand $\\mathcal{O}(|V|^2)$",
        "Adjazenzliste mit Speicheraufwand $\\mathcal{O}(|V|+|E|)$",
        "Kantenliste mit Speicheraufwand $\\mathcal{O}(|E|)$",
        "Inzidenzmatrix mit Speicheraufwand $\\mathcal{O}(|V|\\cdot|E|)$",
        "Hash-basierte Nachbarschaft mit Speicheraufwand $\\mathcal{O}(|E|\\log|V|)$"
      ],
      "answer": 0,
      "explanation": "Bei dichten Graphen ist $|E|\\approx |V|^2$; die Adjazenzmatrix nutzt diesen Platz systematisch.",
      "weight": 2,
      "topic": "LE 17: Graphentheorie",
      "extended_explanation": {
        "titel": "Dichte vs. dünne Graphen",
        "schritte": [
          "Dicht: $|E|\\in\\Theta(|V|^2)$; dünn: $|E|\\in\\Theta(|V|)$.",
          "Adjazenzmatrix speichert für jedes Knotenpaar einen Eintrag; Speicher $\\mathcal{O}(|V|^2)$.",
          "Adjazenzlisten sind vorteilhaft bei dünnen Graphen mit Speicher $\\mathcal{O}(|V|+|E|)$.",
          "Viele Matrixoperationen sind auf der Adjazenzmatrix **vektorisierbar**, was Rechenzeit spart.",
          "Für dichte Graphen ist der konstante Faktor der Matrixdarstellung oft kleiner als der Listen-Overhead."
        ]
      },
      "mini_glossary": {
        "Adjazenzmatrix": "Binäre/gewichtete $|V|\\times |V|$-Matrix; Eintrag $(i,j)$ gibt (gewichtete) Kante an.",
        "Adjazenzliste": "Für jeden Knoten eine Liste seiner Nachbarn (ggf. mit Gewichten); speichereffizient bei dünnen Graphen.",
        "Dichter Graph": "Hoher Kantenanteil relativ zu $|V|^2$; typischerweise $|E|\\approx c\\,|V|^2$ mit konstantem $c>0$."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "31. Unter welcher Voraussetzung liefert der Dijkstra-Algorithmus korrekte kürzeste Wege?",
      "options": [
        "Alle Kantengewichte sind nicht-negativ.",
        "Alle Kantengewichte sind gleich.",
        "Der Graph ist zusammenhängend und ungerichtet.",
        "Es existieren keine Zyklen.",
        "Der Startknoten hat minimalen Grad."
      ],
      "answer": 0,
      "explanation": "Nicht-negative Gewichte sind erforderlich; bei negativen Kanten bricht die Korrektheit.",
      "weight": 3,
      "topic": "LE 17: Graphentheorie",
      "extended_explanation": {
        "titel": "Warum Dijkstra keine negativen Kanten verträgt",
        "schritte": [
          "Dijkstra nutzt eine **Greedy-Invariante**: Der entnommene Knoten hat endgültig minimalen Abstand.",
          "Bei nicht-negativen Kanten kann ein späterer Pfad den Abstand nicht mehr verkürzen.",
          "Eine negative Kante kann einen bereits finalen Abstand verringern und die Invariante zerstören.",
          "Bei negativen Gewichten verwendet man **Bellman–Ford** oder **Johnson** (mit Reweighting).",
          "Für nicht-negative Gewichte erreicht man mit einer Prioritätswarteschlange Komplexität $\\mathcal{O}((|V|+|E|)\\log|V|)$."
        ]
      },
      "mini_glossary": {
        "Greedy-Algorithmus": "Wählt lokal optimale Schritte; Korrektheit erfordert eine Monotonie-/Optimalitätsinvariante.",
        "Bellman–Ford": "Dynamische Programmierung für allgemeine Gewichte; erkennt negative Zyklen.",
        "Kürzeste Wege": "Minimiere die Summe der Kantengewichte $\\sum w(e)$ entlang eines Pfads."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "32. Wann ist ein Graph $G=(V,E)$ zusammenhängend?",
      "options": [
        "Wenn es zwischen jedem Knotenpaar einen Pfad gibt.",
        "Wenn $|E|\\ge |V|$ gilt.",
        "Wenn der Graph keine Zyklen enthält.",
        "Wenn alle Kanten gleich gewichtet sind.",
        "Wenn jeder Knoten den gleichen Grad hat."
      ],
      "answer": 0,
      "explanation": "Zusammenhängend bedeutet: Jedes Knotenpaar ist über eine Kantenfolge verbunden.",
      "weight": 2,
      "topic": "LE 17: Graphentheorie",
      "extended_explanation": {
        "titel": "Pfad-Definition der Zusammenhangseigenschaft",
        "schritte": [
          "Ungerichtet: Für alle $u,v\\in V$ existiert ein Pfad $u\\leadsto v$.",
          "Der Graph zerfällt in **Zusammenhangskomponenten**, maximal zusammenhängende Teilgraphen.",
          "Gerichtet: **Stark zusammenhängend** bedeutet gegenseitige Erreichbarkeit; **schwach** ignoriert Richtungen.",
          "Praktische Prüfung mittels BFS oder DFS in $\\mathcal{O}(|V|+|E|)$.",
          "Mengenweise Charakterisierung: Es gibt keine Trennung $V=S\\cup T$, $S,T\\ne\\emptyset$, ohne Kante zwischen $S$ und $T$."
        ]
      },
      "mini_glossary": {
        "Pfad": "Sequenz von Knoten mit benachbarten Paaren; Länge ist Anzahl Kanten oder Gewichts-Summe.",
        "Zyklus": "Geschlossener Pfad mit Länge $\\ge 1$; in ungerichteten Graphen ohne Richtung.",
        "Komponente": "Maximal zusammenhängender Teilgraph; Graph ist zusammenhängend genau dann, wenn er eine Komponente hat."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "33. Was ist ein Pfad in einem Graphen?",
      "options": [
        "Eine Knotenfolge, in der aufeinanderfolgende Knoten durch eine Kante verbunden sind.",
        "Eine Kantenmenge, die alle Knoten verbindet.",
        "Eine Folge, die am selben Knoten beginnt und endet.",
        "Die vollständige Liste der Nachbarn eines Knotens.",
        "Eine Teilmenge der Kanten ohne gemeinsame Knoten."
      ],
      "answer": 0,
      "explanation": "Ein Pfad ist eine gültige Verbindungskette Kante-für-Kante durch den Graphen.",
      "weight": 1,
      "topic": "LE 17: Graphentheorie",
      "mini_glossary": {
        "Einfacher Pfad": "Pfad ohne Wiederholung von Knoten (und damit Kanten). Vermeidet Schleifen/Mehrfachbesuche.",
        "Weglänge": "Anzahl der Kanten (ungewichtet) oder Summe der Gewichte (gewichtet).",
        "Teilgraph": "Graph, der aus einer Teilmenge von Knoten und/oder Kanten gebildet wird; kann Pfade isolieren."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "34. Ein Graph mit $n$ Knoten ist genau dann ein Baum, wenn er ...",
      "options": [
        "zusammenhängend ist und $n-1$ Kanten besitzt.",
        "zyklenfrei ist und $n$ Kanten besitzt.",
        "zusammenhängend ist und $n$ Kanten besitzt.",
        "zyklenfrei ist und $n-2$ Kanten besitzt.",
        "$n$ Knoten, $n-1$ Kanten und einen Zyklus besitzt."
      ],
      "answer": 0,
      "explanation": "Charakterisierung von Bäumen: zusammenhängend und zyklenfrei; in endlichen Graphen entspricht das $|E|=n-1$.",
      "weight": 1,
      "topic": "LE 18: Bäume",
      "mini_glossary": {
        "Baum": "Zusammenhängender, zyklenfreier Graph. Alternative Charakterisierungen sind äquivalent (z.B. eindeutiger Pfad zwischen Knoten).",
        "Kantenanzahl": "Für jeden Baum gilt $|E|=|V|-1$; folgt per Induktion oder aus dem Handschlag-Lemma.",
        "Zyklus": "Geschlossener Pfad; das Fehlen von Zyklen ist definierend für Bäume."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "35. Welche Aussage zu gewurzelten Bäumen trifft zu?",
      "options": [
        "Die Höhe ist die Anzahl der Kanten auf dem längsten Pfad von der Wurzel zu einem Blatt.",
        "Ein Blatt ist ein Knoten ohne Elternknoten.",
        "Die Tiefe eines Knotens ist die Anzahl seiner Kinder.",
        "Die Wurzel hat stets Eingangsgrad 0 und Ausgangsgrad 0.",
        "Ein Knoten kann mehrere Elternknoten haben."
      ],
      "answer": 0,
      "explanation": "Die Höhe misst die Länge des längsten Wurzel–Blatt-Pfads in Kanten.",
      "weight": 2,
      "topic": "LE 18: Bäume",
      "extended_explanation": {
        "titel": "Höhe, Tiefe, Blatt präzisieren",
        "schritte": [
          "Tiefe eines Knotens $v$ ist die Anzahl der Kanten im Pfad von der Wurzel zu $v$.",
          "Höhe des Baumes ist das Maximum der Tiefen aller Knoten.",
          "Ein Blatt besitzt **keine Kinder**, nicht notwendigerweise keinen Elternknoten.",
          "Jeder Knoten außer der Wurzel hat genau **einen** Elternknoten.",
          "Bezeichnungen hängen von der Wurzelwahl ab, nicht von der ungerichteten Struktur."
        ]
      },
      "mini_glossary": {
        "Wurzel": "Ausgewählter Startknoten, von dem Eltern-Kind-Beziehungen orientiert werden.",
        "Höhe": "Maximale Tiefe eines Knotens; Anzahl der Kanten entlang des längsten Wurzel–Blatt-Pfads.",
        "Blatt": "Knoten ohne Kinder; in Suchbäumen tragen sie Datenendpunkte."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "36. Welche Datenstruktur wird typischerweise zur Implementierung der Breitensuche (BFS) verwendet?",
      "options": [
        "Eine Queue (Warteschlange).",
        "Ein Stack (Stapelspeicher).",
        "Eine Adjazenzmatrix.",
        "Ein binärer Suchbaum.",
        "Eine Hashtabelle."
      ],
      "answer": 0,
      "explanation": "BFS arbeitet Ebene für Ebene und nutzt eine FIFO-Queue.",
      "weight": 2,
      "topic": "LE 18: Bäume",
      "extended_explanation": {
        "titel": "BFS-Mechanik in Kürze",
        "schritte": [
          "Initialisiere Distanzen mit $\\infty$, setze Startknoten $s$ auf 0, füge $s$ in die Queue.",
          "Entnimm vorn, relaxiere unbesuchte Nachbarn und füge sie hinten an; so entstehen **Ebenen**.",
          "Die entstehende Baumstruktur heißt **BFS-Baum** und enthält kürzeste Wege bei ungewichteten Graphen.",
          "Zeitkomplexität $\\mathcal{O}(|V|+|E|)$ bei Adjazenzlisten; Speicher $\\mathcal{O}(|V|)$.",
          "BFS testet Zusammenhang, berechnet Exzentrizitäten und dient als Basis für viele Graphalgorithmen."
        ]
      },
      "mini_glossary": {
        "BFS": "Breitensuche; verarbeitet Knoten in aufsteigender Entfernung vom Startknoten.",
        "Queue": "First-In-First-Out-Struktur; garantiert Ebenenreihenfolge.",
        "Traversierung": "Systematische Durchquerung der Knoten/Kanten entsprechend einer Strategie (BFS/DFS)."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "37. Welche Eigenschaft charakterisiert einen binären Suchbaum (BST)?",
      "options": [
        "Linker Teilbaum: Werte $\\le k$, rechter Teilbaum: Werte $\\ge k$ für jeden Knoten $k$.",
        "Jeder Knoten hat entweder 0 oder 2 Kinder.",
        "Alle Blätter liegen auf gleicher Ebene.",
        "Die Knoten wurden in BFS-Reihenfolge eingefügt.",
        "Der Baum ist stets perfekt balanciert."
      ],
      "answer": 0,
      "explanation": "BST-Ordnung: Alle Schlüssel im linken Teilbaum sind $\\le$ Schlüssel des Knotens, rechts $\\ge$.",
      "weight": 3,
      "topic": "LE 18: Bäume",
      "extended_explanation": {
        "titel": "BST-Ordnungsinvariante",
        "schritte": [
          "Für jeden Knoten mit Schlüssel $k$ gilt: alle Schlüssel im linken Teilbaum $\\le k$, im rechten $\\ge k$.",
          "Inorder-Traversierung liefert eine **sortierte** Folge der Schlüssel.",
          "Suchen, Einfügen, Löschen sind im Durchschnitt $\\mathcal{O}(\\log n)$, im Worst-Case $\\mathcal{O}(n)$.",
          "Duplikate benötigen eine Festlegung, z.B. $\\le$ links, $>$ rechts.",
          "Balancierte Varianten (AVL, Rot-Schwarz) erzwingen Höhen $\\in\\Theta(\\log n)$."
        ]
      },
      "mini_glossary": {
        "BST": "Baumstruktur mit Ordnungsinvariante; erlaubt binäre Suche entlang der Äste.",
        "In-Order": "Links–Wurzel–Rechts; gibt in einem BST die Schlüssel aufsteigend aus.",
        "Balancierung": "Techniken zur Begrenzung der Höhe (AVL, Rot-Schwarz, Splay), um Operationen effizient zu halten."
      },
      "cognitive_level": "Analyse"
    },
    {
      "question": "38. Welche Traversierung besucht die Knoten eines Baumes Ebene für Ebene?",
      "options": [
        "Breitensuche (BFS)",
        "Tiefensuche (DFS) – Preorder",
        "Tiefensuche (DFS) – Inorder",
        "Tiefensuche (DFS) – Postorder",
        "Dijkstra-Algorithmus"
      ],
      "answer": 0,
      "explanation": "Ebenenweise Traversierung ist das Kennzeichen der BFS.",
      "weight": 1,
      "topic": "LE 18: Bäume",
      "mini_glossary": {
        "Preorder": "Reihenfolge: Wurzel, dann linker und rechter Teilbaum; nützlich für Kopieren/Serialisieren.",
        "Inorder": "Links, Wurzel, Rechts; liefert bei BSTs sortierte Folge.",
        "Postorder": "Linker und rechter Teilbaum, dann Wurzel; nützlich zum Löschen/Freeing."
      },
      "cognitive_level": "Reproduktion"
    },
    {
      "question": "39. Wie viele Basisvektoren besitzt $\\mathbb{R}^n$ und was folgt für die Dimension?",
      "options": [
        "$n$ Basisvektoren; die Dimension ist $n$.",
        "$n-1$ Basisvektoren; die Dimension ist $n-1$.",
        "Beliebig viele Basisvektoren; die Dimension ist undefiniert.",
        "$2n$ Basisvektoren; die Dimension ist $2n$.",
        "Ein Basisvektor; die Dimension ist 1."
      ],
      "answer": 0,
      "explanation": "Der Standardraum $\\mathbb{R}^n$ hat per Definition eine Basis mit $n$ Vektoren; damit ist die Dimension $n$.",
      "weight": 2,
      "topic": "LE 12: Lineare Unabhängigkeit, Basis, Dimension",
      "extended_explanation": {
        "titel": "Basis und Dimension im $\\mathbb{R}^n$",
        "schritte": [
          "Die **Standardbasis** $(e_1,\\dots,e_n)$ ist linear unabhängig und erzeugend.",
          "Jeder Vektor $x\\in\\mathbb{R}^n$ besitzt eine eindeutige Darstellung $x=\\sum_{i=1}^n x_i e_i$.",
          "Alle Basen eines endlichen Vektorraums haben dieselbe Kardinalität; diese Zahl heißt **Dimension**.",
          "Basiswechsel wird durch invertible Matrizen beschrieben; Koordinaten transformieren via $T^{-1}$.",
          "Rang-Interpretation: Die Spalten von $I_n$ bilden eine Basis; Rang$(I_n)=n$."
        ]
      },
      "mini_glossary": {
        "Standardbasis": "$e_i$ mit einer 1 an Position $i$ und sonst 0; bildet Identität $I_n$ als Spalten.",
        "Dimension": "Invariante eines endlichen Vektorraums; Anzahl Vektoren jeder Basis.",
        "Linear unabhängig": "Nur die triviale Linearkombination ergibt $0$; prüfbar über Rang einer Spaltenmatrix."
      },
      "cognitive_level": "Anwendung"
    },
    {
      "question": "40. Welche Aussage über die Matrixzerlegung $A=P D P^{-1}$ trifft bei diagonalisierbaren Matrizen zu?",
      "options": [
        "Die Diagonaleinträge von $D$ sind die Eigenwerte von $A$ und die Spalten von $P$ die zugehörigen Eigenvektoren.",
        "Die Diagonaleinträge von $D$ sind die Singularwerte von $A$ und $P$ ist orthogonal.",
        "Die Matrix $P$ enthält die Zeilenvektoren von $A$ und $D$ die Spur von $A$.",
        "Die Matrix $D$ ist stets die Einheitsmatrix und $P$ beliebig.",
        "Die Zerlegung existiert nur für symmetrische Matrizen."
      ],
      "answer": 0,
      "explanation": "Bei Diagonalisierbarkeit bilden die Eigenvektoren die Spalten von $P$, und $D$ sammelt die Eigenwerte auf der Diagonalen.",
      "weight": 3,
      "topic": "LE 15: Eigenwerte und Eigenvektoren",
      "extended_explanation": {
        "titel": "Diagonalisierung interpretieren",
        "schritte": [
          "Eine Matrix $A$ ist diagonalisierbar $\\Leftrightarrow$ es existiert eine Basis aus Eigenvektoren von $A$.",
          "Dann gilt $A P=P D$ mit $P=[v_1\\,\\dots\\,v_n]$ und $D=\\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$.",
          "Äquivalent: $P^{-1} A P=D$; Potenzen ergeben sich als $A^k=P D^k P^{-1}$.",
          "Notwendig: Summe der geometrischen Vielfachheiten $=n$; hinreichend, wenn algebraische und geometrische Vielfachheiten je Eigenwert übereinstimmen.",
          "Für reell symmetrische $A$ kann man $P$ **orthogonal** wählen (Spektralsatz)."
        ]
      },
      "mini_glossary": {
        "Diagonalisierbarkeit": "Existenz einer Eigenvektorbasis; scheitert, wenn die Summe geometrischer Vielfachheiten $<n$ ist.",
        "Spektralzerlegung": "Darstellung über Eigenwerte/-vektoren; bei Normalmatrizen existiert unitäre Diagonalisierung.",
        "Basiswechsel": "Transformation $P^{-1} A P=D$; vereinfacht $A$-Potenzen und Funktionsberechnungen $f(A)$ via $f(D)$."
      },
      "cognitive_level": "Analyse"
    }
  ]
}
