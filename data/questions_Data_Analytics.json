{
  "meta": {
    "title": "Data Analytics & Big Data",
    "target_audience": "Fortgeschrittene",
    "question_count": 278,
    "difficulty_profile": {
      "leicht": 68,
      "mittel": 137,
      "schwer": 73
    },
    "time_per_weight_minutes": {
      "1": 0.5,
      "2": 0.75,
      "3": 1.0
    },
    "additional_buffer_minutes": 5,
    "test_duration_minutes": 215,
    "language": "de",
    "updated": "2026-02-09"
  },
  "questions": [
    {
      "question": "1. Was ist der Hauptzweck von Docker in einem Data-Science-Projekt?",
      "options": [
        "Um die Performance von Python-Skripten zu beschleunigen – im Kontext.",
        "Um eine reproduzierbare und isolierte Umgebung für die Software zu schaffen.",
        "Um Jupyter Notebooks direkt in der Cloud auszuführen – im Kontext.",
        "Um die Größe von Datensätzen zu reduzieren – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Docker packt eine Anwendung und ihre Abhängigkeiten in einen Container, der überall gleich läuft. Das löst das 'Bei mir funktioniert es aber'-Problem und sorgt für Reproduzierbarkeit.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "2. Welcher Befehl wird verwendet, um eine Streamlit-App zu starten?",
      "options": [
        "python run app.py",
        "streamlit start app.py",
        "streamlit run app.py",
        "start streamlit app.py"
      ],
      "answer": 2,
      "explanation": "Der Befehl 'streamlit run [Dateiname].py' startet den Streamlit-Server und öffnet die App im Browser.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "3. Was ist ein Pandas DataFrame?",
      "options": [
        "Eine ein-dimensionale Datenstruktur für numerische Daten.",
        "Eine zweidimensionale, tabellarische Datenstruktur mit Spalten und Zeilen.",
        "Eine Bibliothek zur Erstellung von interaktiven Diagrammen.",
        "Ein Machine-Learning-Modell zur Klassifikation – im Kontext."
      ],
      "answer": 1,
      "explanation": "Ein Pandas DataFrame ist die zentrale Datenstruktur in Pandas und ähnelt einer Excel-Tabelle oder einer SQL-Tabelle. Sie ist für die Handhabung und Analyse von strukturierten Daten optimiert.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "4. Was ist der Hauptunterschied zwischen Supervised und Unsupervised Learning?",
      "options": [
        "Supervised braucht mehr Rechenleistung.",
        "Unsupervised nur für Text – im Kontext.",
        "Supervised nutzt Labels, Unsupervised nicht.",
        "Unsupervised ist immer genauer – im Kontext."
      ],
      "answer": 2,
      "explanation": "Der Kernunterschied ist das Vorhandensein von 'Antworten' in den Trainingsdaten. Supervised Learning lernt von Beispielen mit bekannten Ergebnissen (Labels), während Unsupervised Learning Muster in ungelabelten Daten sucht.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "5. Welcher Algorithmus gehört zum Unsupervised Learning?",
      "options": [
        "Decision Tree",
        "K-Nearest Neighbors (KNN)",
        "K-Means Clustering",
        "Logistic Regression"
      ],
      "answer": 2,
      "explanation": "K-Means ist ein Clustering-Algorithmus, der versucht, Datenpunkte ohne vordefinierte Labels in 'k' Gruppen (Cluster) einzuteilen. Decision Tree, KNN und Logistic Regression sind Supervised-Learning-Algorithmen.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "6. Was ist der Zweck der 'Elbow-Methode'?",
      "options": [
        "Die optimale Anzahl der 'Nachbarn' (k) für KNN zu finden.",
        "Die optimale Anzahl der 'Cluster' (k) für K-Means zu finden.",
        "Die optimale Tiefe eines Decision Trees zu bestimmen.",
        "Die optimale Lernrate für ein Neuronales Netz zu finden."
      ],
      "answer": 1,
      "explanation": "Die Elbow-Methode plottet die 'Inertia' (Summe der quadrierten Abstände zu den Cluster-Zentren) für verschiedene k-Werte. Der 'Ellenbogen' in der Kurve deutet auf ein gutes 'k' hin.",
      "weight": 3,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Analyse",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "7. Was ist die Hauptfunktion einer Aktivierungsfunktion in einem Neuronalen Netz?",
      "options": [
        "Sie normalisiert die Eingabedaten.",
        "Sie führt Nichtlinearität in das Modell ein.",
        "Sie berechnet den Fehler des Modells.",
        "Sie initialisiert die Gewichte des Netzwerks."
      ],
      "answer": 1,
      "explanation": "Ohne nichtlineare Aktivierungsfunktionen wäre ein Neuronales Netz nur eine Kaskade von linearen Operationen, was es auf die Modellierung linearer Zusammenhänge beschränken würde. Die Nichtlinearität ermöglicht das Lernen komplexer Muster.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "8. Welche Aktivierungsfunktion wird typischerweise in der Ausgabeschicht eines Neuronalen Netzes für eine Multi-Klassen-Klassifikation verwendet?",
      "options": [
        "ReLU (Begriff) – im Kontext.",
        "Sigmoid – im Kontext.",
        "Tanh (Begriff) – im Kontext.",
        "Softmax – im Kontext."
      ],
      "answer": 3,
      "explanation": "Die Softmax-Funktion wandelt die rohen Ausgabe-Scores (Logits) des Netzes in eine Wahrscheinlichkeitsverteilung über alle Klassen um, wobei die Summe der Wahrscheinlichkeiten 1 ergibt.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "9. Was ist der Hauptvorteil von Convolutional Neural Networks (CNNs) gegenüber normalen Neuronalen Netzen bei der Bildverarbeitung?",
      "options": [
        "Nur numerische Daten sind erlaubt – im Kontext – in der Praxis.",
        "Datenqualität beeinflusst Modellleistung und Generalisierbarkeit.",
        "Mehr Daten sind immer besser, unabhängig von Qualität.",
        "Datenqualität ist nur für Deep Learning relevant – im Kontext."
      ],
      "answer": 1,
      "explanation": "CNNs verwenden Filter (Kernel), deren Gewichte über das gesamte Bild geteilt werden. Das macht sie extrem parameter-effizient und gut darin, lokale räumliche Muster zu erkennen, was für Bilder ideal ist.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "cognitive_level": "Analyse",
      "concept": "Spezialthemen & Methoden",
      "mini_glossary": {
        "Computer Vision": "Automatische Analyse von Bilddaten.",
        "Explainable AI (XAI)": "Methoden, die Modellentscheidungen nachvollziehbar machen.",
        "Transfer Learning": "Nutzung eines vortrainierten Modells für neue Aufgaben."
      }
    },
    {
      "question": "10. Was ist Data Augmentation?",
      "options": [
        "Manuelles Hinzufügen neuer, gelabelter Daten.",
        "Künstliche Erzeugung neuer Daten durch Transformationen.",
        "Beschleunigung des Trainings durch Hardware.",
        "Entfernen fehlerhafter oder verzerrter Daten."
      ],
      "answer": 1,
      "explanation": "Data Augmentation ist eine Technik, um Overfitting zu reduzieren, indem man dem Modell zur Trainingszeit leicht veränderte Versionen der Bilder zeigt. Dadurch lernt das Modell, robustere und allgemeinere Merkmale zu erkennen.",
      "weight": 2,
      "topic": "Spezialthemen & Methoden",
      "cognitive_level": "Anwendung",
      "concept": "Spezialthemen & Methoden",
      "mini_glossary": {
        "Computer Vision": "Automatische Analyse von Bilddaten.",
        "Explainable AI (XAI)": "Methoden, die Modellentscheidungen nachvollziehbar machen.",
        "Transfer Learning": "Nutzung eines vortrainierten Modells für neue Aufgaben."
      }
    },
    {
      "question": "11. Was versteht man unter Transfer Learning?",
      "options": [
        "Training von Grund auf – im Kontext – in der Praxis.",
        "Vortrainiertes Modell als Ausgangspunkt für neue Aufgabe.",
        "Übertragung zwischen Programmiersprachen – im Kontext.",
        "Mehrere Modelle auf Datenteilen trainieren."
      ],
      "answer": 1,
      "explanation": "Beim Transfer Learning nutzt man das 'Wissen' (die gelernten Features) eines Modells, das auf einem riesigen Datensatz (z.B. ImageNet) trainiert wurde, und passt es mit einem kleineren, aufgabenspezifischen Datensatz an. Dies spart enorm viel Zeit und Daten.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "12. Was ist der Zweck des QUA³CK-Prozessmodells?",
      "options": [
        "`dtype` legt den Datentyp von Spalten fest.",
        "`dtype` steuert das Layout von Diagrammen.",
        "`dtype` bestimmt die Plot‑Größe.",
        "`dtype` ist nur für Strings."
      ],
      "answer": 2,
      "explanation": "QUA³CK ist ein am KIT entwickeltes Prozessmodell, das die Phasen Question, Understanding, die A³-Schleife (Algorithm, Adapting, Adjusting), Conclude und Knowledge Transfer umfasst, um ML-Projekte systematisch zu bearbeiten.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "13. Welches Tool wird im Kurs primär für das Experiment-Tracking und die Modell-Verwaltung (Model Registry) verwendet?",
      "options": [
        "TensorBoard (primär für DL-Logs) – im Kontext.",
        "Weights & Biases (alternatives Tracking-Tool)",
        "MLflow (Experiment-Tracking & Model Registry)",
        "DVC (Daten-Versionierung) – im Kontext."
      ],
      "answer": 2,
      "explanation": "MLflow wird im Kurs als zentrales MLOps-Tool verwendet, um Experimente (Parameter, Metriken) zu tracken und trainierte Modelle in einer 'Model Registry' zu versionieren und zu verwalten.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Analyse",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "14. Was ist der Hauptzweck einer `requirements.txt`-Datei?",
      "options": [
        "Enthält den Python-Code – im Kontext.",
        "Listet Abhängigkeiten und Versionen.",
        "Beschreibt die NN-Architektur.",
        "Enthält Trainingsdaten – im Kontext."
      ],
      "answer": 1,
      "explanation": "Die `requirements.txt` Datei ermöglicht es, eine Python-Umgebung mit genau den richtigen Abhängigkeiten zu reproduzieren, was für die Zusammenarbeit und das Deployment entscheidend ist. Man installiert sie mit `pip install -r requirements.txt`.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "15. Welcher Python-Befehl wird verwendet, um eine Spalte namens 'Alter' aus einem Pandas DataFrame 'df' auszuwählen?",
      "options": [
        "df.get('Alter') – im Kontext.",
        "df['Alter'] – im Kontext.",
        "df.column('Alter') – im Kontext.",
        "df.select('Alter') – im Kontext."
      ],
      "answer": 1,
      "explanation": "Die Standardmethode, um auf eine Spalte in einem Pandas DataFrame zuzugreifen, ist die Verwendung der eckigen Klammern mit dem Spaltennamen als String: df['Alter'].",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "16. Was ist der Unterschied zwischen `st.write()` und `st.dataframe()` in Streamlit?",
      "options": [
        "`st.write()` zeigt Tabellen immer als DataFrame.",
        "`st.dataframe()` ist flexibler für alle Datentypen.",
        "`st.write()` ist generisch; `st.dataframe()` für Tabellen.",
        "Beide identisch – im Kontext – in der Praxis."
      ],
      "answer": 2,
      "explanation": "`st.write()` ist ein Allzweck-Befehl. Wenn man ihm einen Pandas DataFrame übergibt, zeigt er eine statische Tabelle an. `st.dataframe()` hingegen rendert eine interaktive Tabelle mit Sortier- und Filterfunktionen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "17. Was beschreibt der Begriff 'Overfitting' im Machine Learning?",
      "options": [
        "Zur Visualisierung nur nach dem Training.",
        "Für Training, Evaluation und Debugging des Modells.",
        "Nur für Datenbereinigung – im Kontext – in der Praxis.",
        "Nur für Deployment‑Monitoring – im Kontext."
      ],
      "answer": 1,
      "explanation": "Overfitting tritt auf, wenn ein Modell zu komplex ist im Verhältnis zur Datenmenge. Es passt sich perfekt an die Trainingsdaten an, verliert aber die Fähigkeit, auf ungesehenen Daten gute Vorhersagen zu machen.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "18. Welcher der folgenden Filter wird typischerweise zur Kantenerkennung in der Bildverarbeitung verwendet?",
      "options": [
        "Mean-Filter (Blur)",
        "Median-Filter",
        "Gauß-Filter",
        "Sobel-Filter"
      ],
      "answer": 3,
      "explanation": "Der Sobel-Filter ist ein klassischer Kantenerkennungs-Operator, der die Ableitung (den Gradienten) der Bildintensität berechnet, um Kanten hervorzuheben. Blur-Filter hingegen glätten das Bild.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "19. Was ist der Hauptvorteil der Verwendung von Transformer-Modellen (wie BERT oder GPT) gegenüber LSTMs für NLP-Aufgaben?",
      "options": [
        "Kein Unterschied in Leistung oder Skalierung.",
        "Parallelisierung + Self-Attention für lange Abhängigkeiten.",
        "Benötigen keine Trainingsdaten oder Labels – im Kontext.",
        "Nur für sehr kurze Texte und Sätze geeignet – im Kontext."
      ],
      "answer": 1,
      "explanation": "LSTMs verarbeiten Sequenzen Wort für Wort, was die Parallelisierung erschwert. Transformer verarbeiten alle Wörter einer Sequenz gleichzeitig mithilfe des 'Attention'-Mechanismus, was sie ideal für moderne Hardware wie GPUs und TPUs macht.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "20. Was ist der Zweck eines `Dockerfile`?",
      "options": [
        "Es ist ein Python-Skript zum Trainieren von ML-Modellen – im Kontext.",
        "Es ist eine Konfigurationsdatei für Jupyter Notebooks – im Kontext.",
        "Es ist eine Textdatei, die die Anweisungen zum Bauen eines Docker-Images enthält.",
        "Es ist eine Log-Datei, die alle Docker-Befehle aufzeichnet – im Kontext."
      ],
      "answer": 2,
      "explanation": "Ein Dockerfile ist wie ein Rezept. Es listet alle Schritte auf, die notwendig sind, um eine lauffähige Umgebung für eine Anwendung zu erstellen, inklusive Basis-Image, Abhängigkeiten und Startbefehlen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "21. Welches Argument wird bei `train_test_split` von Scikit-learn verwendet, um sicherzustellen, dass die Klassenverteilung in Trainings- und Testset gleich bleibt?",
      "options": [
        "shuffle=True – im Kontext.",
        "stratify=y – im Kontext.",
        "balance=True – im Kontext.",
        "keep_distribution=True"
      ],
      "answer": 1,
      "explanation": "Das `stratify`-Argument sorgt für eine geschichtete Aufteilung. Wenn man ihm die Label-Variable `y` übergibt, stellt es sicher, dass der prozentuale Anteil jeder Klasse in Trainings- und Testdaten dem des Gesamtdatensatzes entspricht.",
      "weight": 3,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Analyse",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "22. Was ist der Zweck des 'Pooling'-Layers in einem CNN?",
      "options": [
        "Die Anzahl der Features (Filter) zu erhöhen – im Kontext.",
        "Die räumliche Dimension der Feature Maps zu reduzieren (Downsampling).",
        "Dem Bild Rauschen hinzuzufügen, um Overfitting zu vermeiden.",
        "Die Farben des Bildes zu normalisieren – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Pooling (z.B. Max-Pooling) reduziert die Höhe und Breite der Feature Maps. Dies verringert die Anzahl der Parameter und den Rechenaufwand in nachfolgenden Schichten und macht das Modell robuster gegenüber kleinen Verschiebungen im Bild.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "23. Welches Python-Tool wird im Kurs verwendet, um ML-Modelle als REST API bereitzustellen?",
      "options": [
        "Streamlit",
        "Flask",
        "Django",
        "FastAPI"
      ],
      "answer": 3,
      "explanation": "FastAPI ist ein modernes, schnelles Web-Framework für Python, das sich hervorragend für die Erstellung von performanten APIs eignet, insbesondere für das Servieren von ML-Modellen.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "24. Was ist der Unterschied zwischen `Accuracy` und `Precision` als Metrik?",
      "options": [
        "Es gibt keinen Unterschied zwischen den Kennzahlen – im Kontext.",
        "Accuracy: Anteil korrekter Vorhersagen; Precision: Anteil korrekter Positiver.",
        "Precision ist immer höher als Accuracy bei Klassifikation – im Kontext.",
        "Accuracy nur für Regression, Precision nur für Klassifikation."
      ],
      "answer": 1,
      "explanation": "Accuracy = (TP+TN)/(TP+TN+FP+FN). Precision = TP/(TP+FP). Precision ist wichtig, wenn die Kosten für 'False Positives' hoch sind (z.B. Spam-Filter, der wichtige E-Mails fälschlicherweise als Spam markiert).",
      "weight": 3,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Analyse",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "25. Wie kann man in Streamlit interaktive Widgets wie Schieberegler oder Buttons in einer Seitenleiste platzieren?",
      "options": [
        "Widgets gehen nicht in der Sidebar (falsch).",
        "Mit `st.sidebar()` als Funktion (so nicht vorgesehen).",
        "Mit `st.sidebar.<widget>()`, z. B. `st.sidebar.slider()`.",
        "Mit CSS `position: sidebar` (nicht möglich)."
      ],
      "answer": 2,
      "explanation": "Jeder Streamlit-Befehl, dem `st.sidebar.` vorangestellt wird, rendert das entsprechende Element in der Seitenleiste anstatt im Hauptbereich der App.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "26. Was ist ein 'Hyperparameter'?",
      "options": [
        "Gewicht/Bias, das während des Trainings gelernt wird.",
        "Vor dem Training festgelegter Steuerparameter (z. B. Lernrate).",
        "Leistungsmaß auf dem Testset, z. B. Accuracy – im Kontext.",
        "Ausgabe der Verlustfunktion nach einer Epoche – im Kontext."
      ],
      "answer": 1,
      "explanation": "Hyperparameter sind die 'Stellschrauben' eines Modells, die nicht durch das Training gelernt, sondern vom Entwickler festgelegt werden. Die Suche nach den optimalen Hyperparametern ist ein wichtiger Teil des ML-Prozesses.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "27. Welcher Befehl wird in einem Dockerfile verwendet, um die notwendigen Python-Pakete zu installieren?",
      "options": [
        "INSTALL requirements.txt",
        "RUN pip install -r requirements.txt",
        "EXECUTE pip install -r requirements.txt",
        "ADD requirements.txt"
      ],
      "answer": 1,
      "explanation": "Der `RUN`-Befehl führt Shell-Kommandos innerhalb des Docker-Images aus. `pip install -r requirements.txt` ist der Standardbefehl, um alle in der `requirements.txt`-Datei gelisteten Pakete zu installieren.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "28. Was ist der Zweck der `GlobalAveragePooling2D`-Schicht in einem CNN?",
      "options": [
        "Sie erhöht die Auflösung der Feature‑Maps.",
        "Sie reduziert jede Feature‑Map auf einen Mittelwert.",
        "Sie fügt eine zusätzliche Convolution hinzu.",
        "Sie verdoppelt die Anzahl der Kanäle."
      ],
      "answer": 1,
      "explanation": "Anstatt die Feature Maps zu 'flatten' (was zu sehr vielen Parametern führt), berechnet Global Average Pooling den Durchschnitt jeder einzelnen Feature Map. Das Ergebnis ist ein Vektor, der direkt an die Dense-Layer weitergegeben werden kann, was das Modell oft robuster gegen Overfitting macht.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "29. Welches Problem löst der 'Attention'-Mechanismus in Transformer-Modellen?",
      "options": [
        "Vanishing Gradient in tiefen Netzen – im Kontext.",
        "Lange Abhängigkeiten in Sequenzen besser erfassen.",
        "Reduziert Parameterzahl in CNNs – im Kontext.",
        "Ersetzt Daten-Normalisierung – im Kontext."
      ],
      "answer": 1,
      "explanation": "Der Attention-Mechanismus erlaubt es dem Modell, für jedes Wort in einer Sequenz die Wichtigkeit jedes anderen Wortes zu bewerten. Dadurch kann es direkte Verbindungen zwischen weit entfernten Wörtern herstellen, ein Problem, mit dem rekurrente Architekturen (RNNs) zu kämpfen haben.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "30. Was ist der Hauptvorteil von `docker-compose` gegenüber einzelnen `docker run`-Befehlen?",
      "options": [
        "`docker-compose` ist schneller beim Bauen von Images – im Kontext.",
        "Mehrere abhängige Services per Datei und einem Befehl verwalten/starten.",
        "`docker-compose` benötigt weniger Speicher – im Kontext.",
        "`docker-compose` ist nur für Webserver geeignet – im Kontext."
      ],
      "answer": 1,
      "explanation": "`docker-compose` ist ein Orchestrierungstool. Es ermöglicht die Definition einer Multi-Container-Anwendung (z.B. eine Web-App, eine Datenbank und ein Caching-Service) in einer `docker-compose.yml`-Datei und deren gemeinsame Verwaltung.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "31. Welche Python-Bibliothek wird hauptsächlich für die Erstellung von interaktiven Web-Dashboards im Kurs verwendet?",
      "options": [
        "Flask (Begriff)",
        "Django",
        "Streamlit",
        "Plotly"
      ],
      "answer": 2,
      "explanation": "Streamlit ist das zentrale Framework im Kurs, um schnell und einfach interaktive Web-Anwendungen und Dashboards für Data-Science- und ML-Projekte zu erstellen.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "32. Was ist der Zweck der `fit()`-Methode bei einem Scikit-learn Modell?",
      "options": [
        "Sie macht Vorhersagen auf neuen Daten.",
        "Sie evaluiert die Genauigkeit des Modells.",
        "Sie trainiert das Modell mit den Trainingsdaten.",
        "Sie speichert das trainierte Modell auf der Festplatte."
      ],
      "answer": 2,
      "explanation": "Die `fit(X_train, y_train)`-Methode ist der zentrale Schritt im Trainingsprozess, bei dem das Modell die Muster und Zusammenhänge aus den Trainingsdaten lernt.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "33. Welcher der 'Big 3' Algorithmen ist ein distanzbasierter Algorithmus?",
      "options": [
        "Decision Tree (baum-basiert)",
        "K-Means Clustering (clusterbasiert)",
        "K-Nearest Neighbors (distanzbasiert)",
        "Random Forest (Ensemble aus Bäumen)"
      ],
      "answer": 2,
      "explanation": "KNN klassifiziert einen neuen Datenpunkt basierend auf der Mehrheitsklasse seiner 'k' nächsten Nachbarn. Die 'Nähe' wird dabei durch ein Distanzmaß (z.B. Euklidischer Abstand) bestimmt.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "34. Was ist eine 'Feature Map' im Kontext von CNNs?",
      "options": [
        "Ein Symbol für fehlende Werte.",
        "Ein Algorithmus zur Dimensionsreduktion.",
        "Ein Plot‑Typ für Zeitreihen.",
        "Ein Verfahren zur Hyperparameter‑Suche."
      ],
      "answer": 1,
      "explanation": "Jeder Filter in einer Convolutional-Schicht erzeugt eine Feature Map. Diese Karte zeigt, wo im Bild das vom Filter gesuchte Muster (z.B. eine vertikale Kante, ein Auge) gefunden wurde.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "35. Was ist der Zweck der `requirements.cloud.txt` Datei im `07_Deployment_Portfolio` Ordner?",
      "options": [
        "Anforderungen fürs Cloud-Training.",
        "Pakete für Deployment auf Streamlit Cloud.",
        "Backup der normalen requirements.txt.",
        "Enthält Cloud-Zugangsdaten."
      ],
      "answer": 1,
      "explanation": "Deployment-Plattformen wie Streamlit Cloud benötigen eine `requirements.txt`-Datei, um zu wissen, welche Pakete für die Ausführung der App installiert werden müssen. Die `requirements.cloud.txt` ist speziell für diesen Zweck optimiert.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "36. Welcher Datentyp wird in Python verwendet, um eine unveränderliche Liste von Elementen zu speichern?",
      "options": [
        "list (veränderlich)",
        "dict (Schlüssel/Werte)",
        "set (Menge)",
        "tuple (unveränderlich)"
      ],
      "answer": 3,
      "explanation": "Ein Tupel (`tuple`) ist ähnlich wie eine Liste, aber seine Elemente können nach der Erstellung nicht mehr geändert, hinzugefügt oder entfernt werden. Dies macht es nützlich für Daten, die konstant bleiben sollen.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "37. Was ist der Unterschied zwischen `iloc` und `loc` in Pandas?",
      "options": [
        "`loc` wählt nach Labels, `iloc` nach Positionsindex.",
        "`iloc` ist immer schneller als `loc`.",
        "`loc` wählt nur Zeilen, `iloc` nur Spalten.",
        "Es gibt keinen funktionalen Unterschied."
      ],
      "answer": 0,
      "explanation": "`loc` ist label-basiert, z.B. `df.loc[0, 'Alter']`. `iloc` ist integer-positions-basiert, z.B. `df.iloc[0, 1]`. Die Verwendung des falschen Indexers führt oft zu Fehlern.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Analyse",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "38. Was ist der 'Vanishing Gradient' Problem in tiefen Neuronalen Netzen?",
      "options": [
        "Gradienten werden zu groß (Exploding).",
        "Gradienten werden sehr klein und Lernen stoppt.",
        "Netz vergisst frühere Epochen – im Kontext.",
        "Aktivierungen verschwinden vollständig."
      ],
      "answer": 1,
      "explanation": "Bei der Backpropagation wird der Fehlergradient durch das Netzwerk zurückpropagiert. Bei tiefen Netzen und bestimmten Aktivierungsfunktionen (wie Sigmoid) kann dieser Gradient exponentiell kleiner werden, was das Update der Gewichte in den vorderen Schichten verhindert.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "39. Welche Technik wird verwendet, um das 'Vanishing Gradient'-Problem zu mildern?",
      "options": [
        "Dropout",
        "Batch Normalization",
        "Gradient Clipping",
        "Label Smoothing"
      ],
      "answer": 2,
      "explanation": "Die ReLU-Aktivierungsfunktion hat für positive Eingaben eine konstante Ableitung von 1, was den Gradientenfluss erleichtert. Residual Connections (in ResNets) schaffen 'Kurzschlüsse', die es dem Gradienten ermöglichen, Schichten zu überspringen.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "40. Was ist der Zweck der Datei `.gitignore` in einem Git-Repository?",
      "options": [
        "Feature Engineering",
        "Normalization",
        "Data Leakage",
        "Sampling"
      ],
      "answer": 1,
      "explanation": "Die `.gitignore`-Datei ist entscheidend, um das Repository sauber zu halten, indem man verhindert, dass generierte Dateien, Abhängigkeiten (wie `node_modules`) oder sensible Informationen versehentlich committet werden.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "41. Was ist der Unterschied zwischen einem `Conv2D`-Layer und einem `Dense`-Layer in Keras?",
      "options": [
        "Einzelne Testdaten im Training.",
        "Training nur mit Labels – im Kontext.",
        "Automatische Datenbereinigung.",
        "Mehr Trainingsdaten als Testdaten."
      ],
      "answer": 1,
      "explanation": "Diese unterschiedliche Konnektivität ist der Kernunterschied. `Dense` (oder Fully-Connected) Layer lernen globale Muster, während `Conv2D`-Layer durch ihre lokalen rezeptiven Felder und das Weight Sharing lokale, räumliche Muster lernen.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "42. Was ist eine 'Epoche' im Kontext des Trainings von Machine-Learning-Modellen?",
      "options": [
        "Die Verarbeitung eines einzelnen Datenpunktes – im Kontext – in der Praxis.",
        "Ein kompletter Durchlauf des Algorithmus durch den gesamten Trainingsdatensatz.",
        "Die Zeit, die für das Training eines Modells benötigt wird – im Kontext.",
        "Ein einzelner Schritt der Gewichtsaktualisierung – im Kontext."
      ],
      "answer": 1,
      "explanation": "Eine Epoche ist abgeschlossen, wenn das Modell jeden Datenpunkt des Trainingsdatensatzes einmal gesehen hat. Das Training eines Modells erstreckt sich typischerweise über viele Epochen.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "43. Welches Streamlit-Kommando wird verwendet, um einen Schieberegler für Zahlen zu erstellen?",
      "options": [
        "st.number_input() – im Kontext.",
        "st.slider() – im Kontext.",
        "st.range() – im Kontext.",
        "st.numeric_selector()"
      ],
      "answer": 1,
      "explanation": "`st.slider()` ist das spezifische Widget in Streamlit, um einen interaktiven Schieberegler zu erstellen, mit dem Benutzer einen numerischen Wert aus einem definierten Bereich auswählen können.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "44. Was ist der Zweck der `predict()`-Methode bei einem trainierten Scikit-learn Modell?",
      "options": [
        "Trainiert das Modell neu.",
        "Gibt gelernte Parameter zurück.",
        "Erzeugt Vorhersagen für neue Daten.",
        "Berechnet die Trainingsgenauigkeit."
      ],
      "answer": 2,
      "explanation": "Nachdem ein Modell mit `fit()` trainiert wurde, wird die `predict()`-Methode verwendet, um es auf neue Daten (z.B. das Testset) anzuwenden und die entsprechenden Vorhersagen (Klassen oder Werte) zu generieren.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "45. Was ist der Hauptunterschied zwischen einem `Dockerfile` und einem `docker-compose.yml`?",
      "options": [
        "Dockerfile baut ein Image; compose orchestriert Services.",
        "Dockerfile nur für Dev, compose nur für Prod.",
        "Dockerfile ist Python, compose ist YAML – im Kontext.",
        "Beides ist austauschbar in Projekten – im Kontext."
      ],
      "answer": 0,
      "explanation": "Ein `Dockerfile` ist die Bauanleitung für einen einzelnen Container. `docker-compose` ist ein Werkzeug, um Multi-Container-Anwendungen zu definieren und auszuführen, wobei jeder Container auf seinem eigenen `Dockerfile` basieren kann.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "46. Welcher der folgenden Algorithmen ist ein Ensemble-Modell?",
      "options": [
        "K-Nearest Neighbors (KNN)",
        "Decision Tree – im Kontext.",
        "Random Forest – im Kontext.",
        "Linear Regression – im Kontext."
      ],
      "answer": 2,
      "explanation": "Ein Random Forest ist ein Ensemble-Modell, das aus vielen einzelnen Decision Trees besteht. Er trifft Vorhersagen, indem er die Vorhersagen der einzelnen Bäume aggregiert (z.B. durch Mehrheitsentscheid), was oft zu robusteren und genaueren Ergebnissen führt.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "47. Was ist der Zweck der `Flatten`-Schicht in einem CNN?",
      "options": [
        "Glättet das Eingabebild.",
        "Formt Feature-Maps zu 1D-Vektor.",
        "Reduziert Farbtiefe – im Kontext.",
        "Führt eine Faltung aus – im Kontext."
      ],
      "answer": 1,
      "explanation": "Nach den Convolutional- und Pooling-Layern liegen die Daten als mehrdimensionale Tensoren (Feature Maps) vor. Um sie an die nachfolgenden `Dense`-Layer (die einen Vektor als Input erwarten) übergeben zu können, muss dieser Tensor 'geglättet' oder 'plattgedrückt' werden.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "48. Was ist ein 'REST API'?",
      "options": [
        "Ein spezieller ML-Modelltyp für Klassifikation.",
        "HTTP-Schnittstelle mit standardisierten Ressourcen.",
        "Tool zur Versionskontrolle von Code, z. B. Git.",
        "Große Datenbank für Rohdaten und Logs."
      ],
      "answer": 1,
      "explanation": "REST (Representational State Transfer) ist ein Architekturstil für verteilte Systeme. Eine REST API ermöglicht es einem Client (z.B. eine Web-App), Daten von einem Server (z.B. einem ML-Modell-Server) über Standard-HTTP-Methoden (GET, POST, etc.) anzufordern.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "49. Welches Python-Keyword wird verwendet, um eine Funktion zu definieren?",
      "options": [
        "function (kein Python-Keyword)",
        "def (Funktionen definieren)",
        "fun (kein Python-Keyword)",
        "define (kein Python-Keyword)"
      ],
      "answer": 1,
      "explanation": "In Python wird das Keyword `def` verwendet, um eine neue Funktion zu deklarieren, gefolgt vom Funktionsnamen, den Parametern in Klammern und einem Doppelpunkt.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "50. Was ist der Zweck der `BatchNormalization`-Schicht in einem Neuronalen Netz?",
      "options": [
        "Erhöht die Parameterzahl des Modells – im Kontext.",
        "Normalisiert Aktivierungen und stabilisiert Training.",
        "Ersetzt Aktivierungsfunktionen vollständig.",
        "Wirkt nur in der ersten Schicht – im Kontext."
      ],
      "answer": 1,
      "explanation": "Batch Normalization normalisiert die Ausgaben einer Schicht, bevor sie an die nächste weitergegeben werden. Dies wirkt dem Problem des 'Internal Covariate Shift' entgegen, erlaubt höhere Lernraten und macht das Training insgesamt stabiler und schneller.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "51. Welcher Befehl wird verwendet, um alle laufenden Docker-Container anzuzeigen?",
      "options": [
        "docker show all – im Kontext.",
        "docker list – im Kontext.",
        "docker ps – im Kontext.",
        "docker containers"
      ],
      "answer": 2,
      "explanation": "`docker ps` listet alle aktuell laufenden Container auf. Um auch gestoppte Container anzuzeigen, verwendet man `docker ps -a`.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "52. Wie kann man in Streamlit eine Datei-Upload-Funktion erstellen?",
      "options": [
        "st.upload_file()",
        "st.file_uploader()",
        "st.input(type='file')",
        "st.load_file()"
      ],
      "answer": 1,
      "explanation": "Das Widget `st.file_uploader()` erstellt eine Schaltfläche, mit der Benutzer Dateien von ihrem lokalen System in die Streamlit-App hochladen können, die dann z.B. mit Pandas verarbeitet werden können.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "53. Was ist der Unterschied zwischen `pd.read_csv()` und `pd.read_excel()` in Pandas?",
      "options": [
        "Beide laden identisch CSV und Excel – im Kontext.",
        "`read_csv()` liest CSV, `read_excel()` liest Excel‑Dateien.",
        "`read_excel()` ist für JSON‑Dateien – im Kontext.",
        "`read_csv()` benötigt immer eine Datenbank – im Kontext."
      ],
      "answer": 1,
      "explanation": "Pandas bietet spezifische Funktionen für verschiedene Dateiformate. `pd.read_csv()` ist für CSV-Dateien optimiert, während `pd.read_excel()` die komplexere Struktur von Excel-Arbeitsmappen (inkl. verschiedener Blätter) handhaben kann.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "54. Was ist eine 'Confusion Matrix'?",
      "options": [
        "Eine Matrix der Feature‑Korrelationen – im Kontext.",
        "Tabelle der Klassifikationsergebnisse (TP, TN, FP, FN).",
        "Eine Visualisierung hochdimensionaler Daten.",
        "Eine Technik zur Hyperparameter‑Optimierung."
      ],
      "answer": 1,
      "explanation": "Die Confusion Matrix ist ein wichtiges Werkzeug zur Evaluation von Klassifikationsmodellen. Sie zeigt detailliert, welche Klassen das Modell gut unterscheidet und wo es zu Verwechslungen kommt.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "55. Welcher der 'Big 3' Algorithmen ist am besten für die Interpretation und Erklärung von Entscheidungen geeignet?",
      "options": [
        "K‑Means‑Clustering – im Kontext.",
        "k‑Nearest Neighbors (kNN) – im Kontext.",
        "Ein Entscheidungsbaum (Decision Tree)",
        "Alle sind gleich gut interpretierbar"
      ],
      "answer": 2,
      "explanation": "Decision Trees (Entscheidungsbäume) sind von Natur aus sehr gut interpretierbar, da ihre Struktur einer Reihe von verständlichen Ja/Nein-Fragen entspricht. Man kann den Entscheidungspfad für jede Vorhersage nachvollziehen.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "56. Was ist der Zweck der `Dropout`-Schicht in einem Neuronalen Netz?",
      "options": [
        "Sie glättet die Eingabedaten im Modell.",
        "Sie deaktiviert zufällig Neuronen im Training.",
        "Sie erhöht die Lernrate automatisch.",
        "Sie fügt neue Neuronen hinzu – im Kontext."
      ],
      "answer": 1,
      "explanation": "Indem in jedem Trainingsschritt zufällig Neuronen deaktiviert werden, zwingt Dropout das Netzwerk, robustere und weniger voneinander abhängige Features zu lernen. Dies wirkt Overfitting entgegen und verbessert die Generalisierungsfähigkeit.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "57. Was ist der Unterschied zwischen 'semantischer Segmentierung' und 'Instanzensegmentierung' in der Computer Vision?",
      "options": [
        "Es gibt keinen Unterschied in der Praxis – im Kontext – in der Praxis.",
        "Semantisch: Klasse pro Pixel; Instanz: trennt einzelne Objekte gleicher Klasse.",
        "Instanzensegmentierung ist immer einfacher – im Kontext – in der Praxis.",
        "Semantische Segmentierung nutzt CNNs, Instanzensegmentierung nicht."
      ],
      "answer": 1,
      "explanation": "Beispiel: Bei einem Bild mit zwei Katzen würde die semantische Segmentierung alle Katzenpixel als 'Katze' markieren. Die Instanzensegmentierung würde sie als 'Katze 1' und 'Katze 2' unterscheiden.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "58. Was ist der Zweck der `fine-tuning`-Phase beim Transfer Learning?",
      "options": [
        "Den vortrainierten Teil komplett neu trainieren.",
        "Nur den neuen Klassifikator trainieren – im Kontext.",
        "Obere Schichten mit kleiner Lernrate an neue Aufgabe anpassen.",
        "Die Anzahl der Layer im Modell reduzieren – im Kontext."
      ],
      "answer": 2,
      "explanation": "Nachdem der neue Klassifikator trainiert wurde (Feature Extraction), werden einige der oberen Schichten des Basis-Modells 'aufgetaut'. Mit einer sehr kleinen Lernrate werden diese Gewichte dann vorsichtig an die Nuancen des neuen Datensatzes angepasst, ohne das wertvolle vortrainierte Wissen zu zerstören.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "59. Welches Hugging Face Modell wird im Kurs als Beispiel für Textgenerierung verwendet?",
      "options": [
        "BERT (Begriff) – im Kontext.",
        "GPT‑2 – im Kontext.",
        "ResNet‑50 – im Kontext.",
        "Word2Vec – im Kontext."
      ],
      "answer": 2,
      "explanation": "Im Notebook `02_NLP_und_Text_Generation.ipynb` wird `distilgpt2`, eine kleinere und schnellere Version von GPT-2, als Beispiel für eine Textgenerierungs-Pipeline mit der Hugging Face `transformers`-Bibliothek verwendet.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "60. Was bedeutet der Begriff 'CI/CD' im Kontext von MLOps?",
      "options": [
        "Continuous Integration/Deployment: Build, Test, Deploy automatisieren.",
        "Complex Intelligence/Complex Deployment (kein Standard).",
        "Code Inspection/Code Delivery (kein Standard) – im Kontext.",
        "Cloud Infrastructure/Cloud Database (kein Standard) – im Kontext."
      ],
      "answer": 0,
      "explanation": "CI/CD ist ein Kernprinzip von DevOps und MLOps. Continuous Integration automatisiert das Testen bei jeder Code-Änderung, während Continuous Deployment den Prozess automatisiert, neue Versionen in die Produktion zu bringen.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "61. Welcher Datentyp in Python wird verwendet, um Schlüssel-Wert-Paare zu speichern?",
      "options": [
        "list",
        "tuple",
        "set",
        "dict"
      ],
      "answer": 3,
      "explanation": "Ein Dictionary (`dict`) ist eine ungeordnete Sammlung von Daten in einem Schlüssel-Wert-Format. Es ist optimiert für das schnelle Nachschlagen von Werten anhand ihrer Schlüssel.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "62. Wie kann man in Pandas alle Zeilen eines DataFrames `df` anzeigen, in denen der Wert der Spalte 'Alter' größer als 30 ist?",
      "options": [
        "df.filter('Alter' > 30)",
        "df[df['Alter'] > 30]",
        "df.select('Alter' > 30)",
        "df.where('Alter' > 30)"
      ],
      "answer": 1,
      "explanation": "Dies wird als 'boolean indexing' oder 'boolean masking' bezeichnet. `df['Alter'] > 30` erzeugt eine Serie von `True`/`False`-Werten, die dann verwendet wird, um die entsprechenden Zeilen aus dem DataFrame zu filtern.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "63. Was ist der Zweck von `st.cache_data` in Streamlit?",
      "options": [
        "Speichert die ganze App im Browser – im Kontext.",
        "Cached Funktionsresultate bei gleichen Argumenten.",
        "Komprimiert nur die Anzeige der Daten.",
        "Leert den Cache automatisch – im Kontext."
      ],
      "answer": 1,
      "explanation": "Caching ist entscheidend für die Performance von Streamlit-Apps. Langsame Operationen wie das Laden großer Datensätze oder das Trainieren von Modellen sollten mit `@st.cache_data` oder `@st.cache_resource` versehen werden, um unnötige Neuberechnungen bei jedem App-Rerun zu vermeiden.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Analyse",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "64. Was ist der Unterschied zwischen Regression und Klassifikation?",
      "options": [
        "Regression sagt Klassen voraus; Klassifikation Zahlen.",
        "Regression sagt kontinuierliche Werte; Klassifikation Klassen.",
        "Beide sind identisch in der Praxis – im Kontext.",
        "Klassifikation ist nur für Zeitreihen – im Kontext."
      ],
      "answer": 1,
      "explanation": "Dies ist die grundlegendste Unterscheidung bei Supervised-Learning-Problemen. Die Wahl des Modells, der Verlustfunktion und der Evaluationsmetriken hängt direkt davon ab, ob man einen numerischen Wert oder eine Kategorie vorhersagen möchte.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "65. Warum ist die Skalierung von Features (z.B. mit `StandardScaler`) für den KNN-Algorithmus wichtig?",
      "options": [
        "Sie ist nicht wichtig für KNN und hat keinen Einfluss auf Distanzen.",
        "Sie wandelt alle Features in Ganzzahlen um, damit KNN rechnen kann.",
        "KNN nutzt Distanzen; ohne Skalierung dominieren große Wertebereiche die Berechnung.",
        "Sie reduziert nur die Anzahl der Features, nicht deren Skala."
      ],
      "answer": 2,
      "explanation": "KNN ist ein distanzbasierter Algorithmus. Wenn die Features unterschiedliche Skalen haben, werden die Distanzen von den Features mit den größten Wertebereichen dominiert. Die Skalierung (z.B. auf einen Mittelwert von 0 und eine Standardabweichung von 1) stellt sicher, dass alle Features gleichberechtigt zur Distanzberechnung beitragen.",
      "weight": 3,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Analyse",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "66. Was ist ein 'Autoencoder'?",
      "options": [
        "Supervised‑Modell zur Bildklassifikation.",
        "Netz, das Eingaben rekonstruiert (Bottleneck).",
        "Algorithmus zur automatischen Code‑Generierung.",
        "Spezialtyp eines RL‑Agenten."
      ],
      "answer": 1,
      "explanation": "Ein Autoencoder besteht aus einem Encoder, der die Eingabe in einen niedrigdimensionalen Code komprimiert, und einem Decoder, der versucht, aus diesem Code die ursprüngliche Eingabe zu rekonstruieren. Er wird für Dimensionsreduktion, Feature Learning und Anomalieerkennung verwendet.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "67. Was ist der Zweck der `padding='same'`-Einstellung in einem `Conv2D`-Layer?",
      "options": [
        "Sie hält die Feature‑Map‑Größe konstant.",
        "Sie entfernt Padding, damit das Bild kleiner wird.",
        "Sie erhöht die Anzahl der Filter automatisch.",
        "Sie setzt die Lernrate des Layers."
      ],
      "answer": 1,
      "explanation": "Ohne Padding würde die Größe der Feature Map bei jeder Faltung kleiner werden. `padding='same'` fügt dem Rand der Eingabe implizit Nullen hinzu (Zero-Padding), sodass die Ausgabe die gleiche Höhe und Breite wie die Eingabe hat. Dies ist nützlich, um sehr tiefe Netzwerke zu bauen.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "68. Was ist ein 'Token' im Kontext von Natural Language Processing (NLP)?",
      "options": [
        "Ein Netzwerkpaket im Internet.",
        "Ein Wort oder Subwort im Text.",
        "Eine Zeile in einer CSV‑Datei.",
        "Ein Python‑Modul in einem Projekt."
      ],
      "answer": 1,
      "explanation": "Tokenisierung ist der erste Schritt in den meisten NLP-Pipelines. Dabei wird ein Rohtext in eine Liste von Tokens zerlegt, die dann in numerische Vektoren (Embeddings) umgewandelt werden können, die das Modell verarbeiten kann.",
      "weight": 2,
      "topic": "Deep Learning",
      "cognitive_level": "Anwendung",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "69. Was ist der Zweck des `EXPOSE`-Befehls in einem Dockerfile?",
      "options": [
        "Er öffnet den Port auf dem Host‑System – im Kontext.",
        "Dokumentiert den Container‑Port, veröffentlicht ihn aber nicht.",
        "Er installiert einen Webserver im Container – im Kontext.",
        "Er macht den Container automatisch im Netzwerk sichtbar."
      ],
      "answer": 1,
      "explanation": "`EXPOSE` ist eine Form der Dokumentation zwischen dem Ersteller des Images und der Person, die den Container ausführt. Um den Port tatsächlich zu veröffentlichen und vom Host aus zugänglich zu machen, muss man die Option `-p` oder `-P` beim `docker run`-Befehl verwenden.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "70. Was ist der Unterschied zwischen `pip` und `conda`?",
      "options": [
        "pip nur für Python 2, conda nur für Python 3 – im Kontext.",
        "pip installiert Pakete; conda verwaltet zusätzlich Umgebungen.",
        "conda ersetzt pip komplett in allen Fällen – im Kontext.",
        "Kein Unterschied zwischen beiden Tools – im Kontext."
      ],
      "answer": 1,
      "explanation": "`pip` ist der Standard-Paketmanager für Python. `conda` ist Teil der Anaconda-Distribution und kann nicht nur Python-Pakete, sondern auch komplexe Abhängigkeiten (wie C-Bibliotheken) und isolierte Umgebungen verwalten, was es in der Data Science sehr beliebt macht.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "71. Was ist der Zweck der `groupby()`-Funktion in Pandas?",
      "options": [
        "Sortiert den DataFrame nach Spalte – im Kontext.",
        "Gruppiert nach Spalten und erlaubt Aggregationen.",
        "Wählt eine Gruppe von Zeilen aus – im Kontext.",
        "Benennt Spalten automatisch um – im Kontext."
      ],
      "answer": 1,
      "explanation": "Die `groupby()`-Operation ist ein extrem mächtiges Werkzeug für die Datenanalyse. Sie folgt dem 'Split-Apply-Combine'-Muster: Daten aufteilen, eine Funktion anwenden und die Ergebnisse wieder zusammenführen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "72. Wie kann man in Streamlit den Inhalt auf mehrere Spalten aufteilen?",
      "options": [
        "Durch die Verwendung von HTML-Tabellen.",
        "Durch die Verwendung von `st.columns()`.",
        "Durch die Verwendung von `st.split()`.",
        "Das ist in Streamlit nicht möglich."
      ],
      "answer": 1,
      "explanation": "Der Befehl `col1, col2 = st.columns(2)` erstellt zwei Spalten. Anschließend kann man mit `with col1:` und `with col2:` Inhalte in die jeweilige Spalte platzieren, um komplexere Layouts zu erstellen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "73. Was ist der Unterschied zwischen `EarlyStopping` und `ModelCheckpoint` Callbacks in Keras?",
      "options": [
        "Beide speichern nur das beste Modell – im Kontext.",
        "EarlyStopping stoppt Training; ModelCheckpoint speichert.",
        "ModelCheckpoint reduziert Lernrate – im Kontext.",
        "EarlyStopping speichert automatisch Gewichte."
      ],
      "answer": 1,
      "explanation": "Beide sind nützliche Callbacks. `EarlyStopping` verhindert Overfitting, indem es das Training abbricht, wenn z.B. der Validierungsfehler nicht mehr sinkt. `ModelCheckpoint` stellt sicher, dass man am Ende nicht ein schlechteres Modell hat, nur weil das Training zu lange lief, indem es die Version mit der besten Leistung auf dem Validierungsset speichert.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "74. Was ist der Zweck der `random_state`-Parameters in vielen Scikit-learn Funktionen?",
      "options": [
        "Steuert Zufälligkeit für Reproduzierbarkeit.",
        "Setzt Modellzustand zufällig bei jedem Lauf.",
        "Wählt zufällige Features für das Training.",
        "Wird ignoriert und hat keine Wirkung."
      ],
      "answer": 0,
      "explanation": "Viele Algorithmen haben eine stochastische (zufällige) Komponente (z.B. die Initialisierung der Gewichte). Durch das Setzen von `random_state` auf einen festen Integer-Wert wird sichergestellt, dass der Zufallszahlengenerator immer im gleichen Zustand startet, was zu identischen Ergebnissen bei wiederholten Durchläufen führt.",
      "weight": 1,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Reproduktion",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "75. Welcher der folgenden ist KEIN Hyperparameter eines Decision Tree?",
      "options": [
        "max_depth",
        "min_samples_split",
        "feature_importance",
        "criterion (gini/entropy)"
      ],
      "answer": 2,
      "explanation": "`max_depth`, `min_samples_split` und `criterion` sind alles Hyperparameter, die vor dem Training festgelegt werden, um die Struktur und das Verhalten des Baumes zu steuern. `feature_importance` ist ein Attribut des trainierten Modells, das nach dem Training berechnet wird.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "76. Was ist der Unterschied zwischen einem `Dense` und einem `Embedding` Layer in Keras?",
      "options": [
        "Beide sind identisch im Verhalten und Einsatz – im Kontext.",
        "Embedding mappt Indizes auf Vektoren; Dense verbindet Eingaben.",
        "Dense reduziert immer die Dimension der Eingaben.",
        "Embedding ist nur für Bilder und Videos – im Kontext."
      ],
      "answer": 1,
      "explanation": "Ein `Embedding`-Layer ist eine effiziente Methode, um hochdimensionale, dünn besetzte kategoriale Daten (wie Wörter in einem Vokabular, die als Integer repräsentiert werden) in dichte, niedrigdimensionale Vektoren umzuwandeln. Es ist im Wesentlichen eine lernbare Nachschlagetabelle, während ein `Dense`-Layer eine vollständige lineare Transformation durchführt.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "77. Was ist ein 'Volume' in Docker?",
      "options": [
        "Die Größe bzw. das Volumen des Docker‑Images – im Kontext.",
        "Persistenter Speicher außerhalb des Containers, der Daten erhält.",
        "Ein Netzwerk‑Interface des Containers – im Kontext.",
        "Ein Maß für die Rechenleistung eines Containers – im Kontext."
      ],
      "answer": 1,
      "explanation": "Container sind standardmäßig zustandslos (stateless). Wenn ein Container entfernt wird, gehen alle darin geschriebenen Daten verloren. Volumes ermöglichen es, Daten (z.B. Datenbankdateien, Logs) persistent auf dem Host-System zu speichern.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "78. Welches Diagramm eignet sich am besten, um die Beziehung zwischen zwei kontinuierlichen Variablen zu visualisieren?",
      "options": [
        "Balkendiagramm (Bar Chart)",
        "Kreisdiagramm (Pie Chart)",
        "Histogramm – im Kontext.",
        "Streudiagramm (Scatter Plot)"
      ],
      "answer": 3,
      "explanation": "Ein Streudiagramm plottet jeden Datenpunkt als Punkt in einem 2D-Koordinatensystem. Dies macht es ideal, um Korrelationen, Cluster und Ausreißer zwischen zwei kontinuierlichen Variablen zu erkennen.",
      "weight": 1,
      "topic": "Datenvisualisierung",
      "cognitive_level": "Reproduktion",
      "concept": "Datenvisualisierung",
      "mini_glossary": {
        "Diagrammtyp": "Passende visuelle Darstellung für eine Fragestellung (z. B. Balken, Linie, Box).",
        "Skalierung": "Art der Achsendarstellung (linear, logarithmisch) beeinflusst die Interpretation.",
        "Lesbarkeit": "Klares Layout, Beschriftungen und Farben erleichtern das Verständnis."
      }
    },
    {
      "question": "79. Was ist der Zweck der `predict_proba()`-Methode bei vielen Scikit-learn Klassifikationsmodellen?",
      "options": [
        "Gibt dasselbe wie predict() – im Kontext.",
        "Gibt Klassen-Wahrscheinlichkeiten zurück.",
        "Schätzt die Korrektheit des Modells.",
        "Nur für Regression verfügbar – im Kontext."
      ],
      "answer": 1,
      "explanation": "Während `predict()` die 'harte' Klassenzuweisung (z.B. Klasse 'Hund') zurückgibt, gibt `predict_proba()` die 'weiche' Zuweisung in Form von Wahrscheinlichkeiten für jede Klasse zurück. Dies ist nützlich, um die Konfidenz des Modells zu bewerten.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "80. Was ist der Hauptzweck der Phase 'K' (Knowledge Transfer) im QUA³CK-Modell?",
      "options": [
        "Neues Wissen über das Problem sammeln – im Kontext.",
        "Das Modell mit mehr Wissen trainieren – im Kontext.",
        "Ergebnisse in eine Anwendung überführen und dokumentieren.",
        "Das Wissen der Entwickler testen – im Kontext."
      ],
      "answer": 2,
      "explanation": "Die K-Phase schließt den Kreis, indem sie sicherstellt, dass die gewonnenen Erkenntnisse und das entwickelte Modell nicht nur in einem Notebook bleiben, sondern in eine Form gebracht werden, die für Endbenutzer oder andere Systeme von Nutzen ist.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "81. Was ist der Unterschied zwischen einem Python `list` und einem `set`?",
      "options": [
        "Listen und Sets sind identisch – im Kontext – in der Praxis.",
        "`set` ist geordnet, `list` ist ungeordnet – im Kontext.",
        "`set` speichert eindeutige Elemente; `list` erlaubt Duplikate.",
        "`list` ist unveränderlich, `set` ist veränderlich."
      ],
      "answer": 2,
      "explanation": "Sets sind für schnelle Mitgliedschaftstests optimiert und entfernen automatisch Duplikate. Listen behalten die Reihenfolge der Elemente bei und erlauben Duplikate.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Reproduktion",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "82. Welcher Pandas-Befehl wird verwendet, um fehlende Werte (NaN) in einem DataFrame `df` mit dem Wert 0 zu füllen?",
      "options": [
        "df.replace(NaN, 0) – im Kontext.",
        "df.remove_nan(0) – im Kontext.",
        "df.fillna(0) – im Kontext.",
        "df.set_nan(0) – im Kontext."
      ],
      "answer": 2,
      "explanation": "Die `fillna()`-Methode ist das Standardwerkzeug in Pandas, um fehlende Werte zu behandeln. Man kann sie mit einem konstanten Wert, dem Mittelwert, dem Median oder anderen Strategien verwenden.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "83. Was ist der Zweck von `st.session_state` in Streamlit?",
      "options": [
        "Es speichert den Zustand der Browser‑Session.",
        "Speichert Variablen über mehrere Reruns hinweg.",
        "Speichert die App‑Konfiguration – im Kontext.",
        "Ist eine veraltete Funktion – im Kontext."
      ],
      "answer": 1,
      "explanation": "Streamlit führt das Skript bei jeder Interaktion neu aus. Um Informationen (wie Zähler, Benutzereingaben, Chat-Verläufe) zwischen diesen Reruns zu speichern, wird das `st.session_state`-Objekt verwendet, das wie ein Dictionary funktioniert.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Analyse",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "84. Was ist der 'Curse of Dimensionality' (Fluch der Dimensionalität)?",
      "options": [
        "Das Phänomen, dass die Leistung von ML-Modellen mit zunehmender Anzahl von Features abnimmt.",
        "Die Tatsache, dass Daten in hochdimensionalen Räumen sehr spärlich werden und Distanzmaße ihre Aussagekraft verlieren.",
        "Die Notwendigkeit, bei hochdimensionalen Daten immer Deep Learning zu verwenden – im Kontext.",
        "Ein Fehler, der auftritt, wenn ein Datensatz mehr Spalten als Zeilen hat – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "In hochdimensionalen Räumen liegen die Datenpunkte tendenziell sehr weit voneinander entfernt. Dies macht Algorithmen, die auf Distanzmessungen basieren (wie KNN), weniger effektiv und erfordert exponentiell mehr Daten, um den Raum abzudecken.",
      "weight": 3,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Analyse",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "85. Was ist der Unterschied zwischen einem 'Validation Set' und einem 'Test Set'?",
      "options": [
        "Es gibt keinen Unterschied.",
        "Validation dient Modellwahl; Test ist für die finale, unabhängige Bewertung.",
        "Testdaten werden zum Training genutzt, Validation nicht.",
        "Validation ist nur für Regression relevant."
      ],
      "answer": 2,
      "explanation": "Das Validation Set wird wiederholt während der Entwicklung verwendet, um das Modell zu justieren (z.B. für Early Stopping). Das Test Set wird idealerweise nur ein einziges Mal am Ende verwendet, um eine unverfälschte Schätzung der Leistung des finalen Modells zu erhalten.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "86. Was ist ein 'Residual Connection' (oder Skip Connection), wie sie in ResNets verwendet wird?",
      "options": [
        "Verbindet Ausgabe direkt mit Eingabe.",
        "Addiert Block‑Eingabe zur Block‑Ausgabe (Skip).",
        "Reduziert die Zahl der Neuronen – im Kontext.",
        "Eine spezielle Dropout‑Variante – im Kontext."
      ],
      "answer": 1,
      "explanation": "Residual Connections ermöglichen es dem Gradienten, beim Backpropagation direkt durch einige Schichten 'hindurchzufließen'. Dies erleichtert das Training von sehr tiefen Netzwerken (z.B. ResNet-152), indem es dem Vanishing-Gradient-Problem entgegenwirkt.",
      "weight": 3,
      "topic": "Deep Learning",
      "cognitive_level": "Analyse",
      "concept": "Deep Learning",
      "mini_glossary": {
        "Neuronales Netz": "Modell aus Schichten verknüpfter Neuronen.",
        "Backpropagation": "Verfahren zur Berechnung der Gradienten beim Training.",
        "Aktivierungsfunktion": "Nichtlineare Funktion, die Neuronen-Ausgaben formt."
      }
    },
    {
      "question": "87. Was ist der Zweck eines 'API-Keys'?",
      "options": [
        "Ein öffentlicher Passwort‑Ersatz für alle Nutzer.",
        "Ein geheimer Schlüssel zur Authentifizierung bei APIs.",
        "Ein Tool zum API‑Monitoring – im Kontext.",
        "Eine Methode zum Daten‑Caching – im Kontext."
      ],
      "answer": 1,
      "explanation": "Viele APIs erfordern einen API-Key, um sicherzustellen, dass nur autorisierte Benutzer oder Anwendungen darauf zugreifen können. Er wird oft verwendet, um Nutzungsquoten (Rate Limiting) durchzusetzen.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "88. Welches Dateiformat wird oft für die Speicherung von großen, strukturierten Datensätzen empfohlen, da es spaltenorientiert und komprimiert ist?",
      "options": [
        "CSV (zeilenorientiert, wenig komprimiert)",
        "JSON (textbasiert, verschachtelt)",
        "Parquet (spaltenorientiert, komprimiert)",
        "TXT (unstrukturiert, ohne Schema)"
      ],
      "answer": 2,
      "explanation": "Parquet ist ein spaltenorientiertes Speicherformat, das für Big-Data-Workflows optimiert ist. Es ermöglicht eine sehr effiziente Kompression und Abfrage-Performance, da nur die benötigten Spalten gelesen werden müssen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "89. Was ist der Hauptzweck von Reinforcement Learning (RL)?",
      "options": [
        "Klassifikation mit Labels optimieren.",
        "Lernen durch Belohnung zur Maximierung langfristiger Rewards.",
        "Regressionsmodelle schneller trainieren.",
        "Daten in Cluster gruppieren."
      ],
      "answer": 2,
      "explanation": "Beim RL lernt ein Agent durch Versuch und Irrtum (Trial and Error). Er interagiert mit einer Umgebung und erhält Belohnungen oder Bestrafungen für seine Aktionen, mit dem Ziel, eine Strategie (Policy) zu lernen, die die langfristige Belohnung maximiert.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "90. Was ist der 'Exploration-Exploitation Trade-off' im Reinforcement Learning?",
      "options": [
        "Kompromiss zwischen Speicherbedarf und Geschwindigkeit.",
        "Abwägen zwischen Neues erkunden und Bekanntes nutzen.",
        "Kompromiss zwischen einfachem und komplexem Modell.",
        "Kompromiss zwischen Trainingszeit und Genauigkeit."
      ],
      "answer": 1,
      "explanation": "Ein RL-Agent muss entscheiden, ob er eine Aktion wählt, von der er bereits weiß, dass sie gut ist (Exploitation), oder ob er eine neue, unbekannte Aktion ausprobiert, die potenziell noch besser sein könnte (Exploration). Dies ist eine zentrale Herausforderung im RL.",
      "weight": 3,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Analyse",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "91. Was ist der Unterschied zwischen einem `list` und einem `numpy.array`?",
      "options": [
        "Es gibt keinen Unterschied in der Praxis – im Kontext.",
        "NumPy‑Arrays sind numerisch/vektorisiert, Listen flexibler.",
        "Listen können nur Strings enthalten – im Kontext.",
        "NumPy‑Arrays gehören zur Standardbibliothek – im Kontext."
      ],
      "answer": 1,
      "explanation": "NumPy ist die Grundlage für wissenschaftliches Rechnen in Python. Seine Array-Struktur ist in C implementiert, was Operationen auf großen Datenmengen deutlich schneller macht als mit reinen Python-Listen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "92. Welcher Plotly Express Befehl wird verwendet, um ein interaktives Streudiagramm zu erstellen?",
      "options": [
        "px.bar()",
        "px.line()",
        "px.scatter()",
        "px.histogram()"
      ],
      "answer": 2,
      "explanation": "`plotly.express.scatter` (üblicherweise als `px.scatter` importiert) ist die High-Level-Funktion zur Erstellung von interaktiven Streudiagrammen.",
      "weight": 1,
      "topic": "Datenvisualisierung",
      "cognitive_level": "Reproduktion",
      "concept": "Datenvisualisierung",
      "mini_glossary": {
        "Diagrammtyp": "Passende visuelle Darstellung für eine Fragestellung (z. B. Balken, Linie, Box).",
        "Skalierung": "Art der Achsendarstellung (linear, logarithmisch) beeinflusst die Interpretation.",
        "Lesbarkeit": "Klares Layout, Beschriftungen und Farben erleichtern das Verständnis."
      }
    },
    {
      "question": "93. Was ist der Zweck der `__init__`-Methode in einer Python-Klasse?",
      "options": [
        "Sie initialisiert die Klasse selbst – im Kontext – in der Praxis.",
        "Konstruktor, der beim Erzeugen einer Instanz den Anfangszustand setzt.",
        "Sie zerstört das Objekt, wenn es nicht mehr benötigt wird.",
        "Sie ist eine normale Methode ohne Sonderrolle – im Kontext."
      ],
      "answer": 1,
      "explanation": "Die `__init__`-Methode wird automatisch aufgerufen, wenn eine neue Instanz einer Klasse erzeugt wird. Sie wird verwendet, um die Attribute des Objekts zu initialisieren.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "94. Was ist ein 'Webhook' im Kontext von CI/CD und MLOps?",
      "options": [
        "Ein Hardware‑Haken, um Server im Rack zu befestigen.",
        "Ereignis‑Benachrichtigung per HTTP‑Request an eine definierte URL.",
        "Ein Sicherheitsprotokoll für APIs – im Kontext – in der Praxis.",
        "Ein Werkzeug zur automatischen Code‑Formatierung – im Kontext."
      ],
      "answer": 1,
      "explanation": "Webhooks sind der 'Klebstoff' für Automatisierung. Zum Beispiel kann GitHub einen Webhook an einen CI/CD-Server senden, wenn neuer Code gepusht wird, was dann automatisch einen neuen Build auslöst.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Analyse",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "95. Was ist der Unterschied zwischen einem 'Dense' und einem 'Sparse' Vektor?",
      "options": [
        "Dense enthält nur Nullen, Sparse nur Einsen.",
        "Dense speichert alle Werte, Sparse nur Nicht‑Nullen.",
        "Dense ist immer kürzer als Sparse – im Kontext.",
        "Sparse ist nicht für ML nutzbar – im Kontext."
      ],
      "answer": 1,
      "explanation": "Sparse Vektoren sind sehr effizient für Daten mit vielen Nullen, wie z.B. bei der One-Hot-Kodierung von großen Vokabularen im NLP. Anstatt eines riesigen Vektors mit meist Nullen speichert man nur die Positionen der Nicht-Null-Werte.",
      "weight": 2,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Anwendung",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "96. Welches der folgenden ist ein Beispiel für eine Metrik zur Evaluation von Regressionsmodellen?",
      "options": [
        "Accuracy (Anteil korrekter Klassifikationen)",
        "F1‑Score (Mittel aus Precision und Recall)",
        "Root Mean Squared Error (RMSE) – im Kontext.",
        "AUC‑ROC (Fläche unter der ROC‑Kurve)"
      ],
      "answer": 2,
      "explanation": "RMSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten. Es ist eine der gebräuchlichsten Metriken für Regressionsprobleme. Accuracy, F1-Score und AUC-ROC sind hingegen Klassifikationsmetriken.",
      "weight": 1,
      "topic": "Machine Learning Grundlagen",
      "cognitive_level": "Reproduktion",
      "concept": "Machine Learning Grundlagen",
      "mini_glossary": {
        "Supervised Learning": "Lernen mit gelabelten Beispielen (Eingabe und Ziel bekannt).",
        "Unsupervised Learning": "Lernen ohne Labels, z. B. Clustering oder Mustererkennung.",
        "Feature": "Eingangsvariable, die dem Modell zur Vorhersage dient."
      }
    },
    {
      "question": "97. Was ist der Zweck der `self`-Variable in Python-Klassenmethoden?",
      "options": [
        "Sie ist eine Klassenvariable für alle Instanzen.",
        "Sie referenziert die aktuelle Instanz in Methoden.",
        "Sie ist ein optionales Keyword‑Argument.",
        "Sie speichert den Rückgabewert einer Methode."
      ],
      "answer": 2,
      "explanation": "`self` ist das erste Argument jeder Instanzmethode und wird von Python automatisch übergeben. Es ermöglicht der Methode, auf die Daten (Attribute) und anderen Methoden zuzugreifen, die zu diesem spezifischen Objekt gehören.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "98. Was ist der Hauptvorteil der Verwendung von `plotly.express` gegenüber `plotly.graph_objects`?",
      "options": [
        "`plotly.express` ist nur für 3D‑Plots – im Kontext – in der Praxis.",
        "`plotly.express` bietet eine High‑Level‑API für schnelle Standard‑Plots.",
        "`plotly.express` erzeugt keine interaktiven Grafiken – im Kontext.",
        "`plotly.graph_objects` unterstützt keine Anpassungen – im Kontext."
      ],
      "answer": 1,
      "explanation": "`plotly.express` (px) ist eine Wrapper-Bibliothek um `plotly.graph_objects`. Sie vereinfacht die Erstellung von gängigen Diagrammtypen erheblich. Für sehr komplexe Visualisierungen kann man immer noch auf die detailliertere `graph_objects`-Syntax zurückgreifen.",
      "weight": 2,
      "topic": "Datenvisualisierung",
      "cognitive_level": "Anwendung",
      "concept": "Datenvisualisierung",
      "mini_glossary": {
        "Diagrammtyp": "Passende visuelle Darstellung für eine Fragestellung (z. B. Balken, Linie, Box).",
        "Skalierung": "Art der Achsendarstellung (linear, logarithmisch) beeinflusst die Interpretation.",
        "Lesbarkeit": "Klares Layout, Beschriftungen und Farben erleichtern das Verständnis."
      }
    },
    {
      "question": "99. Was ist ein 'Container Registry' wie Docker Hub oder GitHub Container Registry?",
      "options": [
        "Ort zum Ausführen von Containern – im Kontext.",
        "Repository zum Speichern/Verteilen von Images.",
        "Tool zum Container-Monitoring – im Kontext.",
        "Datenbank für Container-Metadaten – im Kontext."
      ],
      "answer": 1,
      "explanation": "Eine Container Registry ist wie ein 'App Store' für Docker-Images. Man kann seine eigenen Images dorthin 'pushen' (hochladen) und Images von anderen 'pullen' (herunterladen), was die Zusammenarbeit und das Deployment vereinfacht.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "100. Was ist der Zweck der `main`-Block-Konstruktion `if __name__ == '__main__':` in einem Python-Skript?",
      "options": [
        "Sie definiert die Hauptfunktion des Skripts – im Kontext.",
        "Code im Block läuft nur beim direkten Start, nicht beim Import als Modul.",
        "Sie ist notwendig, um globale Variablen zu deklarieren – im Kontext.",
        "Sie markiert nur den Anfang des Python‑Codes – im Kontext."
      ],
      "answer": 1,
      "explanation": "Diese Konstruktion ist eine Best Practice in Python. Sie ermöglicht es, ein Skript sowohl als eigenständiges Programm als auch als wiederverwendbares Modul zu schreiben, ohne dass beim Import sofort Code ausgeführt wird.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge",
      "mini_glossary": {
        "Workflow": "Abfolge typischer Arbeitsschritte von der Datenbeschaffung bis zum Ergebnis.",
        "Reproduzierbarkeit": "Ergebnisse sind unter gleichen Bedingungen wiederholbar.",
        "Container": "Gekapselte Umgebung, die Software samt Abhängigkeiten konsistent bereitstellt."
      }
    },
    {
      "question": "101. Welcher der folgenden Begriffe beschreibt am besten das Ziel von `Data Leakage` in einem Machine-Learning-Kontext?",
      "options": [
        "Die unbeabsichtigte Verwendung von Daten aus der Testmenge im Trainingsprozess.",
        "Ein Prozess zur absichtlichen Entfernung von sensiblen Daten aus dem Trainingsdatensatz.",
        "Ein Algorithmus, der fehlende Werte in einem Datensatz automatisch imputiert.",
        "Die Anwendung von Feature Engineering, um die Modellleistung zu steigern."
      ],
      "answer": 0,
      "explanation": "Data Leakage (Datenlecks) tritt auf, wenn Informationen, die nicht aus der Trainingsmenge stammen, zur Erstellung des Modells verwendet werden. Dies führt zu überoptimistischen Leistungsschätzungen und einer schlechten Generalisierung auf neue, ungesehene Daten. Eine unbeabsichtigte Verwendung von Testdaten während des Trainings ist ein klassisches Beispiel für Data Leakage.",
      "weight": 3,
      "topic": "Datenqualität & Leakage",
      "konzept": "Data Leakage verstehen",
      "cognitive_level": "Reproduktion",
      "extended_explanation": {
        "titel": "Details zu Data Leakage",
        "schritte": [
          "Data Leakage ist ein schwerwiegender Fehler im ML-Workflow, der das Modell scheinbar besser macht, als es in der Realität ist.",
          "Es kann auftreten, wenn Informationen aus der Zukunft (z.B. eine Spalte, die erst nach dem Ereignis bekannt ist) im Trainingsset verwendet werden oder wenn die Validierungsstrategie (z.B. `Cross-Validation`) nicht korrekt angewendet wird.",
          "Um Data Leakage zu vermeiden, ist eine strikte Trennung von Trainings- und Testdaten vor jeglicher Vorverarbeitung essenziell. Alle Schritte des Workflows (wie `Feature Scaling` oder `Feature Engineering`) müssen separat auf den Trainings- und Testdaten durchgeführt werden."
        ]
      },
      "mini_glossary": {
        "Data Leakage": "Unbeabsichtigter Informationsfluss vom Testset in den Trainingsprozess (z. B. gemeinsames Fitten von Scaler oder Encoding); verfälscht Metriken und führt zu optimistischeren Ergebnissen.",
        "Feature Engineering": "Der Prozess der Erstellung neuer Features aus bestehenden Daten, um die Vorhersagekraft eines Machine-Learning-Modells zu verbessern.",
        "Trainingsmenge": "Der Teil eines Datensatzes, der zum Trainieren eines Machine-Learning-Modells verwendet wird."
      },
      "concept": "Datenqualität & Leakage"
    },
    {
      "question": "102. Was ist der Hauptunterschied zwischen **Overfitting** und **Underfitting**?",
      "options": [
        "Unterbringung des Codes in Docker – im Kontext.",
        "Verhindern, dass Infos aus Testdaten ins Training gelangen.",
        "Daten durch Zufall splitten – im Kontext – in der Praxis.",
        "Modelle schneller trainieren – im Kontext – in der Praxis."
      ],
      "answer": 2,
      "explanation": "Overfitting beschreibt eine Situation, in der ein Modell die Trainingsdaten 'auswendig lernt', anstatt die zugrundeliegenden Muster zu generalisieren. Dies führt zu einer ausgezeichneten Leistung auf dem Trainingsdatensatz, aber einer schlechten Leistung auf neuen, ungesehenen Daten. Underfitting hingegen beschreibt ein Modell, das zu einfach ist, um die zugrundeliegenden Muster überhaupt zu erfassen, was zu einer schlechten Leistung auf Trainings- und Testdaten führt.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "konzept": "Modelle mit Overfitting vs. Underfitting vergleichen",
      "cognitive_level": "Analyse",
      "extended_explanation": {
        "titel": "Verständnis von Overfitting und Underfitting",
        "schritte": [
          "Overfitting kann durch ein zu komplexes Modell (z.B. zu tiefe `Decision Trees`) oder durch eine zu lange Trainingsdauer verursacht werden.",
          "Underfitting entsteht oft, wenn das Modell nicht genügend Komplexität besitzt, um die Beziehungen in den Daten abzubilden (z.B. ein lineares Modell für nicht-lineare Daten).",
          "Das Ziel ist es, ein Modell zu finden, das die richtige Balance zwischen beiden Extremen herstellt. Methoden wie `Regularisierung`, `Cross-Validation` und die Auswahl der richtigen Modellkomplexität sind entscheidend."
        ]
      },
      "mini_glossary": {
        "Overfitting": "Modell lernt Rauschen oder Zufallsschwankungen des Trainingssets statt der zugrunde liegenden Struktur – erkennbar an hoher Trainings-, aber niedriger Testleistung.",
        "Underfitting": "Das Problem, bei dem ein Machine-Learning-Modell nicht komplex genug ist, um die grundlegenden Muster in den Trainingsdaten zu erfassen, was zu einer schlechten Leistung führt.",
        "Regularisierung": "Techniken, die die Komplexität eines Modells reduzieren, um Overfitting zu vermeiden, indem sie große Koeffizienten bestrafen."
      },
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "103. Welche der folgenden Techniken ist **KEIN** Bestandteil der `Explorativen Datenanalyse` (EDA)?",
      "options": [
        "Cross‑Validation",
        "Overfitting",
        "Data Leakage",
        "Regularisierung"
      ],
      "answer": 2,
      "explanation": "Die Explorative Datenanalyse (`EDA`) dient dazu, die Daten zu verstehen und ihre Eigenschaften, Muster und Anomalien zu identifizieren. Methoden wie die Visualisierung mit `Boxplots`, die Identifikation von Ausreißern oder die Korrelationsanalyse sind dabei zentrale Werkzeuge. Das `Hyperparameter-Tuning` ist jedoch ein Schritt, der erst nach der `EDA` und während des Modelltrainings erfolgt, um die Leistung des Modells zu optimieren.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "cognitive_level": "Reproduktion",
      "concept": "Datenvorbereitung & EDA",
      "mini_glossary": {
        "Datenbereinigung": "Entfernen oder Korrigieren fehlerhafter und fehlender Werte.",
        "Feature Engineering": "Erstellen oder Transformieren von Merkmalen zur besseren Modellleistung.",
        "EDA (Explorative Datenanalyse)": "Erste Analyse zur Struktur, Verteilung und Auffälligkeiten der Daten."
      }
    },
    {
      "question": "104. Was ist ein typischer Nachteil von `Decision Trees` ohne weitere Maßnahmen?",
      "options": [
        "Label‑Daten sind immer schlecht – im Kontext.",
        "Labels sind notwendig für überwachte Verfahren.",
        "Labels braucht man nur bei Regression.",
        "Labels ersetzen Features – im Kontext."
      ],
      "answer": 1,
      "explanation": "`Decision Trees` können sehr komplexe und verzweigte Strukturen annehmen, die sich perfekt an die Trainingsdaten anpassen. Dies führt zu einer hohen Sensitivität gegenüber Rauschen und Ausreißern im Trainingsdatensatz, was wiederum eine starke Neigung zu Overfitting zur Folge hat. Techniken wie das Beschneiden (`Pruning`) des Baumes oder der Einsatz von Ensemble-Methoden wie `Random Forests` werden genutzt, um dieses Problem zu beheben.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Overfitting bei `Decision Trees`",
        "schritte": [
          "Ein einzelner, unbeschränkter `Decision Tree` kann für jeden Datenpunkt im Trainingsset eine eigene, spezifische Regel erlernen.",
          "Diese starke Anpassung führt dazu, dass das Modell zwar auf den Trainingsdaten eine perfekte Genauigkeit erreichen kann, aber auf neuen Daten kaum generalisiert.",
          "Im Gegensatz dazu verwenden Ensemble-Methoden wie `Random Forests` die Aggregation vieler unkorrelierter Bäume, um die Varianz zu reduzieren und das Overfitting-Problem zu mildern."
        ]
      },
      "mini_glossary": {
        "Decision Tree": "Ein Klassifikations- oder Regressionsmodell, das durch eine baumartige Struktur Entscheidungen trifft, indem es Daten basierend auf den Werten von Features aufteilt.",
        "Overfitting": "Modell lernt Rauschen oder Zufallsschwankungen des Trainingssets statt der zugrunde liegenden Struktur – erkennbar an hoher Trainings-, aber niedriger Testleistung.",
        "Random Forest": "Ensemble vieler Entscheidungsbäume auf Bootstrap-Samples und Feature-Subsets; reduziert Varianz und Overfitting gegenüber Einzelbäumen."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "105. Welche der folgenden Maßnahmen erhöht die Reproduzierbarkeit von Data-Science-Projekten am effektivsten?",
      "options": [
        "Overfitting ist gutes Generalisieren – im Kontext.",
        "Overfitting: gut auf Training, schlecht auf neue Daten.",
        "Overfitting verbessert immer die Testleistung.",
        "Overfitting betrifft nur lineare Modelle – im Kontext."
      ],
      "answer": 1,
      "explanation": "Die Reproduzierbarkeit eines Data-Science-Projekts hängt davon ab, ob Code, Daten und die gesamte Laufzeitumgebung für Dritte oder auch für den Ersteller selbst zu einem späteren Zeitpunkt wiederhergestellt werden können. `Docker-Container` kapseln alle Abhängigkeiten, Bibliotheken und das Betriebssystem, wodurch die Umgebung exakt reproduzierbar wird. `Jupyter Notebooks` können hilfreich sein, aber sie garantieren nicht die Reproduzierbarkeit der Umgebung, in der sie ausgeführt werden.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "extended_explanation": {
        "titel": "Reproduzierbarkeit in der Praxis",
        "schritte": [
          "Ein `Docker-Container` enthält alle notwendigen Komponenten, um eine Anwendung zu starten: den Code, die `Python`-Version, die Bibliotheken und die Systemkonfiguration.",
          "Durch die Bereitstellung eines `Dockerfiles` kann jeder die exakt gleiche Umgebung auf seinem eigenen System erstellen und das Projekt so reproduzieren, wie es ursprünglich entwickelt wurde.",
          "Dies ist besonders im `MLOps`-Kontext entscheidend, um die Konsistenz zwischen Entwicklungs-, Test- und Produktionsumgebungen zu gewährleisten."
        ]
      },
      "mini_glossary": {
        "Docker-Container": "Eine leichtgewichtige, portable und ausführbare Software-Einheit, die alles enthält, was zur Ausführung einer Anwendung benötigt wird, einschließlich Code, Laufzeitumgebung und Bibliotheken.",
        "Reproduzierbarkeit": "Die Fähigkeit, die Ergebnisse eines Experiments oder Projekts zu einem späteren Zeitpunkt und in einer anderen Umgebung exakt zu replizieren.",
        "MLOps": "Gesamtheit der Prozesse und Tools, die Entwicklung, Deployment und Betrieb von ML-Modellen verzahnen und überwachen (z. B. Experiment-Tracking, CI/CD, Monitoring)."
      },
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge"
    },
    {
      "question": "106. Welche Aussage zu `Feature Engineering` ist korrekt?",
      "options": [
        "Ersetzt das Training des Modells vollständig.",
        "Erstellt/transformiert Merkmale zur besseren Leistung.",
        "Wählt automatisch den Algorithmus aus – im Kontext.",
        "Entfernt immer alle Features – im Kontext."
      ],
      "answer": 1,
      "explanation": "Feature Engineering ist der Prozess der Nutzung von Domänenwissen zur Erstellung von Features, die die Modellgüte verbessern. Indem man rohe Daten in ein Format umwandelt, das für das Modell nützlicher ist, können komplexe Beziehungen für das Modell leichter erfassbar gemacht werden. Dies kann in praktisch jedem Bereich des maschinellen Lernens angewendet werden und ist oft ein entscheidender Faktor für den Erfolg eines Projekts.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Der Wert von Feature Engineering",
        "schritte": [
          "Ein einfaches Beispiel für `Feature Engineering` ist die Umwandlung eines Datums in separate Spalten für `Jahr`, `Monat` und `Wochentag`.",
          "Ein weiteres Beispiel ist die Kombination von zwei numerischen Features zu einem neuen, wie die Berechnung des BMI aus Größe und Gewicht.",
          "Durch solche Transformationen kann das Modell Muster erkennen, die in den rohen Daten nicht offensichtlich waren, und somit seine Vorhersagekraft signifikant steigern."
        ]
      },
      "mini_glossary": {
        "Feature Engineering": "Der Prozess der Erstellung neuer, aussagekräftiger Merkmale (Features) aus bestehenden Daten, um die Leistung eines Machine-Learning-Modells zu verbessern.",
        "Deployment": "Bereitstellung eines trainierten Modells in einer produktiven Umgebung (API, Streamlit, Batch) inklusive Infrastruktur, Skalierung und Monitoring.",
        "Modellgüte": "Ein Maß dafür, wie gut ein Machine-Learning-Modell Vorhersagen treffen kann, oft bewertet durch Metriken wie Genauigkeit, Präzision oder F1-Score."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "107. Welche Methode ist am besten geeignet, um die Robustheit eines Modells gegen **Ausreißer** zu testen?",
      "options": [
        "Weil es keine Labels gibt.",
        "Weil es Labels gibt.",
        "Weil es nur um Clustering geht.",
        "Weil es nur um Regression geht."
      ],
      "answer": 0,
      "explanation": "Ausreißer können die Leistung eines Modells stark beeinflussen. `Cross-Validation` ist eine robuste Methode, um die Stabilität eines Modells zu bewerten, indem es wiederholt auf verschiedenen Teilmengen der Daten trainiert und getestet wird. Wenn das Modell in den verschiedenen Folds, die potenziell Ausreißer enthalten, signifikant unterschiedliche Ergebnisse liefert, deutet dies auf eine mangelnde Robustheit hin.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Robustheit von Modellen",
        "schritte": [
          "Robuste Modelle sind weniger empfindlich gegenüber kleinen Änderungen in den Daten, wie sie durch Rauschen oder Ausreißer verursacht werden können.",
          "Neben `Cross-Validation` können auch andere Methoden wie die Verwendung von robusten Skalierungsverfahren (z.B. `RobustScaler` anstelle von `StandardScaler`) oder der Einsatz von Algorithmen, die von Natur aus weniger anfällig für Ausreißer sind (z.B. `Random Forests` im Vergleich zu linearen Modellen), die Robustheit erhöhen.",
          "Die Robustheit ist ein wichtiges Kriterium, da reale Daten oft unvollkommen und verrauscht sind."
        ]
      },
      "mini_glossary": {
        "Cross-Validation": "Validierungstechnik, bei der Trainingsdaten in Folds geteilt werden; jedes Fold dient einmal als Validierung, wodurch die Varianz der Metrik sinkt.",
        "Ausreißer": "Beobachtungen, die deutlich außerhalb des typischen Wertebereichs liegen und Modelle stark beeinflussen können; benötigen spezielle Behandlung.",
        "Robustheit": "Die Fähigkeit eines Modells, stabil und zuverlässig zu funktionieren, auch wenn die Eingabedaten Rauschen, Fehlern oder Ausreißern unterliegen."
      },
      "cognitive_level": "Analyse",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "108. Was ist ein typisches Symptom für **`Data Leakage`**?",
      "options": [
        "Schlechte Trainingsleistung, hohe Testleistung.",
        "Sehr niedrige Trainingsleistung, aber gute Generalisierung.",
        "Stabile, realistische Testleistung.",
        "Sehr hohe Val/Test-Scores, die im Einsatz einbrechen."
      ],
      "answer": 3,
      "explanation": "Ein verräterisches Zeichen für `Data Leakage` ist, wenn ein Modell auf den Trainings- und Testdaten eine nahezu perfekte Leistung zeigt, aber in der realen Anwendung oder auf komplett neuen Daten versagt. Die Testgenauigkeit ist künstlich aufgebläht, weil das Modell unbeabsichtigt Informationen aus dem Testset während des Trainings 'gesehen' hat. Dies führt zu einer falschen Einschätzung der Modellgüte.",
      "weight": 3,
      "topic": "Datenqualität & Leakage",
      "extended_explanation": {
        "titel": "Symptome und Ursachen von `Data Leakage`",
        "schritte": [
          "Data Leakage kann subtil sein und ist oft schwer zu erkennen. Es resultiert typischerweise in einem übermäßig optimistischen Testergebnis.",
          "Ein Beispiel ist das Vorverarbeiten des gesamten Datensatzes (Trainings- und Testset) vor dem Aufteilen. Wenn beispielsweise die `Standardisierung` oder `Imputation` von fehlenden Werten auf dem gesamten Datensatz erfolgt, 'sieht' das Modell Informationen über die Verteilung der Testdaten, was zu einer unfairen Leistung führt.",
          "Das beste Gegenmittel ist die strikte Einhaltung der korrekten Reihenfolge: Daten aufteilen, dann vorverarbeiten."
        ]
      },
      "mini_glossary": {
        "Data Leakage": "Unbeabsichtigter Informationsfluss vom Testset in den Trainingsprozess (z. B. gemeinsames Fitten von Scaler oder Encoding); verfälscht Metriken und führt zu optimistischeren Ergebnissen.",
        "Generalisierung": "Die Fähigkeit eines Machine-Learning-Modells, gut auf neuen, ungesehenen Daten zu funktionieren, die nicht Teil der Trainingsmenge waren.",
        "Testgenauigkeit": "Ein Maß für die Leistung eines Modells auf den Testdaten, das angibt, wie gut das Modell auf neuen Daten generalisiert."
      },
      "cognitive_level": "Analyse",
      "concept": "Datenqualität & Leakage"
    },
    {
      "question": "109. Welche Bibliothek ist für das Tracking von Experimenten und Modellversionen in einem `Data Analytics` Projekt vorgesehen?",
      "options": [
        "NumPy (Numerik) – im Kontext.",
        "MLflow (Experiment‑Tracking)",
        "Matplotlib (Visualisierung)",
        "Pandas (Datenanalyse)"
      ],
      "answer": 1,
      "explanation": "`MLflow` ist eine Open-Source-Plattform, die speziell für das Management des Machine-Learning-Lebenszyklus entwickelt wurde. Ihre Hauptfunktionen sind das Experiment-Tracking, die Reproduzierbarkeit von Runs und das Modell-Deployment. Im Gegensatz dazu ist `TensorBoard` eher für die Visualisierung von `TensorFlow`-Experimenten und `Matplotlib` für die allgemeine Datenvisualisierung gedacht, während `Flask` ein Web-Framework ist.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "extended_explanation": {
        "titel": "Die Rolle von `MLflow` im MLOps-Kontext",
        "schritte": [
          "`MLflow` besteht aus vier Hauptkomponenten: `Tracking`, `Projects`, `Models` und `Model Registry`.",
          "Das `Tracking` ermöglicht es, Parameter, Metriken und Artefakte (wie trainierte Modelle) für jeden Lauf zu protokollieren.",
          "Die `Model Registry` bietet eine zentrale Plattform zur Verwaltung des Lebenszyklus von Modellen, von der Staging-Umgebung bis zur Produktion. All dies erleichtert die Zusammenarbeit in Teams und die Reproduzierbarkeit von Ergebnissen."
        ]
      },
      "mini_glossary": {
        "MLflow": "Open-Source-Plattform zum Nachverfolgen von ML-Experimenten, Verwalten von Modellen und Bereitstellen in Registry oder Serving.",
        "Experiment-Tracking": "Systematische Erfassung von Trainingsläufen (Parameter, Metriken, Artefakte), um Modellversionen reproduzierbar zu vergleichen und auditierbar zu halten.",
        "MLOps": "Gesamtheit der Prozesse und Tools, die Entwicklung, Deployment und Betrieb von ML-Modellen verzahnen und überwachen (z. B. Experiment-Tracking, CI/CD, Monitoring)."
      },
      "cognitive_level": "Anwendung",
      "concept": "Grundlagen & Werkzeuge"
    },
    {
      "question": "110. Was ist ein Vorteil von `Random Forests` gegenüber einzelnen `Decision Trees`?",
      "options": [
        "Sie bestehen nur aus einem einzelnen Baum.",
        "Sie mitteln viele Bäume und generalisieren meist besser.",
        "Sie brauchen keine Hyperparameter.",
        "Sie sind immer leichter interpretierbar als ein einzelner Baum."
      ],
      "answer": 1,
      "explanation": "`Random Forests` sind eine Ensemble-Methode, die aus vielen einzelnen `Decision Trees` besteht. Jeder Baum wird auf einer zufälligen Teilmenge der Daten und der Features trainiert. Durch die Aggregation der Vorhersagen all dieser Bäume (z.B. durch Mehrheitsentscheidung) wird die Varianz der Vorhersagen reduziert und die Neigung zu Overfitting, die bei einzelnen Bäumen stark ausgeprägt ist, deutlich verringert.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Ensemble-Lernen: `Random Forests`",
        "schritte": [
          "Ein `Random Forest` ist ein gutes Beispiel für das Konzept des `Bagging` (Bootstrap Aggregating).",
          "Dabei wird jeder `Decision Tree` auf einem zufällig gezogenen Datensatz mit Zurücklegen trainiert (Bootstrap).",
          "Die zufällige Auswahl der Features bei jedem Split (Random Subspace) sorgt für Unkorreliertheit der einzelnen Bäume, was die Varianzreduktion weiter fördert."
        ]
      },
      "mini_glossary": {
        "Random Forest": "Ensemble vieler Entscheidungsbäume auf Bootstrap-Samples und Feature-Subsets; reduziert Varianz und Overfitting gegenüber Einzelbäumen.",
        "Ensemble-Lernen": "Eine Methode im Machine Learning, bei der mehrere Modelle (`Ensemble`) kombiniert werden, um eine bessere Leistung zu erzielen als ein einzelnes Modell.",
        "Varianz": "Maß dafür, wie stark Modellvorhersagen bei Variation des Trainingssets schwanken; hohe Varianz deutet auf Überanpassung hin."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "111. Welche Aussage zu `K-Nearest Neighbors` (`KNN`) ist korrekt?",
      "options": [
        "Gute Generalisierung, schlecht auf Training – im Kontext.",
        "Gute Trainingsleistung, kann auf neue Daten schwächer sein.",
        "Nur zu viele Features verursachen Probleme – im Kontext.",
        "Nur zu wenige Daten erklären alles – im Kontext."
      ],
      "answer": 1,
      "explanation": "`K-Nearest Neighbors` ist ein sogenanntes instanzbasiertes oder speicherbasiertes Lernverfahren. Im Gegensatz zu Modellen wie `Decision Trees` oder linearen Modellen, die einen expliziten Algorithmus erlernen und Gewichte oder Parameter speichern, merkt sich `KNN` einfach den gesamten Trainingsdatensatz. Bei einer Vorhersage sucht es dann nur die $k$ ähnlichsten Datenpunkte im Trainingsset, um die Entscheidung zu treffen.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Details zum `KNN`-Algorithmus",
        "schritte": [
          "Im `KNN`-Algorithmus gibt es keine separate 'Lernphase'. Das eigentliche 'Lernen' besteht darin, die Trainingsdaten zu speichern.",
          "Die Berechnung der Ähnlichkeit erfolgt in der Regel über Distanzmaße wie die euklidische Distanz ($d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$) oder die Manhattandistanz.",
          "Aus diesem Grund kann `KNN` sehr rechenintensiv werden, wenn der Datensatz sehr groß ist, da bei jeder Vorhersage die Distanz zu allen anderen Datenpunkten berechnet werden muss."
        ]
      },
      "mini_glossary": {
        "K-Nearest Neighbors (`KNN`)": "Nicht-parametrischer Klassifikator: weist einem Punkt das Mehrheitslabel der k nächsten Trainingspunkte im Feature-Raum zu (distanzbasiert).",
        "Instanzbasiertes Lernen": "Ein Lernparadigma, bei dem das Modell alle Trainingsinstanzen speichert und Vorhersagen für neue Instanzen auf der Grundlage ihrer Ähnlichkeit mit den gespeicherten Instanzen trifft.",
        "Euklidische Distanz": "Ein gebräuchliches Distanzmaß, das den kürzesten Abstand zwischen zwei Punkten in einem euklidischen Raum berechnet: $d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "112. Was ist ein typischer Anwendungsfall für die `Principal Component Analysis` (`PCA`)?",
      "options": [
        "Er nutzt Distanz zu Nachbarn für Vorhersagen.",
        "Er nutzt neuronale Netze für Vorhersagen.",
        "Er lernt keine Parameter.",
        "Er ist nur für Klassifikation."
      ],
      "answer": 1,
      "explanation": "`PCA` ist ein statistisches Verfahren, das zur Dimensionsreduktion verwendet wird. Es transformiert die Daten in eine neue Basis, wobei die ersten Komponenten die größte Varianz in den Daten enthalten. Dadurch können hochdimensionale Daten in eine niedrigere Dimension (z.B. zwei oder drei Dimensionen) projiziert werden, was die Visualisierung erleichtert und die Rechenzeit für nachfolgende Modelle reduziert.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Details zu `PCA`",
        "schritte": [
          "`PCA` identifiziert die Hauptkomponenten der Daten, die die Richtungen der größten Varianz darstellen.",
          "Es wird häufig als Vorverarbeitungsschritt eingesetzt, um die Anzahl der Features zu verringern, was die Modellierung beschleunigen und das Risiko von `Overfitting` reduzieren kann (`Curse of Dimensionality`).",
          "Die Technik ist linear und kann die ursprüngliche Varianz der Daten nicht-linear abbilden, was ein potenzieller Nachteil sein kann."
        ]
      },
      "mini_glossary": {
        "Principal Component Analysis (`PCA`)": "Eine statistische Methode zur Dimensionsreduktion, die hochdimensionale Daten in eine niedrigere Dimension projiziert, während sie die maximale Varianz behält.",
        "Dimensionsreduktion": "Der Prozess der Verringerung der Anzahl von Features in einem Datensatz, um die Modellierung zu vereinfachen und die Leistung zu verbessern.",
        "Overfitting": "Modell lernt Rauschen oder Zufallsschwankungen des Trainingssets statt der zugrunde liegenden Struktur – erkennbar an hoher Trainings-, aber niedriger Testleistung."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "113. Welche Rolle spielt die Standardisierung von Features bei der Anwendung von `KNN`?",
      "options": [
        "Sie ist nur bei Regression nötig – im Kontext.",
        "Sie verhindert, dass große Skalen die Distanz dominieren.",
        "Sie ersetzt die Wahl von k – im Kontext – in der Praxis.",
        "Sie macht KNN unabhängig von der Distanzmetrik."
      ],
      "answer": 1,
      "explanation": "`KNN` misst die Ähnlichkeit zwischen Datenpunkten anhand ihrer Distanz im Merkmalsraum. Wenn die Features unterschiedliche Skalen aufweisen (z.B. eine Spalte von 0 bis 1000 und eine andere von 0 bis 1), dominiert das Feature mit dem größeren Wertebereich die Distanzberechnung. Die Standardisierung (`Min-Max Scaling` oder `Z-Score-Normalisierung`) stellt sicher, dass alle Features einen ähnlichen Wertebereich haben und somit gleichmäßig zur Distanzmessung beitragen.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Die Notwendigkeit von Feature-Skalierung",
        "schritte": [
          "Stellen Sie sich vor, Sie haben ein Feature 'Einkommen' (Werte in Tausenden) und ein Feature 'Alter' (Werte zwischen 0 und 100).",
          "Ohne Skalierung würde die euklidische Distanz fast ausschließlich vom 'Einkommen' bestimmt werden, da die Unterschiede dort viel größer sind.",
          "Durch die Skalierung werden beide Features auf eine ähnliche Skala gebracht, sodass ihre relativen Abstände gleichmäßig zur Berechnung der Nachbarschaft beitragen."
        ]
      },
      "mini_glossary": {
        "Standardisierung": "Ein Datenvorverarbeitungsschritt, bei dem die Werte von Features so transformiert werden, dass sie eine einheitliche Skala haben und eine Normalverteilung mit Mittelwert 0 und Standardabweichung 1 aufweisen.",
        "KNN": "Kurzform für K-Nearest Neighbors; siehe Eintrag „K-Nearest Neighbors (`KNN`)“. ",
        "Distanzmaß": "Eine mathematische Funktion, die den Abstand zwischen zwei Punkten oder Vektoren in einem Raum misst."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "114. Welche Aussage zu **`Cross-Validation`** ist korrekt?",
      "options": [
        "Einmalige Aufteilung in Train und Testdaten.",
        "Mehrfaches Trainieren und Validieren auf verschiedenen Folds.",
        "Nutzung des Testsets fürs Tuning der Hyperparameter.",
        "Ersetzt das Testset vollständig durch Training."
      ],
      "answer": 1,
      "explanation": "Die `Cross-Validation` ist eine robuste Methode zur Modellbewertung, die die Varianz der Leistungsschätzung reduziert. Im Gegensatz zur einfachen `Holdout-Validierung`, bei der das Modell nur einmal auf einer einzelnen Testmenge bewertet wird, verwendet `Cross-Validation` mehrere Folds (Teilmengen). Das Modell wird mehrfach auf unterschiedlichen Trainings- und Testsets bewertet, und die Ergebnisse werden gemittelt. Dies führt zu einer stabileren und zuverlässigeren Schätzung der Modellgüte.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Vorteile der `K-Fold Cross-Validation`",
        "schritte": [
          "Bei der `K-Fold Cross-Validation` wird der Datensatz in `$k$` gleich große Teile aufgeteilt.",
          "In `$k$` aufeinanderfolgenden Iterationen wird ein Teil als Testset verwendet, während die restlichen `$k-1$` Teile zum Training dienen.",
          "Dieser Prozess stellt sicher, dass jeder Datenpunkt sowohl im Trainings- als auch im Testset verwendet wird, was zu einer gründlicheren und zuverlässigeren Leistungsbewertung führt als bei einer einfachen Aufteilung."
        ]
      },
      "mini_glossary": {
        "Cross-Validation": "Validierungstechnik, bei der Trainingsdaten in Folds geteilt werden; jedes Fold dient einmal als Validierung, wodurch die Varianz der Metrik sinkt.",
        "Varianz": "Maß dafür, wie stark Modellvorhersagen bei Variation des Trainingssets schwanken; hohe Varianz deutet auf Überanpassung hin.",
        "Holdout-Validierung": "Eine einfache Methode zur Modellbewertung, bei der der Datensatz einmalig in eine Trainings- und eine Testmenge aufgeteilt wird."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "115. Was ist ein typischer Nachteil von linearen Modellen bei komplexen Datensätzen?",
      "options": [
        "Bias‑Variance ist irrelevant.",
        "Zu einfaches Modell = hoher Bias; zu komplex = hohe Varianz.",
        "Bias steigt immer mit Datenmenge.",
        "Varianz ist nur bei linearen Modellen."
      ],
      "answer": 2,
      "explanation": "Lineare Modelle, wie die lineare Regression oder die logistische Regression, gehen von einem linearen Zusammenhang zwischen den Features und der Zielvariablen aus. Wenn die zugrunde liegenden Daten jedoch eine nicht-lineare Beziehung aufweisen, sind diese Modelle nicht in der Lage, die komplexen Muster zu erfassen, was zu einem `Underfitting` führt. `Polynomiale Regression` oder `Decision Trees` sind Beispiele für Modelle, die besser für nicht-lineare Daten geeignet sind.",
      "weight": 1,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Reproduktion",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "116. Welche Aussage zu `Data Augmentation` im Kontext von `Computer Vision` ist korrekt?",
      "options": [
        "Durch lineare Kernels – im Kontext.",
        "Durch nichtlinearen Kernel (z. B. RBF).",
        "Durch kleinere Datensätze – im Kontext.",
        "Durch Entfernen von Features – im Kontext."
      ],
      "answer": 1,
      "explanation": "`Data Augmentation` ist eine Technik, die die Größe und Qualität eines Trainingsdatensatzes durch die Erstellung modifizierter Versionen von Bildern künstlich erhöht. Durch Transformationen wie Rotation, Spiegelung, Skalierung oder Farbverschiebung werden neue, aber realistische Trainingsbeispiele generiert. Dies vergrößert das Trainingsset, macht das Modell robuster und hilft effektiv, `Overfitting` zu reduzieren, da das Modell weniger die spezifischen Trainingsbilder auswendig lernt.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Anwendung von `Data Augmentation`",
        "schritte": [
          "Ein typisches Beispiel im `Computer Vision` ist die Erstellung von gespiegelten Versionen von Bildern. Ein Modell, das gelernt hat, eine Katze auf einem Bild zu erkennen, sollte auch in der Lage sein, die Katze zu erkennen, wenn das Bild gespiegelt wird.",
          "Andere gängige Transformationen umfassen das Hinzufügen von zufälligem Rauschen, das Zuschneiden (`Cropping`) von Bildausschnitten oder das Ändern des Kontrasts.",
          "Da `Data Augmentation` die Varianz der Trainingsdaten erhöht, muss das Modell robuster und generalisierungsfähiger werden."
        ]
      },
      "mini_glossary": {
        "Data Augmentation": "Gezielte Erzeugung synthetischer Varianten (z. B. Drehung, Spiegelung) zur Erweiterung des Trainingssets und Verringerung von Overfitting.",
        "Computer Vision": "Ein Fachgebiet der Künstlichen Intelligenz, das sich mit der Entwicklung von Systemen befasst, die in der Lage sind, Informationen aus visuellen Daten wie Bildern und Videos zu interpretieren und zu verstehen.",
        "Overfitting": "Modell lernt Rauschen oder Zufallsschwankungen des Trainingssets statt der zugrunde liegenden Struktur – erkennbar an hoher Trainings-, aber niedriger Testleistung."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "117. Was ist ein typischer Vorteil von `Transfer Learning` im Vergleich zu `Training from Scratch`?",
      "options": [
        "Schlechtere Generalisierung",
        "Nur für tabellarische Daten geeignet",
        "Geringerer Datenbedarf und schnellere Konvergenz",
        "Keine Vorteile, da Modelle immer neu trainiert werden müssen"
      ],
      "answer": 2,
      "explanation": "`Transfer Learning` nutzt ein bereits auf einem großen Datensatz vortrainiertes Modell (z.B. auf `ImageNet`) und passt es für eine neue, verwandte Aufgabe an. Der Hauptvorteil ist, dass man nicht von Grund auf neu anfangen muss (`Training from Scratch`). Dies spart erhebliche Rechenressourcen und Zeit und ermöglicht es, gute Ergebnisse mit deutlich weniger Daten zu erzielen, da die gelernten Features aus der ursprünglichen Aufgabe wiederverwendet werden.",
      "weight": 3,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Das Konzept hinter `Transfer Learning`",
        "schritte": [
          "Stellen Sie sich vor, ein Modell lernt, Millionen von Bildern zu klassifizieren. Dabei lernt es in den frühen Schichten, grundlegende Merkmale wie Kanten, Formen und Texturen zu erkennen.",
          "Beim `Transfer Learning` friert man diese frühen Schichten ein und trainiert nur die letzten Schichten des Modells neu, um sich auf die spezifische, neue Aufgabe anzupassen.",
          "Dies ist besonders nützlich bei Aufgaben, bei denen nur wenige Trainingsdaten verfügbar sind, da die bereits gelernten allgemeinen Features wiederverwendet werden können."
        ]
      },
      "mini_glossary": {
        "Transfer Learning": "Übernahme vortrainierter Modelle/Feature-Extraktoren auf neue, verwandte Aufgaben, um Trainingsaufwand und Datenbedarf zu reduzieren.",
        "Training from Scratch": "Die Methode, ein Machine-Learning-Modell von Grund auf neu zu trainieren, ohne auf vorab gelernten Parametern aufzubauen.",
        "ImageNet": "Ein großer visueller Datensatz, der von Forschern zur Entwicklung von Algorithmen für maschinelles Sehen verwendet wird."
      },
      "cognitive_level": "Analyse",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "118. Welche Aussage zu **Hyperparametern** ist korrekt?",
      "options": [
        "`Hyperparameter` werden während des Trainings automatisch gelernt.",
        "`Hyperparameter` müssen vor dem Training festgelegt und ggf. optimiert werden.",
        "`Hyperparameter` sind nur bei linearen Modellen relevant – im Kontext.",
        "`Hyperparameter` sind identisch mit Modellgewichten – im Kontext."
      ],
      "answer": 1,
      "explanation": "Hyperparameter sind externe Konfigurationen, die nicht aus den Daten gelernt werden. Sie müssen vor dem Trainingsprozess festgelegt werden, um zu steuern, wie das Modell lernt. Beispiele sind die Lernrate in neuronalen Netzen, die Anzahl der Bäume in einem `Random Forest` oder der Wert von $k$ in einem `KNN`-Modell. Im Gegensatz dazu sind Modellgewichte oder Parameter Werte, die der Algorithmus während des Trainings lernt.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung",
      "cognitive_level": "Reproduktion",
      "concept": "Modellbewertung & Validierung",
      "mini_glossary": {
        "Metrik": "Maß zur Bewertung der Modellgüte (z. B. Accuracy, F1, MSE).",
        "Train/Test-Split": "Aufteilung der Daten in Trainings- und Testmenge.",
        "Cross-Validation": "Mehrfache Trainings/Test-Aufteilung zur stabileren Bewertung."
      }
    },
    {
      "question": "119. Was ist ein typischer Fehler bei der Modellbewertung?",
      "options": [
        "Nutzung von `Cross-Validation`",
        "Nutzung der Testdaten für `Hyperparameter-Tuning`",
        "Nutzung von Trainingsdaten für das Training",
        "Nutzung von Validierungsdaten für die Modellbewertung"
      ],
      "answer": 1,
      "explanation": "Der zentrale Fehler bei der Modellbewertung ist die unsaubere Trennung von Daten. Die Testdaten sind für die finale, unvoreingenommene Bewertung der Modellleistung reserviert. Wenn diese Daten für das `Hyperparameter-Tuning` verwendet werden, wird die Leistung auf dem Testset künstlich optimiert, was zu einer Überbewertung der Generalisierungsfähigkeit des Modells führt. Die Testdaten müssen vollständig ungesehen bleiben, bis die Modellentwicklung abgeschlossen ist.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Die Bedeutung der Datenaufteilung",
        "schritte": [
          "Der korrekte Workflow sieht vor, den Datensatz in drei Teile zu teilen: Trainings-, Validierungs- und Testset.",
          "Das **Trainingsset** dient dem Lernen der Modellparameter.",
          "Das **Validierungsset** wird für das `Hyperparameter-Tuning` verwendet, um das beste Modell zu finden.",
          "Das **Testset** wird nur einmal am Ende verwendet, um eine ehrliche und unabhängige Schätzung der Modellgüte zu erhalten."
        ]
      },
      "mini_glossary": {
        "Hyperparameter-Tuning": "Systematische Suche nach optimalen Steuerparametern eines Modells (z. B. GridSearchCV, RandomizedSearchCV); erfolgt auf Validierungsdaten, ohne das Testset zu berühren.",
        "Testdaten": "Ein Teil des Datensatzes, der ausschließlich zur unvoreingenommenen, finalen Bewertung der Modellleistung nach dem Training und der `Hyperparameter-Optimierung` verwendet wird.",
        "Generalisierungsfähigkeit": "Die Fähigkeit eines Modells, eine gute Leistung auf neuen, ungesehenen Daten zu erbringen."
      },
      "cognitive_level": "Analyse",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "120. Welche Aussage zu `Explainable AI` (`XAI`) ist korrekt?",
      "options": [
        "Nur für Deep Learning relevant – im Kontext.",
        "Macht Entscheidungen von Modellen nachvollziehbar.",
        "Verschlechtert immer die Modellgüte – im Kontext.",
        "Nur für Bilddaten relevant – im Kontext."
      ],
      "answer": 1,
      "explanation": "`Explainable AI` (`XAI`) ist ein Sammelbegriff für Methoden, die es ermöglichen, die Ergebnisse und Entscheidungen von KI-Systemen nachvollziehbar, transparent und interpretierbar zu machen. Dies ist besonders wichtig bei komplexen Modellen, die oft als 'Black Box' gelten, um das Vertrauen der Nutzer zu gewinnen, Fehler zu identifizieren und die Modelle in kritischen Anwendungen (z.B. in der Medizin) zu validieren.",
      "weight": 2,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Methoden und Nutzen von `XAI`",
        "schritte": [
          "Es gibt verschiedene `XAI`-Methoden, darunter globale Interpretierbarkeit (`LIME`, `SHAP`), die die Bedeutung von Features für die Gesamtvorhersage zeigen.",
          "Lokale Interpretierbarkeit hingegen erklärt, warum eine spezifische Vorhersage für einen einzelnen Datenpunkt getroffen wurde.",
          "Der Nutzen von `XAI` reicht von der Fehlersuche über die Einhaltung regulatorischer Anforderungen bis zur Steigerung des Nutzervertrauens, da Entscheidungen nicht mehr mysteriös, sondern verständlich sind."
        ]
      },
      "mini_glossary": {
        "Explainable AI (`XAI`)": "Methoden, die Entscheidungsprozesse komplexer Modelle verständlich machen (z. B. SHAP, LIME, Feature-Attributionen).",
        "Black Box": "Modell, dessen Entscheidungslogik für Anwender kaum nachvollziehbar ist – etwa tiefe Netze ohne zusätzliche Interpretierbarkeit.",
        "Interpretierbarkeit": "Die Eigenschaft eines Machine-Learning-Modells, bei dem die Ursache-Wirkungs-Beziehung zwischen den Eingaben und den Vorhersagen verstanden werden kann."
      },
      "cognitive_level": "Anwendung",
      "concept": "Spezialthemen & Methoden"
    },
    {
      "question": "121. Was ist der Unterschied zwischen der `Spearman`- und der `Pearson`-Korrelation?",
      "options": [
        "Die `Spearman`-Korrelation misst nur lineare, die `Pearson`-Korrelation misst nicht-lineare Zusammenhänge.",
        "Die `Spearman`-Korrelation ist für kategoriale, die `Pearson`-Korrelation für numerische Daten – im Kontext.",
        "Die `Spearman`-Korrelation basiert auf Rängen der Daten, während die `Pearson`-Korrelation auf der tatsächlichen Linearität basiert.",
        "Die `Spearman`-Korrelation ist empfindlicher gegenüber Ausreißern als die `Pearson`-Korrelation – im Kontext."
      ],
      "answer": 2,
      "explanation": "Die `Pearson`-Korrelation misst die Stärke und Richtung eines **linearen** Zusammenhangs zwischen zwei Variablen. Sie ist empfindlich gegenüber Ausreißern. Im Gegensatz dazu misst die `Spearman`-Korrelation die Stärke eines **monotonen** Zusammenhangs, indem sie die Rangfolge der Datenpunkte betrachtet. Da sie auf Rängen und nicht auf den Originalwerten basiert, ist sie robuster gegenüber Ausreißern und kann auch nicht-lineare, aber monotone Beziehungen erfassen.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Korrelationskoeffizienten im Detail",
        "schritte": [
          "Die `Pearson`-Korrelation wird als $r$ bezeichnet und liegt im Bereich von $[-1, 1]$. Ein Wert von 1 bedeutet eine perfekte positive lineare Korrelation, -1 eine perfekte negative, und 0 keine lineare Korrelation.",
          "Die `Spearman`-Korrelation, oft als $\\rho$ (rho) bezeichnet, misst die Rangkorrelation. Wenn $x$ und $y$ monotone Funktionen voneinander sind, ist der `Spearman`-Koeffizient 1.",
          "Für eine visuelle Veranschaulichung der Unterschiede ist es hilfreich, Streudiagramme zu betrachten: Die `Spearman`-Korrelation würde eine perfekte monotone Kurve mit einem Koeffizienten von 1 abbilden, auch wenn die `Pearson`-Korrelation, aufgrund der Nicht-Linearität, niedriger wäre."
        ]
      },
      "mini_glossary": {
        "Pearson-Korrelation": "Ein statistisches Maß für die Stärke eines linearen Zusammenhangs zwischen zwei Variablen.",
        "Spearman-Korrelation": "Ein statistisches Maß für die Stärke eines monotonen Zusammenhangs zwischen zwei Variablen, basierend auf den Rängen der Daten.",
        "Monotoner Zusammenhang": "Eine Beziehung zwischen zwei Variablen, bei der eine Variable kontinuierlich zunimmt, wenn die andere Variable zunimmt, oder kontinuierlich abnimmt, wenn die andere abnimmt, ohne dass die Beziehung notwendigerweise linear sein muss."
      },
      "cognitive_level": "Analyse",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "122. Wie können Sie in `Python` mit der Bibliothek `Scikit-learn` die Performance eines Klassifikationsmodells umfassend evaluieren?",
      "options": [
        "Durch die alleinige Nutzung der `model.score()`-Methode – im Kontext – in der Praxis – im beschriebenen Fall.",
        "Indem Sie nur die Trainingsgenauigkeit (`accuracy`) messen und als finale Metrik verwenden.",
        "Durch die Erstellung einer `Confusion Matrix` und die Berechnung von Metriken wie `Precision`, `Recall` und `F1-Score`.",
        "Indem Sie die Laufzeit des Modells messen und diese als Performance-Metrik ansehen – im Kontext."
      ],
      "answer": 2,
      "explanation": "Die `Confusion Matrix` ist ein grundlegendes Werkzeug zur Bewertung von Klassifikationsmodellen. Aus ihr lassen sich detailliertere Metriken als die einfache Genauigkeit (`accuracy`) ableiten. `Precision` misst die Exaktheit (Wie viele der als positiv vorhergesagten sind wirklich positiv?), `Recall` misst die Vollständigkeit (Wie viele der tatsächlichen Positiven wurden erkannt?) und der `F1-Score` ist das harmonische Mittel dieser beiden, was ein ausgewogenes Bild der Modellgüte liefert.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Detaillierte Evaluation von Klassifikatoren",
        "schritte": [
          "Die `Confusion Matrix` zeigt die vier möglichen Ausgänge einer Klassifikation: `True Positives` (TP), `True Negatives` (TN), `False Positives` (FP) und `False Negatives` (FN).",
          "Daraus können die Metriken berechnet werden: `Precision` = $\\frac{TP}{TP+FP}$, `Recall` = $\\frac{TP}{TP+FN}$ und `F1-Score` = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$.",
          "Diese Metriken sind besonders wichtig bei unausgewogenen Datensätzen, bei denen die `accuracy` allein irreführend sein kann. Ein Modell, das immer die Mehrheitsklasse vorhersagt, könnte eine hohe `accuracy` haben, aber einen `Recall` von null für die Minderheitsklasse."
        ]
      },
      "mini_glossary": {
        "Confusion Matrix": "Eine Tabelle, die die Leistung eines Klassifikationsmodells visuell darstellt und die Anzahl der `True Positives`, `True Negatives`, `False Positives` und `False Negatives` zeigt.",
        "Precision": "Ein Maß für die Genauigkeit der positiven Vorhersagen des Modells.",
        "Recall": "Ein Maß für die Fähigkeit des Modells, alle positiven Instanzen zu identifizieren."
      },
      "cognitive_level": "Analyse",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "123. Diskutieren Sie die Bedeutung von `Data Leakage` im Machine Learning und wie man es vermeiden kann.",
      "options": [
        "Es ist ein Prozess zur Datenbereinigung und wird durch `Cross-Validation` behoben – im Kontext – in der Praxis – im beschriebenen Fall.",
        "Es ist ein Problem, das entsteht, wenn die Trainings- und Testdaten nicht strikt getrennt werden, und kann durch einen korrekten Workflow vermieden werden.",
        "Es ist eine Technik zur Datenaugmentation, die die Modellleistung verbessert, und wird mit `MLflow` umgesetzt – im Kontext.",
        "Es ist die unbeabsichtigte Verwendung von historischen Daten, die durch `Feature Engineering` behoben wird – im Kontext."
      ],
      "answer": 1,
      "explanation": "`Data Leakage` ist ein ernsthaftes Problem, das zu einer unrealistisch hohen Modellgüte führt, da das Modell Informationen aus der Testmenge oder der Zukunft 'sieht'. Die Lösung ist ein disziplinierter Workflow, bei dem die Daten vor jeglicher Vorverarbeitung in Trainings- und Testset aufgeteilt werden. Erst danach werden Skalierungs-, Imputations- oder Feature-Engineering-Schritte separat auf den beiden Datensätzen angewendet.",
      "weight": 3,
      "topic": "Datenqualität & Leakage",
      "extended_explanation": {
        "titel": "Wie man `Data Leakage` in der Praxis verhindert",
        "schritte": [
          "Das wichtigste Prinzip ist '`Split first, then pre-process`'. Teilen Sie die Daten in Trainings- und Testset auf, bevor Sie irgendwelche Transformationen anwenden.",
          "Wenden Sie Fit-Operationen (z.B. die Berechnung von Mittelwert und Standardabweichung für die `Standardisierung`) nur auf den Trainingsdaten an. Verwenden Sie die gelernten Parameter dann, um die Testdaten zu transformieren.",
          "Besonders vorsichtig muss man bei `TimeSeries`-Daten sein. Hier muss der Split zeitbasiert erfolgen, um zu verhindern, dass das Modell Informationen aus der Zukunft für seine Vorhersagen nutzt."
        ]
      },
      "mini_glossary": {
        "Data Leakage": "Unbeabsichtigter Informationsfluss vom Testset in den Trainingsprozess (z. B. gemeinsames Fitten von Scaler oder Encoding); verfälscht Metriken und führt zu optimistischeren Ergebnissen.",
        "Data Pre-processing": "Alle Schritte, die zur Vorbereitung der Daten für ein Machine-Learning-Modell erforderlich sind, wie z.B. das Bereinigen, Skalieren oder Transformieren von Features.",
        "TimeSeries-Daten": "Daten, die über die Zeit in einer bestimmten Reihenfolge gesammelt werden."
      },
      "cognitive_level": "Analyse",
      "concept": "Datenqualität & Leakage"
    },
    {
      "question": "124. Skizzieren Sie die Schritte zur Entwicklung einer `Streamlit`-App für ein ML-Modell, das auf tabellarischen Daten basiert.",
      "options": [
        "Installieren Sie `Streamlit` und `Scikit-learn`, laden Sie das trainierte Modell und erstellen Sie die Benutzeroberfläche (`UI`) mit `st.write()` und `st.text_input()`.",
        "Installieren Sie `MLflow` und `Flask`, laden Sie die Daten in eine Datenbank und erstellen Sie die `UI` mit `HTML` und `CSS` – im Kontext.",
        "Trainieren Sie das Modell, speichern Sie es als `.txt`-Datei und erstellen Sie eine `UI` mit `Matplotlib` und `Seaborn` – im Kontext.",
        "Laden Sie die Daten in ein `Jupyter Notebook`, trainieren Sie das Modell und teilen Sie das Notebook, um die App bereitzustellen."
      ],
      "answer": 0,
      "explanation": "Die Entwicklung einer `Streamlit`-App zur Bereitstellung eines ML-Modells ist ein relativ einfacher Prozess. Man beginnt mit der Installation der benötigten Bibliotheken. Anschließend lädt man das trainierte Modell in die App. Schließlich erstellt man die Benutzeroberfläche (`UI`) mit `Streamlit`s einfachen Widgets wie `st.text_input` für die Eingabe von Feature-Werten und `st.write` zur Anzeige der Vorhersagen. `Streamlit` übernimmt die gesamte Komplexität des Web-Deployments, sodass man sich auf den Code konzentrieren kann.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "extended_explanation": {
        "titel": "Von der Idee zur App mit `Streamlit`",
        "schritte": [
          "Ein typischer `Streamlit`-Workflow beginnt mit dem Python-Skript. Man lädt die Daten und das trainierte Modell (`z.B. mit joblib.load()`) am Anfang des Skripts.",
          "Anschließend erstellt man die interaktive Benutzeroberfläche, indem man die Daten des Benutzers sammelt. Widgets wie `st.slider()`, `st.selectbox()` und `st.text_input()` sind dafür ideal.",
          "Zuletzt wird die Vorhersagefunktion aufgerufen, und das Ergebnis wird mit `st.write()` oder anderen `Streamlit`-Funktionen angezeigt. Das Skript kann dann einfach über die Kommandozeile (`streamlit run your_app.py`) ausgeführt werden."
        ]
      },
      "mini_glossary": {
        "Streamlit": "Ein Open-Source-Framework in `Python`, das die Erstellung und Bereitstellung von interaktiven Web-Apps für Data Science und Machine Learning extrem vereinfacht.",
        "User Interface (UI)": "Die Benutzeroberfläche, über die ein Nutzer mit einer Anwendung interagiert.",
        "Deployment": "Bereitstellung eines trainierten Modells in einer produktiven Umgebung (API, Streamlit, Batch) inklusive Infrastruktur, Skalierung und Monitoring."
      },
      "cognitive_level": "Analyse",
      "concept": "Grundlagen & Werkzeuge"
    },
    {
      "question": "125. Vergleichen Sie die Vor- und Nachteile von `Random Forests` und `K-Nearest Neighbors` für Klassifikationsaufgaben.",
      "options": [
        "`Random Forests` sind langsamer, aber robuster als `KNN`.",
        "`Random Forests` sind sehr interpretierbar, während `KNN` eine 'Black Box' ist.",
        "`Random Forests` sind rechenintensiv im Training, `KNN` hingegen im `Deployment`.",
        "`Random Forests` können nicht-lineare Zusammenhänge abbilden, `KNN` ist auf lineare Probleme beschränkt."
      ],
      "answer": 2,
      "explanation": "`Random Forests` sind rechenintensiv während des Trainings, da sie viele `Decision Trees` erstellen und optimieren müssen. Die Vorhersagezeit ist jedoch relativ schnell. `KNN` hat keine Trainingsphase im klassischen Sinne, da es einfach alle Daten speichert. Der Rechenaufwand entsteht bei der Vorhersage (`Deployment`), da die Distanz zu allen Trainingsdatenpunkten berechnet werden muss, um die $k$ Nachbarn zu finden.",
      "weight": 3,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Detaillierter Vergleich der Modelle",
        "schritte": [
          "`Random Forests` sind Ensemble-Methoden, die gut mit `Overfitting` umgehen, sehr gut auf großen Datensätzen performen und die Wichtigkeit der Features angeben können.",
          "`KNN` ist sehr einfach zu verstehen und zu implementieren, leidet aber unter Skalierbarkeitsproblemen bei großen Datensätzen und ist sehr anfällig für die Wahl des richtigen Distanzmaßes und der Skalierung der Features.",
          "Der Hauptunterschied liegt also in der Rechenintensität während des Trainings (für `Random Forests`) versus während der Vorhersage (für `KNN`), sowie in der Anfälligkeit für `Overfitting` und der Skalierbarkeit."
        ]
      },
      "mini_glossary": {
        "Random Forest": "Ensemble vieler Entscheidungsbäume auf Bootstrap-Samples und Feature-Subsets; reduziert Varianz und Overfitting gegenüber Einzelbäumen.",
        "K-Nearest Neighbors (`KNN`)": "Nicht-parametrischer Klassifikator: weist einem Punkt das Mehrheitslabel der k nächsten Trainingspunkte im Feature-Raum zu (distanzbasiert).",
        "Overfitting": "Modell lernt Rauschen oder Zufallsschwankungen des Trainingssets statt der zugrunde liegenden Struktur – erkennbar an hoher Trainings-, aber niedriger Testleistung."
      },
      "cognitive_level": "Analyse",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "126. Erklären Sie, wie Sie mit `MLflow` ein Experiment-Tracking für verschiedene Modellvarianten aufsetzen würden und welche Vorteile dies bietet.",
      "options": [
        "Nur zum Deployment in Docker nutzen – im Kontext.",
        "Für automatische Hyperparameter-Suche verwenden.",
        "Run starten, Parameter/Metriken loggen, Modell speichern.",
        "Für Installation von Python-Paketen nutzen."
      ],
      "answer": 2,
      "explanation": "`MLflow` ist für das Experiment-Tracking konzipiert. Man startet einen Run (`with mlflow.start_run():`), loggt die verwendeten Hyperparameter, die gemessenen Metriken (z.B. Genauigkeit, `F1-Score`) und die erzeugten Artefakte, wie das trainierte Modell. Der Hauptvorteil ist, dass man die Ergebnisse verschiedener Modellvarianten (z.B. mit unterschiedlichen Hyperparametern) systematisch vergleichen, reproduzieren und die beste Version leicht identifizieren kann. Dies ist im `MLOps`-Kontext entscheidend für die Transparenz und Verwaltung des Modelllebenszyklus.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "extended_explanation": {
        "titel": "Praktisches `MLflow`-Tracking",
        "schritte": [
          "Ein `MLflow` Run ist ein einzelner Trainingslauf. Jeder Run wird mit einer eindeutigen ID versehen und speichert alle geloggten Informationen.",
          "Das `MLflow` UI ermöglicht es, alle Runs zu vergleichen, indem man Metriken und Parameter in einer Tabelle oder in Graphen visualisiert.",
          "Dies hilft, den Fortschritt zu verfolgen, Hypothesen zu testen und die Reproduzierbarkeit zu gewährleisten. Man kann einfach auf einen vorherigen Run zurückgreifen, wenn man eine bestimmte Konfiguration reproduzieren muss."
        ]
      },
      "mini_glossary": {
        "MLflow": "Open-Source-Plattform zum Nachverfolgen von ML-Experimenten, Verwalten von Modellen und Bereitstellen in Registry oder Serving.",
        "Experiment-Tracking": "Systematische Erfassung von Trainingsläufen (Parameter, Metriken, Artefakte), um Modellversionen reproduzierbar zu vergleichen und auditierbar zu halten.",
        "Modell-Lebenszyklus": "Die aufeinanderfolgenden Phasen im Machine Learning, von der Datenbeschaffung über die Modellentwicklung und das Training bis zur Bereitstellung, Überwachung und Wartung in der Produktion."
      },
      "cognitive_level": "Analyse",
      "concept": "Grundlagen & Werkzeuge"
    },
    {
      "question": "127. Beschreiben Sie, wie Sie mit `Transfer Learning` ein `Computer-Vision`-Projekt umsetzen würden.",
      "options": [
        "Netz von Grund auf trainieren, ohne Vorwissen oder Pretraining.",
        "Alle Schichten löschen und komplett neu bauen – im Kontext – in der Praxis.",
        "Vortrainiertes Modell nutzen, Layer einfrieren, neuen Klassifikator trainieren.",
        "Vortrainiertes Modell ohne Anpassung direkt einsetzen – im Kontext."
      ],
      "answer": 2,
      "explanation": "Der typische Workflow für `Transfer Learning` im `Computer Vision` sieht vor, dass man ein Modell wie `ResNet` oder `VGG16`, das auf einem großen Datensatz wie `ImageNet` vortrainiert wurde, verwendet. Da die frühen Schichten bereits gelernt haben, allgemeine Merkmale (Kanten, Texturen) zu erkennen, friert man diese ein. Man ersetzt dann die letzten Schichten (die für die Klassifikation spezifisch sind) durch neue, die für die eigene Aufgabe geeignet sind. Schließlich trainiert man nur diese neuen Schichten auf dem kleineren, spezifischen Datensatz, was den Prozess beschleunigt und Overfitting reduziert.",
      "weight": 3,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Vorteile und Herausforderungen von `Transfer Learning`",
        "schritte": [
          "Vorteile: Signifikante Reduzierung des Rechenaufwands, der Trainingszeit und des Datenbedarfs. Ermöglicht die Nutzung hochmoderner Architekturen, ohne sie von Grund auf neu trainieren zu müssen.",
          "Nachteile: Nicht immer ist das vortrainierte Modell für die neue Aufgabe geeignet. Manchmal müssen auch die eingefrorenen Schichten 'fein-getuned' werden, um eine optimale Leistung zu erzielen.",
          "Das Konzept funktioniert am besten, wenn die neue Aufgabe ähnlich der ursprünglichen Aufgabe ist, für die das Modell trainiert wurde."
        ]
      },
      "mini_glossary": {
        "Transfer Learning": "Übernahme vortrainierter Modelle/Feature-Extraktoren auf neue, verwandte Aufgaben, um Trainingsaufwand und Datenbedarf zu reduzieren.",
        "Vortrainiertes Modell": "Ein Machine-Learning-Modell, das bereits auf einem großen Datensatz trainiert wurde, bevor es für eine neue Aufgabe weiterverwendet wird.",
        "ResNet": "Eine bekannte neuronale Netzwerkarchitektur, die oft als Basis für `Transfer Learning` in der Bildverarbeitung verwendet wird."
      },
      "cognitive_level": "Analyse",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "128. Diskutieren Sie die Rolle von `Cross-Validation` und `Hyperparameter-Tuning` für die Modellgüte. Wie gehen Sie dabei praktisch vor?",
      "options": [
        "CV nur Datenaufteilung, Tuning manuell – im Kontext.",
        "Beide unabhängig ohne Zusammenhang – im Kontext.",
        "CV liefert robuste Schätzung, Tuning optimiert Parameter.",
        "Tuning auf Testdaten, CV auf Training – im Kontext."
      ],
      "answer": 2,
      "explanation": "`Cross-Validation` und `Hyperparameter-Tuning` sind eng miteinander verbunden. Bei der `Grid Search` oder `Randomized Search` wird `Cross-Validation` in jeder Iteration verwendet, um die Leistung eines Modells mit einem bestimmten Satz von Hyperparametern zu bewerten. Durch die Mittelung der Ergebnisse über alle Folds erhält man eine zuverlässige Schätzung der Güte. Das `Tuning`-Verfahren sucht dann systematisch nach dem besten Parametersatz, der die beste durchschnittliche Leistung über alle Folds liefert. Die Kombination beider Verfahren stellt sicher, dass man ein optimales Modell findet, dessen Leistungsschätzung nicht durch eine zufällige Aufteilung der Daten verzerrt ist.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Praktische Kombination beider Techniken",
        "schritte": [
          "Ein typischer Ansatz ist die Verwendung von `GridSearchCV` oder `RandomizedSearchCV` aus `Scikit-learn`.",
          "Diese Funktionen nehmen als Eingabe ein Modell, einen Parameter-Grid und die `Cross-Validation`-Strategie entgegen.",
          "Für jede mögliche Kombination von Hyperparametern wird das Modell auf den `Cross-Validation`-Folds trainiert und bewertet, und die durchschnittliche Leistung wird aufgezeichnet. Am Ende wählt das Verfahren den Parametersatz, der die höchste mittlere Leistung erbracht hat."
        ]
      },
      "mini_glossary": {
        "Cross-Validation": "Validierungstechnik, bei der Trainingsdaten in Folds geteilt werden; jedes Fold dient einmal als Validierung, wodurch die Varianz der Metrik sinkt.",
        "Hyperparameter-Tuning": "Systematische Suche nach optimalen Steuerparametern eines Modells (z. B. GridSearchCV, RandomizedSearchCV); erfolgt auf Validierungsdaten, ohne das Testset zu berühren.",
        "Grid Search": "Eine Methode zur `Hyperparameter-Optimierung`, die systematisch alle möglichen Kombinationen von Hyperparameter-Werten in einem vordefinierten Raster ausprobiert."
      },
      "cognitive_level": "Analyse",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "129. Analysieren Sie, wie Sie mit `Exploratory Data Analysis` (`EDA`) typische Fehlerquellen und Ausreißer in einem Datensatz identifizieren und adressieren würden.",
      "options": [
        "Nur Durchschnittswerte vergleichen, keine Visualisierung.",
        "Nur ML-Modell auf Rohdaten anwenden – im Kontext.",
        "Mit Boxplots/Streudiagrammen + Kennzahlen Ausreißer finden.",
        "Alle Abweichungen pauschal löschen – im Kontext."
      ],
      "answer": 2,
      "explanation": "`EDA` ist der erste und wichtigste Schritt im `Data Analytics`-Workflow. Visuelle Werkzeuge wie `Boxplots` oder `Streudiagramme` sind besonders effektiv, um die Verteilung der Daten zu verstehen, Ausreißer zu erkennen und unerwartete Beziehungen zwischen Variablen zu identifizieren. Statistische Kennzahlen (`Mittelwert`, `Median`, etc.) helfen, diese visuellen Eindrücke quantitativ zu untermauern. Fehlerquellen wie fehlende Werte oder falsche Datentypen werden in dieser Phase ebenfalls identifiziert.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Der Nutzen von `EDA`",
        "schritte": [
          "`EDA` ist wie Detektivarbeit: Man sucht nach Hinweisen, die helfen, die Daten zu verstehen und mögliche Probleme zu identifizieren.",
          "Ein `Boxplot` kann zum Beispiel Ausreißer visuell als Punkte außerhalb der 'Whisker' darstellen. Ein `Streudiagramm` kann unerwartete Cluster oder nicht-lineare Zusammenhänge aufzeigen.",
          "Fehlerquellen können auch im Datenformat liegen, z.B. wenn eine numerische Spalte plötzlich Text enthält. Solche Inkonsistenzen werden oft bei der ersten Analyse der Datenstrukturen sichtbar."
        ]
      },
      "mini_glossary": {
        "Exploratory Data Analysis (`EDA`)": "Der Prozess der Untersuchung von Datensätzen, um ihre Hauptmerkmale zusammenzufassen, oft mit visuellen Methoden.",
        "Boxplot": "Ein Diagramm, das die Verteilung eines Datensatzes über seine Quartile darstellt und oft zur Identifizierung von Ausreißern verwendet wird.",
        "Ausreißer": "Beobachtungen, die deutlich außerhalb des typischen Wertebereichs liegen und Modelle stark beeinflussen können; benötigen spezielle Behandlung."
      },
      "cognitive_level": "Analyse",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "130. Reflektieren Sie, wie `Explainable AI` (`XAI`) die Akzeptanz von ML-Modellen in Unternehmen beeinflussen kann.",
      "options": [
        "Es macht die Modelle komplexer und weniger verständlich für Nicht-Experten.",
        "Es hilft, die Modellgüte zu reduzieren, um Fehler zu verringern – im Kontext.",
        "Es ermöglicht eine erhöhte Transparenz, stärkt das Vertrauen und erleichtert die Fehlerbehebung.",
        "Es ist ein rein akademisches Konzept ohne praktischen Nutzen in der Industrie."
      ],
      "answer": 2,
      "explanation": "Die Akzeptanz von ML-Modellen in Unternehmen hängt stark von ihrem Vertrauen in die Entscheidungen der Modelle ab. `XAI` bricht die 'Black-Box'-Natur vieler Modelle auf, indem es erklärt, warum eine bestimmte Vorhersage getroffen wurde. Dies schafft Transparenz und Vertrauen, was besonders in regulierten Branchen wie dem Finanz- oder Gesundheitswesen entscheidend ist. Zudem erleichtert `XAI` die Fehlersuche und hilft, Voreingenommenheiten (Bias) in den Daten oder Modellen zu erkennen und zu beheben.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Die wirtschaftlichen und ethischen Vorteile von `XAI`",
        "schritte": [
          "In Branchen, in denen Entscheidungen weitreichende Konsequenzen haben (z.B. Kreditvergabe oder medizinische Diagnose), ist es oft gesetzlich vorgeschrieben, die Entscheidungsfindung zu erklären.",
          "`XAI` hilft nicht nur bei der Einhaltung solcher Vorschriften, sondern ermöglicht es auch, die Modelle kontinuierlich zu verbessern, indem man erkennt, auf welche Features sie übermäßig reagieren oder welche Vorhersagen unlogisch sind.",
          "Zudem kann `XAI` die Zusammenarbeit zwischen Data Scientists und Fachexperten verbessern, da beide eine gemeinsame Grundlage für die Diskussion der Modellergebnisse haben."
        ]
      },
      "mini_glossary": {
        "Explainable AI (`XAI`)": "Methoden, die Entscheidungsprozesse komplexer Modelle verständlich machen (z. B. SHAP, LIME, Feature-Attributionen).",
        "Black Box": "Modell, dessen Entscheidungslogik für Anwender kaum nachvollziehbar ist – etwa tiefe Netze ohne zusätzliche Interpretierbarkeit.",
        "Bias (Voreingenommenheit)": "Eine systemische Verzerrung in einem Datensatz oder Algorithmus, die zu unfairen oder diskriminierenden Ergebnissen führen kann."
      },
      "cognitive_level": "Analyse",
      "concept": "Spezialthemen & Methoden"
    },
    {
      "question": "131. Welche Aussage beschreibt **Bias-Variance-Trade-off** im Kontext von $ML$ am treffendsten?",
      "options": [
        "Hoher Bias und hohe Varianz führen stets zu guter Generalisierung – im Kontext – in der Praxis.",
        "Niedriger Bias und niedrige Varianz sind bei realen Daten leicht zu erreichen – im Kontext.",
        "Ein Modell mit hohem Bias ist tendenziell **unterfittet**, eines mit hoher Varianz tendenziell **überfittet**.",
        "Bias und Varianz sind unabhängig voneinander und beeinflussen die Generalisierung nicht.",
        "Hohe Varianz ist in der Praxis immer besser als hoher Bias – im Kontext – in der Praxis."
      ],
      "answer": 2,
      "explanation": "**Bias** misst die systematische Abweichung (Unteranpassung), **Varianz** die Empfindlichkeit gegenüber Datenfluktuation (Überanpassung). Gute Generalisierung erfordert ein ausgewogenes Verhältnis.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung",
      "cognitive_level": "Reproduktion",
      "concept": "Modellbewertung & Validierung",
      "mini_glossary": {
        "Metrik": "Maß zur Bewertung der Modellgüte (z. B. Accuracy, F1, MSE).",
        "Train/Test-Split": "Aufteilung der Daten in Trainings- und Testmenge.",
        "Cross-Validation": "Mehrfache Trainings/Test-Aufteilung zur stabileren Bewertung."
      }
    },
    {
      "question": "132. Welche Maßnahme reduziert **Data Leakage** in einem klassischen Analytics-Workflow am zuverlässigsten?",
      "options": [
        "Die gesamte Vorverarbeitung auf dem vollständigen Datensatz durchführen.",
        "Die Vorverarbeitung strikt innerhalb von Cross-Validation-Folds fitten und anwenden.",
        "Vorverarbeitung nur auf dem Testset fitten und auf das Train-Set anwenden.",
        "Keine Vorverarbeitung durchführen, um keine Informationen zu leaken.",
        "Feature-Engineering ausschließlich nach der Modellbewertung durchführen."
      ],
      "answer": 1,
      "explanation": "**Leckagen** entstehen, wenn Informationen aus dem Testset in den Trainingsprozess einfließen. Daher müssen Transformationen wie `StandardScaler` innerhalb der Folds gelernt (fit) und angewendet (transform) werden.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Leckagen durch saubere Pipeline-Scopes vermeiden",
        "schritte": [
          "**Fit** von Transformationen (z. B. `StandardScaler`) nur auf Train-Folds, **Transform** auf Train- und Val-Fold separat.",
          "Nutzung von `Pipeline`/`ColumnTransformer` stellt konsistente Reihenfolge und Scope sicher.",
          "Finales Refitting ausschließlich auf dem kompletten Trainingssplit; Testset bleibt unangetastet."
        ]
      },
      "mini_glossary": {
        "Cross-Validation": "Aufteilung der Trainingsdaten in mehrere Folds; jeder Fold dient einmal als Validierung. Reduziert Varianz der Schätzung.",
        "Data Leakage": "Unbeabsichtigter Informationsfluss von Test- auf Trainingsprozess (z. B. durch gemeinsames Fitten von Scaler), führt zu zu optimistischen Ergebnissen."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "133. Wofür wird **`StandardScaler`** in `scikit-learn` primär eingesetzt?",
      "options": [
        "Zum One-Hot-Encoding kategorialer Variablen – im Kontext.",
        "Zur Zentrierung auf Mittelwert 0 und Skalierung auf Standardabweichung 1.",
        "Zur Reduktion der Dimensionalität auf zwei Hauptkomponenten.",
        "Zur Erzeugung künstlicher Datenpunkte für Data Augmentation.",
        "Zur Binarisierung kontinuierlicher Ziele – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "`StandardScaler` transformiert numerische Features auf Mittelwert 0 und Standardabweichung 1, was distanz- und gradientenbasierte Verfahren stabiler macht.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "cognitive_level": "Reproduktion",
      "concept": "Datenvorbereitung & EDA",
      "mini_glossary": {
        "Datenbereinigung": "Entfernen oder Korrigieren fehlerhafter und fehlender Werte.",
        "Feature Engineering": "Erstellen oder Transformieren von Merkmalen zur besseren Modellleistung.",
        "EDA (Explorative Datenanalyse)": "Erste Analyse zur Struktur, Verteilung und Auffälligkeiten der Daten."
      }
    },
    {
      "question": "134. Welche Kennzahl ist für **binäre Klassifikation** am robustesten bei **unausgeglichenen Klassen**?",
      "options": [
        "`Accuracy`",
        "`ROC-AUC`",
        "`MSE`",
        "`Explained Variance`",
        "`R^2`"
      ],
      "answer": 1,
      "explanation": "Bei stark unbalancierten Klassen kann `Accuracy` täuschen. `ROC-AUC` misst die Ranking-Qualität über Schwellen und ist robuster gegenüber Imbalance.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Warum `ROC-AUC` bei Imbalance oft sinnvoller ist",
        "schritte": [
          "`ROC-AUC` berücksichtigt die Sensitivität und 1−Spezifität über alle Schwellen.",
          "Damit wird die **Ranking-Fähigkeit** bewertet statt eine feste Schwelle.",
          "Für extrem unausgeglichene Probleme ist `PR-AUC` oft zusätzlich aussagekräftig."
        ]
      },
      "mini_glossary": {
        "ROC-AUC": "Fläche unter der Receiver-Operating-Characteristic-Kurve; misst die Trennschärfe unabhängig von einer Schwelle.",
        "PR-AUC": "Fläche unter Precision-Recall-Kurve; besonders informativ bei starker Klassenimbalance."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "135. Welche Vorteil bringt die Verwendung von **`Pipeline`** in `scikit-learn`?",
      "options": [
        "Sie beschleunigt das Training durch GPU – im Kontext.",
        "Sie bündelt Schritte und verhindert Data Leakage im CV‑Prozess.",
        "Sie reduziert die Anzahl der Features automatisch.",
        "Sie ersetzt Hyperparameter‑Suche – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "`Pipeline` bündelt Vorverarbeitung und Modell zu einem Ablauf, der konsistent trainiert und validiert wird—wichtig gegen **Data Leakage** und für Reproduzierbarkeit.",
      "weight": 1,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Reproduktion",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "136. Welche Technik ist **keine** typische Komponente der **Exploratory Data Analysis (EDA)**?",
      "options": [
        "Univariate und bivariate Visualisierungen (z. B. Histogramme, Scatterplots).",
        "Berechnung von Lage- und Streuungsmaßen – im Kontext – in der Praxis.",
        "Hyperparameter-Tuning eines Modells – im Kontext – in der Praxis.",
        "Identifikation von Ausreißern – im Kontext – in der Praxis.",
        "Korrelationsanalyse numerischer Variablen – im Kontext – in der Praxis."
      ],
      "answer": 2,
      "explanation": "**EDA** dient Verständnis und Datenqualitätsprüfung; Hyperparameter-Tuning gehört zur Modellierungsphase, nicht zur reinen Exploration.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Grenze zwischen Exploration und Modellierung",
        "schritte": [
          "EDA beantwortet „Was liegt vor?“: Struktur, Qualität, Verteilungen, Zusammenhänge.",
          "Tuning optimiert Modellhyperparameter und setzt ein trainierbares Setup voraus.",
          "Saubere Trennung verhindert voreilige Schlussfolgerungen und Leakage."
        ]
      },
      "mini_glossary": {
        "EDA": "Explorative Analysephase zur Hypothesengenerierung und Datenprüfung.",
        "Hyperparameter-Tuning": "Systematische Suche (z. B. `GridSearchCV`) nach nicht-gelernten Steuerparametern eines Modells."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "137. Welche Maßnahme verbessert die **Robustheit gegenüber Ausreißern** in linearen Modellen am ehesten?",
      "options": [
        "HuberRegressor statt LinearRegression.",
        "Stärkere L2-Regularisierung – im Kontext.",
        "Lernrate vergrößern – im Kontext.",
        "Zielvariable standardisieren.",
        "Bias-Term entfernen – im Kontext."
      ],
      "answer": 0,
      "explanation": "Der `HuberRegressor` nutzt einen robusten Verlust, der Ausreißer weniger stark gewichtet als das quadratische Fehlermaß der klassischen `LinearRegression`.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Robuster Verlust statt quadratischem Fehler",
        "schritte": [
          "Quadratischer Loss bestraft große Residuen stark → Ausreißer dominieren.",
          "Huber-Loss wechselt ab einem Schwellwert in eine lineare Penalisierung.",
          "Ergebnis: stabilere Schätzungen und geringere Sensitivität."
        ]
      },
      "mini_glossary": {
        "Huber-Loss": "Stückweise quadratischer/linearer Verlust; kombiniert Robustheit und Effizienz.",
        "Ridge ($L_2$)": "Quadratische Regularisierung; reduziert Varianz, aber nicht speziell Ausreißerempfindlichkeit."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "138. Welche Aussage trifft **am ehesten** auf **`PCA`** zu?",
      "options": [
        "`PCA` maximiert die Klassengenauigkeit – im Kontext.",
        "`PCA` ist eine überwachte Methode zur Feature-Selektion.",
        "`PCA` projiziert auf orthogonale Richtungen maximaler Varianz.",
        "`PCA` ist nur für binäre Ziele geeignet – im Kontext.",
        "`PCA` ersetzt immer Feature-Engineering – im Kontext."
      ],
      "answer": 2,
      "explanation": "`PCA` ist **unüberwacht** und findet orthogonale Komponenten, die maximale Varianz erklären; sie dient der **Dimensionsreduktion** und Visualisierung.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Varianzorientierte Projektion",
        "schritte": [
          "Kovarianzmatrix zerlegen und Eigenvektoren als Achsen verwenden.",
          "Höchste Eigenwerte → Komponenten mit größter erklärter Varianz.",
          "Reduktion kann Rauschen verringern und Modelle stabilisieren."
        ]
      },
      "mini_glossary": {
        "Erklärte Varianz": "Anteil der Gesamtvarianz, den eine Komponente abdeckt.",
        "Eigenvektor/-wert": "Richtungen und zugehörige Varianzstärken der Kovarianzmatrix."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "139. Warum ist **Feature-Scaling** für `KNN` besonders wichtig?",
      "options": [
        "Weil `KNN` keine Distanzmaße verwendet – im Kontext – in der Praxis.",
        "Weil `KNN` auf Distanzvergleichen basiert und Skalenunterschiede verzerren.",
        "Weil `KNN` sonst keine kategorialen Variablen verarbeitet.",
        "Weil `KNN` nur mit normalverteilten Merkmalen funktioniert.",
        "Weil `KNN` sonst die Nachbarn zufällig wählt – im Kontext."
      ],
      "answer": 1,
      "explanation": "`KNN` nutzt Distanzen; Features mit größeren Skalen dominieren sonst die Nachbarschaftssuche, weshalb **Standardisierung/Normalisierung** nötig ist.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "cognitive_level": "Reproduktion",
      "concept": "Datenvorbereitung & EDA",
      "mini_glossary": {
        "Datenbereinigung": "Entfernen oder Korrigieren fehlerhafter und fehlender Werte.",
        "Feature Engineering": "Erstellen oder Transformieren von Merkmalen zur besseren Modellleistung.",
        "EDA (Explorative Datenanalyse)": "Erste Analyse zur Struktur, Verteilung und Auffälligkeiten der Daten."
      }
    },
    {
      "question": "140. Welche Praxis ist bei **Class Imbalance** für die **Schwellenwahl** sinnvoll?",
      "options": [
        "Immer Schwelle 0.5, unabhängig von Kosten.",
        "Schwelle per Youden-Index/Kostenmatrix anhand ROC/PR.",
        "Schwelle so, dass Accuracy maximal wird.",
        "Schwellen zufällig sampeln und mitteln.",
        "Schwellenwahl ignorieren, sie ändert nichts."
      ],
      "answer": 1,
      "explanation": "Bei unausgeglichenen Klassen optimiert eine feste Schwelle selten die gewünschten **Kosten/Nutzen**. Kurvenbasierte Verfahren erlauben zielabhängige Auswahl.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Schwellen daten- und zielabhängig bestimmen",
        "schritte": [
          "`ROC`/`PR`-Kurven über alle Schwellen erzeugen.",
          "Youden-Index oder kostenbasierte Optimierung auf Geschäftsziele abstimmen.",
          "Schwelle regelmäßig rekalibrieren, wenn Daten-Drift vorliegt."
        ]
      },
      "mini_glossary": {
        "Youden-Index": "Maximiert Sensitivität + Spezifität − 1; liefert eine Schwellenempfehlung.",
        "Daten-Drift": "Änderung der Verteilung über die Zeit; erfordert Nachkalibrierung."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "141. Welche Methode ist **keine** gängige Strategie zur **Feature-Selektion**?",
      "options": [
        "Filtermethoden (z. B. Korrelation, Mutual Information).",
        "Wrappermethoden (z. B. rekursives Weglassen).",
        "Embedded-Methoden (z. B. `Lasso`).",
        "Zufälliges Entfernen von Features ohne Bewertung.",
        "Stabilitätsselektion über resampelte Daten."
      ],
      "answer": 3,
      "explanation": "**Randomes Entfernen** ohne Bewertung ist keine systematische Methode; etablierte Ansätze nutzen statistische Kriterien oder modellinduzierte Gewichte.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Selektion erfordert Kriterium oder Modellkopplung",
        "schritte": [
          "Filter stützen sich auf datengetriebene Maße (z. B. Korrelation).",
          "Wrapper bewerten Feature-Sets über wiederholtes Train/Val.",
          "Embedded nutzen Regularisierung/Gewichte im Modell (z. B. `L1`)."
        ]
      },
      "mini_glossary": {
        "Lasso (`L1`)": "Sparsame Regularisierung; kann Koeffizienten exakt auf 0 setzen.",
        "Stabilitätsselektion": "Wählt Features, die über Resamples hinweg konsistent wichtig sind."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "142. Welche Aussage trifft auf **`StratifiedKFold`** am besten zu?",
      "options": [
        "Teilt Daten zufällig ohne Rücksicht auf die Zielverteilung.",
        "Erhält die Klassenverteilung in jedem Fold näherungsweise bei.",
        "Ist nur für Regression geeignet.",
        "Verwendet deterministisch immer dieselben Splits ohne Seed.",
        "Erfordert, dass alle Features bereits skaliert sind."
      ],
      "answer": 1,
      "explanation": "`StratifiedKFold` sorgt dafür, dass die **Klassenverteilung** in jedem Fold ähnlich der Gesamtheit ist—wichtig bei Imbalance.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung",
      "cognitive_level": "Reproduktion",
      "concept": "Modellbewertung & Validierung",
      "mini_glossary": {
        "Metrik": "Maß zur Bewertung der Modellgüte (z. B. Accuracy, F1, MSE).",
        "Train/Test-Split": "Aufteilung der Daten in Trainings- und Testmenge.",
        "Cross-Validation": "Mehrfache Trainings/Test-Aufteilung zur stabileren Bewertung."
      }
    },
    {
      "question": "143. Welche Aussage zur **Kalibrierung** von Klassifikationsmodellen ist korrekt?",
      "options": [
        "Kalibrierung verändert die Rangfolge der Scores nicht.",
        "Kalibrierung maximiert zwangsläufig `Accuracy`.",
        "Kalibrierung ist bei logistischen Modellen nicht nötig.",
        "Kalibrierung ist identisch mit Regularisierung.",
        "Kalibrierung ersetzt die Schwellenwahl."
      ],
      "answer": 0,
      "explanation": "Methoden wie **Platt Scaling** oder **Isotonic Regression** justieren **Wahrscheinlichkeiten**, nicht das Ranking; `ROC-AUC` bleibt oft unverändert, `Brier`/`LogLoss` verbessern sich.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Wahrscheinlichkeiten auf reale Häufigkeiten abbilden",
        "schritte": [
          "Auf Val-Daten wird ein Kalibrierungsmodell auf Scores gefittet.",
          "Output sind besser kalibrierte Wahrscheinlichkeiten (z. B. 0.7 ≈ 70 %).",
          "Nützlich für kostenbasierte Entscheidungen und Risikoabschätzungen."
        ]
      },
      "mini_glossary": {
        "Platt Scaling": "Sigmoid-Modell auf Scores; mappt zu Wahrscheinlichkeiten.",
        "Isotone Regression": "Monotone, nichtlineare Anpassung der Score→Proba-Abbildung."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "144. Welche Option beschreibt **`TimeSeriesSplit`** richtig?",
      "options": [
        "Randomisiert Daten ohne Reihenfolge.",
        "Respektiert Zeitordnung bei Validierung.",
        "Mischt Train/Test in einem Fold.",
        "Wählt zufällige Subsets – im Kontext."
      ],
      "answer": 1,
      "explanation": "`TimeSeriesSplit` respektiert **Zeitkausalität**: Train auf Vergangenheit, Val auf Zukunft—verhindert **Look-ahead Leakage**.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Kausal korrekte Validierung für Zeitreihen",
        "schritte": [
          "Splits wachsen kumulativ im Training und schieben das Val-Fenster vor.",
          "Keine Durchmischung über die Zeit; Ordnung bleibt erhalten.",
          "Geeignet für Prognosen, Drift-Erkennung und Modellwahl."
        ]
      },
      "mini_glossary": {
        "Look-ahead Leakage": "Zukünftige Informationen beeinflussen Training; führt zu zu guten Schätzungen.",
        "Drift": "Systematische Verschiebung der Datenverteilung über die Zeit."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "145. Was ist der **Hauptzweck** von **`MLflow Tracking`** im Analytics-Stack?",
      "options": [
        "GPU-Training von Netzen im Stack.",
        "Tracking von Parametern, Metriken, Artefakten und Modellen.",
        "Automatische Hyperparameter-Optimierung im Hintergrund.",
        "Berechnung von SHAP-Werten für Erklärbarkeit.",
        "Direktes Kubernetes-Deployment ohne weitere Tools."
      ],
      "answer": 1,
      "explanation": "`MLflow Tracking` protokolliert **Parameter, Metriken, Artefakte und Modelldateien** und erleichtert Vergleich und Reproduzierbarkeit.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "extended_explanation": {
        "titel": "Warum strukturierte Experiment-Historie entscheidend ist",
        "schritte": [
          "Konsistente Runs mit identischen Seeds und Umgebungen protokollieren.",
          "Artefakte (Plots, Modelle) zentral ablegen und vergleichen.",
          "Reproduzierbare Entscheidungen durch nachvollziehbare Historie ermöglichen."
        ]
      },
      "mini_glossary": {
        "Artefakt": "Begleitobjekt eines Runs (z. B. Figure, Confusion Matrix, Modell).",
        "Run": "Einzelner Trainings-/Evaluationsdurchlauf mit Logeinträgen in `MLflow`."
      },
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment"
    },
    {
      "question": "146. Welche Aussage beschreibt **Regularisierung** korrekt?",
      "options": [
        "`L1` erhöht Varianz, `L2` erhöht Bias – im Kontext – in der Praxis.",
        "`L1` kann Koeffizienten exakt nullen, `L2` schrumpft kontinuierlich.",
        "`L2` führt stets zu sparsamen, exakt Null-Koeffizienten.",
        "Regularisierung ist nur bei Deep Learning sinnvoll – im Kontext.",
        "Regularisierung ersetzt Datenqualität – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "`L1` (Lasso) induziert **Sparsamkeit** durch Nullsetzen, `L2` (Ridge) glättet und reduziert Varianz ohne harte Nullung.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Bias–Varianz gezielt beeinflussen",
        "schritte": [
          "`L1` fördert Feature-Selektion durch koeffiziente Nullung.",
          "`L2` verteilt Gewichte und stabilisiert Vorhersagen.",
          "Kombination (`Elastic Net`) vereint Vorteile beider Welten."
        ]
      },
      "mini_glossary": {
        "Elastic Net": "Kombiniert `L1` und `L2`; kontrolliert Sparsamkeit und Glättung.",
        "Sparsamkeit": "Viele Koeffizienten exakt 0; vereinfacht das Modell."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "147. Welche **Docker**-Praxis unterstützt **Reproduzierbarkeit** in Analytics-Projekten am stärksten?",
      "options": [
        "Installationen auf Host ohne Versionsangaben (schlecht).",
        "Versioniertes Dockerfile + pinned Requirements/Environment.",
        "Nur Readme mit Setup-Hinweisen, ohne Versionen.",
        "Abhängigkeiten zur Laufzeit updaten (nicht reproduzierbar).",
        "Basisimage ohne Tag verwenden (wechselnd)."
      ],
      "answer": 1,
      "explanation": "Ein **fixiertes** `Dockerfile` mit **versionierten** Abhängigkeiten erzeugt portable, reproduzierbare Umgebungen über Systeme hinweg.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "extended_explanation": {
        "titel": "Umgebung einfrieren – Ergebnisse stabil halten",
        "schritte": [
          "Basisimage und Paketversionen explizit deklarieren.",
          "Build-Caching durch frühes Kopieren der Anforderungen nutzen.",
          "Identische Images über Dev/CI/Prod ausrollen."
        ]
      },
      "mini_glossary": {
        "Image": "Schreibgeschützte Vorlage eines Containers mit allen Abhängigkeiten.",
        "Container": "Laufende Instanz eines Images mit isolierter Umgebung."
      },
      "cognitive_level": "Anwendung",
      "concept": "MLOps & Deployment"
    },
    {
      "question": "148. Welche Aussage beschreibt **`RandomForest`** gegenüber einem einzelnen **Decision Tree** am besten?",
      "options": [
        "`RandomForest` erhöht systematisch Overfitting.",
        "`RandomForest` reduziert Varianz durch Bagging vieler Bäume.",
        "`RandomForest` braucht zwingend weniger Daten.",
        "`RandomForest` ist deterministisch ohne Zufallseinfluss.",
        "`RandomForest` eignet sich nicht für Klassifikation."
      ],
      "answer": 1,
      "explanation": "Durch **Bagging** (Bootstrapping + Merkmalssampling) mittelt der Wald über viele Bäume und reduziert so **Varianz** und Overfitting-Tendenzen einzelner Bäume.",
      "weight": 2,
      "topic": "Modelle & Algorithmen",
      "extended_explanation": {
        "titel": "Varianzreduktion durch Diversität",
        "schritte": [
          "Bootstraps und zufällige Feature-Subsets erzeugen diverse Bäume.",
          "Aggregation (Mehrheit/Mittel) glättet Einzelschätzungen.",
          "Resultat: stabilere Performance und höhere Robustheit."
        ]
      },
      "mini_glossary": {
        "Bagging": "Parallel trainierte Modelle auf Bootstraps; Aggregation der Vorhersagen.",
        "Feature-Sampling": "Zufällige Auswahl von Merkmalen pro Split zur Entkorrelation."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modelle & Algorithmen"
    },
    {
      "question": "149. Welche Metrik eignet sich **besonders** zur Bewertung einer **Regression**?",
      "options": [
        "F1-Score (Klassifikation) – im Kontext.",
        "MSE (mittlere quadratische Abweichung)",
        "Recall (Klassifikation) – im Kontext.",
        "ROC-AUC (Klassifikation) – im Kontext.",
        "Accuracy (Klassifikation) – im Kontext."
      ],
      "answer": 1,
      "explanation": "In Regressionsaufgaben sind Fehlermaße wie **`MSE`** oder `MAE` üblich; Klassifikationsmetriken wie `F1`, `Accuracy`, `ROC-AUC` sind ungeeignet.",
      "weight": 1,
      "topic": "Modellbewertung & Validierung",
      "cognitive_level": "Reproduktion",
      "concept": "Modellbewertung & Validierung",
      "mini_glossary": {
        "Metrik": "Maß zur Bewertung der Modellgüte (z. B. Accuracy, F1, MSE).",
        "Train/Test-Split": "Aufteilung der Daten in Trainings- und Testmenge.",
        "Cross-Validation": "Mehrfache Trainings/Test-Aufteilung zur stabileren Bewertung."
      }
    },
    {
      "question": "150. Welche Maßnahme ist zur **Erklärung** von Modellentscheidungen in tabellarischen Daten **am praktikabelsten**?",
      "options": [
        "`LIME` oder `SHAP` zur lokalen/ globalen Feature-Beitragsanalyse.",
        "Nur die Koeffizienten eines Ridge-Modells betrachten.",
        "Gewichte eines `RandomForest` ignorieren, da sie uninterpretierbar sind.",
        "Stets nur Korrelationsmatrix zeigen.",
        "Feature-Skalierung abschalten, um Interpretierbarkeit zu erhöhen."
      ],
      "answer": 0,
      "explanation": "**`LIME`/`SHAP`** liefern **lokale** und **globale** Beiträge von Features—geeignet für komplexe Modelle und Audit-Zwecke.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Lokale Erklärungen für individuelle Vorhersagen",
        "schritte": [
          "`LIME` approximiert lokal ein einfaches Modell um eine Vorhersage.",
          "`SHAP` nutzt Shapley-Werte aus der Spieltheorie für konsistente Beiträge.",
          "Kombination erlaubt Debugging, Kommunikation und Governance."
        ]
      },
      "mini_glossary": {
        "SHAP": "Shapley Additive Explanations; additiv zerlegbare Feature-Beiträge.",
        "LIME": "Local Interpretable Model-agnostic Explanations; lokale Linearisierung."
      },
      "cognitive_level": "Analyse",
      "concept": "Spezialthemen & Methoden"
    },
    {
      "question": "151. Welche Praxis ist bei **SQL-Analysen** für Kennzahlenbildung korrekt?",
      "options": [
        "Kennzahlen direkt aus Roh-Events ohne Aggregation berichten.",
        "Window-Functions verwenden, um rollierende Metriken zu berechnen.",
        "Nur `GROUP BY` reicht immer für Kohortenanalysen.",
        "`JOIN`s vermeiden, um Performance zu sichern.",
        "Zeitstempel in Text konvertieren, um Zonenprobleme zu umgehen."
      ],
      "answer": 1,
      "explanation": "**Window-Functions** (`OVER`-Klausel) erlauben **rollierende**, **kumulative** und rangbasierte Auswertungen ohne Informationsverlust durch Aggregation.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Zeitbezogene Metriken korrekt bilden",
        "schritte": [
          "Mit `PARTITION BY` und `ORDER BY` Kohorten/Zeitläufe definieren.",
          "Funktionen wie `SUM() OVER` oder `AVG() OVER` für rollierende Werte nutzen.",
          "So bleiben Zeilengranularität und Metrikbildung vereinbar."
        ]
      },
      "mini_glossary": {
        "Window Function": "Aggregation über ein gleitendes Fenster statt Gruppenverdichtung.",
        "`OVER`-Klausel": "Definiert Partition und Ordnung für Window-Berechnungen."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "152. Welche Option trifft auf **A/B-Tests** methodisch zu?",
      "options": [
        "Die Stichprobengröße kann nach ersten Ergebnissen beliebig angepasst werden.",
        "Vorab festgelegte **Power** und **Signifikanzniveau** bestimmen die benötigte Stichprobengröße.",
        "Der p-Wert gibt die Wahrscheinlichkeit der Nullhypothese an – im Kontext.",
        "Mehrfaches Zwischenstopp-Look erhöht nicht die Fehlerwahrscheinlichkeit – im Kontext.",
        "Kein Randomisieren nötig, wenn Gruppen ähnlich wirken – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Testdesign verlangt **a priori** definierte **$\\u03b1$** (Signifikanz) und **Power** (1−$\\u03b2$). Daraus ergibt sich die notwendige **Sample Size**.",
      "weight": 3,
      "topic": "Spezialthemen & Methoden",
      "extended_explanation": {
        "titel": "Sauberes Testdesign vor Datensammlung",
        "schritte": [
          "Effektgröße, $α$ und Power festlegen → Stichprobengröße berechnen.",
          "Randomisieren und vorab Analyseplan fixieren (Pre-Registration).",
          "Zwischenanalysen nur mit Alpha-Spending/Group-Sequential-Design."
        ]
      },
      "mini_glossary": {
        "Power": "Wahrscheinlichkeit, einen echten Effekt zu entdecken (1−$β$).",
        "Signifikanzniveau ($α$)": "Fehlerrate 1. Art; üblicherweise 0.05."
      },
      "cognitive_level": "Analyse",
      "concept": "Spezialthemen & Methoden"
    },
    {
      "question": "153. Welche Maßnahme verbessert **Reproduzierbarkeit** zusätzlich zu `Docker`?",
      "options": [
        "Seeds und Splits zufällig halten (nicht reproduzierbar).",
        "requirements.txt ohne Versions-Pins verwenden.",
        "Random-Seeds setzen und Splits deterministisch speichern.",
        "Artefakte nicht versionieren oder sichern.",
        "Notebook-Zellen beliebig neu ausführen."
      ],
      "answer": 2,
      "explanation": "**Deterministische Seeds** und persistierte **Splits** sichern identische Bedingungen für Folgeruns—wichtig für Vergleichbarkeit.",
      "weight": 1,
      "topic": "MLOps & Deployment",
      "cognitive_level": "Reproduktion",
      "concept": "MLOps & Deployment",
      "mini_glossary": {
        "Deployment": "Bereitstellung eines Modells für den produktiven Einsatz.",
        "Monitoring": "Überwachung von Modellleistung und Daten drift im Betrieb.",
        "Pipeline": "Automatisierte Abfolge von Schritten vom Training bis zur Auslieferung."
      }
    },
    {
      "question": "154. Wodurch unterscheidet sich **`GridSearchCV`** von **`RandomizedSearchCV`**?",
      "options": [
        "GridSearch zufällig, Randomized vollständig.",
        "GridSearch prüft alle Kombos, Randomized sampelt.",
        "Beide liefern identische Ergebnisse – im Kontext.",
        "Randomized braucht keine CV – im Kontext.",
        "GridSearch ist immer schneller – im Kontext."
      ],
      "answer": 1,
      "explanation": "**Grid** prüft alle Gitterpunkte; **Randomized** sampelt und kann schneller gute Regionen finden—insbesondere bei großen Suchräumen.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Suchstrategien im Parameterraum",
        "schritte": [
          "Grid: Exhaustiv, aber teuer bei vielen Dimensionen.",
          "Randomized: Stochastisch, erkundet Breite effizienter.",
          "Praxis: Randomized → Feingrid um gute Regionen."
        ]
      },
      "mini_glossary": {
        "Suchraum": "Kartesisches Produkt aller Hyperparameterbereiche.",
        "Validierungsstrategie": "Art der Datenpartitionierung während der Suche (z. B. K-Fold)."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "155. Welche Strategie ist **zweckmäßig**, um **Ausreißer** **vor** dem Modelltraining zu behandeln?",
      "options": [
        "Alle Ausreißer immer entfernen, ohne Prüfung – im Kontext.",
        "Winsorisierung/robuste Transformationen prüfen und validieren.",
        "Ausreißer ignorieren, Modelle lernen das – im Kontext.",
        "Nur Zielvariable clippen, Features unverändert.",
        "Nur Log-Transformationen anwenden, immer – im Kontext."
      ],
      "answer": 1,
      "explanation": "**Ausreißer-Handling** ist kontextabhängig. Robuste Alternativen (Winsorize, Huber/Quantile-Transformer) sollten **validiert** statt dogmatisch angewandt werden.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Kontext und Validierung statt Dogma",
        "schritte": [
          "Ausreißer detektieren (z. B. IQR/robuste Z-Scores).",
          "Optionen wie Winsorisierung/robuste Scaler testen.",
          "Wirksamkeit per Cross-Validation und Domänenmetriken prüfen."
        ]
      },
      "mini_glossary": {
        "Winsorisierung": "Beschneidet Extremwerte auf Perzentilschwellen, statt sie zu verwerfen.",
        "IQR": "Interquartilsabstand $Q3-Q1$; robustes Streuungsmaß."
      },
      "cognitive_level": "Anwendung",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "156. Welche Eigenschaft zeichnet **`LogisticRegression`** in `scikit-learn` aus?",
      "options": [
        "Sie liefert kalibrierte Wahrscheinlichkeiten ohne weitere Maßnahmen.",
        "Sie kann mit `liblinear` oder `lbfgs` optimiert werden.",
        "Sie ist ein Regressionsmodell für kontinuierliche Ziele.",
        "Sie benötigt keine Regularisierung.",
        "Sie kann keine multiklassigen Probleme lösen."
      ],
      "answer": 1,
      "explanation": "Die Implementierung unterstützt verschiedene **Solver** wie `liblinear`/`lbfgs`; Regularisierung ist standardmäßig aktiv (`C`).",
      "weight": 1,
      "topic": "Modelle & Algorithmen",
      "cognitive_level": "Reproduktion",
      "concept": "Modelle & Algorithmen",
      "mini_glossary": {
        "Modell": "Mathematische Abbildung von Eingaben auf Ausgaben.",
        "Algorithmus": "Verfahren zur Modellbildung oder -optimierung.",
        "Hyperparameter": "Einstellungen, die vor dem Training festgelegt werden (z. B. Baumtiefe)."
      }
    },
    {
      "question": "157. Welche Visualisierung eignet sich **direkt** zur Diagnose von **Klassifikationsfehlern**?",
      "options": [
        "Elbow-Plot der Clusteranzahl.",
        "Confusion-Matrix mit Normalisierung.",
        "Scree-Plot der PCA-Varianzanteile.",
        "Lag-Plot für Zeitreihen – im Kontext.",
        "QQ-Plot der Residuen – im Kontext."
      ],
      "answer": 1,
      "explanation": "Eine **Confusion-Matrix** zeigt Fehlklassifikationen je Klasse, optional normalisiert—nützlich für zielgerichtete Fehleranalysen.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Fehlerstruktur sichtbar machen",
        "schritte": [
          "Matrix je Klasse: True/False Positives/Negatives.",
          "Normalisierung macht Imbalance vergleichbar.",
          "Ableiten gezielter Maßnahmen (Daten, Schwelle, Kosten)."
        ]
      },
      "mini_glossary": {
        "False Positive": "Fälschlich als positiv klassifiziert.",
        "False Negative": "Fälschlich als negativ klassifiziert."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "158. Welche Aussage zu **`Train/Test Split`** ist **richtig**?",
      "options": [
        "Testset für Tuning nutzen (falsch) – im Kontext.",
        "Train/Val/Test trennen; Test nur finale Schätzung.",
        "Testset größer als Trainset erzwingen.",
        "Validierungsset unnötig bei CV – im Kontext.",
        "Stratifizierung nur bei Regression – im Kontext."
      ],
      "answer": 1,
      "explanation": "**Rollen-Trennung**: Tuning erfolgt ohne Testwissen (Val/CV), die finale Leistung wird **einmalig** auf dem **Testset** geschätzt.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "extended_explanation": {
        "titel": "Saubere Rollen für belastbare Schätzungen",
        "schritte": [
          "Entwicklung/Tuning ohne Testkontakt.",
          "Test nur für die Schlussmessung reservieren.",
          "Reproduzierbare Splits dokumentieren und versionieren."
        ]
      },
      "mini_glossary": {
        "Holdout": "Feste Aufteilung in Train/Val/Test.",
        "Leckage": "Informationsübertrag von Test in Training/Tuning."
      },
      "cognitive_level": "Anwendung",
      "concept": "Modellbewertung & Validierung"
    },
    {
      "question": "159. Welche Maßnahme verbessert die **Modellüberwachung** nach Deployment?",
      "options": [
        "Verzicht auf Logging zur Performance-Steigerung – im Kontext.",
        "Drift-Detektion, Re-Calibration und periodisches Re-Training nach SLAs.",
        "Ausschließlich manuelle Stichprobenprüfung ohne Metriken.",
        "Nur Hardware-Monitoring (CPU/RAM) betrachten – im Kontext.",
        "Feature-Statistiken nie speichern, um Speicherplatz zu sparen."
      ],
      "answer": 1,
      "explanation": "**MLOps** verlangt Monitoring von **Daten-/Kontextdrift**, **Metriken** und **Re-Training**-Regeln, um Leistung stabil zu halten.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "extended_explanation": {
        "titel": "Kontinuierliche Qualitätssicherung im Betrieb",
        "schritte": [
          "Eingangsdaten und Predictions statistisch überwachen.",
          "Schwellen/SLAs definieren und Alerts etablieren.",
          "Re-Training/Neu-Kalibrierung bei Verletzungen anstoßen."
        ]
      },
      "mini_glossary": {
        "SLA": "Service Level Agreement; Zielwerte für Qualität/Verfügbarkeit.",
        "Drift-Detektion": "Statistische Verfahren zum Erkennen von Verteilungsänderungen."
      },
      "cognitive_level": "Analyse",
      "concept": "MLOps & Deployment"
    },
    {
      "question": "160. Welche Praxis ist bei **Feature-Encoding** für **Baumverfahren** sinnvoll?",
      "options": [
        "Kategoriale High-Cardinality-Features immer one-hot-encoden.",
        "Zyklische Features (z. B. Monat) stets als Integer lassen.",
        "Target-Encoding nur strikt CV-intern anwenden, um Leakage zu vermeiden.",
        "Numerische Features stets normalisieren – im Kontext – in der Praxis.",
        "Label-Encoding ist immer besser als One-Hot – im Kontext."
      ],
      "answer": 2,
      "explanation": "**Target-Encoding** kann starken Leakage verursachen; daher muss es **fold-intern** fit/transformiert werden. Bäume benötigen kein Scaling, aber Encoding muss leakage-sicher sein.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "extended_explanation": {
        "titel": "Hochkardinale Kategorien ohne Leckagen encoden",
        "schritte": [
          "Target-Statistiken nur aus Trainingsfold berechnen.",
          "Auf Val/Test dieselben Regeln ohne Zielinformation anwenden.",
          "Regulierung (Smoothing) gegen Overfitting der Kategorien nutzen."
        ]
      },
      "mini_glossary": {
        "Target-Encoding": "Ersetzt Kategorie durch aggregierte Zielstatistik (z. B. Mittelwert).",
        "Smoothing": "Mischung aus Kategorie- und Global-Statistik zur Stabilisierung."
      },
      "cognitive_level": "Analyse",
      "concept": "Datenvorbereitung & EDA"
    },
    {
      "question": "161. Welche Aussage beschreibt \"Big Data\" am treffendsten?",
      "options": [
        "Daten, die wegen Umfang und Vielfalt nur mit skalierbarer Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen geringer Vielfalt nur mit manueller Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen Umfang und Vielfalt nur mit lokaler Verarbeitung sinnvoll nutzbar sind.",
        "Daten, die wegen geringer Vielfalt nur mit skalierbarer Verarbeitung sinnvoll nutzbar sind."
      ],
      "answer": 0,
      "explanation": "Big Data meint Daten, deren Umfang und Vielfalt klassische Einzelrechner-Ansätze überfordern. Entscheidend ist die Notwendigkeit skalierbarer Verarbeitung, um die Daten sinnvoll auszuwerten. Die anderen Optionen verdrehen entweder Vielfalt oder Skalierung.",
      "weight": 1,
      "topic": "Big Data & Systeme",
      "concept": "Skalierbarkeit",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Big Data",
          "definition": "Daten- und Verarbeitungskontext, in dem Umfang und Vielfalt so groß sind, dass skalierbare Systeme nötig werden."
        },
        {
          "term": "Umfang",
          "definition": "Menge an Daten, die gespeichert und verarbeitet werden muss (z. B. sehr große Datenvolumina)."
        },
        {
          "term": "Vielfalt",
          "definition": "Heterogenität der Datenformen, z. B. strukturierte Tabellen, Logs, Texte oder Ereignisdaten."
        },
        {
          "term": "Skalierbare Verarbeitung",
          "definition": "Verarbeitung, die durch Verteilung auf mehrere Knoten wächst, statt nur auf einem einzelnen Rechner zu laufen."
        }
      ]
    },
    {
      "question": "162. Welche Aussage unterscheidet Data Lake und Data Warehouse am korrektesten?",
      "options": [
        "Ein Data Lake nutzt oft Schema-on-read, ein Data Warehouse nutzt typischerweise Schema-on-write.",
        "Ein Data Lake nutzt oft Schema-on-write, ein Data Warehouse nutzt typischerweise Schema-on-read.",
        "Ein Data Lake benötigt immer Schema-on-write, ein Data Warehouse benötigt immer Schema-on-read.",
        "Ein Data Lake und ein Data Warehouse nutzen typischerweise beide Schema-on-read."
      ],
      "answer": 0,
      "explanation": "Data Lakes speichern Rohdaten flexibel und interpretieren das Schema häufig erst beim Lesen (Schema-on-read). Data Warehouses modellieren Daten vorab für Analysen (Schema-on-write). Die übrigen Optionen vertauschen oder verabsolutieren die Konzepte.",
      "weight": 1,
      "topic": "Big Data & Systeme",
      "concept": "Schema-Strategie",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Data Lake",
          "definition": "Zentraler Speicher für Roh- und Halbstrukturierte Daten, oft mit späterer Interpretation je nach Use Case."
        },
        {
          "term": "Data Warehouse",
          "definition": "Für Analysen modellierter Datenbestand mit kuratierten, konsistenten Strukturen."
        },
        {
          "term": "Schema-on-read",
          "definition": "Das Schema wird beim Auslesen angewendet; Speicherung bleibt flexibel, Auswertung definiert die Struktur."
        },
        {
          "term": "Schema-on-write",
          "definition": "Das Schema wird vor dem Schreiben festgelegt; Daten werden beim Laden transformiert und validiert."
        }
      ]
    },
    {
      "question": "163. Du halbstrukturierte Logdaten für spätere Analysen. Welche Ablage ist am geeignetsten?",
      "options": [
        "Logs im Object Storage, gespeichert als Parquet, mit Schema-on-read für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Parquet, mit Schema-on-write für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Klartext, mit Schema-on-read für flexible Auswertung.",
        "Logs im Object Storage, gespeichert als Klartext, mit Schema-on-write für flexible Auswertung."
      ],
      "answer": 0,
      "explanation": "Parquet ist für analytische Abfragen effizient und Object Storage ist skalierbar. Für Logs ist Schema-on-read oft passend, weil Felder sich ändern können. Die anderen Optionen sind entweder ineffizient (Klartext) oder widersprüchlich (Schema-on-write als \"flexibel\").",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Analytische Ablage",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum diese Ablage für Logs funktioniert",
        "content": "Logs ändern sich häufig, daher passt Schema-on-read gut zum Log-Charakter. Parquet reduziert I/O und beschleunigt Analysen, besonders bei selektiven Abfragen. Object Storage bietet Skalierung ohne komplexe Clusterverwaltung für reine Speicherung.",
        "steps": [
          "Wähle Object Storage, wenn du skalierbar und kosteneffizient speichern willst.",
          "Nutze Parquet, um Analysen durch spaltenorientierte Speicherung effizient zu machen.",
          "Setze Schema-on-read ein, wenn Logfelder sich im Zeitverlauf ändern.",
          "Definiere Auswertungs-Schemata in der Analyse, nicht beim Schreiben."
        ]
      },
      "mini_glossary": [
        {
          "term": "Logs",
          "definition": "Ereignis- und Statusdaten aus Systemen/Anwendungen, oft halbstrukturiert und zeitlich geordnet."
        },
        {
          "term": "Object Storage",
          "definition": "Skalierbarer Speicher für Objekte/Dateien, typischerweise günstig und für große Datenmengen geeignet."
        },
        {
          "term": "Parquet",
          "definition": "Spaltenorientiertes Dateiformat, optimiert für analytische Abfragen und Kompression."
        },
        {
          "term": "Schema-on-read",
          "definition": "Schema wird erst beim Lesen angewendet; erleichtert flexible Nutzung sich ändernder Daten."
        }
      ]
    },
    {
      "question": "164. Du in Apache Kafka die Reihenfolge von Ereignissen pro Nutzer erhalten. Welche Konfiguration ist am passendsten?",
      "options": [
        "Apache Kafka so nutzen, dass der Key pro Nutzer gesetzt wird, damit alle Events pro Nutzer in einer Partition landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer zufällig gesetzt wird, damit alle Events pro Nutzer in einer Partition landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer gesetzt wird, damit alle Events pro Nutzer in mehreren Partitionen landen und Ordering erhalten bleibt.",
        "Apache Kafka so nutzen, dass der Key pro Nutzer zufällig gesetzt wird, damit alle Events pro Nutzer in mehreren Partitionen landen und Ordering erhalten bleibt."
      ],
      "answer": 0,
      "explanation": "In Kafka gilt Ordering zuverlässig innerhalb einer Partition. Mit einem stabilen Key pro Nutzer landen dessen Events konsistent in derselben Partition. Zufällige Keys oder mehrere Partitionen brechen die Ordnung pro Nutzer.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Partitionierung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Ordering pro Nutzer in Kafka",
        "content": "Kafka garantiert die Reihenfolge pro Partition, nicht über Partitionen hinweg. Deshalb ist ein stabiler Key die typische Lösung, wenn Ordering pro Entität (z. B. Nutzer) nötig ist. So bleiben Events pro Nutzer geordnet, während andere Nutzer parallel verarbeitet werden können.",
        "steps": [
          "Nutze Kafka-Partitionen als Ordering-Grenze.",
          "Setze den Key stabil auf die Nutzer-ID, damit Routing deterministisch bleibt.",
          "Vermeide zufällige Keys, weil sie Events über Partitionen verteilen.",
          "Plane die Partition-Anzahl so, dass sie Parallelität erlaubt, ohne Ordering-Anforderungen zu verletzen."
        ]
      },
      "mini_glossary": [
        {
          "term": "Apache Kafka",
          "definition": "Verteiltes Log-System für Event-Streaming, bei dem Topics in Partitionen aufgeteilt werden."
        },
        {
          "term": "Partition",
          "definition": "Unterteilung eines Kafka-Topics; Ordering ist innerhalb einer Partition garantiert."
        },
        {
          "term": "Key",
          "definition": "Wert, der das Routing eines Events zu einer Partition bestimmt (typisch per Hashing)."
        },
        {
          "term": "Ordering",
          "definition": "Garantierte Reihenfolge von Events innerhalb einer Partition beim Lesen durch Konsumenten."
        }
      ]
    },
    {
      "question": "165. Du führst wiederholte Transformationen auf demselben Datensatz aus. Welche Verarbeitung ist am geeignetsten?",
      "options": [
        "Apache Spark nutzen, weil In-Memory-Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "Apache Spark nutzen, weil Disk-basierte Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "MapReduce nutzen, weil In-Memory-Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt.",
        "MapReduce nutzen, weil Disk-basierte Verarbeitung Iteration effizient macht und wiederholte Transformationen beschleunigt."
      ],
      "answer": 0,
      "explanation": "Iterative Workloads profitieren stark von In-Memory-Verarbeitung. Spark ist dafür ausgelegt, Zwischenzustände im Speicher zu halten und Iteration zu beschleunigen. MapReduce ist typischerweise stärker disk-orientiert und daher für Iteration oft langsamer.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Iterative Workloads",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Spark vs. MapReduce bei Iteration",
        "steps": [
          "Erkenne Iteration: derselbe Datenbestand wird mehrfach verarbeitet.",
          "Bevorzuge In-Memory-Verarbeitung, um wiederholte I/O-Kosten zu reduzieren.",
          "Wähle Spark, wenn du Transformationen als Pipeline wiederholt ausführen willst.",
          "Nutze MapReduce eher für streng batch-orientierte, weniger iterative Jobs."
        ],
        "content": "Bei iterativen Algorithmen ist das Wiederverwenden von Zwischenresultaten zentral. Spark kann diese im Speicher halten und vermeidet unnötiges Schreiben/Lesen zwischen Iterationsschritten. MapReduce-Workflows sind häufig stärker an diskbasierte Phasen gekoppelt und verlieren dadurch Zeit bei vielen Iterationen."
      },
      "mini_glossary": [
        {
          "term": "Apache Spark",
          "definition": "Verteiltes Verarbeitungssystem, das Transformationen über Daten resilient und oft speicherbasiert ausführt."
        },
        {
          "term": "MapReduce",
          "definition": "Batch-Verarbeitungsmodell mit Map- und Reduce-Phasen, häufig mit starkem Fokus auf diskbasierte Zwischenergebnisse."
        },
        {
          "term": "In-Memory-Verarbeitung",
          "definition": "Verarbeitung, bei der Daten/Zwischenergebnisse im Arbeitsspeicher gehalten werden, um I/O zu reduzieren."
        },
        {
          "term": "Iteration",
          "definition": "Wiederholte Anwendung von Verarbeitungsschritten auf denselben oder abgeleiteten Datenbestand."
        }
      ]
    },
    {
      "question": "166. Ein Join in einem verteilten Job ist sehr langsam, weil wenige Schlüssel extrem häufig auftreten. Welche Maßnahme ist am passendsten?",
      "options": [
        "Join-Skew adressieren, indem du Salting einsetzt oder einen Broadcast Join nutzt, um Daten-Skew zu reduzieren.",
        "Join-Skew adressieren, indem du Salting einsetzt oder einen Broadcast Join nutzt, um Daten-Skew zu erhöhen.",
        "Join-Skew adressieren, indem du die Anzahl der Joins reduzierst, um Daten-Skew zu reduzieren.",
        "Join-Skew adressieren, indem du die Anzahl der Joins reduzierst, um Daten-Skew zu erhöhen."
      ],
      "answer": 0,
      "explanation": "Wenn wenige Schlüssel dominieren, entsteht Daten-Skew und einzelne Tasks werden Hotspots. Salting verteilt schwere Schlüssel auf mehrere Partitionen, Broadcast Join vermeidet Shuffle für kleine Tabellen. Die anderen Optionen ändern das Problem nicht oder verschlechtern es.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Skew-Handling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Skew bei Joins beheben",
        "steps": [
          "Erkenne Daten-Skew: einzelne Schlüssel erzeugen unverhältnismäßig viel Arbeit.",
          "Nutze Broadcast Join, wenn eine Join-Seite klein genug ist.",
          "Nutze Salting, um große Skew-Schlüssel auf mehrere Partitionen zu verteilen.",
          "Überprüfe danach die Task-Laufzeiten, um Hotspots zu verifizieren."
        ],
        "content": "Skew erzeugt ungleichmäßige Lastverteilung, was die Parallelität des Clusters aushebelt. Broadcast Join hilft, indem ein kleiner Datensatz repliziert wird und teure Shuffles vermieden werden. Salting hilft, wenn beide Seiten groß sind, indem es die Last pro Schlüssel künstlich verteilt."
      },
      "mini_glossary": [
        {
          "term": "Join",
          "definition": "Operation, die Datensätze anhand gemeinsamer Schlüssel zusammenführt."
        },
        {
          "term": "Daten-Skew",
          "definition": "Ungleichverteilung von Daten/Schlüsseln, die zu Hotspots und langen Task-Laufzeiten führt."
        },
        {
          "term": "Broadcast Join",
          "definition": "Join-Strategie, bei der ein kleiner Datensatz an alle Worker verteilt wird, um Shuffle zu vermeiden."
        },
        {
          "term": "Salting",
          "definition": "Technik, die Schlüssel künstlich erweitert, um Skew-Schlüssel auf mehrere Partitionen zu verteilen."
        }
      ]
    },
    {
      "question": "167. Du modellierst Time-Series-Daten aus Sensoren mit sehr hohem Schreibdurchsatz. Welche Datenmodellierung ist am geeignetsten?",
      "options": [
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Sensor und TTL für automatische Datenablaufregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Sensor und TTL für automatische Datenaufbauregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Messwert und TTL für automatische Datenablaufregeln.",
        "Time-Series in Cassandra speichern, mit einem Partition Key pro Messwert und TTL für automatische Datenaufbauregeln."
      ],
      "answer": 0,
      "explanation": "Für Time-Series sind Schreiblast und sequentielle Zugriffe pro Sensor typisch. Ein Partition Key pro Sensor hält zusammengehörige Daten logisch zusammen, TTL sorgt für automatisches Löschen alter Daten. Partition Key pro Messwert ist meist unpraktisch und zerlegt die Zugriffe.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Time-Series-Modell",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum Cassandra gut zu Time-Series passt",
        "steps": [
          "Ordne Time-Series-Daten nach Sensor als natürliche Partitionseinheit.",
          "Wähle den Partition Key so, dass typische Abfragen effizient bleiben.",
          "Nutze TTL, um Retention automatisch umzusetzen.",
          "Vermeide zu feingranulare Partition Keys, die Abfragen fragmentieren."
        ],
        "content": "Time-Series-Workloads bestehen oft aus vielen Writes und Abfragen pro Entität (Sensor). Cassandra kann hohe Schreibdurchsätze liefern, wenn der Partition Key die Zugriffsmuster unterstützt. TTL vereinfacht Retention, weil Daten automatisch auslaufen, ohne zusätzliche Löschjobs."
      },
      "mini_glossary": [
        {
          "term": "Time-Series",
          "definition": "Zeitlich geordnete Messwerte, typischerweise mit Zeitstempel und Entität (z. B. Sensor)."
        },
        {
          "term": "Cassandra",
          "definition": "Verteilte Wide-Column-NoSQL-Datenbank, optimiert für hohe Schreiblasten und horizontale Skalierung."
        },
        {
          "term": "Partition Key",
          "definition": "Schlüssel, der bestimmt, wie Daten auf Knoten verteilt und gruppiert werden."
        },
        {
          "term": "TTL",
          "definition": "Time-to-live; Mechanismus, der Daten nach einer festgelegten Zeit automatisch entfernt."
        }
      ]
    },
    {
      "question": "168. Du willst analytische Abfragen beschleunigen, die nur wenige Spalten einer großen Tabelle benötigen. Welche Wahl ist am geeignetsten?",
      "options": [
        "Parquet als Spaltenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "Parquet als Zeilenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "JSON als Spaltenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen.",
        "JSON als Zeilenformat nutzen, damit Predicate Pushdown und Kompression Analysen effizient machen."
      ],
      "answer": 0,
      "explanation": "Spaltenformate wie Parquet lesen nur die benötigten Spalten und profitieren von Kompression. Predicate Pushdown reduziert zusätzlich die gelesenen Daten. JSON ist für Analysen meist weniger effizient und kein echtes Spaltenformat.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Columnar Analytics",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Warum Spaltenformate Analysen beschleunigen",
        "steps": [
          "Identifiziere selektive Abfragen, die nur wenige Spalten benötigen.",
          "Nutze Parquet als Spaltenformat, um unnötiges Lesen zu vermeiden.",
          "Aktiviere Predicate Pushdown, um Filter früh anzuwenden.",
          "Nutze Kompression, um I/O zu reduzieren."
        ],
        "content": "Analytische Workloads profitieren von spaltenorientierter Speicherung, weil sie typischerweise wenige Spalten über viele Zeilen aggregieren. Parquet unterstützt effizientes Lesen, Filterung und Kompression. Dadurch sinkt die gelesene Datenmenge und Abfragen laufen schneller."
      },
      "mini_glossary": [
        {
          "term": "Parquet",
          "definition": "Spaltenorientiertes Dateiformat, das für analytische Workloads optimiert ist."
        },
        {
          "term": "Spaltenformat",
          "definition": "Speicherlayout, bei dem Werte spaltenweise gespeichert werden, um selektives Lesen zu ermöglichen."
        },
        {
          "term": "Predicate Pushdown",
          "definition": "Optimierung, bei der Filterbedingungen so früh wie möglich angewendet werden, um weniger Daten zu lesen."
        },
        {
          "term": "Kompression",
          "definition": "Verdichtung von Daten zur Reduktion von Speicherplatz und I/O bei Lesen/Schreiben."
        }
      ]
    },
    {
      "question": "169. Du willst in einem Streaming-Job doppelte Verarbeitung vermeiden, auch bei Neustarts. Welche Strategie ist am passendsten?",
      "options": [
        "Exactly-once anstreben, indem du Offsets zuverlässig verwaltest und Transaktionen mit Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zufällig verwaltest und Transaktionen mit Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zuverlässig verwaltest und Transaktionen ohne Idempotenz kombinierst.",
        "Exactly-once anstreben, indem du Offsets zufällig verwaltest und Transaktionen ohne Idempotenz kombinierst."
      ],
      "answer": 0,
      "explanation": "Exactly-once erfordert robuste Offset-Verwaltung und atomische Verarbeitung/Commit-Logik. Transaktionen helfen, Konsum und Ausgabe konsistent zu koppeln, Idempotenz reduziert die Wirkung von Wiederholungen. Zufällige Offsets oder fehlende Idempotenz machen Duplikate wahrscheinlicher.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Exactly-once",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Wie Exactly-once praktisch erreicht wird",
        "steps": [
          "Verwalte Offsets deterministisch, damit Wiederanläufe an der richtigen Stelle fortsetzen.",
          "Nutze Transaktionen, um Lesen und Schreiben atomar zu koppeln.",
          "Nutze Idempotenz, um Wiederholungen ohne doppelte Wirkung zu machen.",
          "Teste Recovery-Szenarien, um Duplikate bei Neustarts zu prüfen."
        ],
        "content": "Exactly-once ist weniger ein einzelner Schalter als ein Zusammenspiel aus Offset-Management, atomischen Updates und idempotenten Schreiboperationen. Transaktionen sorgen für Konsistenz zwischen Eingangsfortschritt und Ausgangszustand. Idempotenz reduziert die Auswirkungen von Retrys, die in verteilten Systemen normal sind."
      },
      "mini_glossary": [
        {
          "term": "Exactly-once",
          "definition": "Verarbeitungsziel, bei dem jedes Event genau einmal wirksam wird, auch bei Fehlern und Neustarts."
        },
        {
          "term": "Offset",
          "definition": "Positionsmarker in einem Event-Log/Stream, der den konsumierten Fortschritt repräsentiert."
        },
        {
          "term": "Transaktionen",
          "definition": "Mechanismus für atomische, konsistente Änderungen, sodass Teilergebnisse nicht sichtbar werden."
        },
        {
          "term": "Idempotenz",
          "definition": "Eigenschaft, dass eine Operation bei Wiederholung denselben Effekt hat wie bei einmaliger Ausführung."
        }
      ]
    },
    {
      "question": "170. Du nutzt Windowing in Stream Processing und bekommst verspätete Events. Welche Einstellung ist am passendsten?",
      "options": [
        "Event Time mit Watermarking nutzen, damit Late Events kontrolliert in Window-Berechnungen einfließen können.",
        "Event Time mit Watermarking nutzen, damit Late Events kontrolliert aus Window-Berechnungen ausgeschlossen werden.",
        "Processing Time mit Watermarking nutzen, damit Late Events kontrolliert in Window-Berechnungen einfließen können.",
        "Processing Time mit Watermarking nutzen, damit Late Events kontrolliert aus Window-Berechnungen ausgeschlossen werden."
      ],
      "answer": 0,
      "explanation": "Bei verspäteten Events ist Event Time die relevante Zeitbasis. Watermarks markieren den Fortschritt und erlauben definierte Toleranzen für Late Events in Windows. Processing Time passt oft nicht zur inhaltlichen Ereigniszeit und führt zu verzerrten Aggregationen.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Windowing",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Late Events korrekt behandeln",
        "steps": [
          "Nutze Event Time, wenn die Ereigniszeit fachlich relevant ist.",
          "Setze Watermarks, um erwartete Verzögerungen abzubilden.",
          "Konfiguriere Windowing so, dass Late Events bis zur Watermark berücksichtigt werden.",
          "Definiere klar, was nach Ablauf der Watermark passieren soll (z. B. Drop oder Korrekturpfad)."
        ],
        "content": "Late Events entstehen durch Netzwerk- oder Systemverzögerungen. Event Time stellt sicher, dass Aggregationen nach der tatsächlichen Ereigniszeit erfolgen. Watermarks geben dem System eine kontrollierte Grenze, bis wann späte Events noch in Windows eingehen dürfen."
      },
      "mini_glossary": [
        {
          "term": "Event Time",
          "definition": "Zeitstempel des Ereignisses selbst, nicht der Verarbeitungszeit im System."
        },
        {
          "term": "Watermark",
          "definition": "Schätzwert/Marker, bis zu welchem Event-Time-Zeitpunkt Events als \"weitgehend vollständig\" gelten."
        },
        {
          "term": "Late Events",
          "definition": "Events, die nach dem erwarteten Zeitpunkt eintreffen, z. B. durch Verzögerungen in der Pipeline."
        },
        {
          "term": "Window",
          "definition": "Zeitliches Aggregationsfenster, in dem Events gesammelt und berechnet werden."
        }
      ]
    },
    {
      "question": "171. Du speicherst Nutzerdaten für Analysen und willst DSGVO-konform arbeiten. Welche Aussage ist am passendsten?",
      "options": [
        "Pseudonymisierung reduziert den Personenbezug, während Anonymisierung den Personenbezug entfernt, wenn sie tatsächlich irreversibel ist.",
        "Pseudonymisierung entfernt den Personenbezug, während Anonymisierung den Personenbezug reduziert, wenn sie tatsächlich irreversibel ist.",
        "Pseudonymisierung reduziert den Personenbezug, während Anonymisierung den Personenbezug entfernt, wenn sie tatsächlich reversibel ist.",
        "Pseudonymisierung entfernt den Personenbezug, während Anonymisierung den Personenbezug reduziert, wenn sie tatsächlich reversibel ist."
      ],
      "answer": 0,
      "explanation": "Pseudonymisierung ersetzt Identifikatoren, lässt aber prinzipiell eine Re-Identifikation zu und bleibt daher oft personenbezogen. Anonymisierung entfernt den Personenbezug nur dann, wenn sie wirklich irreversibel ist. Die anderen Optionen vertauschen oder verwässern diese Abgrenzung.",
      "weight": 2,
      "topic": "Datenqualität & Leakage",
      "concept": "Datenschutz",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Pseudonymisierung vs. Anonymisierung",
        "steps": [
          "Bestimme, ob ein Personenbezug noch hergestellt werden kann.",
          "Nutze Pseudonymisierung, wenn du Identifikatoren ersetzen willst, aber ggf. Rückbezug brauchst.",
          "Nutze Anonymisierung nur, wenn Re-Identifikation praktisch ausgeschlossen ist.",
          "Dokumentiere Verarbeitung und Schutzmaßnahmen im DSGVO-Kontext."
        ],
        "content": "DSGVO-Konformität hängt stark am Personenbezug. Pseudonymisierung ist ein Sicherheits- und Minimierungsmechanismus, beseitigt aber den Personenbezug nicht automatisch. Anonymisierung ist nur dann wirklich anonym, wenn eine Re-Identifikation nicht mehr möglich ist."
      },
      "mini_glossary": [
        {
          "term": "DSGVO",
          "definition": "Datenschutz-Grundverordnung; EU-Regelwerk zur Verarbeitung personenbezogener Daten."
        },
        {
          "term": "Personenbezug",
          "definition": "Bezug einer Information zu einer identifizierten oder identifizierbaren Person."
        },
        {
          "term": "Pseudonymisierung",
          "definition": "Ersetzung direkter Identifikatoren durch Pseudonyme, sodass eine Zuordnung nur mit Zusatzinformation möglich ist."
        },
        {
          "term": "Anonymisierung",
          "definition": "Entfernung des Personenbezugs, sodass eine Re-Identifikation nicht mehr möglich ist."
        }
      ]
    },
    {
      "question": "172. Du willst in einer Pipeline früh Datenfehler erkennen. Welche Maßnahme ist am passendsten?",
      "options": [
        "Data Quality durch Schema-Validierung und Constraints absichern, damit fehlerhafte Daten früh blockiert oder markiert werden.",
        "Data Quality durch Schema-Validierung und Constraints absichern, damit fehlerhafte Daten spät blockiert oder markiert werden.",
        "Data Quality durch manuelle Sichtprüfung absichern, damit fehlerhafte Daten früh blockiert oder markiert werden.",
        "Data Quality durch manuelle Sichtprüfung absichern, damit fehlerhafte Daten spät blockiert oder markiert werden."
      ],
      "answer": 0,
      "explanation": "Automatisierte Schema-Validierung und Constraints sind skalierbar und erkennen Fehler früh. Das verhindert, dass schlechte Daten downstream Analysen verfälschen. Manuelle Sichtprüfung ist bei Big Data nicht zuverlässig skalierbar.",
      "weight": 2,
      "topic": "Datenqualität & Leakage",
      "concept": "Validierung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Frühe Data-Quality-Gates",
        "steps": [
          "Definiere ein Schema, das Typen und Felder eindeutig beschreibt.",
          "Formuliere Constraints, z. B. Wertebereiche oder Nicht-Null-Regeln.",
          "Validiere eingehende Daten automatisch beim Ingest oder vor kritischen Transformationen.",
          "Leite fehlerhafte Daten in Quarantäne oder markiere sie für Korrektur."
        ],
        "content": "Data Quality ist in Big-Data-Pipelines ein Systemthema, kein manueller Prozess. Schema-Validierung erkennt strukturelle Abweichungen, Constraints erkennen inhaltliche Verletzungen. Beides ermöglicht frühzeitige, reproduzierbare Qualitätssicherung."
      },
      "mini_glossary": [
        {
          "term": "Data Quality",
          "definition": "Maß dafür, ob Daten korrekt, vollständig, konsistent und für den Zweck geeignet sind."
        },
        {
          "term": "Schema",
          "definition": "Strukturbeschreibung von Daten, z. B. Felder, Datentypen und erlaubte Formen."
        },
        {
          "term": "Validierung",
          "definition": "Automatisiertes Prüfen, ob Daten einem Schema und Regeln entsprechen."
        },
        {
          "term": "Constraint",
          "definition": "Regel/Restriktion, z. B. Wertebereich, Eindeutigkeit oder Nicht-Null-Anforderung."
        }
      ]
    },
    {
      "question": "173. Du in Elasticsearch häufig Aggregationen über große Datenmengen ausführen. Welche Konfiguration ist am geeignetsten?",
      "options": [
        "Elasticsearch so modellieren, dass das Mapping Aggregation-geeignete Felder definiert und der Inverted Index effiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-geeignete Felder definiert und der Inverted Index ineffiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-ungeeignete Felder definiert und der Inverted Index effiziente Abfragen unterstützt.",
        "Elasticsearch so modellieren, dass das Mapping Aggregation-ungeeignete Felder definiert und der Inverted Index ineffiziente Abfragen unterstützt."
      ],
      "answer": 0,
      "explanation": "Für Aggregationen müssen Felder passend gemappt sein, damit sie effizient aggregierbar sind. Der Inverted Index unterstützt schnelle Suche und Filter, was Aggregationen vorbereitet. Falsches Mapping bremst oder verhindert sinnvolle Aggregationen.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Aggregation Modeling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Aggregationen in Elasticsearch effizient machen",
        "content": "Elasticsearch lebt von korrekter Feldmodellierung. Wenn das Mapping Aggregationsfelder sauber definiert, können Aggregationen effizient ausgeführt werden, während der Inverted Index Filter und Suche beschleunigt. Unpassende Feldtypen führen zu teuren Workarounds oder unbrauchbaren Ergebnissen.",
        "steps": [
          "Definiere im Mapping, welche Felder für Aggregation gedacht sind.",
          "Nutze den Inverted Index für schnelle Filterung vor der Aggregation.",
          "Vermeide ungeeignete Feldtypen für Aggregationen.",
          "Teste Aggregationen mit realistischen Query-Patterns und Datenvolumen."
        ]
      },
      "mini_glossary": [
        {
          "term": "Elasticsearch",
          "definition": "Such- und Analyse-Engine, die Daten indexiert und Abfragen sowie Aggregationen schnell ausführt."
        },
        {
          "term": "Mapping",
          "definition": "Feld- und Typdefinition in Elasticsearch, die bestimmt, wie Daten indexiert und genutzt werden."
        },
        {
          "term": "Inverted Index",
          "definition": "Indexstruktur, die von Begriffen auf Dokumente verweist und schnelle Suche/Filter ermöglicht."
        },
        {
          "term": "Aggregation",
          "definition": "Zusammenfassung/Statistik über Daten, z. B. Counts, Summen oder Verteilungen nach Gruppen."
        }
      ]
    },
    {
      "question": "174. Du brauchst hohe Haltbarkeit in Object Storage und willst Kosten senken. Welche Wahl ist am passendsten?",
      "options": [
        "Erasure Coding statt vollständiger Replikation nutzen, um Haltbarkeit mit weniger Speicher-Overhead zu erreichen.",
        "Erasure Coding statt vollständiger Replikation nutzen, um Haltbarkeit mit mehr Speicher-Overhead zu erreichen.",
        "Vollständige Replikation statt Erasure Coding nutzen, um Haltbarkeit mit weniger Speicher-Overhead zu erreichen.",
        "Vollständige Replikation statt Erasure Coding nutzen, um Haltbarkeit mit mehr Speicher-Overhead zu erreichen."
      ],
      "answer": 0,
      "explanation": "Erasure Coding verteilt Redundanz effizienter als vollständige Replikation und senkt typischerweise den Speicher-Overhead bei hoher Haltbarkeit. Replikation ist einfacher, aber meist speicherintensiver. Die anderen Optionen verdrehen diese Zusammenhänge.",
      "weight": 2,
      "topic": "Big Data & Systeme",
      "concept": "Durability vs Cost",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Redundanzstrategien für Haltbarkeit",
        "steps": [
          "Definiere die geforderte Haltbarkeit für dein Object Storage.",
          "Vergleiche Replikation und Erasure Coding hinsichtlich Speicher-Overhead.",
          "Wähle Erasure Coding, wenn du Haltbarkeit bei geringerem Overhead brauchst.",
          "Berücksichtige Wiederherstellungs- und Latenz-Tradeoffs je nach System."
        ],
        "content": "Erasure Coding erreicht Haltbarkeit durch kodierte Redundanz über mehrere Fragmente und reduziert damit häufig den Speicherbedarf gegenüber Mehrfachkopien. Replikation ist konzeptionell simpel, benötigt aber meist mehr Speicher. In vielen Object-Storage-Systemen wird Erasure Coding daher für kosteneffiziente Haltbarkeit eingesetzt."
      },
      "mini_glossary": [
        {
          "term": "Object Storage",
          "definition": "Skalierbarer Speicher für Objekte/Dateien, häufig mit eingebauter Redundanz für Haltbarkeit."
        },
        {
          "term": "Haltbarkeit",
          "definition": "Wahrscheinlichkeit, dass Daten über Zeit nicht verloren gehen, typischerweise durch Redundanzmechanismen."
        },
        {
          "term": "Replikation",
          "definition": "Speichern mehrerer vollständiger Kopien derselben Daten auf unterschiedlichen Knoten/Standorten."
        },
        {
          "term": "Erasure Coding",
          "definition": "Kodierung, bei der Daten in Fragmente plus Redundanzfragmente zerlegt werden, um Verluste zu tolerieren."
        }
      ]
    },
    {
      "question": "175. Du betreibst nur Stream Processing und willst die Komplexität minimieren. Welche Architektur ist am passendsten?",
      "options": [
        "Kappa Architecture statt Lambda Architecture wählen, weil Stream Processing den Datenpfad vereinheitlicht und Batch entfällt.",
        "Kappa Architecture statt Lambda Architecture wählen, weil Stream Processing den Datenpfad aufspaltet und Batch entfällt.",
        "Lambda Architecture statt Kappa Architecture wählen, weil Stream Processing den Datenpfad vereinheitlicht und Batch entfällt.",
        "Lambda Architecture statt Kappa Architecture wählen, weil Stream Processing den Datenpfad aufspaltet und Batch entfällt."
      ],
      "answer": 0,
      "explanation": "Kappa setzt auf einen einheitlichen Stream-Pfad und vermeidet getrennte Batch- und Speed-Layer. Lambda führt typischerweise einen Batch- und einen Stream-Pfad parallel, was Komplexität erhöht. Wenn Batch nicht benötigt wird, passt Kappa oft besser.",
      "weight": 3,
      "topic": "Big Data & Systeme",
      "concept": "Lambda vs Kappa",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Wann Kappa einfacher ist als Lambda",
        "steps": [
          "Prüfe, ob du einen separaten Batch-Pfad wirklich brauchst.",
          "Bewerte die Komplexität von zwei parallelen Datenpfaden in Lambda.",
          "Wähle Kappa, wenn Stream Processing alle Anforderungen abdeckt.",
          "Stelle sicher, dass Reprocessing und Backfills über den Stream-Pfad sauber möglich sind."
        ],
        "content": "Lambda Architecture trennt Batch und Stream, um sowohl Genauigkeit als auch niedrige Latenz abzudecken, zahlt aber mit zwei Pfaden und doppelter Logik. Kappa Architecture reduziert diese Komplexität, indem sie Stream Processing zum zentralen Pfad macht. Das ist besonders attraktiv, wenn Batch-Anforderungen gering sind oder anders gelöst werden können."
      },
      "mini_glossary": [
        {
          "term": "Lambda Architecture",
          "definition": "Architektur mit getrenntem Batch- und Stream-Pfad, die Ergebnisse zusammenführt."
        },
        {
          "term": "Kappa Architecture",
          "definition": "Architektur mit einem einheitlichen Stream-Pfad, in dem auch Reprocessing über den Stream erfolgt."
        },
        {
          "term": "Stream Processing",
          "definition": "Kontinuierliche Verarbeitung von Events, typischerweise mit geringer Latenz."
        },
        {
          "term": "Batch",
          "definition": "Periodische Verarbeitung großer Datenmengen in diskreten Jobs mit höherer Latenz."
        }
      ]
    },
    {
      "question": "176. Ein System erlebt eine Partition. Welche Aussage ist im Sinne des CAP-Theorems am passendsten?",
      "options": [
        "Bei Partitionstoleranz muss ein System zwischen Konsistenz und Verfügbarkeit abwägen, weil nicht beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System zwischen Konsistenz und Verfügbarkeit abwägen, weil beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System Konsistenz und Verfügbarkeit nicht abwägen, weil nicht beides gleichzeitig garantiert werden kann.",
        "Bei Partitionstoleranz muss ein System Konsistenz und Verfügbarkeit nicht abwägen, weil beides gleichzeitig garantiert werden kann."
      ],
      "answer": 0,
      "explanation": "CAP beschreibt den Tradeoff zwischen Konsistenz und Verfügbarkeit unter Partitionen. Wenn Partitionstoleranz nötig ist, kann das System bei einer Partition nicht gleichzeitig starke Konsistenz und volle Verfügbarkeit garantieren. Die falschen Optionen widersprechen diesem Grundsatz.",
      "weight": 3,
      "topic": "Big Data & Systeme",
      "concept": "CAP Tradeoff",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "CAP-Theorem unter Partitionen",
        "content": "Unter Netzwerkpartitionen sind vollständige Garantien schwer. Systeme müssen entscheiden, ob sie lieber konsistente Antworten liefern (und dann ggf. Requests ablehnen) oder lieber verfügbar bleiben (und dann ggf. inkonsistente Zustände zulassen). Diese Entscheidung sollte aus den Anforderungen und Fehlertoleranzen abgeleitet werden.",
        "steps": [
          "Nimm Partitionstoleranz als realistische Anforderung in verteilten Systemen an.",
          "Entscheide, ob Konsistenz oder Verfügbarkeit im Partition-Fall wichtiger ist.",
          "Passe Datenmodell und Client-Verhalten an die gewählte Priorität an.",
          "Dokumentiere die Semantik, damit Nutzer die Tradeoffs verstehen."
        ]
      },
      "mini_glossary": [
        {
          "term": "CAP-Theorem",
          "definition": "Aussage, dass bei Partitionen Konsistenz und Verfügbarkeit nicht gleichzeitig vollständig garantiert werden können."
        },
        {
          "term": "Konsistenz",
          "definition": "Eigenschaft, dass alle Clients einen logisch einheitlichen Zustand sehen, z. B. nach einem Write."
        },
        {
          "term": "Verfügbarkeit",
          "definition": "Eigenschaft, dass das System auf Requests antwortet, auch bei Teilfehlern."
        },
        {
          "term": "Partitionstoleranz",
          "definition": "Eigenschaft, dass das System trotz Netzwerkpartitionen weiter funktioniert (mit Tradeoffs)."
        }
      ]
    },
    {
      "question": "177. Du willst Data Lake und Warehouse vereinen, inklusive Transaktionssicherheit. Welche Aussage ist am passendsten?",
      "options": [
        "Ein Lakehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Analysen zusammenzubringen.",
        "Ein Lakehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Schreibzugriffe zu trennen.",
        "Ein Warehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Analysen zusammenzubringen.",
        "Ein Warehouse kombiniert Data Lake mit ACID-Eigenschaften und einem Metadatenkatalog, um Governance und Schreibzugriffe zu trennen."
      ],
      "answer": 0,
      "explanation": "Lakehouse ist ein Konzept, das Data-Lake-Flexibilität mit Warehouse-ähnlicher Zuverlässigkeit verbindet. ACID-Eigenschaften und ein Metadatenkatalog unterstützen Governance und konsistente Analysen. Die anderen Optionen verwechseln Begriffe oder Ziele.",
      "weight": 3,
      "topic": "Datenqualität & Leakage",
      "concept": "Lakehouse",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Warum Lakehouse ein Brückenkonzept ist",
        "content": "Ein reiner Data Lake ist flexibel, aber kann ohne zusätzliche Mechanismen in Governance und Konsistenz schwächeln. Lakehouse adressiert genau diese Lücke durch ACID-ähnliche Semantik und Katalogisierung. So können Daten kuratiert, nachvollziehbar und analytisch nutzbar werden, ohne die Flexibilität vollständig aufzugeben.",
        "steps": [
          "Nutze Data Lake als flexible Ablage für Roh- und Kurationsdaten.",
          "Füge ACID-Eigenschaften hinzu, um konsistente Tabellen-Operationen zu ermöglichen.",
          "Setze einen Metadatenkatalog ein, um Governance und Auffindbarkeit zu verbessern.",
          "Bewerte, ob Warehouse-Anforderungen (z. B. SLAs) durch das Lakehouse erfüllt werden."
        ]
      },
      "mini_glossary": [
        {
          "term": "Lakehouse",
          "definition": "Architekturidee, die Data-Lake-Speicherung mit Warehouse-ähnlichen Eigenschaften für Analysen und Governance verbindet."
        },
        {
          "term": "Data Lake",
          "definition": "Flexibler Speicher für Roh- und Halbstrukturierte Daten, häufig mit Schema-on-read."
        },
        {
          "term": "ACID",
          "definition": "Transaktionseigenschaften (Atomarität, Konsistenz, Isolation, Dauerhaftigkeit) für zuverlässige Datenoperationen."
        },
        {
          "term": "Metadatenkatalog",
          "definition": "Verzeichnis, das Datenbestände beschreibt, auffindbar macht und Governance unterstützt."
        }
      ]
    },
    {
      "question": "178. Du willst veröffentlichte Statistik-Ergebnisse schützen, ohne sie unbrauchbar zu machen. Welche Aussage ist am passendsten?",
      "options": [
        "Differential Privacy nutzt Rauschen und ein Privacy Budget, um Utility zu erhalten und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen und ein Privacy Budget, um Utility zu entfernen und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen ohne Privacy Budget, um Utility zu erhalten und individuelle Beiträge zu schützen.",
        "Differential Privacy nutzt Rauschen ohne Privacy Budget, um Utility zu entfernen und individuelle Beiträge zu schützen."
      ],
      "answer": 0,
      "explanation": "Differential Privacy fügt kontrolliertes Rauschen hinzu und steuert den Schutz über ein Privacy Budget. Ziel ist ein sinnvolles Gleichgewicht zwischen Utility und Datenschutz. Ohne Budget fehlt eine klare Steuerung der Schutzstärke, und \"Utility entfernen\" ist nicht das Ziel.",
      "weight": 3,
      "topic": "Datenqualität & Leakage",
      "concept": "Differential Privacy",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Utility und Datenschutz balancieren",
        "steps": [
          "Definiere, welche Statistik-Ergebnisse du veröffentlichen willst.",
          "Füge Rauschen so hinzu, dass einzelne Beiträge nicht zuverlässig ableitbar sind.",
          "Steuere die Schutzstärke über ein Privacy Budget.",
          "Überprüfe Utility, damit Ergebnisse weiterhin interpretierbar bleiben."
        ],
        "content": "Differential Privacy ist ein formaler Ansatz, der den Einfluss einzelner Datensätze auf veröffentlichte Statistiken begrenzt. Rauschen sorgt für Schutz, das Privacy Budget steuert die Stärke und akkumuliert über Abfragen. Der Kern ist das Abwägen von Utility und Schutz, nicht das vollständige Unbrauchbarmachen der Ergebnisse."
      },
      "mini_glossary": [
        {
          "term": "Differential Privacy",
          "definition": "Datenschutzkonzept, das den Einfluss einzelner Datensätze auf veröffentlichte Ergebnisse mathematisch begrenzt."
        },
        {
          "term": "Rauschen",
          "definition": "Gezielte Zufallsstörung von Ergebnissen, um Rückschlüsse auf einzelne Beiträge zu erschweren."
        },
        {
          "term": "Privacy Budget",
          "definition": "Parameter, der die Gesamtmenge an \"Datenschutzverbrauch\" über Abfragen beschreibt und die Schutzstärke steuert."
        },
        {
          "term": "Utility",
          "definition": "Nützlichkeit/Aussagekraft der veröffentlichten Ergebnisse trotz Datenschutzmechanismen."
        }
      ]
    },
    {
      "question": "179. Welcher fundamentale Unterschied besteht zwischen Variablen in Python und Java, der für Data Science relevant ist?",
      "options": [
        "Python-Variablen sind statisch typisiert, Java-Variablen dynamisch – im Kontext.",
        "Python ist dynamisch typisiert; Variablen sind Referenzen auf Objekte, keine festen Typ-Container.",
        "In Python müssen Variablen immer vor der Verwendung deklariert werden – im Kontext.",
        "Python erlaubt keine primitiven Datentypen wie Integer oder Float – im Kontext."
      ],
      "answer": 1,
      "explanation": "Python ist dynamisch typisiert. Im Gegensatz zu Java, wo eine Variable `int x` nur Integer speichern kann, ist ein Variablenname in Python nur ein Label (Referenz), das auf beliebige Objekte zeigen kann.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Dynamische Typisierung",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Dynamische Typisierung",
          "definition": "Typüberprüfung zur Laufzeit; Variablen sind nicht an einen Typ gebunden."
        },
        {
          "term": "Referenz",
          "definition": "Verweis auf den Speicherort eines Objekts."
        }
      ]
    },
    {
      "question": "180. Wie wird in Python ein Code-Block (z. B. innerhalb einer Schleife) definiert?",
      "options": [
        "Durch geschweifte Klammern `{ }` wie in Java.",
        "Durch das Schlüsselwort `begin` und `end`.",
        "Durch Einrückung (Indentation).",
        "Durch Semikolons am Zeilenende."
      ],
      "answer": 2,
      "explanation": "Python nutzt Einrückungen (Whitespace), um Blöcke zu strukturieren. Dies erzwingt lesbaren Code und ersetzt die in Java üblichen geschweiften Klammern.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Syntax & Indentation",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Indentation",
          "definition": "Einrückung von Codezeilen zur Strukturierung."
        }
      ]
    },
    {
      "question": "181. Was ist das Ergebnis des Ausdrucks `[x**2 for x in range(4)]`?",
      "options": [
        "Eine Fehlermeldung, da die Syntax falsch ist.",
        "`[0, 1, 4, 9]` – im Kurskontext.",
        "`[1, 4, 9, 16]` – in der Praxis.",
        "`{0, 1, 4, 9}` – im Kurskontext."
      ],
      "answer": 1,
      "explanation": "Dies ist eine List Comprehension. `range(4)` erzeugt 0, 1, 2, 3. Das Quadrat davon ist 0, 1, 4, 9.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "List Comprehensions",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Auflösung der List Comprehension",
        "steps": [
          "Identifiziere den Generator: `range(4)` liefert die Werte 0, 1, 2, 3.",
          "Wende die Operation an: `x**2` quadriert jeden Wert.",
          "Berechne: 0²=0, 1²=1, 2²=4, 3²=9.",
          "Sammle in Liste: `[0, 1, 4, 9]`."
        ],
        "content": "List Comprehensions bieten eine kompakte Syntax, um Listen basierend auf anderen Iterables zu erstellen, und ersetzen oft mehrzeilige For-Schleifen."
      },
      "mini_glossary": [
        {
          "term": "List Comprehension",
          "definition": "Kompakte Syntax zur Erstellung von Listen."
        },
        {
          "term": "range()",
          "definition": "Funktion, die eine Sequenz von Zahlen generiert."
        }
      ]
    },
    {
      "question": "182. Welche Bibliothek ist der De-facto-Standard für numerische Berechnungen und Arrays in Python?",
      "options": [
        "Pandas – in der Praxis.",
        "NumPy – in der Praxis.",
        "Matplotlib – im Kontext.",
        "Scikit-learn – im Kontext."
      ],
      "answer": 1,
      "explanation": "NumPy (Numerical Python) ist die Basisbibliothek für effiziente Arrays und mathematische Operationen.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Bibliotheken-Ökosystem",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "NumPy",
          "definition": "Bibliothek für multidimensionale Arrays und Matrizen."
        }
      ]
    },
    {
      "question": "183. Gegeben ist ein NumPy Array `arr = np.array([1, 2, 3])`. Was passiert bei `arr * 2`?",
      "options": [
        "Das Array wird dupliziert: `[1, 2, 3, 1, 2, 3]`.",
        "Es entsteht ein Fehler, da man Arrays nicht mit Skalaren multiplizieren kann.",
        "Jedes Element wird multipliziert: `[2, 4, 6]`.",
        "Die Länge des Arrays wird verdoppelt."
      ],
      "answer": 2,
      "explanation": "NumPy unterstützt Vektorisierung. Operationen mit Skalaren werden elementweise auf das gesamte Array angewendet (Broadcasting).",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Vektorisierung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Vektorisierung in NumPy",
        "steps": [
          "NumPy erkennt die Multiplikation eines Arrays mit einem Skalar.",
          "Der Skalar (2) wird auf jedes Element des Arrays angewendet.",
          "Rechnung: 1*2=2, 2*2=4, 3*2=6.",
          "Ergebnis ist ein neues Array gleicher Form."
        ],
        "content": "Im Gegensatz zu Java-Listen (wo man eine Schleife bräuchte) oder Python-Listen (wo `*` die Liste wiederholt), rechnet NumPy mathematisch elementweise."
      },
      "mini_glossary": [
        {
          "term": "Vektorisierung",
          "definition": "Ausführung von Operationen auf ganzen Arrays ohne explizite Schleifen."
        },
        {
          "term": "Broadcasting",
          "definition": "Mechanismus zur Behandlung von Arrays unterschiedlicher Form bei Operationen."
        }
      ]
    },
    {
      "question": "184. Sie haben ein Pandas DataFrame `df`. Mit welchem Befehl erhalten Sie eine schnelle statistische Zusammenfassung (Mittelwert, Min, Max etc.) der numerischen Spalten?",
      "options": [
        "`df.info()` (Struktur)",
        "`df.head()` (erste Zeilen)",
        "`df.describe()` (Statistik)",
        "`df.stats()` (kein Standard)"
      ],
      "answer": 2,
      "explanation": "`df.describe()` generiert deskriptive Statistiken für alle numerischen Spalten. `df.info()` zeigt Datentypen und Nicht-Null-Werte.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Daten-Exploration",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "DataFrame",
          "definition": "Zweidimensionale, tabellarische Datenstruktur in Pandas."
        },
        {
          "term": "Deskriptive Statistik",
          "definition": "Zusammenfassende Kennzahlen wie Mittelwert und Standardabweichung."
        }
      ]
    },
    {
      "question": "185. Wie wählen Sie in einem Pandas DataFrame `df` die Spalte 'Preis' aus?",
      "options": [
        "`df.get('Preis')` – in der Praxis.",
        "`df['Preis']` oder `df.Preis`",
        "`df(Preis)` – in der Praxis.",
        "`df->Preis` – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Die Standard-Syntax ist `df['Spaltenname']`. Wenn der Name keine Leerzeichen oder Sonderzeichen enthält, geht oft auch die Attribut-Schreibweise `df.Spaltenname`.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Spalten-Selektion",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Series",
          "definition": "Eindimensionales Array-ähnliches Objekt, das eine Spalte repräsentiert."
        }
      ]
    },
    {
      "question": "186. Gegeben ist `df` mit Index 0 bis 10. Was liefert `df.iloc[0:3]`?",
      "options": [
        "Die Zeilen mit den Index-Labels 0, 1, 2 und 3.",
        "Die Zeilen an den Positionen 0, 1 und 2.",
        "Nur die Zeile an Position 3.",
        "Eine Fehlermeldung."
      ],
      "answer": 1,
      "explanation": "`iloc` arbeitet positionsbasiert (integer-location). Wie beim Python-Slicing ist der Endwert exklusiv. Es werden die Zeilen an Position 0, 1 und 2 zurückgegeben.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "iloc vs loc",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Funktionsweise von iloc",
        "steps": [
          "Verstehe `iloc`: Integer-Location (basierend auf Position, nicht Label).",
          "Analysiere den Slice `0:3`.",
          "Start: 0 (inklusive).",
          "Ende: 3 (exklusiv, typisch für Python).",
          "Ergebnis: Zeilen an Position 0, 1, 2."
        ],
        "content": "Dies ist ein häufiger Stolperstein für Umsteiger: `loc[0:3]` würde (bei numerischem Index) auch die 3 einschließen (Label-basiert), während `iloc` strikt Python-Slicing-Regeln folgt."
      },
      "mini_glossary": [
        {
          "term": "iloc",
          "definition": "Positionsbasierte Indizierung in Pandas."
        },
        {
          "term": "Slicing",
          "definition": "Ausschneiden eines Teilbereichs aus einer Sequenz."
        }
      ]
    },
    {
      "question": "187. Sie möchten Zeilen filtern, in denen die Spalte 'Alter' größer als 30 ist. Welcher Code ist korrekt?",
      "options": [
        "`df[df['Alter'] > 30]`",
        "`df.filter('Alter' > 30)`",
        "`df.where(Alter > 30)`",
        "`if df['Alter'] > 30: return df`"
      ],
      "answer": 0,
      "explanation": "Dies nennt man 'Boolean Indexing' oder 'Masking'. Der innere Ausdruck `df['Alter'] > 30` erzeugt eine Serie von Wahrheitswerten, die dann zum Filtern des DataFrames genutzt wird.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Boolean Indexing",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Boolean Masking",
        "steps": [
          "Erstelle Bedingung: `df['Alter'] > 30` ergibt eine Series aus True/False.",
          "Wende Maske an: `df[...]` nutzt diese Series.",
          "Filterung: Nur Zeilen, wo 'True' steht, bleiben erhalten.",
          "Ergebnis: Ein neuer DataFrame mit den gefilterten Daten."
        ],
        "content": "Dies ist der Standardweg in Pandas und NumPy, um Daten effizient ohne Schleifen zu filtern."
      },
      "mini_glossary": [
        {
          "term": "Boolean Mask",
          "definition": "Array aus Wahrheitswerten zur Filterung von Daten."
        }
      ]
    },
    {
      "question": "188. Ein DataFrame enthält `NaN` Werte. Welche Methode entfernt alle Zeilen, die mindestens einen fehlenden Wert enthalten?",
      "options": [
        "`df.fillna(0)`",
        "`df.dropna()`",
        "`df.remove_nulls()`",
        "`df.isnull()`"
      ],
      "answer": 1,
      "explanation": "`dropna()` entfernt standardmäßig alle Zeilen (`axis=0`), die irgendeinen (`how='any'`) fehlenden Wert enthalten.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Missing Values",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "NaN",
          "definition": "Not a Number; Platzhalter für fehlende Werte."
        },
        {
          "term": "Imputation",
          "definition": "Ersetzen fehlender Werte (Gegenteil von Löschen)."
        }
      ]
    },
    {
      "question": "189. Was bewirkt `df.groupby('Kategorie')['Umsatz'].sum()`?",
      "options": [
        "Es sortiert die Tabelle nach Kategorie und Umsatz – in der Praxis – im Kontext.",
        "Es gruppiert die Daten nach Kategorie und berechnet die Summe der Umsatz-Spalte für jede Gruppe.",
        "Es addiert 'Kategorie' und 'Umsatz' zusammen – im Kurskontext – im Kontext.",
        "Es filtert alle Zeilen, die keinen Umsatz haben – in der Praxis – im Kontext."
      ],
      "answer": 1,
      "explanation": "Dies ist das 'Split-Apply-Combine' Muster. Die Daten werden nach 'Kategorie' gesplittet, und die Summen-Funktion wird auf 'Umsatz' angewendet.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Groupby",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Split-Apply-Combine",
        "steps": [
          "Split: Teile Daten in Gruppen basierend auf eindeutigen Werten in 'Kategorie'.",
          "Select: Wähle die Spalte 'Umsatz' in jeder Gruppe.",
          "Apply: Berechne die Summe für diese Spalte pro Gruppe.",
          "Combine: Füge Ergebnisse zu neuem DataFrame/Series zusammen."
        ],
        "content": "Ähnlich wie `GROUP BY` in SQL ist dies essenziell für aggregierte Analysen."
      },
      "mini_glossary": [
        {
          "term": "Aggregation",
          "definition": "Zusammenfassen mehrerer Werte zu einem (z. B. Summe, Mittelwert)."
        }
      ]
    },
    {
      "question": "190. Sie wollen eine CSV-Datei 'data.csv' einlesen. Welcher Befehl ist korrekt?",
      "options": [
        "`pd.read_file('data.csv')`",
        "`pd.import_csv('data.csv')`",
        "`pd.read_csv('data.csv')`",
        "`new DataFrame('data.csv')`"
      ],
      "answer": 2,
      "explanation": "`pd.read_csv()` ist die Standardfunktion in Pandas zum Einlesen von Comma Separated Values.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Datenimport",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "CSV",
          "definition": "Comma Separated Values; einfaches Textformat für Tabellen."
        }
      ]
    },
    {
      "question": "191. In einem Boxplot ist der Median als Linie eingezeichnet. Was repräsentiert die 'Box' selbst?",
      "options": [
        "Den Bereich von Minimum bis Maximum – in der Praxis – im Kontext.",
        "Den Interquartilsabstand, also den Bereich zwischen dem 25. und 75. Perzentil.",
        "Die Standardabweichung vom Mittelwert – in der Praxis – im Kontext.",
        "Alle Ausreißer – im beschriebenen Szenario – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Die Box in einem Boxplot spannt sich vom 1. Quartil (Q1) bis zum 3. Quartil (Q3) und enthält somit die mittleren 50% der Daten.",
      "weight": 2,
      "topic": "Datenvisualisierung",
      "concept": "Boxplot-Interpretation",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Aufbau eines Boxplots",
        "steps": [
          "Untere Kante der Box: Q1 (25% der Daten sind kleiner).",
          "Obere Kante der Box: Q3 (75% der Daten sind kleiner).",
          "Höhe der Box: IQR = Q3 - Q1.",
          "Linie in der Box: Median (50%)."
        ],
        "content": "Boxplots sind ideal, um Verteilungen und Ausreißer (Punkte außerhalb der 'Whisker') schnell zu erkennen."
      },
      "mini_glossary": [
        {
          "term": "IQR",
          "definition": "Interquartilsabstand; Maß für die Streuung."
        },
        {
          "term": "Perzentil",
          "definition": "Wert, unterhalb dessen ein bestimmter Prozentsatz der Daten liegt."
        }
      ]
    },
    {
      "question": "192. Sie führen folgenden Code aus: `df['A'] = df['B']; df['A'].iloc[0] = 10`. Später stellen Sie fest, dass sich auch `df['B']` geändert hat. Warum?",
      "options": [
        "Das ist ein Bug in Pandas – in der Praxis.",
        "Pandas kopiert Daten standardmäßig immer.",
        "Dies ist eine 'SettingWithCopy'-Situation",
        "Spalte A und B teilen sich physikalisch immer den Speicher."
      ],
      "answer": 2,
      "explanation": "In Python und Pandas führen einfache Zuweisungen oft nur zu Referenzen oder Views auf denselben Speicher. Um eine unabhängige Kopie zu erhalten, muss `.copy()` verwendet werden.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Views vs Copies",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Referenzverhalten analysieren",
        "steps": [
          "Zuweisung `df['A'] = df['B']` erstellt oft keine neuen Daten, sondern verweist auf dieselbe Series.",
          "Änderung in 'A' greift auf denselben Speicher zu wie 'B'.",
          "Lösung: Explizites `df['A'] = df['B'].copy()` nutzen."
        ],
        "content": "Dieses Verhalten ist für Java-Entwickler, die 'Call-by-Value' bei Primitiven gewohnt sind, eine häufige Fehlerquelle bei Objekten."
      },
      "mini_glossary": [
        {
          "term": "Deep Copy",
          "definition": "Vollständige Kopie aller Daten und Unterobjekte."
        },
        {
          "term": "View",
          "definition": "Sicht auf Daten, ohne den Speicher zu duplizieren."
        }
      ]
    },
    {
      "question": "193. Was ist der Hauptvorteil der Verwendung von `apply()` mit einer Lambda-Funktion gegenüber einer Iteration mit `for`-Schleife über Zeilen in Pandas?",
      "options": [
        "`apply()` ist immer 100x schneller als jede andere Methode.",
        "Es ist syntaktisch kürzer und oft schneller als eine naive Python-Schleife",
        "Es erlaubt Zugriff auf private Variablen – in der Praxis.",
        "Es gibt keinen Unterschied – in diesem Beispiel – im Kontext."
      ],
      "answer": 1,
      "explanation": "`apply()` ist performanter als eine explizite `for`-Schleife in reinem Python, da der Loop intern in C optimiert ist, aber es ist immer noch langsamer als echte Vektorisierung (direkte Spalten-Operationen).",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Apply vs Loops",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Performance-Hierarchie",
        "steps": [
          "1. Platz: Vektorisierung (NumPy/Pandas native Ops).",
          "2. Platz: `apply()` (interner Iterator).",
          "3. Platz: `iterrows()` (langsamer Iterator).",
          "4. Platz: Manuelle Index-Schleife (sehr langsam)."
        ],
        "content": "Verwenden Sie `apply` nur, wenn keine vektorisierte Lösung (wie `df['A'] + df['B']`) möglich ist."
      },
      "mini_glossary": [
        {
          "term": "Lambda-Funktion",
          "definition": "Anonyme kleine Funktion, oft inline definiert."
        }
      ]
    },
    {
      "question": "194. Sie haben zwei DataFrames `df1` (Kunden) und `df2` (Bestellungen). Sie wollen diese verbinden, sodass alle Kunden enthalten sind, auch wenn sie keine Bestellung haben. Welchen Join-Typ wählen Sie?",
      "options": [
        "Inner Join – in der Praxis.",
        "Left Join (wenn df1 links steht)",
        "Right Join (wenn df1 links steht)",
        "Cross Join – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Ein Left Join behält alle Schlüssel aus der linken Tabelle (`df1`) und ergänzt Daten aus der rechten Tabelle (`df2`) dort, wo Treffer sind. Sonst entstehen `NaN`.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Merge/Join Logik",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Join-Logik anwenden",
        "steps": [
          "Ziel: Alle Datensätze aus `df1` (Kunden) behalten.",
          "Strategie: `df1` als linke Tabelle setzen.",
          "Join-Typ: `how='left'`.",
          "Ergebnis: Kunden ohne Bestellung haben `NaN` in den Bestellspalten."
        ],
        "content": "Dies entspricht `LEFT OUTER JOIN` in SQL. Ein Inner Join würde Kunden ohne Bestellung verwerfen."
      },
      "mini_glossary": [
        {
          "term": "Merge",
          "definition": "Zusammenführen von DataFrames basierend auf Schlüsseln."
        }
      ]
    },
    {
      "question": "195. Welche Aussage zur Speicherverwaltung von Pandas 'category' Datentyp ist korrekt?",
      "options": [
        "Er verbraucht mehr Speicher als 'object' (String), da er Metadaten speichert.",
        "Er ist nur für numerische Daten geeignet – in der Praxis.",
        "Er spart signifikant Speicher bei String-Spalten mit wenigen eindeutigen Werten.",
        "Er verlangsamt Gruppierungsoperationen – in der Praxis."
      ],
      "answer": 2,
      "explanation": "Der Datentyp 'category' speichert die Strings nur einmal in einer Lookup-Tabelle und verwendet im DataFrame kleine Integer-Codes. Bei vielen Wiederholungen spart das massiv Speicher.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Categorical Data",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Speicheroptimierung mit Categories",
        "steps": [
          "Analyse: Spalte 'Farbe' hat 1 Mio Zeilen, aber nur 'Rot', 'Grün', 'Blau'.",
          "String (Object): Speichert 1 Mio mal den String.",
          "Category: Speichert 3 Strings + 1 Mio kleine Integers (0, 1, 2).",
          "Effekt: Drastische Reduktion des RAM-Verbrauchs."
        ],
        "content": "Zusätzlich beschleunigt 'category' oft `groupby` und Sortieroperationen."
      },
      "mini_glossary": [
        {
          "term": "Kardinalität",
          "definition": "Anzahl der eindeutigen Werte in einer Menge."
        }
      ]
    },
    {
      "question": "196. Was ist das Ergebnis von `np.arange(0, 10, 2)`?",
      "options": [
        "`[0, 2, 4, 6, 8, 10]`",
        "`[0, 2, 4, 6, 8]`",
        "`[2, 4, 6, 8]`",
        "`[0, 1, 2, ... 9]`"
      ],
      "answer": 1,
      "explanation": "`arange(start, stop, step)` generiert Werte von Start bis exklusive Stop mit der Schrittweite Step. 10 ist exklusiv.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Array Erstellung",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "arange",
          "definition": "Array Range; NumPy-Äquivalent zu Python range()."
        }
      ]
    },
    {
      "question": "197. Sie haben Zeitreihendaten als String '2023-01-01'. Wie konvertieren Sie die ganze Spalte effizient in echte Datumsobjekte?",
      "options": [
        "Mit einer For-Schleife und `datetime.parse()`.",
        "`pd.to_datetime(df['Spalte'])`",
        "`df['Spalte'].astype('date')`",
        "`df.convert_objects()`"
      ],
      "answer": 1,
      "explanation": "`pd.to_datetime()` ist der leistungsfähige Parser in Pandas, der ganze Spalten intelligent und schnell in Timestamp-Objekte umwandelt.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Datentyp-Konvertierung",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Timestamp",
          "definition": "Zeitstempel-Objekt in Pandas."
        }
      ]
    },
    {
      "question": "198. Welche Bibliothek baut auf Matplotlib auf und bietet eine einfachere Syntax für statistische Grafiken sowie schönere Default-Styles?",
      "options": [
        "NumPy",
        "Seaborn",
        "SciPy",
        "TensorFlow"
      ],
      "answer": 1,
      "explanation": "Seaborn ist eine High-Level-Visualisierungsbibliothek, die eng mit Pandas DataFrames integriert ist und Matplotlib erweitert.",
      "weight": 1,
      "topic": "Datenvisualisierung",
      "concept": "Bibliotheken",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Seaborn",
          "definition": "Bibliothek für statistische Datenvisualisierung."
        }
      ]
    },
    {
      "question": "199. Sie analysieren Verkaufsdaten. `df.groupby('Produkt')['Umsatz'].mean()` liefert Durchschnittswerte. Aber die Ergebnisse wirken verzerrt durch einige extrem hohe Verkäufe. Welche Aggregation ist robuster gegen Ausreißer?",
      "options": [
        "`sum()` (Summe)",
        "`median()` (Median)",
        "`std()` (Streuung)",
        "`max()` (Maximum)"
      ],
      "answer": 1,
      "explanation": "Der Median (Zentralwert) ist im Gegensatz zum Mittelwert (Mean) robust gegenüber extremen Ausreißern, da er nur auf der Rangfolge basiert.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "concept": "Robuste Statistiken",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Median vs Mean",
        "steps": [
          "Szenario: Werte 1, 2, 3, 100.",
          "Mittelwert: (106)/4 = 26.5 (stark beeinflusst durch 100).",
          "Median: Zwischen 2 und 3 => 2.5 (repräsentiert die 'Mitte' besser).",
          "Analyse: Nutzen Sie Median bei schiefen Verteilungen."
        ],
        "content": "Bei Data Science ist die Wahl der richtigen Kennzahl entscheidend für die korrekte Interpretation der Daten."
      },
      "mini_glossary": [
        {
          "term": "Robustheit",
          "definition": "Unempfindlichkeit statistischer Maßzahlen gegenüber Ausreißern."
        }
      ]
    },
    {
      "question": "200. Was bedeutet `axis=1` in Operationen wie `df.drop('col', axis=1)` oder `df.sum(axis=1)`?",
      "options": [
        "Operation entlang der Zeilen (vertikal).",
        "Operation entlang der Spalten (horizontal).",
        "Operation auf der dritten Dimension.",
        "Es gibt keinen Unterschied zu `axis=0`."
      ],
      "answer": 1,
      "explanation": "`axis=0` bezieht sich auf den Index (Zeilen), `axis=1` bezieht sich auf die Spalten. `drop(..., axis=1)` löscht also eine Spalte.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Achsen-Logik",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Achsen verstehen",
        "steps": [
          "Axis 0: Bewegt sich 'nach unten' (über Zeilen hinweg). `sum(axis=0)` summiert Spalten.",
          "Axis 1: Bewegt sich 'nach rechts' (über Spalten hinweg). `sum(axis=1)` summiert Werte einer Zeile.",
          "Eselsbrücke: 1 sieht aus wie ein stehender Strich (Spalte) -> beeinflusst Spalten."
        ],
        "content": "Die korrekte Achsenwahl ist essenziell für Aggregationen und Manipulationen."
      },
      "mini_glossary": [
        {
          "term": "Axis",
          "definition": "Dimension in einem Array/DataFrame (0=Zeilen, 1=Spalten)."
        }
      ]
    },
    {
      "question": "201. Sie wollen zwei DataFrames vertikal untereinander hängen (konkatenieren). Welcher Befehl?",
      "options": [
        "`pd.merge([df1, df2])` – im Kontext.",
        "`pd.concat([df1, df2], axis=0)`",
        "`pd.join(df1, df2)` – im Kontext.",
        "`df1.append_rows(df2)` – im Kontext."
      ],
      "answer": 1,
      "explanation": "`pd.concat()` ist die universelle Funktion zum Aneinanderhängen. `axis=0` (Standard) stapelt vertikal, `axis=1` würde sie nebeneinander stellen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Konkatenation",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Daten stapeln",
        "steps": [
          "Identifiziere Aufgabe: Tabellen 'untereinander' kleben.",
          "Wähle Tool: `pd.concat()` nimmt eine Liste von Objekten.",
          "Wähle Richtung: `axis=0` (Zeilen hinzufügen).",
          "Prüfung: Spaltennamen sollten idealerweise übereinstimmen."
        ],
        "content": "`merge` ist für SQL-artige Joins (Spalten verbinden), `concat` für physisches Zusammenfügen."
      },
      "mini_glossary": [
        {
          "term": "Konkatenation",
          "definition": "Verkettung von Listen oder Arrays."
        }
      ]
    },
    {
      "question": "202. Welche Python-Struktur entspricht am ehesten einer `HashMap` in Java?",
      "options": [
        "List `[]`",
        "Tuple `()`",
        "Dictionary `{}`",
        "Set `{}` – in der Praxis."
      ],
      "answer": 2,
      "explanation": "Ein Dictionary (`dict`) speichert Key-Value-Paare und bietet O(1) Zugriff auf Schlüssel, genau wie eine HashMap.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Datenstrukturen Vergleich",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Dictionary",
          "definition": "Ungeordnete Sammlung von Schlüssel-Wert-Paaren."
        }
      ]
    },
    {
      "question": "203. Ein Histogramm zeigt eine 'Linksschiefe' (left-skewed / negative skew). Wo liegt der Mittelwert im Vergleich zum Median?",
      "options": [
        "Mittelwert > Median",
        "Mittelwert < Median",
        "Mittelwert == Median",
        "Kann man nicht sagen."
      ],
      "answer": 1,
      "explanation": "Bei einer Linksschiefe (langer Schwanz nach links, zu kleinen Werten) ziehen die kleinen Ausreißer den Mittelwert nach unten. Der Median bleibt weiter rechts.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "concept": "Verteilungsformen",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Schiefe Verteilungen",
        "steps": [
          "Visualisiere: Der 'Berg' ist rechts, der flache Ausläufer links.",
          "Einfluss: Ausreißer im linken Ausläufer (sehr kleine Werte).",
          "Mittelwert: Reagiert stark, wandert nach links.",
          "Median: Bleibt im Zentrum der Masse (rechts vom Mittelwert)."
        ],
        "content": "Verstehen der Schiefe ist wichtig, um zu entscheiden, ob der Mittelwert eine gute Kennzahl ist."
      },
      "mini_glossary": [
        {
          "term": "Skewness",
          "definition": "Schiefe einer Wahrscheinlichkeitsverteilung."
        }
      ]
    },
    {
      "question": "204. Warum ist `for row in df.iterrows(): ...` oft eine schlechte Idee bei großen Datensätzen?",
      "options": [
        "Es ist syntaktisch falsch.",
        "Es ist extrem langsam – im Kontext.",
        "Es führt zu Speicherüberlauf.",
        "Es verändert die Daten ungewollt."
      ],
      "answer": 1,
      "explanation": "`iterrows()` iteriert in Python-Geschwindigkeit und erzeugt Overhead durch Objekterstellung. Vektorisierung ist um Größenordnungen schneller.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Anti-Patterns",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Vermeidung von iterrows",
        "steps": [
          "Problem: Python-Loops sind langsam bei Millionen Durchläufen.",
          "Problem 2: `iterrows` konvertiert jede Zeile in eine neue Series (Overhead).",
          "Lösung: Nutze Spaltenoperationen (`df['a'] + df['b']`)."
        ],
        "content": "Iteration sollte in Pandas das letzte Mittel sein, nicht das erste."
      },
      "mini_glossary": [
        {
          "term": "Overhead",
          "definition": "Zusätzlicher Ressourcenaufwand für Verwaltung."
        }
      ]
    },
    {
      "question": "205. Sie verwenden `df.pivot_table(index='Datum', columns='Produkt', values='Umsatz')`. Was passiert, wenn es für ein Datum und Produkt mehrere Umsatz-Einträge gibt?",
      "options": [
        "Fehler – im beschriebenen Szenario.",
        "Es wird standardmäßig der Mittelwert der Werte berechnet.",
        "Es wird der erste Wert genommen – in der Praxis.",
        "Es wird der letzte Wert genommen – in der Praxis."
      ],
      "answer": 1,
      "explanation": "`pivot_table` hat einen Parameter `aggfunc`, der standardmäßig auf `'mean'` gesetzt ist (anders als einfaches `pivot`, das keine Duplikate erlaubt).",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Pivoting",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Pivot vs Pivot Table",
        "steps": [
          "Situation: Mehrere Werte pro Zelle (Datum/Produkt Kombination).",
          "Notwendigkeit: Aggregation (Zusammenfassung).",
          "Lösung: `pivot_table` aggregiert automatisch (default mean).",
          "Alternative: `pivot` würde einen 'Duplicate index' Fehler werfen."
        ],
        "content": "Pivoting wandelt 'lange' Formate (Datenbank-Style) in 'breite' Formate (Excel-Style) um."
      },
      "mini_glossary": [
        {
          "term": "Pivot",
          "definition": "Drehen von Daten; Umwandlung von Zeilenwerten in Spaltenüberschriften."
        }
      ]
    },
    {
      "question": "206. Welcher Datentyp wird von Pandas verwendet, um fehlende numerische Werte (NaN) zu speichern, und was ist die Konsequenz für Integer-Spalten (vor Pandas 1.0)?",
      "options": [
        "`null`; Integer Spalten bleiben Integer.",
        "`NaN` – im genannten Kontext.",
        "`None`; keine Auswirkung.",
        "`-1`; als Platzhalter – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Traditionell basiert `NaN` auf dem IEEE 754 Floating Point Standard. Daher mussten Integer-Spalten mit fehlenden Werten zu Float gecastet werden (z. B. wird 5 zu 5.0).",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "concept": "NaN und Integers",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Das Int-NaN Problem",
        "steps": [
          "Ursache: Hardware-NaN existiert nur für Floats.",
          "Effekt: Sobald ein `NaN` in eine Int-Spalte kommt, castet Pandas alles zu Float.",
          "Neuere Lösung: Pandas führte 'Int64' (nullable int) ein, um dies zu umgehen.",
          "Analyse: Wichtig beim Einlesen von IDs, die plötzlich Kommazahlen sind."
        ],
        "content": "Achten Sie darauf, wenn IDs plötzlich '.0' am Ende haben."
      },
      "mini_glossary": [
        {
          "term": "IEEE 754",
          "definition": "Standard für Gleitkommaarithmetik."
        }
      ]
    },
    {
      "question": "207. Wie erstellen Sie eine Kopie eines DataFrames, bei der Änderungen NICHT auf das Original zurückwirken?",
      "options": [
        "`df2 = df`",
        "`df2 = df.copy()`",
        "`df2 = new DataFrame(df)`",
        "`df2 = df[:]`"
      ],
      "answer": 1,
      "explanation": "`df.copy()` erstellt eine 'Deep Copy' (standardmäßig). `df2 = df` kopiert nur die Referenz.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Kopieren",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Assignment",
          "definition": "Zuweisung; in Python meist Referenzkopie."
        }
      ]
    },
    {
      "question": "208. Was ist der Unterschied zwischen einer Liste `[1, 2]` und einem Tupel `(1, 2)` in Python?",
      "options": [
        "Listen sind immutable (unveränderlich), Tupel sind mutable.",
        "Tupel sind immutable (unveränderlich), Listen sind mutable.",
        "Listen können nur Zahlen enthalten – im Kontext.",
        "Tupel sind langsamer – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Tupel sind unveränderlich (immutable). Nach der Erstellung können keine Elemente hinzugefügt oder geändert werden. Listen sind dynamisch veränderbar.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Immutability",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Mutable",
          "definition": "Veränderbares Objekt."
        },
        {
          "term": "Immutable",
          "definition": "Unveränderbares Objekt nach Erstellung."
        }
      ]
    },
    {
      "question": "209. Sie haben eine Funktion `def process(data=[]): ...`. Warum ist dieser Default-Parameter gefährlich?",
      "options": [
        "Leere Listen sind in Python nicht erlaubt.",
        "Default wird einmal ausgewertet; Aufrufe teilen die Liste.",
        "Es verursacht einen Syntaxfehler beim Definieren.",
        "Die Typisierung ist unklar und macht Code schwerer lesbar."
      ],
      "answer": 1,
      "explanation": "Dies ist ein klassischer Python-Fallstrick. Wenn die Funktion die Liste ändert, behält sie diese Änderungen bei folgenden Aufrufen, da das Listen-Objekt nur einmal erstellt wird.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Mutable Default Arguments",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Mutable Defaults Falle",
        "steps": [
          "Definition: `def f(x=[]): x.append(1); print(x)`",
          "1. Aufruf: `f()` -> `[1]`",
          "2. Aufruf: `f()` -> `[1, 1]` (NICHT `[1]`!)",
          "Lösung: Nutze `def f(x=None): if x is None: x = []`"
        ],
        "content": "In Java gibt es dieses Verhalten nicht, da Parameter anders behandelt werden. In Python ist die Funktionsdefinition ausführbarer Code."
      },
      "mini_glossary": [
        {
          "term": "Default Argument",
          "definition": "Standardwert für einen Funktionsparameter."
        }
      ]
    },
    {
      "question": "210. Welche Methode nutzen Sie, um Strings in einer Pandas-Series zu bearbeiten, z. B. alle auf Kleinbuchstaben zu setzen?",
      "options": [
        "Loop über alle Elemente.",
        "`df['col'].str.lower()`",
        "`df['col'].lower()`",
        "`lower(df['col'])`"
      ],
      "answer": 1,
      "explanation": "Pandas bietet den `.str` Accessor, der vektorisierte String-Operationen auf Series-Objekten ermöglicht.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "String Accessor",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Der .str Accessor",
        "steps": [
          "Problem: Methoden wie `.lower()` existieren nicht direkt auf der Series.",
          "Lösung: Zugriff über `.str` macht String-Methoden verfügbar.",
          "Anwendung: `series.str.upper()`, `series.str.contains()`, etc.",
          "Vorteil: Behandelt `NaN` automatisch korrekt."
        ],
        "content": "Dies ist analog zum `.dt` Accessor für Datumsangaben."
      },
      "mini_glossary": [
        {
          "term": "Accessor",
          "definition": "Schnittstelle für spezifische Datentyp-Funktionen (str, dt, cat)."
        }
      ]
    },
    {
      "question": "211. Sie plotten zwei Variablen gegeneinander und sehen eine Punktewolke, die sich um eine Gerade von unten links nach oben rechts gruppiert. Was sagt die Korrelation aus?",
      "options": [
        "Korrelation nahe -1.",
        "Korrelation nahe 0.",
        "Korrelation nahe +1.",
        "Keine Korrelation."
      ],
      "answer": 2,
      "explanation": "Eine steigende Gerade deutet auf eine starke positive lineare Beziehung hin, also einen Korrelationskoeffizienten (Pearson) nahe +1.",
      "weight": 2,
      "topic": "Modellbewertung & Validierung",
      "concept": "Korrelation",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Korrelation interpretieren",
        "steps": [
          "Positive Steigung: Wenn X steigt, steigt Y -> Positiv.",
          "Eng an der Linie: Starke Beziehung -> Nahe 1.",
          "Streuung: Punktewolke -> Schwächere Korrelation.",
          "Schlussfolgerung: +1 ist perfekt linear steigend."
        ],
        "content": "Korrelation impliziert keine Kausalität, beschreibt aber die Stärke des linearen Zusammenhangs."
      },
      "mini_glossary": [
        {
          "term": "Pearson Korrelation",
          "definition": "Maß für den linearen Zusammenhang (-1 bis +1)."
        }
      ]
    },
    {
      "question": "212. Wie laden Sie ein Modul 'matplotlib.pyplot' standardkonform, um Tipparbeit zu sparen?",
      "options": [
        "`import matplotlib.pyplot`",
        "`import matplotlib.pyplot as plt`",
        "`include matplotlib.pyplot`",
        "`from matplotlib import pyplot`"
      ],
      "answer": 1,
      "explanation": "`import ... as ...` erlaubt Aliasse. `plt` ist die universelle Konvention für pyplot.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Import Konventionen",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Alias",
          "definition": "Kurzname für ein importiertes Modul."
        }
      ]
    },
    {
      "question": "213. Was passiert, wenn Sie `set([1, 2, 2, 3])` aufrufen?",
      "options": [
        "Ein Fehler – in diesem Beispiel.",
        "Es entsteht `{1, 2, 2, 3}` – in der Praxis.",
        "Es entsteht `{1, 2, 3}` (Duplikate entfernt).",
        "Es entsteht eine sortierte Liste `[1, 2, 3]`."
      ],
      "answer": 2,
      "explanation": "Ein Set ist eine Menge eindeutiger Elemente. Bei der Erstellung werden Duplikate automatisch verworfen.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Sets",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Eigenschaften von Sets",
        "steps": [
          "Eingabe: Liste mit Duplikaten.",
          "Konvertierung zu Set: Prüft Hash-Werte.",
          "Filterung: Behält jeden Hash nur einmal.",
          "Ergebnis: Menge der Unique Values."
        ],
        "content": "Sets sind extrem nützlich, um schnell die Menge der eindeutigen Elemente in einer Liste zu finden."
      },
      "mini_glossary": [
        {
          "term": "Set",
          "definition": "Menge eindeutiger Elemente ohne feste Ordnung."
        }
      ]
    },
    {
      "question": "214. Ein Machine Learning Modell zeigt auf den Trainingsdaten fast 100% Genauigkeit, aber auf den Testdaten nur 60%. Welches Problem liegt wahrscheinlich vor?",
      "options": [
        "Underfitting – im Kontext.",
        "Overfitting – im Kontext.",
        "Falsche Daten – im Kontext.",
        "Zu wenig Trainingsdaten"
      ],
      "answer": 1,
      "explanation": "Overfitting (Überanpassung) bedeutet, dass das Modell die Trainingsdaten (inkl. Rauschen) 'auswendig gelernt' hat, aber nicht auf neue Daten generalisieren kann.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "concept": "Overfitting",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Overfitting Diagnose",
        "steps": [
          "Symptom: Hohe Diskrepanz zwischen Training- und Test-Score.",
          "Ursache: Modell ist zu komplex für die Datenmenge.",
          "Vergleich: Wie ein Schüler, der die Lösungen auswendig lernt, aber das Konzept nicht versteht.",
          "Gegenmaßnahme: Regularisierung, mehr Daten, einfacheres Modell."
        ],
        "content": "Dies ist eines der zentralen Probleme im Machine Learning."
      },
      "mini_glossary": [
        {
          "term": "Generalisierung",
          "definition": "Fähigkeit eines Modells, auf unbekannten Daten gute Ergebnisse zu liefern."
        }
      ]
    },
    {
      "question": "215. Wie greifen Sie auf das letzte Element einer Liste `lst` zu?",
      "options": [
        "`lst[len(lst)]` – im Kontext.",
        "`lst[-1]` – im Kontext.",
        "`lst.last()` – im Kontext.",
        "`lst[end]` – im Kontext."
      ],
      "answer": 1,
      "explanation": "Python unterstützt negative Indizierung. `-1` referenziert das letzte Element, `-2` das vorletzte usw.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Negative Indexing",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Index",
          "definition": "Position eines Elements in einer Sequenz."
        }
      ]
    },
    {
      "question": "216. Sie haben eine Spalte mit Werten '100 USD', '200 USD'. Um damit zu rechnen, müssen Sie ' USD' entfernen und in Zahlen umwandeln. Welche Kette ist am sinnvollsten?",
      "options": [
        "`df['Preis'].str.replace(' USD', '').astype(float)`",
        "`df['Preis'].replace(' USD', '').to_numeric()`",
        "`df['Preis'].trim().int()` – in der Praxis.",
        "Manuelle Schleife – in der Praxis."
      ],
      "answer": 0,
      "explanation": "Zuerst String-Operation (`str.replace`), um den Text zu bereinigen, dann Typ-Konvertierung (`astype`).",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Method Chaining",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Cleaning Pipeline",
        "steps": [
          "1. Zugriff auf String-Methoden: `.str`.",
          "2. Ersetzen: `.replace(' USD', '')` entfernt Suffix.",
          "3. Ergebnis ist immer noch String ('100').",
          "4. Konvertierung: `.astype(float)` macht daraus Zahl (100.0)."
        ],
        "content": "Das Verketten von Methoden (Chaining) macht den Code lesbar und kompakt."
      },
      "mini_glossary": [
        {
          "term": "Casting",
          "definition": "Änderung des Datentyps einer Variable."
        }
      ]
    },
    {
      "question": "217. Was ist der Unterschied zwischen `__init__` in Python und einem Konstruktor in Java?",
      "options": [
        "Es gibt keinen Unterschied – in der Praxis.",
        "`__init__` erstellt das Objekt nicht",
        "`__init__` muss immer einen Rückgabewert haben.",
        "Java hat keine Konstruktoren."
      ],
      "answer": 1,
      "explanation": "Technisch gesehen ist `__new__` der Konstruktor, der den Speicher allokiert. `__init__` ist der Initialisierer, der das bereits erstellte Objekt (`self`) konfiguriert. In der Praxis wird `__init__` aber oft wie ein Konstruktor genutzt.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Objekt-Erstellung",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Python Objekt-Lifecycle",
        "steps": [
          "1. `__new__`: Statische Methode, erstellt Instanz.",
          "2. `__init__`: Instanzmethode, setzt Attribute.",
          "Java: `new` Operator macht beides in einem Schritt (allokieren + Konstruktor aufrufen).",
          "Relevanz: Wichtig bei Vererbung von unveränderlichen Typen (wie str, int)."
        ],
        "content": "Für die meisten Data Science Anwendungen reicht `__init__`, aber das Verständnis ist wichtig für tiefere Python-Kenntnisse."
      },
      "mini_glossary": [
        {
          "term": "Self",
          "definition": "Referenz auf die aktuelle Instanz (analog zu `this` in Java)."
        }
      ]
    },
    {
      "question": "218. Welche Datenstruktur eignet sich am besten, um die Häufigkeit von Wörtern in einem Text zu zählen?",
      "options": [
        "List – in der Praxis.",
        "Set – in der Praxis.",
        "Dictionary – im Kontext.",
        "Tuple (Begriff)"
      ],
      "answer": 2,
      "explanation": "Ein Dictionary mappt Wort -> Anzahl. `collections.Counter` ist eine spezialisierte Subklasse dafür.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Counter",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Zählen mit HashMaps",
        "steps": [
          "Idee: Schlüssel ist das Wort, Wert ist der Zähler.",
          "Algorithmus: Iteriere über Text, wenn Wort existiert: count++, sonst count=1.",
          "Python Shortcut: `Counter(wort_liste)` macht dies automatisch."
        ],
        "content": "In Data Science (NLP) ist das Zählen von Tokens ('Bag of Words') ein fundamentaler Schritt."
      },
      "mini_glossary": [
        {
          "term": "NLP",
          "definition": "Natural Language Processing; Verarbeitung natürlicher Sprache."
        }
      ]
    },
    {
      "question": "219. Betrachten Sie folgenden Code. Was gibt er aus?\n\n```python\n1: daten = [10, 20, 30, 40, 50]\n2: print(daten[1:4])\n```",
      "options": [
        "`[10, 20, 30]` – im Kontext.",
        "`[20, 30, 40, 50]` – im Kontext.",
        "`[20, 30, 40]` – im Kontext.",
        "Einen `IndexOutOfBounds` Fehler."
      ],
      "answer": 2,
      "explanation": "Python Slicing ist inklusiv beim Startindex (1 entspricht dem zweiten Element '20') und exklusiv beim Endindex (4 entspricht dem fünften Element, wird also nicht mehr eingeschlossen). Indizes sind 1, 2, 3.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "List Slicing",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Slicing",
          "definition": "Syntax `[start:stop:step]` zum Extrahieren von Teilen."
        }
      ]
    },
    {
      "question": "220. Welches Problem tritt in diesem Code auf?\n\n```python\n1: import pandas as pd\n2: df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n3: maske = (df['A'] > 1) and (df['B'] < 5)\n```",
      "options": [
        "Die Syntax ist korrekt – in diesem Beispiel – im Kontext.",
        "Python kennt kein `and`, man muss `&&` nutzen – im Kontext.",
        "Für Pandas-Series muss der bitweise Operator `&` verwendet werden",
        "Die Klammern sind überflüssig und verursachen den Fehler."
      ],
      "answer": 2,
      "explanation": "Der Python-Operator `and` erwartet einen einzelnen Wahrheitswert (True/False). Da Pandas-Series viele Werte enthalten, weiß Python nicht, ob die ganze Serie 'Wahr' ist. Man muss den bitweisen Operator `&` für elementweise Vergleiche nutzen.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Bitwise Operators",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Boolean Operators in Pandas",
        "steps": [
          "Problem: `and` prüft `bool(Series)`, was einen `ValueError` wirft.",
          "Lösung: `&` führt die Operation elementweise (Zeile für Zeile) aus.",
          "Wichtig: Klammern um die Bedingungen `(A > 1) & (B < 5)` sind zwingend wegen der Operator-Priorität."
        ],
        "content": "Ein klassischer Fehler für Java-Entwickler, die `&&` oder `and` gewohnt sind."
      },
      "mini_glossary": [
        {
          "term": "Ambiguity Error",
          "definition": "Fehler, wenn der Wahrheitsgehalt einer Array-Struktur nicht eindeutig ist."
        }
      ]
    },
    {
      "question": "221. Was passiert bei der Ausführung dieses Codes?\n\n```python\n1: tupel = (1, 2, 3)\n2: tupel[1] = 5\n3: print(tupel)\n```",
      "options": [
        "Ausgabe: `(1, 5, 3)` – im Kurskontext – im Kontext.",
        "Ausgabe: `[1, 5, 3]` – im Kurskontext – im Kontext.",
        "TypeError: 'tuple' object does not support item assignment",
        "Der Code läuft, aber das Tupel bleibt unverändert."
      ],
      "answer": 2,
      "explanation": "Tupel sind in Python unveränderlich (immutable). Man kann Elemente nach der Erstellung nicht mehr ändern.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Immutability",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "TypeError",
          "definition": "Fehler, wenn eine Operation auf einen unpassenden Datentyp angewendet wird."
        }
      ]
    },
    {
      "question": "222. Finden Sie den Logikfehler in dieser Funktion:\n\n```python\n1: def add_item(item, liste=[]):\n2:     liste.append(item)\n3:     return liste\n4:\n5: print(add_item('A'))\n6: print(add_item('B'))\n```",
      "options": [
        "Zeile 5 gibt `['A']` aus, Zeile 6 gibt `['B']` aus.",
        "Zeile 5 gibt `['A']` aus, Zeile 6 gibt `['A', 'B']` aus.",
        "Es tritt ein Fehler auf, weil `liste` nicht typisiert ist.",
        "Zeile 6 überschreibt Zeile 5, beide geben `['B']` aus."
      ],
      "answer": 1,
      "explanation": "Die Liste im Default-Parameter wird nur *einmal* bei der Definition erstellt. Beide Funktionsaufrufe teilen sich dieselbe Listen-Instanz. Das zweite Element wird an die *bestehende* Liste angehängt.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Mutable Default Arguments",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Gefährliche Default-Werte",
        "steps": [
          "Definiert: `liste` wird im Speicher angelegt (ID: 123).",
          "Aufruf 1: `add_item('A')` nutzt ID 123 -> `['A']`.",
          "Aufruf 2: `add_item('B')` nutzt immer noch ID 123 (welche bereits `['A']` enthält) -> `['A', 'B']`.",
          "Fix: `def add_item(item, liste=None): if liste is None: liste = []`."
        ],
        "content": "Dies ist anders als in Java, wo Default-Parameter so nicht existieren oder per Overloading gelöst werden."
      },
      "mini_glossary": [
        {
          "term": "Side Effect",
          "definition": "Ungewollte Änderung des Zustands außerhalb des lokalen Scopes."
        }
      ]
    },
    {
      "question": "223. Was bewirkt dieser List-Comprehension Code?\n\n```python\n1: zahlen = [1, 2, 3, 4, 5]\n2: res = [x for x in zahlen if x % 2 == 0]\n```",
      "options": [
        "Er quadriert alle geraden Zahlen – im Kontext – in der Praxis.",
        "Er filtert die Liste und behält nur gerade Zahlen: `[2, 4]`.",
        "Er erstellt eine Liste von Booleans: `[False, True, False, True, False]`.",
        "Er gibt Syntaxfehler zurück – im Kontext – in der Praxis."
      ],
      "answer": 1,
      "explanation": "Der Teil `if x % 2 == 0` wirkt als Filter. Nur Elemente, die die Bedingung erfüllen, landen in der neuen Liste `res`.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Filtering List Comprehension",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Filter-Logik",
        "steps": [
          "Iteration: Gehe durch `zahlen`.",
          "Prüfung: Ist Element durch 2 teilbar?",
          "Selektion: Wenn ja, nimm es auf.",
          "Ergebnis: Eine neue Liste mit der Teilmenge."
        ],
        "content": "Dies entspricht in Java etwa: `stream().filter(x -> x % 2 == 0).collect(toList())`."
      },
      "mini_glossary": [
        {
          "term": "Modulo",
          "definition": "Operator `%`, gibt den Rest einer Division zurück."
        }
      ]
    },
    {
      "question": "224. Warum wirft dieser Pandas-Code eine Warnung (SettingWithCopyWarning)?\n\n```python\n1: df_subset = df[df['Age'] > 50]\n2: df_subset['Status'] = 'Senior'\n```",
      "options": [
        "Weil 'Senior' ein String ist und 'Status' numerisch sein sollte.",
        "Weil Pandas nicht weiß – in diesem Beispiel.",
        "Weil man neuen Spalten keine Werte zuweisen darf.",
        "Der Code ist syntaktisch falsch und läuft gar nicht."
      ],
      "answer": 1,
      "explanation": "`df[df['Age'] > 50]` könnte eine View zurückgeben. Eine Zuweisung darauf ist riskant. Pandas warnt, dass man explizit `.copy()` nutzen sollte, wenn man eine eigene Tabelle will, oder `.loc` auf dem Original nutzen sollte.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "concept": "SettingWithCopy",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Chained Indexing vermeiden",
        "steps": [
          "Schritt 1: `df[...]` erzeugt ein temporäres Objekt (View oder Copy).",
          "Schritt 2: `['Status'] = ...` modifiziert dieses temporäre Objekt.",
          "Konsequenz: Die Änderung geht oft verloren oder ist ineffizient.",
          "Lösung: `df.loc[df['Age'] > 50, 'Status'] = 'Senior'`."
        ],
        "content": "Dies ist eine der häufigsten Warnungen in Pandas."
      },
      "mini_glossary": [
        {
          "term": "Chained Assignment",
          "definition": "Zuweisung, die auf das Ergebnis einer vorherigen Indizierung folgt (z.B. `df[][]=`)."
        }
      ]
    },
    {
      "question": "225. Was gibt dieser Code aus?\n\n```python\n1: s = \"Python\"\n2: print(f\"{s} ist toll\")\n```",
      "options": [
        "`{s} ist toll`",
        "`s ist toll` – im Kontext.",
        "`Python ist toll`",
        "SyntaxError – im Kontext."
      ],
      "answer": 2,
      "explanation": "Das `f` vor dem String aktiviert 'f-strings' (ab Python 3.6). Ausdrücke in geschweiften Klammern `{}` werden zur Laufzeit ausgewertet und eingefügt.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "String Formatting",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "f-string",
          "definition": "Formatierter String-Literal zur Interpolation von Variablen."
        }
      ]
    },
    {
      "question": "226. Was ist das Problem bei diesem Dictionary-Zugriff?\n\n```python\n1: data = {'Name': 'Max', 'Alter': 25}\n2: print(data['Adresse'])\n```",
      "options": [
        "Es gibt `None` aus.",
        "Es wirft einen `KeyError`.",
        "Es gibt einen leeren String aus.",
        "Es wirft einen `IndexError`."
      ],
      "answer": 1,
      "explanation": "Der direkte Zugriff mit eckigen Klammern `[]` auf einen nicht existierenden Schlüssel wirft in Python einen `KeyError`. Die Methode `.get('Adresse')` würde stattdessen `None` zurückgeben.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Error Handling",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Sicherer Dictionary-Zugriff",
        "steps": [
          "Direktzugriff `data['key']`: Schnell, aber Fehler bei fehlendem Key.",
          "Methode `data.get('key')`: Sicher, liefert `None` (oder Default-Wert) bei fehlendem Key.",
          "Best Practice: Nutze `.get()`, wenn Unsicherheit über die Keys besteht."
        ],
        "content": "In Java Maps entspricht `.get()` eher dem sicheren Verhalten (gibt null zurück)."
      },
      "mini_glossary": [
        {
          "term": "KeyError",
          "definition": "Ausnahme, wenn ein Dictionary-Schlüssel nicht gefunden wird."
        }
      ]
    },
    {
      "question": "227. Welche Zeile verursacht einen `IndentationError`?\n\n```python\n1: for i in range(5):\n2: print(i)\n3:     print(\"Done\")\n```",
      "options": [
        "Zeile 1 – in der Praxis.",
        "Zeile 2 – in der Praxis.",
        "Zeile 3 – in der Praxis.",
        "Keine, der Code ist korrekt."
      ],
      "answer": 1,
      "explanation": "In Python muss der Körper einer Schleife eingerückt sein. Zeile 2 steht auf derselben Ebene wie `for` und gehört somit syntaktisch nicht zur Schleife, was aber von `for` erwartet wird.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Indentation",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Scope",
          "definition": "Geltungsbereich von Variablen und Codeblöcken."
        }
      ]
    },
    {
      "question": "228. Sie wollen alle negativen Werte in einem NumPy-Array auf 0 setzen. Was macht dieser Code?\n\n```python\n1: import numpy as np\n2: arr = np.array([-1, 2, -3, 4])\n3: arr[arr < 0] = 0\n```",
      "options": [
        "Er setzt das gesamte Array auf 0, wenn irgendein Wert negativ ist.",
        "Er funktioniert korrekt und ändert `arr` zu `[0, 2, 0, 4]`.",
        "Er wirft einen Fehler, da Zuweisungen innerhalb von eckigen Klammern verboten sind.",
        "Er erzeugt eine Kopie, aber das Original `arr` bleibt unverändert."
      ],
      "answer": 1,
      "explanation": "Dies ist korrektes 'Boolean Masking' zur Zuweisung. `arr < 0` wählt die Indizes der negativen Werte aus, und die Zuweisung `= 0` ändert genau diese Stellen im Original-Array.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Conditional Assignment",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Maskierte Zuweisung",
        "steps": [
          "Maske erstellen: `[-1, 2, -3, 4] < 0` -> `[True, False, True, False]`.",
          "Indizierung: `arr[...]` wählt nur Elemente an Position 0 und 2.",
          "Zuweisung: Setze diese auf 0.",
          "Ergebnis: In-Place Modifikation."
        ],
        "content": "Dies ist viel effizienter als eine Schleife mit if-Bedingung."
      },
      "mini_glossary": [
        {
          "term": "In-Place",
          "definition": "Operation ändert die Daten direkt im Speicher, ohne neue Kopie."
        }
      ]
    },
    {
      "question": "229. Was ist das Ergebnis dieses Vergleichs?\n\n```python\n1: x = 0.1 + 0.2\n2: print(x == 0.3)\n```",
      "options": [
        "`True` – im Kontext.",
        "`False` – im Kontext.",
        "`0.30000000000000004`",
        "Fehler (Begriff)"
      ],
      "answer": 1,
      "explanation": "Aufgrund der IEEE 754 Gleitkomma-Darstellung ist 0.1 + 0.2 in binären Systemen minimal größer als 0.3 (etwa 0.30000000000000004). Der strikte Vergleich `==` schlägt daher fehl.",
      "weight": 3,
      "topic": "Modellbewertung & Validierung",
      "concept": "Floating Point Precision",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Gleitkomma-Ungenauigkeit",
        "steps": [
          "Problem: Binärsystem kann 0.1 nicht exakt darstellen (periodischer Bruch).",
          "Rechnung: Summe hat minimalen Rundungsfehler.",
          "Lösung: Nutzen Sie `np.isclose(x, 0.3)` oder prüfen Sie `abs(x - 0.3) < epsilon`."
        ],
        "content": "Dieses Problem existiert auch in Java (`double`), wird aber oft übersehen."
      },
      "mini_glossary": [
        {
          "term": "Epsilon",
          "definition": "Sehr kleine Zahl, die als Toleranzgrenze bei Vergleichen dient."
        }
      ]
    },
    {
      "question": "230. Warum schlägt dieser Code fehl?\n\n```python\n1: import numpy as np\n2: a = np.array([1, 2, 3])\n3: b = np.array([1, 2])\n4: print(a + b)\n```",
      "options": [
        "NumPy kann Arrays nicht addieren – in der Praxis – im Kontext.",
        "ValueError: operands could not be broadcast together with shapes (3,) (2,).",
        "Er addiert die ersten zwei Elemente und ignoriert das dritte.",
        "Er füllt das kürzere Array mit Nullen auf – in der Praxis."
      ],
      "answer": 1,
      "explanation": "NumPy Broadcasting funktioniert nur, wenn Dimensionen kompatibel sind (gleich groß oder eine Dimension ist 1). Länge 3 und Länge 2 sind inkompatibel.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Shape Mismatch",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Broadcasting Regeln",
        "steps": [
          "Regel: Dimensionen werden von rechts nach links verglichen.",
          "Check: Dim 3 vs Dim 2.",
          "Bedingung: Sind sie gleich? Nein. Ist eine davon 1? Nein.",
          "Folge: Fehler. NumPy rät nicht, wie aufgefüllt werden soll."
        ],
        "content": "Shape-Fehler sind die häufigsten Fehler beim Arbeiten mit neuronalen Netzen oder Matrix-Ops."
      },
      "mini_glossary": [
        {
          "term": "Shape",
          "definition": "Tupel, das die Größe der Dimensionen eines Arrays angibt."
        }
      ]
    },
    {
      "question": "231. Was passiert hier mit der Variable `x`?\n\n```python\n1: x = 10\n2: def change():\n3:     x = 20\n4: change()\n5: print(x)\n```",
      "options": [
        "Gibt `20` aus – im Kontext.",
        "Gibt `10` aus – im Kontext.",
        "Gibt Fehler 'UnboundLocalError'.",
        "Gibt `None` aus – im Kontext."
      ],
      "answer": 1,
      "explanation": "Innerhalb der Funktion `change` wird eine *neue lokale* Variable `x` erstellt (Shadowing). Die globale Variable `x` (Zeile 1) bleibt unverändert. Um global zu schreiben, müsste `global x` verwendet werden.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Variable Scope",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Local vs Global Scope",
        "steps": [
          "Zeile 3: `x = 20` sieht aus wie eine Zuweisung.",
          "Python Regel: Zuweisungen im Funktionskörper erzeugen standardmäßig lokale Variablen.",
          "Effekt: Das äußere `x` wird überdeckt, aber nicht geändert.",
          "Resultat: `print(x)` greift auf das unveränderte äußere `x` zu."
        ],
        "content": "Scope-Regeln (LEGB: Local, Enclosing, Global, Built-in) sind essenziell für sauberen Code."
      },
      "mini_glossary": [
        {
          "term": "Shadowing",
          "definition": "Eine Variable in einem inneren Scope hat denselben Namen wie eine äußere und verdeckt diese."
        }
      ]
    },
    {
      "question": "232. Sie wollen eine Liste sortieren. Was ist der Unterschied zwischen diesen beiden Zeilen?\n\n```python\n1: l1 = [3, 1, 2]\n2: l2 = l1.sort()\n3: l3 = sorted(l1)\n```",
      "options": [
        "Kein Unterschied – im beschriebenen Szenario – im Kontext – in der Praxis – im beschriebenen Fall.",
        "`l1.sort()` sortiert in-place und gibt `None` zurück. `sorted` erstellt eine neue sortierte Liste.",
        "`l1.sort()` ist veraltet, man sollte nur `sorted` nutzen – in der Praxis – im Kontext.",
        "`sorted(l1)` sortiert in-place, `l1.sort()` nicht – in der Praxis – im Kontext."
      ],
      "answer": 1,
      "explanation": "Listen-Methoden wie `.sort()`, `.append()`, `.reverse()` ändern das Objekt direkt und geben `None` zurück. Built-in Funktionen wie `sorted()` geben ein neues Objekt zurück.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "In-Place vs Return",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Rückgabewerte beachten",
        "steps": [
          "Fehlerquelle: `liste = liste.sort()` löscht die Liste (setzt sie auf None).",
          "Korrekt: `liste.sort()` (als Anweisung) ODER `neue_liste = sorted(liste)`.",
          "Java-Vergleich: `Collections.sort(list)` ist auch void/in-place."
        ],
        "content": "Wer `l2 = l1.sort()` schreibt, erlebt oft eine böse Überraschung."
      },
      "mini_glossary": [
        {
          "term": "Void",
          "definition": "Rückgabetyp, der anzeigt, dass eine Funktion keinen Wert liefert."
        }
      ]
    },
    {
      "question": "233. Was bewirkt der folgende Code zur Fehlerbehandlung?\n\n```python\n1: try:\n2:     x = 1 / 0\n3: except ZeroDivisionError:\n4:     x = 0\n5: else:\n6:     x = 1\n```",
      "options": [
        "`x` wird 0 – im beschriebenen Szenario.",
        "`x` wird 1 – im beschriebenen Szenario.",
        "Der Code stürzt ab – in diesem Beispiel.",
        "Der `else`-Block wird ausgeführt, bevor der Fehler auftritt."
      ],
      "answer": 0,
      "explanation": "Da `1/0` einen Fehler wirft, wird der `except`-Block ausgeführt (`x=0`). Der `else`-Block in einem try-Konstrukt wird nur ausgeführt, wenn *keine* Ausnahme auftrat.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Try-Except-Else",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Der Else-Zweig in Try-Blöcken",
        "steps": [
          "Try: Versuche Code auszuführen.",
          "Exception: Fehler passiert -> Springe zu Except.",
          "Else: Kein Fehler passiert -> Führe Else aus.",
          "Finally (hier nicht gezeigt): Wird immer ausgeführt."
        ],
        "content": "`else` ist nützlich, um Code auszuführen, der nur bei Erfolg laufen soll, aber nicht mehr vom Error-Handling abgedeckt werden muss."
      },
      "mini_glossary": [
        {
          "term": "Exception Handling",
          "definition": "Strukturierter Umgang mit Laufzeitfehlern."
        }
      ]
    },
    {
      "question": "234. Ein häufiger Fehler beim Kopieren von Listen. Was gibt Zeile 4 aus?\n\n```python\n1: a = [1, 2, 3]\n2: b = a\n3: b[0] = 99\n4: print(a)\n```",
      "options": [
        "`[1, 2, 3]`",
        "`[99, 2, 3]`",
        "`[1, 2, 3, 99]`",
        "Fehler"
      ],
      "answer": 1,
      "explanation": "In Python sind Zuweisungen bei Objekten (wie Listen) nur Referenzkopien (Pointer). `a` und `b` zeigen auf denselben Speicher. Eine Änderung über `b` ist auch in `a` sichtbar.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "References",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Referenz vs Kopie",
        "steps": [
          "Zeile 2: `b = a` kopiert nur die Adresse, nicht die Daten.",
          "Analogie: Zwei Namensschilder an derselben Kiste.",
          "Lösung für echte Kopie: `b = a[:]` oder `b = a.copy()`."
        ],
        "content": "Dies ist identisch zu Java-Objekt-Referenzen, aber oft verwirrend für Anfänger bei simplen Datentypen."
      },
      "mini_glossary": [
        {
          "term": "Shallow Copy",
          "definition": "Kopie der Container-Struktur, Elemente bleiben Referenzen."
        }
      ]
    },
    {
      "question": "235. Sie nutzen `groupby` ohne eine Aggregationsfunktion. Was ist `res`?\n\n```python\n1: res = df.groupby('Kategorie')\n2: print(res)\n```",
      "options": [
        "Eine Fehlermeldung – in der Praxis.",
        "Ein DataFrame, sortiert nach Kategorie.",
        "Ein `DataFrameGroupBy` Objekt.",
        "Eine Liste von DataFrames."
      ],
      "answer": 2,
      "explanation": "`groupby()` allein führt noch keine Berechnung durch. Es bereitet die Gruppierung vor (lazy evaluation) und gibt ein Zwischenobjekt zurück, auf das man `.sum()`, `.mean()` etc. anwenden muss.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Lazy Evaluation",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Das GroupBy Objekt",
        "steps": [
          "Aufruf: `groupby` speichert nur das Mapping (welche Zeile gehört zu welcher Gruppe).",
          "Zweck: Warten auf Anweisung, was berechnet werden soll.",
          "Nutzung: Man kann darüber iterieren (`for name, group in res: ...`) oder aggregieren."
        ],
        "content": "Ein häufiger Anfängerfehler ist zu erwarten, dass `groupby` sofort eine Tabelle zurückgibt."
      },
      "mini_glossary": [
        {
          "term": "Lazy Evaluation",
          "definition": "Auswertung eines Ausdrucks erst dann, wenn das Ergebnis benötigt wird."
        }
      ]
    },
    {
      "question": "236. Welcher Fehler liegt hier vor?\n\n```python\n1: x, y = (10, 20, 30)\n```",
      "options": [
        "SyntaxError: Klammern fehlen – im Kontext.",
        "ValueError: too many values to unpack (expected 2).",
        "x wird 10, y wird 20, 30 wird ignoriert.",
        "x wird 10, y wird (20, 30) – im Kontext."
      ],
      "answer": 1,
      "explanation": "Dies ist 'Tuple Unpacking'. Die Anzahl der Variablen links (2) muss exakt mit der Anzahl der Elemente rechts (3) übereinstimmen, sonst gibt es einen `ValueError`.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Unpacking",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Unpacking Regeln",
        "steps": [
          "Prinzip: 1-zu-1 Zuordnung.",
          "Lösung 1: `x, y, z = ...` (3 Variablen).",
          "Lösung 2: `x, *y = ...` (y fängt den Rest als Liste auf -> `[20, 30]`)."
        ],
        "content": "Unpacking ist sehr elegant, erfordert aber strikte Übereinstimmung der Länge."
      },
      "mini_glossary": [
        {
          "term": "Unpacking",
          "definition": "Entpacken von Sequenzen in einzelne Variablen."
        }
      ]
    },
    {
      "question": "237. Was passiert bei diesem Import-Versuch?\n\n```python\n1: import pandas.DataFrame\n```",
      "options": [
        "Es importiert die Klasse DataFrame.",
        "Es funktioniert, wenn Pandas installiert ist.",
        "ModuleNotFoundError – in der Praxis.",
        "Es importiert alles aus Pandas."
      ],
      "answer": 2,
      "explanation": "Die Syntax `import package.module` erwartet, dass das letzte Element ein Modul (Datei/Ordner) ist. `DataFrame` ist eine Klasse. Korrekt wäre `from pandas import DataFrame`.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Import Syntax",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Package",
          "definition": "Ordner mit Python-Modulen und einer `__init__.py`."
        }
      ]
    },
    {
      "question": "238. Sie möchten eine benutzerdefinierte Funktion auf jede Zeile eines DataFrames anwenden. Welches Argument fehlt oder ist falsch?\n\n```python\n1: def my_func(row):\n2:     return row['A'] + row['B']\n3:\n4: df['C'] = df.apply(my_func)\n```",
      "options": [
        "Nichts fehlt – im beschriebenen Szenario – im Kontext.",
        "Es muss `axis=1` angegeben werden, sonst iteriert `apply` über Spalten.",
        "`my_func` darf keine Argumente haben – in der Praxis – im Kontext.",
        "Pandas unterstützt keine benutzerdefinierten Funktionen."
      ],
      "answer": 1,
      "explanation": "Standardmäßig ist `axis=0` (Spalten). `apply` würde `my_func` eine ganze Spalte als Series übergeben. Da `my_func` aber `row['A']` (Zeilenzugriff) erwartet, crasht es. Mit `axis=1` wird zeilenweise iteriert.",
      "weight": 3,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Apply Axis",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Apply Richtung",
        "steps": [
          "Default: `apply` arbeitet vertikal (auf jeder Spalte).",
          "Ziel: Berechnung quer über Zeilen (Spalte A + Spalte B).",
          "Korrektur: `df.apply(my_func, axis=1)`.",
          "Hinweis: Oft langsam, besser direkt `df['A'] + df['B']` nutzen."
        ],
        "content": "Die Achsen-Logik ist bei `apply` genau umgekehrt zur Intuition mancher Nutzer ('axis=1' -> bewege dich entlang der Spaltenrichtung, also Zeile für Zeile)."
      },
      "mini_glossary": [
        {
          "term": "Row-wise",
          "definition": "Operation, die zeilenweise ausgeführt wird."
        }
      ]
    },
    {
      "question": "239. Welches Schlüsselwort fehlt, um über Schlüssel und Werte eines Dictionaries zu iterieren (analog zu `entrySet` in Java)?\n\n```python\n1: data = {'a': 1, 'b': 2}\n2: for k, v in data.__________:\n3:     print(k, v)\n```",
      "options": [
        "entries()",
        "items()",
        "keys()",
        "values()"
      ],
      "answer": 1,
      "explanation": "In Python liefert `.items()` die Schlüssel-Wert-Paare als Tupel zurück. `.entries()` existiert in Python nicht (das wäre Java-Jargon).",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Dictionary Iteration",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "items()",
          "definition": "Methode, die eine Ansicht der (Key, Value)-Paare eines Dictionaries liefert."
        }
      ]
    },
    {
      "question": "240. Ergänzen Sie den ersten Parameter der Methode, der in Python explizit genannt werden muss (anders als `this` in Java).\n\n```python\n1: class Auto:\n2:     def __init__(__________, marke):\n3:         self.marke = marke\n```",
      "options": [
        "this (Begriff)",
        "self (Begriff)",
        "auto (Begriff)",
        "instance – im Kontext."
      ],
      "answer": 1,
      "explanation": "`self` ist die konventionelle Bezeichnung für die Instanzreferenz in Python. Sie muss als erster Parameter in jeder Instanzmethode definiert werden.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Self Referenz",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "self",
          "definition": "Expliziter Parameter für die aktuelle Instanz (analog zu implizitem `this` in Java)."
        }
      ]
    },
    {
      "question": "241. Welcher Pandas-Befehl wird benötigt, um Zeilen mit fehlenden Werten zu entfernen?\n\n```python\n1: import pandas as pd\n2: df = pd.read_csv('data.csv')\n3: # Entferne Zeilen mit NaNs\n4: df_clean = df.__________()\n```",
      "options": [
        "delete_na – im Kontext.",
        "remove_nulls",
        "dropna (Begriff)",
        "fillna (Begriff)"
      ],
      "answer": 2,
      "explanation": "`dropna()` ist die Standardmethode in Pandas, um Missing Values zu löschen. `fillna()` würde sie ersetzen.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Handling Missing Data",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "dropna",
          "definition": "Methode zum Entfernen von fehlenden Werten."
        }
      ]
    },
    {
      "question": "242. Vervollständigen Sie die List Comprehension.\n\n```python\n1: values = [1, 2, 3, 4]\n2: # Quadriere alle Werte\n3: squares = [x**2 __________ values]\n```",
      "options": [
        "for x in",
        "foreach x in",
        "in",
        "from x in"
      ],
      "answer": 0,
      "explanation": "Die Syntax einer List Comprehension lautet `[ausdruck for item in iterable]`. `foreach` gibt es in Python nicht.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "List Comprehension",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Syntax-Aufbau",
        "steps": [
          "1. Was soll passieren? `x**2` (Ausdruck).",
          "2. Woher kommen die Daten? `for x in values` (Schleifenkopf).",
          "Zusammengesetzt: `[x**2 for x in values]`."
        ],
        "content": "List Comprehensions sind der 'pythonische' Weg, `.map()` oder Schleifen zu ersetzen."
      },
      "mini_glossary": [
        {
          "term": "Iterable",
          "definition": "Objekt, über das man iterieren kann (z.B. Liste, Range)."
        }
      ]
    },
    {
      "question": "243. Welches Statement sorgt für ein sicheres Datei-Handling (automatisches Schließen), ähnlich wie `try-with-resources` in Java?\n\n```python\n1: __________ open('datei.txt', 'r') as f:\n2:     content = f.read()\n```",
      "options": [
        "try",
        "using",
        "with",
        "scope"
      ],
      "answer": 2,
      "explanation": "Das `with`-Statement initiiert einen Context Manager, der sicherstellt, dass die Datei am Ende des Blocks geschlossen wird, auch wenn Fehler auftreten.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Context Manager",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Context Manager",
        "steps": [
          "Java: `try (BufferedReader br = ...) { ... }`",
          "Python: `with open(...) as f: ...`",
          "Funktion: Ruft automatisch `__exit__` (und damit `close`) auf."
        ],
        "content": "Verwenden Sie niemals `open()` ohne `with`, es sei denn, Sie haben einen sehr guten Grund."
      },
      "mini_glossary": [
        {
          "term": "Context Manager",
          "definition": "Objekt, das Ressourcenverwaltung (Setup/Teardown) übernimmt."
        }
      ]
    },
    {
      "question": "244. Sie wollen ein 1D-Array mit 9 Elementen in eine 3x3 Matrix umwandeln. Welcher NumPy-Befehl fehlt?\n\n```python\n1: import numpy as np\n2: arr = np.arange(9)\n3: mtx = arr.__________(3, 3)\n```",
      "options": [
        "resize",
        "reshape",
        "dim",
        "transform"
      ],
      "answer": 1,
      "explanation": "`.reshape(rows, cols)` ändert die Dimensionen eines Arrays, ohne die Daten zu ändern. Die Anzahl der Elemente muss konstant bleiben.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Reshaping",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Reshape",
          "definition": "Ändern der Form (Dimensionen) eines Arrays."
        }
      ]
    },
    {
      "question": "245. Welcher Code fehlt, um den Konstruktor der Elternklasse aufzurufen?\n\n```python\n1: class Tier:\n2:     def __init__(self, name): self.name = name\n3:\n4: class Hund(Tier):\n5:     def __init__(self, name, rasse):\n6:         __________(name) # Rufe Eltern-Init auf\n7:         self.rasse = rasse\n```",
      "options": [
        "super.__init__",
        "super().__init__",
        "Tier(name)",
        "this.super(name)"
      ],
      "answer": 1,
      "explanation": "In Python 3 nutzt man `super().__init__(arguments)`. Beachten Sie die Klammern nach `super`, die in Java nicht nötig sind, in Python aber eine Funktion aufrufen, die das Proxy-Objekt zurückgibt.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Inheritance",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Super() in Python",
        "steps": [
          "Syntax: `super()` gibt ein temporäres Objekt der Elternklasse zurück.",
          "Aufruf: Darauf ruft man `.__init__` auf.",
          "Unterschied Java: Java ruft `super()` (Konstruktor) direkt auf; Python ruft die Methode `__init__` auf dem Super-Objekt."
        ],
        "content": "Dies ist essentiell für korrekte Vererbungsketten."
      },
      "mini_glossary": [
        {
          "term": "Super",
          "definition": "Built-in Funktion für den Zugriff auf Methoden der Elternklasse."
        }
      ]
    },
    {
      "question": "246. Vervollständigen Sie die Lambda-Funktion, um nach dem zweiten Element (Index 1) der Tupel zu sortieren.\n\n```python\n1: data = [('A', 10), ('B', 5), ('C', 20)]\n2: # Sortiere nach der Zahl\n3: data.sort(key=lambda x: __________)\n```",
      "options": [
        "x[0] – im Kontext.",
        "x[1] – im Kontext.",
        "x.get(1) – im Kontext.",
        "x->1 – im Kontext."
      ],
      "answer": 1,
      "explanation": "`x` ist hier ein Tupel, z.B. `('A', 10)`. Das zweite Element ist am Index 1. Also `x[1]`.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Lambda Sort",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Lambda als Key",
        "steps": [
          "Input: Die `sort` Funktion übergibt jedes Element der Liste an `key`.",
          "Lambda: `lambda x: x[1]` nimmt das Tupel `x` und gibt den Wert an Index 1 zurück.",
          "Sortierung: Python sortiert die Liste basierend auf diesen Rückgabewerten (5, 10, 20)."
        ],
        "content": "In Java entspricht dies `Comparator.comparing(x -> x.get(1))`."
      },
      "mini_glossary": [
        {
          "term": "Key Argument",
          "definition": "Funktion, die für jedes Element einen Vergleichswert generiert."
        }
      ]
    },
    {
      "question": "247. Sie nutzen Matplotlib. Der Plot wurde konfiguriert, ist aber noch nicht sichtbar. Welcher Befehl fehlt am Ende?\n\n```python\n1: import matplotlib.pyplot as plt\n2: plt.plot([1, 2, 3], [4, 5, 6])\n3: __________\n```",
      "options": [
        "plt.render()",
        "plt.display()",
        "plt.show()",
        "plt.open()"
      ],
      "answer": 2,
      "explanation": "`plt.show()` ist der Befehl, um das Grafikfenster tatsächlich zu öffnen und den Plot zu rendern. In Jupyter Notebooks passiert dies oft automatisch, in Skripten ist es zwingend.",
      "weight": 2,
      "topic": "Datenvisualisierung",
      "concept": "Plot Display",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Der Show-Befehl",
        "steps": [
          "Konfiguration: Alle `plt.plot`, `plt.title` etc. Befehle ändern den internen Status.",
          "Abschluss: `plt.show()` 'flusht' diesen Status auf den Bildschirm.",
          "Blockierend: In Skripten blockiert `show()` oft die Ausführung, bis das Fenster geschlossen wird."
        ],
        "content": "Ohne `show()` läuft das Skript durch, ohne dass Sie etwas sehen."
      },
      "mini_glossary": [
        {
          "term": "Backend",
          "definition": "Die Engine von Matplotlib, die die Pixel tatsächlich zeichnet (z.B. Qt, TkAgg)."
        }
      ]
    },
    {
      "question": "248. Welcher Operator fehlt, um zu prüfen, ob ein Element in einer Liste enthalten ist (Java: `contains`)?\n\n```python\n1: blacklist = [1, 5, 9]\n2: user_id = 5\n3: if user_id __________ blacklist:\n4:     print('Zugriff verweigert')\n```",
      "options": [
        "exists in (englischer Ausdruck)",
        "contains (Methodenname in anderen Sprachen)",
        "in (Membership-Operator) – im Kontext.",
        "inside (englischer Ausdruck)"
      ],
      "answer": 2,
      "explanation": "Der `in` Operator prüft auf Mitgliedschaft. Er funktioniert für Listen, Tupel, Sets, Dictionaries (Keys) und Strings.",
      "weight": 3,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Membership Operator",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Pythonic Membership Testing",
        "steps": [
          "Syntax: `x in container`.",
          "Rückgabe: `True` oder `False`.",
          "Performance: Bei Listen O(n), bei Sets/Dicts O(1).",
          "Lesbarkeit: Liest sich fast wie englischer Satz."
        ],
        "content": "Vermeiden Sie manuelle Schleifen zum Suchen, nutzen Sie `in`."
      },
      "mini_glossary": [
        {
          "term": "Membership Test",
          "definition": "Überprüfung, ob ein Wert Teil einer Kollektion ist."
        }
      ]
    },
    {
      "question": "249. Welches Schlüsselwort fehlt für die 'else if' Bedingung in Python?\n\n```python\n1: x = 10\n2: if x < 0:\n3:     print('Negativ')\n4: __________ x == 0:\n5:     print('Null')\n6: else:\n7:     print('Positiv')\n```",
      "options": [
        "else if – im Kontext.",
        "elseif (Begriff)",
        "elif (Begriff)",
        "elsif (Begriff)"
      ],
      "answer": 2,
      "explanation": "Python kürzt `else if` strikt zu `elif` ab. `else if` (wie in Java) oder `elseif` (wie in PHP) verursachen einen Syntaxfehler.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Control Flow",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Control Flow",
          "definition": "Steuerung des Programmablaufs durch Bedingungen und Schleifen."
        }
      ]
    },
    {
      "question": "250. Welches Attribut nutzen Sie, um die Dimensionen (Zeilen, Spalten) eines DataFrames abzufragen (ohne Klammern, da es keine Methode ist)?\n\n```python\n1: import pandas as pd\n2: df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n3: # Ausgabe erwartet: (2, 2)\n4: print(df.__________)\n```",
      "options": [
        "size",
        "shape",
        "dim()",
        "length"
      ],
      "answer": 1,
      "explanation": "`.shape` ist ein Attribut (Property) von DataFrames und NumPy Arrays, das ein Tupel `(n_rows, n_cols)` liefert. Es wird ohne Klammern `()` aufgerufen.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "DataFrame Shape",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Attribute",
          "definition": "Eigenschaft eines Objekts, die ohne Klammern abgerufen wird."
        }
      ]
    },
    {
      "question": "251. Sie definieren eine Funktion, die noch nichts tun soll (Placeholder). Welches Schlüsselwort verhindert einen `IndentationError`?\n\n```python\n1: def todo_later():\n2:     __________\n```",
      "options": [
        "null (Begriff)",
        "continue – im Kontext.",
        "pass (Begriff)",
        "void (Begriff)"
      ],
      "answer": 2,
      "explanation": "`pass` ist eine Anweisung, die nichts tut (No-Op). Sie wird benötigt, weil Python leere Blöcke (anders als `{}` in Java) syntaktisch nicht erlaubt.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Placeholder",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "pass",
          "definition": "Null-Operation; Platzhalter für syntaktisch notwendigen Code."
        }
      ]
    },
    {
      "question": "252. Welches Keyword entspricht dem Java-Wert `null`?\n\n```python\n1: result = fetch_data()\n2: if result is __________:\n3:     print('Keine Daten gefunden')\n```",
      "options": [
        "null",
        "Null",
        "nil",
        "None"
      ],
      "answer": 3,
      "explanation": "Das Python-Äquivalent zum Null-Referenztyp ist `None`. Es ist ein Singleton-Objekt und wird üblicherweise mit `is` geprüft.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "NoneType",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Singleton",
          "definition": "Klasse, von der nur eine einzige Instanz existieren kann."
        }
      ]
    },
    {
      "question": "253. Wie greifen Sie auf die Spaltennamen eines DataFrames zu?\n\n```python\n1: df = pd.read_csv('data.csv')\n2: # Liste alle Header auf\n3: headers = df.__________\n```",
      "options": [
        "keys()",
        "columns",
        "names",
        "header"
      ],
      "answer": 1,
      "explanation": "`df.columns` liefert ein Index-Objekt mit den Spaltennamen. `keys()` funktioniert auch, ist aber weniger explizit und eher für Dictionaries typisch.",
      "weight": 1,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Column Access",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Index Object",
          "definition": "Unveränderliche Sequenz, die für Achsenbeschriftungen in Pandas genutzt wird."
        }
      ]
    },
    {
      "question": "254. Welches Wort fehlt, um zu prüfen, ob zwei Variablen auf *dasselbe* Objekt im Speicher zeigen (Referenzgleichheit)?\n\n```python\n1: a = [1, 2]\n2: b = a\n3: if a __________ b:\n4:     print('Identische Objekte')\n```",
      "options": [
        "== (Wertgleichheit)",
        "equals (Methodenname in anderen Sprachen)",
        "is (Identität/Referenzgleichheit)",
        "same (englischer Ausdruck)"
      ],
      "answer": 2,
      "explanation": "`is` prüft die Objekt-Identität (Speicheradresse), ähnlich wie `==` bei Objekten in Java. Der Operator `==` in Python prüft dagegen die Wertegleichheit (`equals()`).",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Identity vs Equality",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "is vs ==",
        "steps": [
          "Vergleich `a == b`: Haben beide den gleichen Inhalt? (Value equality).",
          "Vergleich `a is b`: Sind es dieselben Speicheradressen? (Reference equality).",
          "Java-Analogie: Python `is` entspricht Java `==` (bei Objekten). Python `==` entspricht Java `.equals()`."
        ],
        "content": "Verwenden Sie `is` hauptsächlich für Singletons wie `None`."
      },
      "mini_glossary": [
        {
          "term": "Identity",
          "definition": "Eindeutige Kennung eines Objekts (meist Speicheradresse)."
        }
      ]
    },
    {
      "question": "255. Wie ermitteln Sie global die Länge einer Liste, eines Strings oder eines DataFrames?\n\n```python\n1: my_list = [10, 20, 30]\n2: count = __________(my_list)\n```",
      "options": [
        "my_list.length",
        "my_list.size()",
        "len – in der Praxis.",
        "count – in der Praxis."
      ],
      "answer": 2,
      "explanation": "In Python ist `len()` eine Built-in Funktion, keine Methode des Objekts. Man ruft `len(obj)` auf, nicht `obj.len()`.",
      "weight": 1,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Length Function",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "Built-in",
          "definition": "Funktion, die in Python standardmäßig verfügbar ist ohne Import."
        }
      ]
    },
    {
      "question": "256. Welches Keyword entfernt ein Element aus einem Dictionary oder eine Variable aus dem Speicher?\n\n```python\n1: my_dict = {'a': 1, 'b': 2}\n2: __________ my_dict['a']\n3: # Jetzt enthält my_dict nur noch {'b': 2}\n```",
      "options": [
        "remove (Methodenname)",
        "delete (englischer Begriff)",
        "del (Python-Keyword)",
        "erase (englischer Begriff)"
      ],
      "answer": 2,
      "explanation": "`del` ist ein Statement, das Referenzen löscht. `del my_dict['a']` entfernt den Schlüssel 'a'.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Deletion",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Das del Statement",
        "steps": [
          "Dictionary: `del d['k']` löscht Key-Value-Paar.",
          "Liste: `del l[0]` löscht Element am Index.",
          "Variable: `del v` löscht den Variablennamen aus dem Scope."
        ],
        "content": "Alternativ gibt es oft `.pop()`, was den Wert vor dem Löschen noch zurückgibt."
      },
      "mini_glossary": [
        {
          "term": "Garbage Collection",
          "definition": "Automatische Speicherbereinigung ungenutzter Objekte."
        }
      ]
    },
    {
      "question": "257. Sie wollen Code zu Debugging-Zwecken validieren. Wenn die Bedingung falsch ist, soll ein Fehler fliegen.\n\n```python\n1: x = -5\n2: __________ x >= 0, 'x muss positiv sein'\n```",
      "options": [
        "check",
        "ensure",
        "test",
        "assert"
      ],
      "answer": 3,
      "explanation": "`assert condition, message` prüft eine Bedingung. Ist sie False, wird ein `AssertionError` mit der Nachricht geworfen. Wird oft für interne Konsistenzchecks genutzt.",
      "weight": 2,
      "topic": "Grundlagen & Werkzeuge",
      "concept": "Assertions",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Assertions nutzen",
        "steps": [
          "Zweck: Annahmen im Code sicherstellen.",
          "Syntax: `assert <Bedingung>, <Fehlermeldung>`.",
          "Verhalten: Wenn Bedingung True -> weiter. Wenn False -> Crash.",
          "Produktion: Kann mit `-O` Flag deaktiviert werden."
        ],
        "content": "In Java ähnlich wie `assert condition : message;`."
      },
      "mini_glossary": [
        {
          "term": "Assertion",
          "definition": "Aussage, die im Programmablauf als wahr angenommen wird."
        }
      ]
    },
    {
      "question": "258. Um ein Pandas DataFrame zu transponieren (Zeilen und Spalten tauschen), nutzen Sie welches Attribut?\n\n```python\n1: df = pd.DataFrame([[1, 2], [3, 4]])\n2: df_transposed = df.__________\n```",
      "options": [
        "transpose() (Methode zum Transponieren)",
        "T (Attribut für Transponieren)",
        "swapaxes() (Methode für Achsenwechsel)",
        "flip() (Begriff aus anderen Kontexten)"
      ],
      "answer": 1,
      "explanation": "`.T` ist der Kurz-Accessor (Property) für das Transponieren, analog zu NumPy-Matrizen. `.transpose()` existiert zwar als Methode, `.T` ist aber idiomatisch.",
      "weight": 2,
      "topic": "Datenvorbereitung & EDA",
      "concept": "Transposition",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Transponieren",
        "steps": [
          "Ausgang: Zeilen sind Beobachtungen, Spalten sind Features.",
          "Ziel: Features sollen Zeilen sein.",
          "Code: `df.T` spiegelt den DataFrame an der Hauptdiagonalen.",
          "Typ: Datentypen können sich ändern, wenn Spalten nicht uniform waren."
        ],
        "content": "Häufig nötig, um Daten für bestimmte Plots oder Algorithmen vorzubereiten."
      },
      "mini_glossary": [
        {
          "term": "Transponierte Matrix",
          "definition": "Matrix, die durch Vertauschen von Zeilen und Spalten entsteht."
        }
      ]
    },
    {
      "question": "259. Wofür steht das Akronym QUA³CK im gleichnamigen Prozessmodell?",
      "options": [
        "Question, Understanding, Algorithm-Adapting-Adjusting, Conclude, Knowledge",
        "Quality, Understanding, Analysis-Assessment-Adjustment, Comparison, Knowledge",
        "Question, Utilization, Algorithm-Application-Automation, Conclude, Kommunikation",
        "Query, Understanding, Automation-Acceleration-Adaptation, Classification, Key-Transfer"
      ],
      "answer": 0,
      "explanation": "QUA³CK steht für Question (Fragestellung), Understanding the data (Datenverständnis), A³ (Algorithm selection, Adapting features, Adjusting hyperparameters), Conclude and compare (Schlussfolgerung und Vergleich) sowie Knowledge transfer (Wissenstransfer). Diese fünf Phasen bilden das strukturierte Framework für Machine-Learning-Projekte.",
      "weight": 1,
      "topic": "MLOps & Deployment",
      "concept": "QUA³CK Akronym",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "QUA³CK-Prozessmodell",
          "definition": "Ein strukturiertes Framework für Data-Science-Projekte, entwickelt am KIT, das fünf systematische Phasen von der Fragestellung bis zum Deployment umfasst."
        },
        {
          "term": "A³-Schleife",
          "definition": "Die iterative Phase im QUA³CK-Modell bestehend aus Algorithm selection, Adapting features und Adjusting hyperparameters."
        },
        {
          "term": "Knowledge Transfer",
          "definition": "Die letzte Phase des QUA³CK-Prozesses, in der Ergebnisse dokumentiert, kommuniziert und in produktive Systeme überführt werden."
        },
        {
          "term": "Fragestellung",
          "definition": "Die erste Phase (Q) im QUA³CK-Modell, in der Problem, Zielgruppe und KPIs präzise definiert werden."
        },
        {
          "term": "Datenverständnis",
          "definition": "Die Phase U im QUA³CK-Modell, die explorative Datenanalyse zur Gewinnung von Einblicken in Datenstruktur und -qualität umfasst."
        },
        {
          "term": "Conclude and Compare",
          "definition": "Phase C des QUA³CK-Modells, in der Modelle anhand definierter Metriken bewertet und systematisch verglichen werden."
        }
      ]
    },
    {
      "question": "260. Laut dem Kursmaterial wird prognostiziert, dass der MLOps-Markt von 1,7 Mrd. USD (2024) auf welchen Wert im Jahr 2034 wachsen wird?",
      "options": [
        "39 Mrd. USD",
        "25 Mrd. USD",
        "52 Mrd. USD"
      ],
      "answer": 0,
      "explanation": "Laut Grand View Research (2024) wird der MLOps-Markt von 1,7 Mrd. USD im Jahr 2024 auf prognostizierte 39 Mrd. USD im Jahr 2034 wachsen. Dies entspricht einem jährlichen Wachstum von 40,5 Prozent und unterstreicht die zunehmende Bedeutung strukturierter MLOps-Praktiken in der Industrie.",
      "weight": 1,
      "topic": "MLOps & Deployment",
      "concept": "MLOps-Marktwachstum",
      "cognitive_level": "Reproduction",
      "extended_explanation": null,
      "mini_glossary": [
        {
          "term": "MLOps",
          "definition": "Machine Learning Operations - eine Reihe von Praktiken zur Standardisierung und Rationalisierung von Entwicklung, Einsatz und Wartung von ML-Modellen in der Produktion."
        },
        {
          "term": "Grand View Research",
          "definition": "Ein Marktforschungsunternehmen, das die Prognose zum MLOps-Marktwachstum bis 2034 erstellt hat."
        },
        {
          "term": "Produktionsreife",
          "definition": "Der Zustand, in dem ein ML-Modell vollständig entwickelt, getestet und bereit für den Einsatz in realen Anwendungen ist."
        },
        {
          "term": "ML-Lifecycle",
          "definition": "Der gesamte Lebenszyklus eines Machine-Learning-Projekts von der Konzeption über Entwicklung bis zur Wartung."
        },
        {
          "term": "Kostenreduktion",
          "definition": "Unternehmen mit ausgereiften MLOps-Praktiken berichten von 40 Prozent Kostenreduktion im ML-Lifecycle."
        },
        {
          "term": "Modell-Performance",
          "definition": "Die Leistungsfähigkeit eines ML-Modells, die durch MLOps-Praktiken um bis zu 97 Prozent verbessert werden kann."
        }
      ]
    },
    {
      "question": "261. Ein Unternehmen möchte ein ML-Projekt starten, um Kundenabwanderung vorherzusagen. Welche Elemente müssen in der Q-Phase (Question) zwingend definiert werden?",
      "options": [
        "Problemstellung, Zielgruppe, Erfolgsmetriken (KPIs) und Deployment-Ziel",
        "Datenquellen, Algorithmusauswahl, Trainingszeit und Hardwareanforderungen",
        "Feature Engineering, Hyperparameter, Modellarchitektur und Validierungsstrategie",
        "Explorative Datenanalyse, Visualisierungen, statistische Tests und Korrelationsanalysen",
        "Code-Repository, Dokumentationsformat, Teamzusammensetzung und Budget"
      ],
      "answer": 0,
      "explanation": "In der Q-Phase müssen vier zentrale Elemente definiert werden: die klare Problemstellung (Was soll gelöst werden?), die Zielgruppe (Für wen?), quantitative Erfolgsmetriken bzw. KPIs (Wie wird Erfolg gemessen?) und das Deployment-Ziel (Welche Artefakte entstehen?). Diese Elemente bilden das Fundament und reduzieren das Risiko des Projektscheiterns erheblich.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Problemdefinition",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Kritische Elemente der Q-Phase",
        "steps": [
          "Präzise Problemstellung formulieren, die das konkrete Business-Problem beschreibt",
          "Zielgruppe identifizieren, um die Lösung bedarfsgerecht zu gestalten",
          "Quantifizierbare KPIs definieren (z.B. Genauigkeit > 90%, Prediction Time < 200ms)",
          "Deployment-Ziel festlegen, um die finalen Artefakte zu klären (z.B. Web-App, API)"
        ],
        "content": "Die Q-Phase ist entscheidend, da 85-87 Prozent der Data-Science-Projekte scheitern, hauptsächlich aufgrund unklarer Problemdefinition und fehlender Erfolgsmetriken. Eine systematische Definition dieser vier Elemente legt den Grundstein für alle nachfolgenden Phasen und stellt sicher, dass das Projekt relevante Fragen beantwortet und messbare Ergebnisse liefert."
      },
      "mini_glossary": [
        {
          "term": "Q-Phase",
          "definition": "Die erste Phase des QUA³CK-Modells zur präzisen Definition des Geschäftsproblems und der Projektziele."
        },
        {
          "term": "KPIs",
          "definition": "Key Performance Indicators - quantitative Metriken zur Messung des Projekterfolgs, z.B. Genauigkeit oder Inferenzzeit."
        },
        {
          "term": "Deployment-Ziel",
          "definition": "Die finalen Artefakte eines ML-Projekts, z.B. interaktive Web-App, API oder Dashboard."
        },
        {
          "term": "Problemstellung",
          "definition": "Die konkrete Beschreibung des zu lösenden Business-Problems als Grundlage des ML-Projekts."
        },
        {
          "term": "Zielgruppe",
          "definition": "Die Personen oder Organisationen, für die die ML-Lösung entwickelt wird."
        },
        {
          "term": "Projektscheitern",
          "definition": "85-87 Prozent der Data-Science-Projekte scheitern, hauptsächlich wegen unklarer Problemdefinition."
        }
      ]
    },
    {
      "question": "262. In der U-Phase (Understanding the Data) des Iris-Projekts wurde festgestellt, dass bestimmte Merkmalskombinationen eine bessere Klassifikation ermöglichen als andere. Welche Visualisierungsmethode eignet sich am besten, um die Trennschärfe zwischen Klassen zu beurteilen?",
      "options": [
        "Scatter-Plots für Merkmalskombinationen und Box-Plots für Verteilungen pro Klasse",
        "Histogramme für Häufigkeitsverteilungen und Tortendiagramme für Klassenanteile",
        "Zeitreihendiagramme für zeitliche Entwicklung und Balkendiagramme für Mittelwerte",
        "Heatmaps für Korrelationen und Liniendiagramme für Trendverläufe"
      ],
      "answer": 0,
      "explanation": "Scatter-Plots zeigen die räumliche Verteilung der Datenpunkte verschiedener Klassen in zwei Dimensionen und machen Überlappungen oder Trennungen visuell erkennbar. Box-Plots ergänzen dies, indem sie die Verteilungen pro Klasse darstellen und Varianz sowie Überlappungsbereiche verdeutlichen. Im Iris-Projekt zeigten diese Visualisierungen, dass Petal-Merkmale eine deutlich bessere Trennschärfe aufweisen als Sepal-Merkmale.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Explorative Datenanalyse",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Visualisierungsstrategien in der EDA",
        "steps": [
          "Scatter-Plots erstellen, um die räumliche Verteilung von Merkmalskombinationen zu untersuchen",
          "Überlappungen zwischen Klassen identifizieren und visuell bewerten",
          "Box-Plots verwenden, um Verteilungen, Mediane und Quartile pro Klasse zu analysieren",
          "Aus der Kombination beider Visualisierungen Rückschlüsse auf Feature-Bedeutung ziehen"
        ],
        "content": "Die visuelle Analyse ist ein zentraler Bestandteil der U-Phase und ermöglicht intuitive Einblicke, die bei reinen statistischen Kennzahlen verborgen bleiben könnten. Im Iris-Beispiel zeigte der Scatter-Plot von Petal Length vs. Width eine klare Trennung der Setosa-Klasse und nur geringe Überlappung zwischen Versicolor und Virginica, während Sepal-Merkmale deutlich mehr Überlappung aufwiesen. Diese Erkenntnisse sind wertvoll für die nachfolgende Modellauswahl und Feature-Engineering-Entscheidungen."
      },
      "mini_glossary": [
        {
          "term": "EDA",
          "definition": "Explorative Datenanalyse - systematische Untersuchung von Datensätzen zur Identifikation von Mustern, Anomalien und Zusammenhängen."
        },
        {
          "term": "Scatter-Plot",
          "definition": "Punktdiagramm zur Darstellung der Beziehung zwischen zwei Variablen, zeigt Trennschärfe zwischen Klassen."
        },
        {
          "term": "Box-Plot",
          "definition": "Grafische Darstellung von Verteilungen mit Median, Quartilen und Ausreißern pro Klasse."
        },
        {
          "term": "Trennschärfe",
          "definition": "Das Ausmaß, in dem verschiedene Klassen anhand ihrer Merkmale unterschieden werden können."
        },
        {
          "term": "Petal-Merkmale",
          "definition": "Blütenblattlänge und -breite beim Iris-Datensatz, die bessere Klassifikation ermöglichen als Kelchblatt-Merkmale."
        },
        {
          "term": "Überlappung",
          "definition": "Bereiche, in denen sich Datenpunkte verschiedener Klassen im Merkmalsraum überlagern."
        }
      ]
    },
    {
      "question": "263. Welche der folgenden Aktivitäten gehört zur A³-Schleife des QUA³CK-Modells?",
      "options": [
        "Hyperparameter-Optimierung mit Grid Search zur Verbesserung der Modellleistung",
        "Definition von KPIs und Erfolgsmetriken für das Gesamtprojekt",
        "Erstellung einer Streamlit-App zur Bereitstellung des finalen Modells",
        "Durchführung explorativer Datenanalyse mit Visualisierungen"
      ],
      "answer": 0,
      "explanation": "Die A³-Schleife umfasst Algorithm selection, Adapting features und Adjusting hyperparameters. Die Hyperparameter-Optimierung ist Teil des dritten A (Adjusting) und dient der Feinabstimmung der Modellparameter. KPI-Definition gehört zur Q-Phase, Deployment zur K-Phase und EDA zur U-Phase.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "A³-Komponenten",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Die drei Dimensionen der A³-Schleife",
        "steps": [
          "Algorithm Selection: Auswahl geeigneter Algorithmen basierend auf Problemstellung und Dateneigenschaften",
          "Adapting Features: Transformation und Engineering von Merkmalen zur Verbesserung der Modellleistung",
          "Adjusting Hyperparameters: Feinabstimmung der Modellparameter durch Techniken wie Grid Search oder Random Search"
        ],
        "content": "Die A³-Schleife ist das iterative Herzstück des QUA³CK-Prozesses. Diese drei Schritte werden typischerweise mehrfach durchlaufen, wobei die Ergebnisse jeder Iteration zur kontinuierlichen Verbesserung beitragen. Im MLOps-Kontext werden alle Experimente mit Tools wie MLFlow systematisch protokolliert, um Reproduzierbarkeit und Vergleichbarkeit zu gewährleisten. Die Schleife endet, wenn die definierten KPIs aus der Q-Phase erreicht sind."
      },
      "mini_glossary": [
        {
          "term": "A³-Schleife",
          "definition": "Die iterative Phase im QUA³CK-Modell bestehend aus Algorithm selection, Adapting features und Adjusting hyperparameters."
        },
        {
          "term": "Hyperparameter",
          "definition": "Parameter, die den Lernprozess und die Modellstruktur steuern und vor dem Training festgelegt werden."
        },
        {
          "term": "Grid Search",
          "definition": "Systematische Suchmethode zur Optimierung von Hyperparametern durch Ausprobieren aller Kombinationen."
        },
        {
          "term": "Feature Engineering",
          "definition": "Der Prozess der Erstellung neuer Features aus bestehenden Rohdaten zur Verbesserung der Modellleistung."
        },
        {
          "term": "Algorithm Selection",
          "definition": "Auswahl geeigneter ML-Algorithmen basierend auf Problemstellung, Dateneigenschaften und Anforderungen."
        },
        {
          "term": "Iterativer Prozess",
          "definition": "Wiederholte Durchläufe der A³-Komponenten zur schrittweisen Verbesserung der Modellleistung."
        }
      ]
    },
    {
      "question": "264. Für das Iris-Projekt wurde als Deployment-Ziel eine öffentliche Streamlit Cloud App festgelegt. Welche Phase des QUA³CK-Modells ist primär für die Umsetzung dieses Ziels verantwortlich?",
      "options": [
        "K-Phase (Knowledge Transfer), da hier die Überführung in produktive Systeme erfolgt",
        "A³-Phase (Algorithms), weil dort die Modellarchitektur für das Deployment optimiert wird",
        "C-Phase (Conclude), da hier das beste Modell für die Bereitstellung ausgewählt wird",
        "Q-Phase (Question), weil dort das Deployment-Ziel initial definiert wurde",
        "U-Phase (Understanding), da die Datenstruktur das Deployment-Format bestimmt"
      ],
      "answer": 0,
      "explanation": "Die K-Phase (Knowledge Transfer) ist für die tatsächliche Umsetzung des Deployments verantwortlich. Hier wird das trainierte Modell in eine produktive Anwendung überführt, die Dokumentation erstellt und die Kommunikation der Ergebnisse sichergestellt. Obwohl das Deployment-Ziel in der Q-Phase definiert wurde, erfolgt die technische Implementierung in der K-Phase.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Deployment",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Deployment als Teil des Knowledge Transfers",
        "steps": [
          "Code-Strukturierung für die Integration des trainierten Modells in die Anwendung",
          "Erstellung der Streamlit-Applikation mit Benutzeroberfläche für Eingaben und Vorhersagen",
          "Deployment in der Streamlit Cloud zur öffentlichen Verfügbarkeit",
          "Dokumentation und Portfolio-Integration (GitHub Repository, README)"
        ],
        "content": "Die K-Phase schlägt die Brücke zwischen experimenteller Entwicklung und produktivem Einsatz. Im akademischen Kontext bedeutet dies die Erstellung eines professionellen Portfolios, während in der Praxis die Überführung in produktive Systeme zentral ist. Der AMALEA-Ansatz betont moderne MLOps-Praktiken mit Cloud-Deployment und automatisierter Modellbereitstellung, im Gegensatz zu traditionellen Ansätzen mit lokaler, manueller Bereitstellung."
      },
      "mini_glossary": [
        {
          "term": "K-Phase",
          "definition": "Knowledge Transfer - die letzte QUA³CK-Phase zur Dokumentation, Kommunikation und Überführung in produktive Systeme."
        },
        {
          "term": "Streamlit",
          "definition": "Open-Source Python-Bibliothek zur schnellen Erstellung interaktiver Webanwendungen für Data Science."
        },
        {
          "term": "Deployment",
          "definition": "Der Prozess der Überführung eines trainierten ML-Modells in eine Produktionsumgebung."
        },
        {
          "term": "Cloud-Deployment",
          "definition": "Bereitstellung von Anwendungen in Cloud-Infrastrukturen wie Streamlit Cloud für öffentlichen Zugriff."
        },
        {
          "term": "Portfolio",
          "definition": "Sammlung von Projekten zur Demonstration erworbener Data-Science-Fähigkeiten."
        },
        {
          "term": "Produktive Systeme",
          "definition": "Einsatzbereite Anwendungen, die von Endnutzern für reale Aufgaben verwendet werden können."
        }
      ]
    },
    {
      "question": "265. Ein Data-Science-Team verwendet MLFlow im AMALEA-Ansatz. Welche MLFlow-Funktionalität ist für die systematische Protokollierung von Experimenten in der A³-Phase am wichtigsten?",
      "options": [
        "Logging von Parametern – im Kurskontext – im Kontext.",
        "Automatische Datensatzaufteilung mit mlflow.train_test_split()",
        "Visualisierung von Scatter-Plots mit mlflow.visualize()",
        "Definition von KPIs mit mlflow.set_kpi()"
      ],
      "answer": 0,
      "explanation": "Die zentrale MLFlow-Funktionalität für Experiment Tracking ist das Logging von Parametern (z.B. max_depth, learning_rate), Metriken (z.B. accuracy, F1-score) und Artefakten (z.B. trainierte Modelle). Dies ermöglicht Reproduzierbarkeit, systematischen Vergleich verschiedener Experimente und die Dokumentation des gesamten ML-Entwicklungsprozesses.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Experiment Tracking",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "MLFlow Experiment Tracking in der Praxis",
        "steps": [
          "Experiment erstellen mit mlflow.set_experiment() zur Organisation verschiedener Versuchsreihen",
          "Run starten mit mlflow.start_run() zur Gruppierung zusammengehöriger Logs",
          "Parameter protokollieren mit mlflow.log_param() für alle Hyperparameter-Einstellungen",
          "Metriken aufzeichnen mit mlflow.log_metric() für Evaluierungsergebnisse wie Accuracy"
        ],
        "content": "MLFlow ermöglicht die systematische Protokollierung von Experimenten, wodurch alle Informationen zu Code, Parametern, Metriken und Artefakten nachvollziehbar gespeichert werden. Dies steigert die Reproduzierbarkeit erheblich und vereinfacht Modellvergleiche. Im Gegensatz zu traditionellen Ansätzen mit manuellen Excel-Reports bietet MLFlow automatisierte, strukturierte Dokumentation und ein Dashboard zur Visualisierung. Unternehmen mit ausgereiften MLOps-Praktiken berichten von 40 Prozent Kostenreduktion im ML-Lifecycle."
      },
      "mini_glossary": [
        {
          "term": "MLFlow",
          "definition": "Open-Source-Plattform zur Verwaltung des gesamten ML-Lebenszyklus, einschließlich Experiment Tracking und Modellbereitstellung."
        },
        {
          "term": "Experiment Tracking",
          "definition": "Systematisches Protokollieren und Organisieren von ML-Experimenten mit Parametern, Metriken und Artefakten."
        },
        {
          "term": "mlflow.log_param()",
          "definition": "MLFlow-Funktion zum Protokollieren von Hyperparametern eines Experiments."
        },
        {
          "term": "mlflow.log_metric()",
          "definition": "MLFlow-Funktion zum Aufzeichnen von Evaluierungsmetriken wie Accuracy oder F1-Score."
        },
        {
          "term": "Artefakte",
          "definition": "Dateien oder Objekte, die während eines ML-Experiments erzeugt werden, z.B. trainierte Modelle oder Plots."
        },
        {
          "term": "Reproduzierbarkeit",
          "definition": "Die Fähigkeit, ML-Experimente unter gleichen Bedingungen exakt zu replizieren."
        }
      ]
    },
    {
      "question": "266. Basierend auf den EDA-Erkenntnissen des Iris-Projekts wurde festgestellt, dass Petal-Merkmale prädiktiv stärker sind als Sepal-Merkmale. Welche Schlussfolgerung sollte für die Modellierung in der A³-Phase gezogen werden?",
      "options": [
        "Petal-Merkmale sollten im Modell höher gewichtet werden oder bei Feature Selection priorisiert werden",
        "Sepal-Merkmale sollten vollständig aus dem Datensatz entfernt werden – im Kontext.",
        "Alle Merkmale sollten gleich gewichtet werden, um Bias zu vermeiden – im Kontext.",
        "Nur Petal Length sollte verwendet werden, da es das wichtigste einzelne Merkmal ist",
        "Die Merkmale sollten invertiert werden, um die Modellkomplexität zu erhöhen – im Kontext."
      ],
      "answer": 0,
      "explanation": "Die EDA-Erkenntnis, dass Petal-Merkmale eine bessere Klassentrennung zeigen, sollte in der Modellierung berücksichtigt werden. Dies kann durch höhere Gewichtung, Priorisierung bei Feature Selection oder bewusste Modellwahl (z.B. Entscheidungsbäume, die wichtige Features näher an der Wurzel platzieren) erfolgen. Ein vollständiges Entfernen von Sepal-Merkmalen wäre übertrieben, da sie zusätzliche Information liefern können.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Feature-Bedeutung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Von EDA-Erkenntnissen zu Modellierungsentscheidungen",
        "steps": [
          "EDA-Visualisierungen analysieren und Feature-Bedeutung quantifizieren",
          "Prädiktiv starke Features identifizieren (hier: Petal Length und Width)",
          "Feature Selection oder Feature Weighting basierend auf Erkenntnissen durchführen",
          "Algorithmen wählen, die Feature-Bedeutung berücksichtigen können (z.B. Decision Trees)"
        ],
        "content": "Die systematische Analyse in der U-Phase liefert wertvolle Hinweise für die A³-Phase. Bei interpretierbaren Modellen wie Entscheidungsbäumen würden Petal-Merkmale näher an der Wurzel erscheinen, was die EDA-Erkenntnisse direkt widerspiegelt. Moderne ML-Frameworks bieten zudem Feature Importance-Analysen, die diese Priorisierung automatisch vornehmen. Die Verbindung zwischen U- und A³-Phase ist ein Beispiel für die iterative Natur des QUA³CK-Prozesses."
      },
      "mini_glossary": [
        {
          "term": "Feature Selection",
          "definition": "Der Prozess der Auswahl der relevantesten Merkmale zur Verbesserung der Modellleistung und Reduzierung von Overfitting."
        },
        {
          "term": "Feature Weighting",
          "definition": "Zuweisen unterschiedlicher Gewichte zu Features basierend auf ihrer prädiktiven Bedeutung."
        },
        {
          "term": "Prädiktive Stärke",
          "definition": "Das Ausmaß, in dem ein Feature zur korrekten Vorhersage der Zielvariable beiträgt."
        },
        {
          "term": "Decision Tree",
          "definition": "Baumbasierter Algorithmus, der wichtige Features automatisch näher an der Wurzel platziert."
        },
        {
          "term": "Feature Importance",
          "definition": "Metriken, die die Bedeutung einzelner Features für die Modellvorhersage quantifizieren."
        },
        {
          "term": "Klassentrennung",
          "definition": "Die Fähigkeit von Features, verschiedene Zielklassen im Merkmalsraum zu separieren."
        }
      ]
    },
    {
      "question": "267. Für ein ML-Projekt zur Vorhersage von Maschinenausfällen soll eine passende Erfolgsmetrik definiert werden. Falsch-negative Vorhersagen (tatsächlicher Ausfall wird nicht erkannt) sind kritischer als falsch-positive. Welche Metrik sollte priorisiert werden?",
      "options": [
        "Recall, da es den Anteil der korrekt erkannten tatsächlichen Ausfälle misst",
        "Precision, da es die Genauigkeit der positiven Vorhersagen bewertet",
        "Accuracy, da es den Gesamtanteil korrekter Vorhersagen erfasst",
        "F1-Score, da es beide Metriken gleich gewichtet"
      ],
      "answer": 0,
      "explanation": "Bei kritischen falsch-negativen Vorhersagen sollte Recall priorisiert werden, da diese Metrik den Anteil der tatsächlich positiven Fälle misst, die korrekt identifiziert wurden. Ein hoher Recall bedeutet, dass wenige tatsächliche Ausfälle übersehen werden. Precision wäre wichtiger, wenn falsch-positive Vorhersagen (unnötige Wartungen) kritischer wären.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Metrik-Auswahl",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Kontextabhängige Metrik-Auswahl",
        "steps": [
          "Geschäftskontext analysieren und Kosten verschiedener Fehlertypen bewerten",
          "Kritikalität von False Negatives vs. False Positives bestimmen",
          "Passende Metrik auswählen: Recall bei kritischen FN, Precision bei kritischen FP",
          "Threshold-Anpassung vornehmen, um die priorisierte Metrik zu optimieren"
        ],
        "content": "Die Wahl der Erfolgsmetrik in der Q-Phase ist entscheidend für den Projekterfolg. Recall (auch Sensitivität genannt) ist das Verhältnis der True Positives zu allen tatsächlich positiven Instanzen und beantwortet die Frage: Wie viele der tatsächlichen Ausfälle haben wir erkannt? Bei Maschinenausfällen kann ein übersehener Ausfall zu Produktionsstillstand und hohen Kosten führen, weshalb Recall wichtiger ist als Precision. Der F1-Score würde beide Aspekte gleich gewichten, was in diesem asymmetrischen Kostenkontext nicht optimal ist."
      },
      "mini_glossary": [
        {
          "term": "Recall",
          "definition": "Metrik, die den Anteil der korrekt identifizierten positiven Instanzen an allen tatsächlich positiven Instanzen misst."
        },
        {
          "term": "Precision",
          "definition": "Metrik, die das Verhältnis der True Positives zu allen positiven Vorhersagen bewertet."
        },
        {
          "term": "False Negative",
          "definition": "Fehlertyp, bei dem ein positiver Fall fälschlicherweise als negativ klassifiziert wird."
        },
        {
          "term": "False Positive",
          "definition": "Fehlertyp, bei dem ein negativer Fall fälschlicherweise als positiv klassifiziert wird."
        },
        {
          "term": "F1-Score",
          "definition": "Harmonisches Mittel aus Precision und Recall, das beide Metriken gleich gewichtet."
        },
        {
          "term": "Confusion Matrix",
          "definition": "Tabelle zur Darstellung von True Positives, True Negatives, False Positives und False Negatives."
        }
      ]
    },
    {
      "question": "268. Im Iris-Projekt wurde eine stratifizierte Datenaufteilung (train_test_split mit stratify=y) verwendet. Welcher Vorteil ergibt sich aus dieser Strategie?",
      "options": [
        "Die Klassenverteilung bleibt in Trainings- und Testdaten proportional erhalten",
        "Die Trainingszeit des Modells wird signifikant reduziert – im Kontext.",
        "Overfitting wird vollständig verhindert – in der Praxis – im Kontext.",
        "Die Anzahl der Features wird automatisch optimiert – im Kontext.",
        "Die Hyperparameter werden automatisch angepasst – in der Praxis."
      ],
      "answer": 0,
      "explanation": "Stratifizierte Aufteilung stellt sicher, dass die Proportionen der Klassen in Trainings- und Testdaten gleich bleiben. Dies ist besonders wichtig bei unbalancierten Datensätzen, verhindert aber auch bei balancierten Daten zufällige Verzerrungen. Im Iris-Datensatz mit drei gleich häufigen Klassen garantiert Stratifizierung, dass alle drei Arten proportional in beiden Splits vertreten sind.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Stratified Split",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Stratifizierung für robuste Evaluierung",
        "steps": [
          "Klassenverteilung im Gesamtdatensatz analysieren",
          "Stratify-Parameter in train_test_split setzen, um proportionale Aufteilung zu erzwingen",
          "Verifizieren, dass Trainings- und Testset die gleichen Klassenproportionen aufweisen",
          "Dadurch Bias in der Evaluierung vermeiden und robuste Metriken erhalten"
        ],
        "content": "Die stratifizierte Aufteilung ist eine Best Practice im Machine Learning, um systematische Verzerrungen zu vermeiden. Ohne Stratifizierung könnte bei zufälliger Aufteilung eine Klasse unterrepräsentiert sein, was zu unrealistischen Evaluierungsergebnissen führt. Im Iris-Beispiel mit 50 Instanzen pro Klasse garantiert stratify=y, dass jede der drei Arten mit exakt 33,3 Prozent in Trainings- und Testdaten vertreten ist. Dies ist Teil der robusten Methodologie in der A³-Phase."
      },
      "mini_glossary": [
        {
          "term": "Stratifizierung",
          "definition": "Technik zur proportionalen Aufteilung von Daten, die Klassenverteilungen in allen Splits erhält."
        },
        {
          "term": "Train-Test-Split",
          "definition": "Aufteilung eines Datensatzes in Trainingsmenge (zum Lernen) und Testmenge (zur unabhängigen Evaluierung)."
        },
        {
          "term": "Bias",
          "definition": "Systematische Verzerrung in Daten oder Modellen, die zu unfairen oder ungenauen Ergebnissen führt."
        },
        {
          "term": "Klassenverteilung",
          "definition": "Die Proportionen verschiedener Klassen im Datensatz, z.B. 50-50 oder 70-30."
        },
        {
          "term": "Unbalancierte Daten",
          "definition": "Datensätze, bei denen manche Klassen deutlich häufiger vorkommen als andere."
        },
        {
          "term": "Robuste Evaluierung",
          "definition": "Bewertungsstrategie, die zuverlässige und unverzerrte Leistungsmessungen gewährleistet."
        }
      ]
    },
    {
      "question": "269. Welches Portfolio-Element wird im AMALEA-Kurs besonders betont, um erworbene Data-Science-Fähigkeiten zu demonstrieren?",
      "options": [
        "Öffentliches GitHub-Repository mit README und Streamlit Cloud App",
        "Gedruckte Dissertation mit mathematischen Beweisen",
        "Zertifikate von Online-Kursen – in der Praxis – im Kontext.",
        "Powerpoint-Präsentation für Vorträge – in der Praxis."
      ],
      "answer": 0,
      "explanation": "Der AMALEA-Kurs legt besonderen Wert auf die Erstellung eines professionellen Portfolios mit öffentlichem GitHub-Repository (inklusive strukturierter README zur Projektdokumentation) und einer interaktiven Streamlit Cloud App zur Demonstration des trainierten Modells. Diese Kombination zeigt sowohl technische Fähigkeiten als auch die Kompetenz, Projekte professionell zu dokumentieren und bereitzustellen.",
      "weight": 2,
      "topic": "MLOps & Deployment",
      "concept": "Portfolio-Erstellung",
      "cognitive_level": "Application",
      "extended_explanation": {
        "title": "Professionelles Data-Science-Portfolio",
        "steps": [
          "GitHub-Repository erstellen mit strukturierter README, die Projekt, Methodik und Ergebnisse beschreibt",
          "Streamlit Web-App entwickeln zur interaktiven Demonstration des Modells",
          "App in Streamlit Cloud deployen für öffentlichen Zugriff ohne lokale Installation",
          "Optional: Blog-Post verfassen zur ausführlichen Dokumentation des Projektverlaufs"
        ],
        "content": "Das Portfolio ist in der K-Phase (Knowledge Transfer) verankert und demonstriert die Fähigkeit, komplexe ML-Projekte nicht nur technisch umzusetzen, sondern auch professionell zu kommunizieren. Ein öffentlich zugängliches Repository und eine funktionierende Web-App sind wesentlich aussagekräftiger als theoretische Zertifikate, da sie konkrete, nachvollziehbare Projektergebnisse zeigen. Dies entspricht modernen Industriestandards, wo praktische Fähigkeiten zunehmend wichtiger werden."
      },
      "mini_glossary": [
        {
          "term": "GitHub Repository",
          "definition": "Online-Plattform zur Versionskontrolle und Kollaboration, zentral für professionelle Code-Dokumentation."
        },
        {
          "term": "README",
          "definition": "Markdown-Datei im Repository, die Projekt, Installation, Nutzung und Ergebnisse strukturiert beschreibt."
        },
        {
          "term": "Streamlit Cloud",
          "definition": "Kostenlose Hosting-Plattform für Streamlit-Apps mit einfachem Deployment via GitHub-Integration."
        },
        {
          "term": "Portfolio",
          "definition": "Sammlung von Projekten zur Demonstration erworbener Fähigkeiten für potenzielle Arbeitgeber."
        },
        {
          "term": "Interaktive Web-App",
          "definition": "Anwendung, die es Nutzern erlaubt, mit ML-Modellen zu interagieren und eigene Vorhersagen zu generieren."
        },
        {
          "term": "Blog-Post",
          "definition": "Ausführlicher Artikel zur Dokumentation des Projektverlaufs, veröffentlicht auf Plattformen wie Medium oder LinkedIn."
        }
      ]
    },
    {
      "question": "270. Welches Kriterium ist laut dem Kursmaterial am wichtigsten, um in der C-Phase das beste Modell auszuwählen?",
      "options": [
        "Ein ausgewogener Kompromiss zwischen quantitativen Metriken und qualitativen Kriterien wie Interpretierbarkeit",
        "Ausschließlich die höchste Accuracy auf dem Testdatensatz – in der Praxis – im Kontext.",
        "Die kürzeste Trainingszeit unabhängig von der Leistung – im Kurskontext – im Kontext.",
        "Die größte Anzahl an Parametern für maximale Flexibilität – in der Praxis – im Kontext.",
        "Die Verwendung des neuesten veröffentlichten Algorithmus – in der Praxis – im Kontext."
      ],
      "answer": 0,
      "explanation": "Die C-Phase betont, dass nicht nur die höchste Genauigkeit entscheidend ist, sondern ein ausgewogener Kompromiss zwischen quantitativen Metriken (Accuracy, Precision, Recall, Inferenzzeit) und qualitativen Kriterien (Modellkomplexität, Interpretierbarkeit, Wartungsaufwand). Ein Modell muss neben Genauigkeit auch effizient, interpretierbar und wartbar sein, um in der Praxis erfolgreich eingesetzt zu werden.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Modellauswahl",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Ganzheitliche Modellbewertung",
        "steps": [
          "Quantitative Metriken erheben: Accuracy, Precision, Recall, F1-Score, Inferenzzeit",
          "Qualitative Aspekte bewerten: Komplexität, Interpretierbarkeit, Trainingszeit, Wartungsaufwand",
          "Trade-offs analysieren: Ein einfacheres Modell mit 95 Prozent Accuracy kann einem komplexen mit 96 Prozent vorzuziehen sein",
          "Kontext berücksichtigen: Deployment-Umgebung, Echtzeitanforderungen, Erklärbarkeit für Stakeholder",
          "Entscheidung treffen, die alle Dimensionen ausbalanciert"
        ],
        "content": "Die C-Phase schlägt die Brücke zwischen experimenteller Forschung und praktischer Anwendung. Im Iris-Projekt erreichten Decision Tree und K-Nearest Neighbors beide 97,8 Prozent Accuracy, aber der Decision Tree wurde bevorzugt, da er potenziell interpretierbarer und effizienter ist. Diese ganzheitliche Bewertung ist charakteristisch für den QUA³CK-Ansatz und unterscheidet ihn von rein metrisch-orientierten Methoden. MLOps-Dashboards wie das MLFlow UI ermöglichen dabei einen schnellen, visuellen Vergleich aller relevanten Dimensionen."
      },
      "mini_glossary": [
        {
          "term": "C-Phase",
          "definition": "Conclude and Compare - Phase zur Bewertung und Auswahl des optimalen Modells anhand definierter Metriken."
        },
        {
          "term": "Quantitative Metriken",
          "definition": "Messbare Kennzahlen wie Accuracy, Precision, Recall, die Modellleistung numerisch bewerten."
        },
        {
          "term": "Qualitative Kriterien",
          "definition": "Nicht-numerische Bewertungsdimensionen wie Interpretierbarkeit, Wartbarkeit und Modellkomplexität."
        },
        {
          "term": "Interpretierbarkeit",
          "definition": "Die Verständlichkeit von Modellentscheidungen für Menschen, wichtig für Akzeptanz und Debugging."
        },
        {
          "term": "Trade-offs",
          "definition": "Kompromisse zwischen verschiedenen Zielen, z.B. Genauigkeit vs. Inferenzzeit oder Komplexität vs. Interpretierbarkeit."
        },
        {
          "term": "Inferenzzeit",
          "definition": "Die Zeit, die ein trainiertes Modell für eine einzelne Vorhersage benötigt."
        }
      ]
    },
    {
      "question": "271. Laut Gartner (2017-2019) scheitern 85-87 Prozent der Data-Science-Projekte. Analysieren Sie die Hauptursachen: Welcher strukturelle Faktor ist primär verantwortlich für diese hohe Scheiternrate?",
      "options": [
        "Unklare Problemdefinition und fehlende Abstimmung in der initialen Projektphase",
        "Mangelnde Rechenleistung und Hardware-Limitierungen – im Kontext.",
        "Zu wenige verfügbare Algorithmen und ML-Frameworks – im Kontext.",
        "Fehlende Programmierkenntnisse der Datenwissenschaftler – im Kontext.",
        "Unzureichende Budget-Allokation für Cloud-Dienste – im Kontext."
      ],
      "answer": 0,
      "explanation": "Die häufigsten Gründe für das Scheitern sind laut Material unklare Problemdefinition, fehlende Erfolgsmetriken und mangelnde Stakeholder-Abstimmung - allesamt Defizite in der Q-Phase. Technische Aspekte wie Hardware oder Algorithmen sind selten der limitierende Faktor. Dies unterstreicht die kritische Bedeutung einer präzisen Fragestellung, die das QUA³CK-Modell durch die systematische Q-Phase adressiert.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Ursachenanalyse",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Strukturelle Ursachen des Projektscheiterns",
        "steps": [
          "Probleme in der Q-Phase identifizieren: vage Zielsetzungen, unklare KPIs, fehlende Business-Anbindung",
          "Folgeeffekte analysieren: Ohne klare Fragestellung werden falsche Daten gesammelt und irrelevante Modelle entwickelt",
          "Stakeholder-Misalignment erkennen: Verschiedene Erwartungen führen zu Unzufriedenheit trotz technischer Erfolge",
          "Systematische Prävention: QUA³CK-Modell adressiert diese Probleme explizit durch strukturierte Q-Phase"
        ],
        "content": "Die hohe Scheiternrate von 85-87 Prozent ist primär kein technisches, sondern ein methodisches Problem. Projekte scheitern nicht, weil die Modelle ungenau sind, sondern weil sie das falsche Problem lösen oder niemand weiß, was 'Erfolg' bedeutet. Das QUA³CK-Modell wurde speziell entwickelt, um diese Lücke zu schließen, indem es die Q-Phase als Fundament etabliert. Die Statistik, dass nur 13 Prozent der ML-Projekte die Produktionsreife erreichen, verdeutlicht die Bedeutung strukturierter Prozesse. MLOps-Praktiken können die Erfolgsrate signifikant steigern, aber nur wenn die Grundlage (Q-Phase) stimmt."
      },
      "mini_glossary": [
        {
          "term": "Gartner",
          "definition": "Forschungs- und Beratungsunternehmen, das die 85-87 Prozent Scheiternrate von DS-Projekten dokumentiert hat."
        },
        {
          "term": "Stakeholder-Abstimmung",
          "definition": "Prozess zur Sicherstellung gemeinsamer Erwartungen und Ziele zwischen allen Projektbeteiligten."
        },
        {
          "term": "Problemdefinition",
          "definition": "Präzise Beschreibung des zu lösenden Problems, zentral für den Projekterfolg."
        },
        {
          "term": "Produktionsreife",
          "definition": "Zustand, in dem ein ML-Modell vollständig entwickelt und bereit für den produktiven Einsatz ist."
        },
        {
          "term": "Business-Anbindung",
          "definition": "Verbindung zwischen technischer ML-Lösung und konkretem Geschäftswert oder -nutzen."
        },
        {
          "term": "Methodisches Problem",
          "definition": "Herausforderungen, die aus fehlenden oder unzureichenden Prozessen und Strukturen resultieren."
        }
      ]
    },
    {
      "question": "272. Im Iris-Projekt erreichten Decision Tree und K-Nearest Neighbors beide eine Accuracy von 97,8 Prozent. Der Decision Tree wurde dennoch bevorzugt. Analysieren Sie: Welche Überlegungen rechtfertigen diese Entscheidung trotz identischer Accuracy?",
      "options": [
        "Decision Trees bieten höhere Interpretierbarkeit und potenziell effizientere Inferenz bei gleicher Genauigkeit",
        "K-Nearest Neighbors hat immer eine schlechtere Generalisierung auf neue Daten – im Kontext.",
        "Decision Trees benötigen weniger Speicherplatz für die Trainingsdaten – im Kontext – in der Praxis.",
        "Die Wahl basiert ausschließlich auf persönlichen Präferenzen ohne objektive Kriterien",
        "Decision Trees sind in allen Metriken überlegen, nicht nur in Accuracy – im Kontext."
      ],
      "answer": 0,
      "explanation": "Bei identischer Accuracy werden qualitative Kriterien entscheidend. Decision Trees sind inherent interpretierbarer (Entscheidungsregeln sind nachvollziehbar) und haben oft effizientere Inferenz (nur ein Pfad durch den Baum), während KNN alle Trainingsdaten speichern und mit jedem neuen Punkt vergleichen muss. Diese Überlegungen spiegeln die ganzheitliche Bewertungsstrategie der C-Phase wider.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Trade-off-Analyse",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Tiefenanalyse der Modellwahl",
        "steps": [
          "Quantitative Gleichheit erkennen: Beide Modelle erreichen 97,8 Prozent Accuracy",
          "Interpretierbarkeit analysieren: Decision Trees zeigen explizite If-Then-Regeln, KNN ist black-box-artiger",
          "Inferenzeffizienz bewerten: Decision Tree hat O(log n) Komplexität, KNN benötigt O(n) Distanzberechnungen",
          "Speicheranforderungen vergleichen: KNN muss alle Trainingsdaten vorhalten, Decision Tree nur Baumstruktur",
          "Wartbarkeit einschätzen: Bäume sind einfacher zu debuggen und anzupassen"
        ],
        "content": "Diese Entscheidung exemplifiziert die Philosophie der C-Phase: Accuracy ist nur eine Dimension von vielen. In Produktionsumgebungen sind Faktoren wie Erklärbarkeit (für Stakeholder-Akzeptanz), Inferenzgeschwindigkeit (für Echtzeitanwendungen) und Wartbarkeit (für langfristige Systeme) oft gleichwertig oder wichtiger. Decision Trees ermöglichen zudem Feature Importance-Analysen, die direkt die EDA-Erkenntnisse aus der U-Phase bestätigen (Petal-Features dominieren). Diese ganzheitliche Perspektive unterscheidet reife ML-Entwicklung von rein metrisch-getriebenen Ansätzen."
      },
      "mini_glossary": [
        {
          "term": "Interpretierbarkeit",
          "definition": "Die Fähigkeit, Modellentscheidungen für Menschen nachvollziehbar zu machen, zentral für Vertrauen und Akzeptanz."
        },
        {
          "term": "Inferenzeffizienz",
          "definition": "Geschwindigkeit und Ressourcenbedarf für Vorhersagen in der Produktionsumgebung."
        },
        {
          "term": "K-Nearest Neighbors",
          "definition": "Instanzbasierter Algorithmus, der neue Punkte durch Vergleich mit k nächsten Trainingspunkten klassifiziert."
        },
        {
          "term": "Decision Tree",
          "definition": "Baumbasierter Algorithmus mit If-Then-Regeln, der hohe Interpretierbarkeit bietet."
        },
        {
          "term": "Komplexität",
          "definition": "Maß für den Rechen- oder Speicheraufwand eines Algorithmus, ausgedrückt in Big-O-Notation."
        },
        {
          "term": "Feature Importance",
          "definition": "Analyse, welche Features am stärksten zur Modellvorhersage beitragen, bei Decision Trees direkt ablesbar."
        }
      ]
    },
    {
      "question": "273. Vergleichen Sie den traditionellen Ansatz mit dem AMALEA 2025 MLOps-Ansatz für die C-Phase (Conclude). Welche strukturelle Verbesserung bietet der MLOps-Ansatz?",
      "options": [
        "Automatisierter Modellvergleich via MLFlow UI Dashboard statt manueller Excel-Reports",
        "Vollständige Eliminierung menschlicher Entscheidungen durch KI-gesteuerte Modellselektion",
        "Reduktion der Trainingszeit auf wenige Sekunden durch Cloud-Computing",
        "Automatische Generierung neuer Features ohne menschliches Zutun",
        "Garantierte 100 Prozent Accuracy durch fortgeschrittene Algorithmen"
      ],
      "answer": 0,
      "explanation": "Der AMALEA-Ansatz ersetzt manuelle, fehleranfällige Excel-Reports durch automatisierte Dashboards im MLFlow UI. Dies ermöglicht schnellen, visuellen Vergleich aller Experimente mit Parametern, Metriken und Artefakten. Die Automatisierung steigert Effizienz und Reproduzierbarkeit, ohne menschliche Expertise zu eliminieren - die finale Entscheidung bleibt beim Data Scientist, aber auf Basis besserer Informationen.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "MLOps-Integration",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Evolutionäre Verbesserung durch MLOps",
        "steps": [
          "Traditioneller Ansatz analysieren: Manuelle Erstellung von Vergleichstabellen, fehleranfällig und zeitaufwendig",
          "MLOps-Ansatz verstehen: Automatische Protokollierung aller Experimente in strukturierter Form",
          "Dashboard-Vorteile erkennen: Interaktive Visualisierungen ermöglichen schnelle Mustererkennungen",
          "Reproduzierbarkeit sichern: Alle Informationen zentral gespeichert, jederzeit nachvollziehbar",
          "Kollaboration fördern: Teams können gemeinsam auf dieselbe Experiment-History zugreifen"
        ],
        "content": "Die MLOps-Integration im AMALEA-Ansatz ist keine Revolution, sondern eine Evolution des QUA³CK-Modells. Das Grundprinzip der C-Phase (systematischer Modellvergleich) bleibt erhalten, wird aber durch moderne Tools erheblich verbessert. Unternehmen mit ausgereiften MLOps-Praktiken berichten von 40 Prozent Kostenreduktion im ML-Lifecycle und 97 Prozent Verbesserung der Modell-Performance. Die Kombination aus strukturierter Methodik (QUA³CK) und fortschrittlichen Tools (MLFlow) bereitet optimal auf die Herausforderungen realer Projekte vor."
      },
      "mini_glossary": [
        {
          "term": "MLFlow UI",
          "definition": "Web-basiertes Dashboard zur Visualisierung und Verwaltung von ML-Experimenten, zentral im AMALEA-Ansatz."
        },
        {
          "term": "Automatisierter Modellvergleich",
          "definition": "Systematischer Vergleich verschiedener Modelle basierend auf protokollierten Metriken ohne manuelle Tabellenerstellung."
        },
        {
          "term": "Excel-Reports",
          "definition": "Traditionelle, manuelle Methode zur Dokumentation von Experimentergebnissen, fehleranfällig und ineffizient."
        },
        {
          "term": "Experiment-History",
          "definition": "Vollständige Historie aller durchgeführten ML-Experimente mit allen relevanten Metadaten."
        },
        {
          "term": "Kollaboration",
          "definition": "Zusammenarbeit im Team, durch MLOps-Tools wie MLFlow durch gemeinsamen Zugriff auf Experimente vereinfacht."
        },
        {
          "term": "AMALEA 2025",
          "definition": "Kursansatz, der QUA³CK mit modernen MLOps-Praktiken kombiniert, um Studierenden praxisrelevante Fähigkeiten zu vermitteln."
        }
      ]
    },
    {
      "question": "274. Die EDA im Iris-Projekt zeigte, dass Setosa linear separierbar ist, während Versicolor und Virginica leichte Überlappungen aufweisen. Analysieren Sie: Welche Implikation hat dies für die Modellwahl und erwartete Performance?",
      "options": [
        "Einfache lineare Modelle könnten für Setosa vs. Rest ausreichen",
        "Alle drei Klassen benötigen zwingend Deep Learning mit neuronalen Netzen",
        "Die Überlappung macht eine Klassifikation grundsätzlich unmöglich",
        "Clustering-Algorithmen wie K-Means sind für überlappende Klassen immer besser als überwachte Methoden",
        "Die Überlappung hat keine Auswirkung auf Modellwahl oder Performance"
      ],
      "answer": 0,
      "explanation": "Die lineare Separierbarkeit von Setosa bedeutet, dass selbst einfache Modelle (z.B. logistische Regression) diese Klasse perfekt von den anderen trennen können. Die Überlappung zwischen Versicolor und Virginica erfordert jedoch komplexere Entscheidungsgrenzen, die z.B. Decision Trees oder KNN gut modellieren können. Diese EDA-Erkenntnis sollte die Modellwahl in der A³-Phase informieren.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Datencharakteristika",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Von Datenstruktur zu Modellarchitektur",
        "steps": [
          "Lineare Separierbarkeit erkennen: Setosa ist durch eine einfache Hyperebene von den anderen trennbar",
          "Überlappungsbereiche quantifizieren: Versicolor und Virginica haben gemeinsame Regionen im Merkmalsraum",
          "Modellkomplexität ableiten: One-vs-Rest-Strategien könnten unterschiedliche Komplexität pro Binärproblem benötigen",
          "Performance-Erwartungen setzen: Perfekte Genauigkeit unwahrscheinlich aufgrund inhärenter Überlappung",
          "Fehleranalyse vorbereiten: Missklassifikationen werden primär zwischen Versicolor und Virginica auftreten"
        ],
        "content": "Diese Analyse verbindet U-Phase (Datenverständnis) mit A³-Phase (Modellierung) und C-Phase (Evaluation). Die EDA zeigt nicht nur 'wie die Daten aussehen', sondern gibt strategische Hinweise für die gesamte Modellentwicklung. Die erreichte Accuracy von 97,8 Prozent (nicht 100 Prozent) ist konsistent mit der beobachteten Überlappung. Moderne Ansätze wie Ensemble-Methoden könnten hier helfen, indem sie verschiedene Modelle für verschiedene Klassenpaare kombinieren. Diese mehrschichtige Analyse exemplifiziert die Tiefe, die QUA³CK als strukturiertes Framework ermöglicht."
      },
      "mini_glossary": [
        {
          "term": "Lineare Separierbarkeit",
          "definition": "Eigenschaft von Daten, bei der Klassen durch eine Hyperebene (Gerade, Ebene) vollständig getrennt werden können."
        },
        {
          "term": "Entscheidungsgrenze",
          "definition": "Die Grenze im Merkmalsraum, die verschiedene Klassen voneinander trennt."
        },
        {
          "term": "Überlappung",
          "definition": "Bereiche im Merkmalsraum, wo Datenpunkte verschiedener Klassen ähnliche Merkmalswerte haben."
        },
        {
          "term": "One-vs-Rest",
          "definition": "Strategie zur Mehrkl assenklassifikation, bei der für jede Klasse ein binäres Problem 'Klasse vs. alle anderen' gelöst wird."
        },
        {
          "term": "Hyperebene",
          "definition": "Geometrisches Objekt, das einen Raum in zwei Bereiche teilt, Grundlage linearer Klassifikatoren."
        },
        {
          "term": "Inhärente Überlappung",
          "definition": "Natürliche, unvermeidbare Überschneidungen in Daten, die eine Obergrenze für mögliche Accuracy setzen."
        }
      ]
    },
    {
      "question": "275. Bewerten Sie den vollständigen QUA³CK-Workflow für ein hypothetisches Projekt zur medizinischen Bildklassifikation (z.B. Tumordetektion). In welcher Phase würden die größten Herausforderungen auftreten und warum?",
      "options": [
        "In der Q-Phase aufgrund komplexer ethischer und regulatorischer Anforderungen sowie kritischer Fehlerkosten",
        "In der U-Phase wegen der Größe medizinischer Bilddatensätze – in der Praxis – im Kontext.",
        "In der A³-Phase, da Deep Learning-Modelle schwer zu trainieren sind – im Kontext – in der Praxis.",
        "In der C-Phase durch die Vielzahl verfügbarer Metriken – in der Praxis – im Kontext.",
        "In der K-Phase aufgrund fehlender Cloud-Infrastruktur – in der Praxis – im Kontext."
      ],
      "answer": 0,
      "explanation": "Medizinische Anwendungen haben besondere Herausforderungen in der Q-Phase: ethische Überlegungen (Patientendaten, Bias), regulatorische Anforderungen (FDA, CE-Zertifizierung), extreme Asymmetrie der Fehlerkosten (False Negatives können lebensbedrohlich sein) und komplexe Stakeholder-Landschaft (Ärzte, Patienten, Behörden). Eine unzureichende Q-Phase würde hier zum Scheitern führen, selbst bei technisch exzellenter Umsetzung.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Domänenspezifische Herausforderungen",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Kritische Erfolgsfaktoren im medizinischen Kontext",
        "steps": [
          "Ethik und Compliance definieren: DSGVO, Patienteneinwilligung, Bias-Vermeidung in Trainingsdaten",
          "Asymmetrische Fehlerkosten modellieren: False Negatives (übersehene Tumore) vs. False Positives (unnötige Biopsien)",
          "Stakeholder-Anforderungen balancieren: Ärzte brauchen Erklärbarkeit, Patienten Sicherheit, Kliniken Effizienz",
          "Regulatorische Pfade planen: Welche Zertifizierungen sind nötig? Welche Validierungsstudien gefordert?",
          "Success-Metriken sorgfältig wählen: Sensitivität (Recall) ist oft wichtiger als Accuracy"
        ],
        "content": "Dieses Szenario zeigt, dass technische Exzellenz in A³ und C nicht ausreicht, wenn die Grundlage (Q) fehlerhaft ist. Im medizinischen Bereich könnten selbst 99 Prozent Accuracy unzureichend sein, wenn die verbleibenden 1 Prozent kritische Fälle sind. Die Q-Phase muss dies antizipieren und entsprechende Metriken (z.B. Recall > 99,5 Prozent für bestimmte Tumortypen) sowie Validierungsstrategien (externe Validierung an mehreren Kliniken) definieren. Dies exemplifiziert, warum QUA³CK die Q-Phase als Fundament betont: Sie legt fest, was 'Erfolg' bedeutet, und beeinflusst alle nachfolgenden Phasen."
      },
      "mini_glossary": [
        {
          "term": "Ethische Anforderungen",
          "definition": "Moralische und rechtliche Verpflichtungen bei der Verarbeitung sensibler Daten wie Patienteninformationen."
        },
        {
          "term": "Regulatorische Anforderungen",
          "definition": "Gesetzliche und behördliche Vorgaben wie FDA-Zulassung oder CE-Kennzeichnung für medizinische Software."
        },
        {
          "term": "Asymmetrische Fehlerkosten",
          "definition": "Situation, in der verschiedene Fehlertypen (FP vs. FN) stark unterschiedliche Konsequenzen haben."
        },
        {
          "term": "Bias in Trainingsdaten",
          "definition": "Systematische Verzerrungen in Daten, z.B. Unterrepräsentation bestimmter Patientengruppen."
        },
        {
          "term": "Externe Validierung",
          "definition": "Bewertung eines Modells auf Daten aus anderen Quellen/Kliniken zur Sicherstellung der Generalisierung."
        },
        {
          "term": "Stakeholder-Landschaft",
          "definition": "Die Gesamtheit aller Beteiligten mit unterschiedlichen Interessen und Anforderungen an das System."
        }
      ]
    },
    {
      "question": "276. Analysieren Sie die Interdependenzen zwischen den QUA³CK-Phasen: Wenn in der U-Phase festgestellt wird, dass die verfügbaren Daten die in Q definierten KPIs unmöglich erreichen lassen, welche Strategie ist am sinnvollsten?",
      "options": [
        "Iterativ zur Q-Phase zurückkehren und KPIs basierend auf Datenlage anpassen oder Datenbeschaffung neu planen",
        "Mit unrealistischen KPIs fortfahren und in der C-Phase erklären, warum sie nicht erreicht wurden",
        "Die U-Phase überspringen und direkt mit der Modellierung beginnen – im Kontext – in der Praxis.",
        "Das gesamte Projekt abbrechen, da QUA³CK keine Iterationen erlaubt – im Kontext – in der Praxis.",
        "Nur synthetische Daten generieren, um die ursprünglichen KPIs zu erreichen – im Kontext."
      ],
      "answer": 0,
      "explanation": "QUA³CK ist iterativ, nicht strikt sequenziell. Wenn die U-Phase zeigt, dass die Datenlage die KPIs nicht unterstützt, muss zur Q-Phase zurückgekehrt werden, um entweder realistischere KPIs zu definieren oder die Datenbeschaffungsstrategie anzupassen. Dies verhindert, dass Ressourcen in ein zum Scheitern verurteiltes Projekt investiert werden. Flexibilität bei der Phasendurchführung ist ein Kernprinzip agiler Data-Science-Methodik.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Iterative Anpassung",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Iterative und adaptive Prozessführung",
        "steps": [
          "Problem erkennen: U-Phase deckt Diskrepanz zwischen Datenanforderungen und -verfügbarkeit auf",
          "Impact bewerten: Können KPIs mit vorhandenen Daten prinzipiell erreicht werden?",
          "Optionen entwickeln: A) KPIs anpassen, B) mehr/bessere Daten beschaffen, C) Problem neu definieren",
          "Stakeholder einbeziehen: Entscheidung über Neuausrichtung kommunizieren und Konsens erreichen",
          "Q-Phase revidieren: Angepasste Problemstellung und KPIs dokumentieren, dann U-Phase wiederholen"
        ],
        "content": "Diese Situation illustriert einen wichtigen Aspekt des QUA³CK-Modells: Es ist ein Leitfaden, kein Dogma. Die lineare Darstellung (Q→U→A³→C→K) dient der Klarheit, aber reale Projekte erfordern oft Rückschritte und Iterationen. Die 85-87 Prozent Scheiternrate entsteht oft, weil Teams stur am ursprünglichen Plan festhalten, obwohl frühe Phasen Unvereinbarkeiten aufdecken. QUA³CK ermutigt zu frühem, strukturiertem Scheitern: Lieber in der U-Phase erkennen, dass ein Projekt nicht realisierbar ist, als nach Monaten in der K-Phase feststellen, dass niemand das Ergebnis nutzen kann. Diese Agilität ist kompatibel mit modernen Projektmanagement-Ansätzen und unterscheidet QUA³CK von rigiden Wasserfall-Modellen."
      },
      "mini_glossary": [
        {
          "term": "Iterative Methodik",
          "definition": "Ansatz, bei dem Phasen bei Bedarf wiederholt und angepasst werden, nicht strikt linear durchlaufen."
        },
        {
          "term": "Interdependenzen",
          "definition": "Wechselseitige Abhängigkeiten zwischen verschiedenen Projektphasen."
        },
        {
          "term": "Datenbeschaffung",
          "definition": "Prozess zur Akquisition zusätzlicher oder qualitativ besserer Daten für das ML-Projekt."
        },
        {
          "term": "Realistische KPIs",
          "definition": "Erfolgsmetriken, die basierend auf verfügbaren Ressourcen und Daten tatsächlich erreichbar sind."
        },
        {
          "term": "Frühes Scheitern",
          "definition": "Konzept, Probleme möglichst früh zu identifizieren, um Ressourcenverschwendung zu vermeiden."
        },
        {
          "term": "Agile Data Science",
          "definition": "Flexible, iterative Herangehensweise an DS-Projekte im Gegensatz zu starren Wasserfall-Modellen."
        }
      ]
    },
    {
      "question": "277. Im Kursmaterial wird betont, dass die Wahl von Erfolgsmetriken in der Q-Phase kritisch ist. Kritisch analysiert: Warum ist Accuracy oft eine unzureichende Metrik, und unter welchen Bedingungen sollte sie durch Recall oder Precision ersetzt werden?",
      "options": [
        "Accuracy ignoriert Klassenungleichgewichte und unterschiedliche Fehlerkosten",
        "Accuracy ist immer die beste Metrik für alle Klassifikationsprobleme",
        "Recall und Precision sind veraltet und sollten nie verwendet werden",
        "Accuracy sollte nur bei Regressionsproblemen verwendet werden",
        "Die Wahl der Metrik hat keinen Einfluss auf den Projekterfolg"
      ],
      "answer": 0,
      "explanation": "Accuracy misst nur den Gesamtanteil korrekter Vorhersagen und kann bei unbalancierten Daten irreführend sein (z.B. 95 Prozent Accuracy bei 95 Prozent negativer Klasse durch 'immer negativ' vorhersagen). Recall ist wichtig, wenn False Negatives kritisch sind (z.B. Krankheitsdetektion), Precision wenn False Positives teuer sind (z.B. Marketing-Kampagnen). Die richtige Metrik hängt vom Business-Kontext ab.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "Metriken-Kritik",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Kontextabhängige Metrikwahl",
        "steps": [
          "Accuracy-Limitation erkennen: Bei 1 Prozent positiver Klasse erreicht 'immer negativ' 99 Prozent Accuracy, aber 0 Prozent Recall",
          "Fehlerkosten analysieren: Was kostet ein False Negative vs. False Positive im spezifischen Kontext?",
          "Recall priorisieren: Bei medizinischer Diagnose, Betrugserkennung, Sicherheitssystemen (Kosten von FN >> Kosten von FP)",
          "Precision priorisieren: Bei Spam-Filtern, Empfehlungssystemen, Marketing (Kosten von FP >> Kosten von FN)",
          "F1-Score erwägen: Wenn beide Fehlertypen etwa gleich kritisch sind, harmonisches Mittel nutzen"
        ],
        "content": "Diese Diskussion zeigt, dass die Q-Phase nicht nur formale Fragen beantwortet ('Welches Problem?'), sondern tiefes Domänenwissen und Business-Verständnis erfordert. Die Wahl zwischen Accuracy, Recall und Precision ist keine technische, sondern eine strategische Entscheidung mit direktem Impact auf Modellentwicklung (A³) und -auswahl (C). Im Iris-Projekt war Accuracy angemessen, da alle Klassen gleich häufig und gleichwertig sind. In realen Anwendungen ist dies selten der Fall. Moderne MLOps-Praktiken erlauben Multi-Metrik-Tracking, aber die primäre Metrik zur Entscheidungsfindung muss in Q definiert werden und den tatsächlichen Geschäftswert reflektieren."
      },
      "mini_glossary": [
        {
          "term": "Accuracy",
          "definition": "Anteil aller korrekten Vorhersagen; kann bei unbalancierten Daten irreführend sein."
        },
        {
          "term": "Recall",
          "definition": "Anteil der korrekt identifizierten positiven Instanzen an allen tatsächlich positiven; wichtig bei kritischen False Negatives."
        },
        {
          "term": "Precision",
          "definition": "Anteil der korrekt als positiv klassifizierten an allen als positiv vorhergesagten; wichtig bei kritischen False Positives."
        },
        {
          "term": "Klassenungleichgewicht",
          "definition": "Situation, in der eine Klasse deutlich häufiger vorkommt als andere, macht Accuracy problematisch."
        },
        {
          "term": "Business-Kontext",
          "definition": "Die spezifischen Anforderungen und Rahmenbedingungen des Anwendungsfalls, die Metrikwahl bestimmen."
        },
        {
          "term": "Multi-Metrik-Tracking",
          "definition": "Gleichzeitige Protokollierung mehrerer Metriken in MLOps-Systemen für umfassende Modellbewertung."
        }
      ]
    },
    {
      "question": "278. Synthese: Ein Startup plant ein ML-Projekt für personalisierte Produktempfehlungen. Entwickeln Sie eine QUA³CK-basierte Strategie für die ersten drei Phasen (Q, U, A³) und begründen Sie, wie MLOps-Integration den Prozess verbessern würde.",
      "options": [
        "Q: KPIs definieren – im beschriebenen Szenario – im Kontext.",
        "Direkt mit Deep Learning beginnen ohne Problemdefinition oder Datenanalyse",
        "Nur die K-Phase durchführen und ein vorgefertigtes Modell deployen",
        "Ausschließlich auf manuelle Empfehlungen durch Experten setzen",
        "Q, U und A³ gleichzeitig ohne Struktur durchführen"
      ],
      "answer": 0,
      "explanation": "Eine strukturierte Strategie würde in Q konkrete, messbare KPIs definieren (z.B. Click-Through-Rate > 5 Prozent, Conversion-Rate > 2 Prozent), in U das User-Verhalten und Produktattribute analysieren (Kaufhistorie, Produktkategorien, Seasonality), und in A³ verschiedene Ansätze testen (Collaborative Filtering, Content-Based, Hybrid) mit MLFlow-Tracking für systematische Evaluierung. MLOps ermöglicht kontinuierliche Verbesserung und A/B-Testing in Produktion.",
      "weight": 3,
      "topic": "MLOps & Deployment",
      "concept": "End-to-End Planung",
      "cognitive_level": "Analysis",
      "extended_explanation": {
        "title": "Vollständige QUA³CK-Strategie für Empfehlungssystem",
        "steps": [
          "Q-Phase: Problem präzisieren (personalisierte Empfehlungen zur Umsatzsteigerung), Zielgruppe (E-Commerce-Nutzer), KPIs (CTR, Conversion, Revenue per User), Deployment (API + Web-Integration)",
          "U-Phase: User-Behavior-Logs analysieren (Clickstreams, Kaufhistorie), Produktdaten erkunden (Kategorien, Preise, Attribute), Saisonalität und Trends identifizieren",
          "A³-Phase: Collaborative Filtering (User-Item-Matrix), Content-Based (Produktattribute), Matrix Factorization, Deep Learning (Neural Collaborative Filtering) testen",
          "MLOps-Integration: MLFlow für Experiment-Tracking aller Ansätze, Online-Evaluation mit A/B-Tests, kontinuierliches Retraining bei Konzeptdrift",
          "C+K-Vorbereitung: Modellvergleich via Dashboard, beste Variante als REST-API deployen, Monitoring für Performance-Degradation"
        ],
        "content": "Dieses Szenario demonstriert die Anwendbarkeit von QUA³CK über den akademischen Kontext hinaus. Empfehlungssysteme haben spezifische Herausforderungen (Cold-Start-Problem, Feedback-Loops, Konzeptdrift), die in jeder Phase adressiert werden müssen. Die MLOps-Integration ist hier besonders wertvoll: Im Gegensatz zu statischen Modellen (wie Iris-Klassifikation) erfordern Empfehlungssysteme kontinuierliches Retraining bei sich änderndem User-Verhalten. MLFlow ermöglicht Versionierung, Rollback und graduelle Rollouts. Dies exemplifiziert die Skalierbarkeit des QUA³CK-Ansatzes: Die Grundprinzipien (strukturierte Phasen, klare Metriken, systematische Evaluierung) bleiben gleich, aber die konkrete Umsetzung passt sich dem Anwendungsfall an."
      },
      "mini_glossary": [
        {
          "term": "Collaborative Filtering",
          "definition": "Empfehlungsalgorithmus basierend auf Ähnlichkeiten zwischen Nutzern oder Items ohne Attributinformation."
        },
        {
          "term": "Content-Based Filtering",
          "definition": "Empfehlungsansatz basierend auf Produkt- oder Content-Attributen und Nutzerpräferenzen."
        },
        {
          "term": "Click-Through-Rate",
          "definition": "Metrik für den Anteil der Nutzer, die auf eine Empfehlung klicken (Klicks / Impressions)."
        },
        {
          "term": "A/B-Testing",
          "definition": "Experimentelle Methode zum Vergleich zweier Varianten durch zufällige Nutzeraufteilung."
        },
        {
          "term": "Konzeptdrift",
          "definition": "Veränderung der statistischen Eigenschaften der Zielvariable über Zeit, erfordert Modell-Retraining."
        },
        {
          "term": "Cold-Start-Problem",
          "definition": "Herausforderung bei Empfehlungssystemen für neue Nutzer oder Items ohne historische Daten."
        }
      ]
    }
  ]
}