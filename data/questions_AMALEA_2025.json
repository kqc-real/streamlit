[
  {
    "frage": "1. Was ist der Hauptvorteil von Docker im Kontext von Data Science?",
    "optionen": [
      "Eine schnellere Datenverarbeitung durch optimierte Container.",
      "Die Erstellung reproduzierbarer und isolierter Projektumgebungen.",
      "Eine automatische Bereinigung von inkonsistenten Datensätzen.",
      "Die direkte GPU-Nutzung ohne spezielle Treiberkonfiguration."
    ],
    "loesung": 1,
    "erklaerung": "Docker kapselt Anwendungen und ihre Abhängigkeiten in Container. Dies garantiert, dass alle Teammitglieder mit einer identischen, reproduzierbaren Umgebung arbeiten, was Fehler durch unterschiedliche Systemkonfigurationen vermeidet.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Warum Reproduzierbarkeit und Isolation der Kernvorteil sind",
      "schritte": [
        "Ein Docker-Image friert exakte Versionen von Laufzeit (z. B. Python), Bibliotheken und Systempaketen ein und bildet damit eine konstante Basis.",
        "Ein Container startet überall dieselbe Umgebung, unabhängig von lokalen Systemunterschieden, und verhindert so „funktioniert nur auf meinem Rechner“-Effekte.",
        "Durch isolierte Abhängigkeiten lassen sich Data-Science-Pipelines konsistent ausführen, testen und teilen, wodurch Ergebnisse reproduzierbar werden.",
        "Performance-Booster, automatische Datenreinigung oder GPU-Treiberersatz sind nicht der Zweck von Docker und erklären daher nicht den Hauptvorteil."
      ]
    },
    "mini_glossary": {
      "**Container**": "Leichtgewichtige, isolierte Laufzeitumgebung, die eine Anwendung zusammen mit ihren Abhängigkeiten ausführt, ohne ein eigenes Betriebssystem mitzubringen.",
      "**Image**": "Unveränderliches Vorlage-Artefakt für Container; beschreibt Schichten mit definierten Paket- und Versionsständen (z. B. Basisimage + Python + Bibliotheken).",
      "**Abhängigkeiten**": "Alle benötigten Komponenten einer Anwendung (Bibliotheken, Systempakete, Tools). In Docker werden sie im Image versionsgenau festgelegt.",
      "**Isolation**": "Trennung von Prozessen und Dateisystemen zwischen Container und Host bzw. zwischen Containern; verhindert Konflikte und erhöht Reproduzierbarkeit."
    }
  },
  {
    "frage": "2. Was ist ein DataFrame in der `pandas`-Bibliothek?",
    "optionen": [
      "Ein Datentyp zur Speicherung von Zeitreihen in einer Dimension.",
      "Eine zweidimensionale, tabellarische Datenstruktur mit benannten Spalten und Zeilen.",
      "Ein Container zur Speicherung beliebiger Python-Objekte in einer Liste.",
      "Ein Python-Modul, das ausschließlich für die Datenvisualisierung genutzt wird."
    ],
    "loesung": 1,
    "erklaerung": "Ein `DataFrame` ist die zentrale Datenstruktur in `pandas`. Er repräsentiert Daten in einer tabellarischen Form, ähnlich einer SQL-Tabelle oder einem Spreadsheet, und ist für die Datenmanipulation optimiert.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "DataFrame in Kontext setzen",
      "schritte": [
        "Denken Sie an eine SQL-Tabelle: Spalten haben Namen/Typen, Zeilen sind Observations; genau das bildet ein DataFrame ab.",
        "Vektoroperationen auf Spalten erlauben schnelle Aggregation/Filterung ohne explizite Schleifen.",
        "Indizes ermöglichen schnellen, labelbasierten Zugriff und saubere Joins/Merges zwischen Tabellen."
      ]
    },
    "mini_glossary": {
      "**DataFrame**": "2D-Tabellenobjekt mit benannten Spalten und (optional) Index; Kernstruktur in `pandas`.",
      "**Index**": "Label-Achse für Zeilen/Spalten, über die selektiert, ausgerichtet und zusammengeführt wird.",
      "**Vektoroperation**": "Operation auf ganzen Arrays/Spalten statt elementweise Schleifen, typischerweise schneller."
    }
  },
  {
    "frage": "3. Was ist der Zweck eines `Dockerfile`?",
    "optionen": [
      "Es konfiguriert die Netzwerkeinstellungen zwischen laufenden Containern.",
      "Es enthält die schrittweisen Anweisungen zum Bauen eines Docker-Images.",
      "Es verwaltet die Benutzerrechte und Passwörter innerhalb eines Containers.",
      "Es listet alle verfügbaren Docker-Images aus einer Registry auf."
    ],
    "loesung": 1,
    "erklaerung": "Ein `Dockerfile` ist ein Textdokument, das alle Befehle enthält, die notwendig sind, um ein Docker-Image zu assemblieren. Es dient als Bauplan für die Container-Umgebung.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Vom Dockerfile zum reproduzierbaren Image",
      "schritte": [
        "Das Dockerfile definiert Basisimage, Systempakete und Python-Abhängigkeiten als deterministische Bauanleitung.",
        "Beim Build erzeugt Docker Schichten (Layers), die cachebar sind und so Folge-Builds beschleunigen.",
        "Das resultierende Image lässt sich überall identisch ausführen und versieht Projekte mit reproduzierbaren Umgebungen."
      ]
    },
    "mini_glossary": {
      "**Dockerfile**": "Rezeptdatei mit Befehlen wie `FROM`, `RUN`, `COPY`, die ein Image deterministisch erzeugen.",
      "**Layer**": "Cachbare Bauschicht eines Images; Änderungen invalidieren nur nachfolgende Schichten.",
      "**Image-Build**": "Prozess, der die Dockerfile-Anweisungen in ein ausführbares Image überführt."
    }
  },
  {
    "frage": "4. Welcher Befehl wandelt eine Zelle in einem Jupyter Notebook in eine Code-Zelle um?",
    "optionen": [
      "Die Taste `M` im Command Mode",
      "Die Taste `Y` im Command Mode",
      "Die Tastenkombination `Shift+Enter`",
      "Die Taste `A` im Command Mode"
    ],
    "loesung": 1,
    "erklaerung": "Im Command Mode (erreichbar durch Drücken von `Esc`) wandelt die Taste `Y` eine Zelle in eine ausführbare Code-Zelle um. `M` wandelt sie in eine Markdown-Zelle um.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Jupyter Zelltypen sicher umschalten",
      "schritte": [
        "`Esc` bringt in den Command Mode; dann setzt `Y` den Typ auf Code, `M` auf Markdown.",
        "`Shift+Enter` führt aus und springt weiter, ändert aber nicht den Zelltyp.",
        "Konsistente Nutzung der Modi beschleunigt Workflow bei Analyse und Dokumentation."
      ]
    },
    "mini_glossary": {
      "**Command Mode**": "Jupyter-Modus für Zell-Operationen (Löschen, Typ ändern); via `Esc` aktivierbar.",
      "**Markdown-Zelle**": "Zelle für formatierte Dokumentation (Text, LaTeX, Überschriften) neben Code."
    }
  },
  {
    "frage": "5. Was ist der fundamentale Unterschied zwischen einem Docker-Image und einem Docker-Container?",
    "optionen": [
      "Ein Image ist eine laufende Instanz, während ein Container eine schreibgeschützte Vorlage ist.",
      "Ein Container ist eine laufende, beschreibbare Instanz, die aus einem schreibgeschützten Image erstellt wird.",
      "Es gibt keinen fundamentalen Unterschied; die Begriffe sind austauschbar.",
      "Ein Image enthält nur den Anwendungscode, ein Container enthält zusätzlich die Daten."
    ],
    "loesung": 1,
    "erklaerung": "Ein Image ist eine unveränderliche Vorlage (Blueprint), die alles Nötige für eine Anwendung enthält. Ein Container ist eine aktive, laufende Instanz dieses Images.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Blueprint vs. Laufzeitinstanz",
      "schritte": [
        "Das Image ist die unveränderliche Momentaufnahme der Umgebung (readonly Layers).",
        "Beim Start wird darüber ein beschreibbarer Layer gelegt: das ist der Container.",
        "Mehrere Container können aus demselben Image parallel und unabhängig laufen."
      ]
    },
    "mini_glossary": {
      "**Image**": "Readonly-Vorlage mit OS-Basis, Laufzeiten und App-Dateien.",
      "**Container**": "Prozess + isoliertes Dateisystem auf Basis eines Images mit schreibbarem Top-Layer."
    }
  },
  {
    "frage": "6. Was ist der primäre Zweck einer `docker-compose.yml`-Datei?",
    "optionen": [
      "Die Verwaltung von Python-Paketabhängigkeiten ähnlich einer `requirements.txt`.",
      "Die Definition und Orchestrierung einer Multi-Container-Anwendung.",
      "Die Konfiguration der Docker-Engine auf dem Host-System.",
      "Die automatische Formatierung von Code innerhalb eines Containers."
    ],
    "loesung": 1,
    "erklaerung": "`Docker Compose` ist ein Werkzeug zur Definition und Ausführung von Anwendungen, die aus mehreren Containern bestehen. Die `docker-compose.yml` beschreibt die Services, Netzwerke und Volumes.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Mehrere Services konsistent starten",
      "schritte": [
        "In `docker-compose.yml` werden Services (z. B. Web, DB, Cache) deklarativ beschrieben.",
        "Mit `docker compose up` wird die komplette Anwendung inklusive Netzwerken/Volumes hochgefahren.",
        "Einheitliche Konfiguration erleichtert Reproduzierbarkeit und Teamarbeit."
      ]
    },
    "mini_glossary": {
      "**Service**": "Benannter Container-Baustein in Compose (Image, Ports, Env, Volumes).",
      "**Volume**": "Persistenter Speicher, der in Services gemountet wird.",
      "**Netzwerk**": "Virtuelles Netz, über das Services per DNS-Name kommunizieren."
    }
  },
  {
    "frage": "7. Was ist das Kernziel der Q-Phase im QUA³CK-Modell?",
    "optionen": [
      "Die Bereinigung und Vorverarbeitung der Rohdaten.",
      "Die präzise Formulierung der geschäftlichen Fragestellung und der Projektziele.",
      "Die Auswahl und das Training des finalen Machine-Learning-Algorithmus.",
      "Die Visualisierung und Präsentation der finalen Ergebnisse."
    ],
    "loesung": 1,
    "erklaerung": "Die Q-Phase (Question) legt das Fundament des Projekts. Hier werden die genaue Fragestellung, die Ziele und die Erfolgskriterien in Absprache mit den Stakeholdern definiert.",
    "gewichtung": 2,
    "thema": "QUA³CK & MLOps",
    "extended_explanation": {
      "title": "Vom Business-Need zur messbaren Frage",
      "schritte": [
        "Stakeholder-Anforderungen in konkrete, messbare Fragen überführen.",
        "Erfolgskriterien (KPIs) und Randbedingungen definieren.",
        "Deliverables, Datenbedarf und Risiken früh festhalten."
      ]
    },
    "mini_glossary": {
      "**Q-Phase**": "Question-Phase: präzise Problemdefinition und Zielsetzung.",
      "**Erfolgskriterien (KPIs)**": "Messgrößen, an denen Projekterfolg objektiv bewertet wird.",
      "**Stakeholder**": "Interessengruppen wie Fachbereich, IT, Management."
    }
  },
  {
    "frage": "8. Welche Funktion wird in `pandas` standardmäßig zum Einlesen einer CSV-Datei verwendet?",
    "optionen": [
      "`pd.read_table()`",
      "`pd.read_csv()`",
      "`pd.load_csv()`",
      "`pd.import_file()`"
    ],
    "loesung": 1,
    "erklaerung": "Die Funktion `pd.read_csv()` ist die flexible und performante Standardmethode in `pandas`, um Daten aus CSV-Dateien in einen DataFrame zu laden.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "CSV robust laden",
      "schritte": [
        "`pd.read_csv()` liest Textdateien mit Trennzeichen (Standard `,`) in DataFrames.",
        "Parameter wie `sep`, `dtype`, `parse_dates` adressieren häufige Importprobleme.",
        "Sauberer Import ist Grundlage für korrekte Analyse und Typinferenzen."
      ]
    },
    "mini_glossary": {
      "**CSV**": "Textformat mit zeilenweisen Datensätzen und Spaltentrennern (meist Komma/Strichpunkt).",
      "**`parse_dates`**": "Option, um Spalten als Datumsobjekte zu interpretieren."
    }
  },
  {
    "frage": "9. Welcher Fehler tritt typischerweise auf, wenn man in `pandas` auf eine nicht existierende Spalte zugreift?",
    "optionen": [
      "Ein `KeyError`",
      "Ein `SyntaxError`",
      "Ein `ValueError`",
      "Ein `ImportError`"
    ],
    "loesung": 0,
    "erklaerung": "Ein `KeyError` wird ausgelöst, wenn versucht wird, auf einen Spalten- oder Zeilen-Index zuzugreifen, der im DataFrame nicht existiert. Dies geschieht oft durch Tippfehler im Spaltennamen.",
    "gewichtung": 2,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "KeyError vermeiden",
      "schritte": [
        "Spaltennamen vor Zugriff prüfen (`df.columns`, `in`-Check) oder via `get` arbeiten.",
        "Einheitliche Benennung/Normalisierung (lowercase, trim) reduziert Tippfehler.",
        "Schemas per Tests validieren (z. B. `assert set(cols) <= set(df.columns)`)."
      ]
    },
    "mini_glossary": {
      "**KeyError**": "Fehler bei nicht vorhandenem Schlüssel/Label in Mapping-ähnlichen Strukturen.",
      "**Schema**": "Erwartete Struktur eines DataFrames (Spaltennamen/-typen)."
    }
  },
  {
    "frage": "10. Was ist ein wesentlicher Unterschied zwischen `pip` und `conda` als Paketmanager?",
    "optionen": [
      "`pip` kann nur Pakete deinstallieren, während `conda` sie auch installiert.",
      "`pip` ist auf Python-Pakete beschränkt, während `conda` auch Umgebungen und Nicht-Python-Pakete verwalten kann.",
      "`conda` ist der offizielle Paketmanager für PyPI, während `pip` für Anaconda entwickelt wurde.",
      "`pip` ist ausschließlich für Windows verfügbar, `conda` für Linux und macOS."
    ],
    "loesung": 1,
    "erklaerung": "Während `pip` sich auf die Installation von Python-Paketen aus dem Python Package Index (PyPI) konzentriert, ist `conda` ein sprachunabhängiger Paket- und Umgebungsmanager.",
    "gewichtung": 2,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Pakete vs. Umgebungen",
      "schritte": [
        "`pip` installiert Python-Pakete aus PyPI in die aktive Umgebung.",
        "`conda` verwaltet komplette Umgebungen inkl. Binärpaketen (z. B. MKL, CUDA).",
        "Für komplexe Stacks (NumPy/SciPy/GPU) ist `conda` oft problemloser."
      ]
    },
    "mini_glossary": {
      "**PyPI**": "Zentrale Paketquelle für Python-Bibliotheken.",
      "**Umgebung**": "Isolierter Paket- und Interpreter-Kontext mit eigener Versionslandschaft."
    }
  },
  {
    "frage": "11. Was ist ein zentraler Vorteil von Jupyter Notebooks für die explorative Datenanalyse?",
    "optionen": [
      "Sie erzwingen eine strikt lineare Ausführung des gesamten Codes.",
      "Sie ermöglichen die interaktive Ausführung von Code in Zellen, kombiniert mit Text und Visualisierungen.",
      "Sie sind ausschließlich für die Programmiersprache R optimiert.",
      "Sie laufen nativ als hochperformante Desktop-Anwendungen."
    ],
    "loesung": 1,
    "erklaerung": "Jupyter Notebooks sind ideal für die explorative Analyse, da sie es erlauben, Code in kleinen, isolierten Blöcken auszuführen, die Ergebnisse sofort zu inspizieren und den Prozess mit Markdown zu dokumentieren.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Code, Text und Grafik eng verzahnt",
      "schritte": [
        "Zellenweise Ausführung beschleunigt Hypothesentests und Dateninspektion.",
        "Markdown und Visualisierungen halten Kontext und Befunde direkt neben dem Code fest.",
        "Notebooks unterstützen Reproduzierbarkeit via Versionierung/Parameterisierung."
      ]
    },
    "mini_glossary": {
      "**Zelle**": "Ausführungseinheit im Notebook; enthält Code oder Markdown.",
      "**Explorative Analyse**": "Hypothesenfindung und Mustererkennung vor formaler Modellierung."
    }
  },
  {
    "frage": "12. Was ist der primäre Anwendungsfall für Streamlit im Data-Science-Kontext?",
    "optionen": [
      "Das Training von neuronalen Netzen direkt im Browser.",
      "Die Erstellung interaktiver Web-Anwendungen zur Visualisierung von Analysen und Modellen.",
      "Die Verwaltung von Python-Umgebungen ähnlich wie `conda`.",
      "Die direkte Ausführung von SQL-Abfragen auf Big-Data-Clustern."
    ],
    "loesung": 1,
    "erklaerung": "`Streamlit` ist ein Framework, das es ermöglicht, aus Datenanalyse-Skripten mit wenigen Zeilen Python-Code interaktive und ansprechende Web-Anwendungen zu erstellen.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Vom Notebook zum interaktiven Frontend",
      "schritte": [
        "Widgets (Slider, Selects) binden Parameter und erlauben Live-Exploration.",
        "Visualisierungen/Tabellen werden ohne Web-Framework-Wissen gerendert.",
        "Schnelle Prototypen senken die Hürde zum Stakeholder-Feedback."
      ]
    },
    "mini_glossary": {
      "**Widget**": "Interaktives UI-Element (Slider, Button) zur Parametrisierung.",
      "**Dashboard**": "Kompakte Oberfläche zur Anzeige von KPIs/Plots/Steuerung."
    }
  },
  {
    "frage": "13. Was unterscheidet eine Python-Liste fundamental von einem Dictionary?",
    "optionen": [
      "Listen speichern Schlüssel-Wert-Paare, Dictionaries nur geordnete Werte.",
      "Listen sind geordnete Sammlungen mit numerischem Index, Dictionaries speichern ungeordnete Schlüssel-Wert-Paare.",
      "Listen können nur Elemente des gleichen Datentyps enthalten, Dictionaries beliebige.",
      "Dictionaries sind in ihrer Größe beschränkt, während Listen dynamisch wachsen können."
    ],
    "loesung": 1,
    "erklaerung": "Eine Liste ist eine geordnete Sequenz, auf die über einen Integer-Index zugegriffen wird. Ein Dictionary ist eine ungeordnete Sammlung von Paaren aus einem eindeutigen Schlüssel und einem Wert.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Sequenz vs. Mapping",
      "schritte": [
        "Liste: Position (0,1,2,…) bestimmt Zugriff; Reihenfolge bleibt erhalten.",
        "Dictionary: Zugriff über Schlüssel; Einfügereihenfolge wird gemerkt, logische Ordnung ist keybasiert.",
        "Use-Case: Listen für Sequenzen, Dicts für schnelle Zuordnung (Hash-Lookup)."
      ]
    },
    "mini_glossary": {
      "**Liste**": "Geordnete Sammlung, zugreifbar per Integer-Index.",
      "**Dictionary**": "Hash-basiertes Mapping von Schlüsseln auf Werte (Amortisiert O(1)-Zugriff)."
    }
  },
  {
    "frage": "14. Mit welchem Schlüsselwort wird in Python eine Funktion definiert?",
    "optionen": [
      "`function`",
      "`def`",
      "`fun`",
      "`define`"
    ],
    "loesung": 1,
    "erklaerung": "Das Schlüsselwort `def` (für 'define') leitet die Definition einer Funktion in Python ein, gefolgt vom Funktionsnamen und den Parametern in Klammern.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Funktionen sauber deklarieren",
      "schritte": [
        "`def name(params):` eröffnet den Funktionsblock; Einrückung definiert den Körper.",
        "Docstring und Typannotationen verbessern Lesbarkeit und Toolsupport.",
        "Rückgaben via `return` machen Verhalten explizit testbar."
      ]
    },
    "mini_glossary": {
      "**`def`**": "Python-Schlüsselwort zur Definition einer Funktion.",
      "**Docstring**": "Erste String-Literal-Zeile im Body; dokumentiert Zweck und Parameter."
    }
  },
  {
    "frage": "15. Welchen Zweck erfüllt die Datei `requirements.txt` in einem Python-Projekt?",
    "optionen": [
      "Sie enthält den Quellcode der Hauptanwendung.",
      "Sie listet die notwendigen Python-Paketabhängigkeiten und deren Versionen auf.",
      "Sie speichert Konfigurationsparameter wie Datenbank-Zugangsdaten.",
      "Sie definiert die Befehle zum Starten des Projekts in einem Docker-Container."
    ],
    "loesung": 1,
    "erklaerung": "Eine `requirements.txt`-Datei ermöglicht es, alle externen Python-Pakete, die ein Projekt benötigt, mit einem einzigen Befehl (`pip install -r requirements.txt`) zu installieren und so die Umgebung reproduzierbar zu machen.",
    "gewichtung": 2,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Deterministische Abhängigkeitsinstallation",
      "schritte": [
        "Pinne Versionen (`pkg==x.y.z`) für reproduzierbare Builds.",
        "Installation per `pip install -r requirements.txt` vereinheitlicht Setups.",
        "Zusammen mit virtuellen Umgebungen verhindert das Konflikte zwischen Projekten."
      ]
    },
    "mini_glossary": {
      "**Pinning**": "Festschreiben konkreter Paketversionen zur Reproduzierbarkeit.",
      "**Virtuelle Umgebung**": "Isolierter Python-Interpreter mit eigenem Site-Packages-Verzeichnis."
    }
  },
  {
    "frage": "16. Welcher Fehler tritt auf, wenn man versucht, ein nicht installiertes Python-Modul zu importieren?",
    "optionen": [
      "`ModuleNotFoundError`",
      "`ValueError`",
      "`KeyError`",
      "`TypeError`"
    ],
    "loesung": 0,
    "erklaerung": "Ein `ModuleNotFoundError` (in älteren Python-Versionen `ImportError`) wird ausgelöst, wenn der Python-Interpreter ein importiertes Modul nicht in den konfigurierten Pfaden finden kann, meist weil es nicht installiert ist.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Importfehler systematisch beheben",
      "schritte": [
        "Prüfen, ob das Modul in der aktiven Umgebung installiert ist (`pip list`).",
        "Bei mehreren Umgebungen sicherstellen, dass Interpreter und `pip` übereinstimmen.",
        "Pfadprobleme vermeiden (relativer Import, korrekte `PYTHONPATH`-Konfiguration)."
      ]
    },
    "mini_glossary": {
      "**ModuleNotFoundError**": "Ausnahme, wenn ein importiertes Modul nicht gefunden wird.",
      "**PYTHONPATH**": "Suchpfade, in denen der Interpreter nach Modulen/Packages sucht."
    }
  },
  {
    "frage": "17. Was ist der Unterschied zwischen den Docker-Befehlen `ps` und `images`?",
    "optionen": [
      "`docker ps` zeigt alle laufenden Container an, während `docker images` alle lokal verfügbaren Images auflistet.",
      "`docker ps` listet alle Images auf, während `docker images` alle gestoppten Container anzeigt.",
      "`docker images` startet einen Container, während `docker ps` dessen Status prüft.",
      "Beide Befehle zeigen die gleiche Liste aller laufenden und gestoppten Container an."
    ],
    "loesung": 0,
    "erklaerung": "Mit `docker ps` (process status) inspiziert man die laufenden Container. Mit `docker images` verwaltet man die lokal heruntergeladenen Image-Vorlagen.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Laufzeit vs. Vorlagen unterscheiden",
      "schritte": [
        "`docker ps` listet Prozesse/Container-Instanzen mit Status/Ports.",
        "`docker images` zeigt Image-Templates, aus denen Container erzeugt werden.",
        "Verwaltung trennt bewusst Build-Artefakte (Images) von Laufzeit (Container)."
      ]
    },
    "mini_glossary": {
      "**`docker ps`**": "Befehl zur Anzeige laufender (mit `-a` auch gestoppter) Container.",
      "**`docker images`**": "Befehl zur Auflistung lokaler Images inkl. Tags/Größe."
    }
  },
  {
    "frage": "18. Was ist ein wesentlicher Vorteil von `Docker Compose` gegenüber einzelnen `docker run`-Befehlen?",
    "optionen": [
      "Es kann nur einen Container pro Konfigurationsdatei verwalten.",
      "Es ermöglicht die deklarative Verwaltung und Verknüpfung mehrerer Services mit einem einzigen Befehl.",
      "Es macht die Erstellung von Docker-Images überflüssig.",
      "Es ist ausschließlich für die lokale Entwicklungsumgebung konzipiert."
    ],
    "loesung": 1,
    "erklaerung": "`Docker Compose` vereinfacht die Verwaltung von komplexen Anwendungen, die aus mehreren miteinander verbundenen Containern (z.B. App, Datenbank, Cache) bestehen, über eine einzige `yml`-Datei.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Deklarativ statt imperative Startsequenzen",
      "schritte": [
        "Services, Abhängigkeiten und Netzwerke in YAML definieren statt viele `docker run`-Befehle zu skripten.",
        "Ein einziger Start-/Stop-Befehl (`up`/`down`) orchestriert die gesamte Topologie.",
        "Versionierbare Konfiguration erhöht Nachvollziehbarkeit und CI/CD-Tauglichkeit."
      ]
    },
    "mini_glossary": {
      "**YAML**": "Menschenlesbares Konfigurationsformat für Compose-Dateien.",
      "**Orchestrierung**": "Koordination mehrerer Container/Services inkl. Startreihenfolge/Netzwerke."
    }
  },
  {
    "frage": "19. Wofür wird das `st.sidebar`-Objekt in Streamlit primär verwendet?",
    "optionen": [
      "Um kritische Fehlermeldungen und Warnungen anzuzeigen.",
      "Um Steuerelemente und Navigationsoptionen in einer Seitenleiste zu platzieren.",
      "Um den `session_state` der Anwendung persistent zu speichern.",
      "Um den Hauptinhalt der Seite horizontal zu zentrieren."
    ],
    "loesung": 1,
    "erklaerung": "Mit `st.sidebar` können Widgets wie Slider, Selectboxen oder Buttons in einer separaten Leiste am linken Rand der App platziert werden, um die Hauptansicht aufgeräumt zu halten.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "UI entrümpeln durch Seitenleiste",
      "schritte": [
        "Steuer-Widgets wandern in `st.sidebar`, Inhalte bleiben im Hauptbereich.",
        "Einheitliche Interaktion verbessert Benutzbarkeit auf kleinen Bildschirmen.",
        "Gruppierte Controls (Filter, Parameter) erhöhen Lesbarkeit von Dashboards."
      ]
    },
    "mini_glossary": {
      "**`st.sidebar`**": "Namensraum für UI-Elemente in der linken Seitenleiste.",
      "**Widget**": "Interaktive Steuerelemente wie Slider, Selectbox, Button."
    }
  },
  {
    "frage": "20. Was ist der Hauptzweck des QUA³CK-Prozessmodells?",
    "optionen": [
      "Die Optimierung der Hardware-Auslastung bei Deep-Learning-Aufgaben.",
      "Die systematische Strukturierung eines Data-Science-Projekts von der Frage bis zum Transfer.",
      "Die Verwaltung von Python-Abhängigkeiten in großen Teams.",
      "Die Erstellung und Versionierung von relationalen Datenbank-Schemata."
    ],
    "loesung": 1,
    "erklaerung": "Das QUA³CK-Modell bietet einen strukturierten, iterativen Rahmen für Data-Science-Projekte, um sicherzustellen, dass alle Phasen von der Fragestellung bis zur Wissensvermittlung durchlaufen werden.",
    "gewichtung": 2,
    "thema": "QUA³CK & MLOps",
    "extended_explanation": {
      "title": "Projektfluss von Frage bis Transfer",
      "schritte": [
        "Durchläuft Phasen Q, U, A³, C, K, um Zielklarheit, Analyse, Modellierung, Bewertung und Transfer abzudecken.",
        "Iterative Schleifen erlauben Anpassungen basierend auf Zwischenergebnissen.",
        "Artefakte/Entscheidungen werden je Phase explizit dokumentiert."
      ]
    },
    "mini_glossary": {
      "**QUA³CK**": "Rahmenwerk mit Phasen Question, Understanding, Algorithm/Adapting/Adjusting, Conclude, Knowledge.",
      "**MLOps**": "Praktiken zur Operationalisierung von ML (Versionierung, Deployment, Monitoring)."
    }
  },
  {
    "frage": "21. Welche Methode wird in `pandas` verwendet, um die ersten `n` Zeilen eines DataFrames anzuzeigen?",
    "optionen": [
      "`df.show(n)`",
      "`df.head(n)`",
      "`df.top(n)`",
      "`df.first(n)`"
    ],
    "loesung": 1,
    "erklaerung": "Die Methode `df.head()` (standardmäßig n=5) ist ein unverzichtbares Werkzeug für einen ersten schnellen Blick auf die Struktur und den Inhalt eines DataFrames.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Schneller Struktur-Check",
      "schritte": [
        "`df.head()` zeigt Top-Zeilen und hilft, Spaltennamen/Typen/Beispiele zu prüfen.",
        "Kombination mit `df.info()` und `df.describe()` gibt kompakten Überblick.",
        "Früher Blick verhindert spätere Fehlannahmen über Datentypen."
      ]
    },
    "mini_glossary": {
      "**`head()`**": "Gibt standardmäßig die ersten 5 Zeilen eines DataFrames zurück.",
      "**Profiling**": "Frühe, kurze Bestandsaufnahme von Struktur und Qualität der Daten."
    }
  },
  {
    "frage": "22. Was ist ein wesentlicher Vorteil der Verwendung von Funktionen in der Programmierung?",
    "optionen": [
      "Sie erhöhen die Komplexität und Laufzeit des Codes.",
      "Sie fördern die Wiederverwendbarkeit von Code und die logische Strukturierung.",
      "Sie verhindern die Verwendung von lokalen Variablen innerhalb ihres Gültigkeitsbereichs.",
      "Sie sind ausschließlich für komplexe mathematische Berechnungen vorgesehen."
    ],
    "loesung": 1,
    "erklaerung": "Funktionen kapseln eine bestimmte Logik, die dann beliebig oft wiederverwendet werden kann. Dies reduziert Codeduplizierung und verbessert die Lesbarkeit und Wartbarkeit.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Modularität zahlt sich aus",
      "schritte": [
        "Funktionen kapseln Verhalten und reduzieren Wiederholungen.",
        "Tests werden einfacher, da Einheiten klar abgrenzbar sind.",
        "Änderungen bleiben lokal statt globalen Code zu beeinflussen."
      ]
    },
    "mini_glossary": {
      "**Modularität**": "Zerlegung in kleine, wiederverwendbare Bausteine.",
      "**Wartbarkeit**": "Leichtigkeit, mit der Code geändert, erweitert und getestet werden kann."
    }
  },
  {
    "frage": "23. Was unterscheidet die Docker-Befehle `stop` und `rm`?",
    "optionen": [
      "`docker stop` löscht einen Container permanent, während `docker rm` ihn nur anhält.",
      "`docker stop` hält einen laufenden Container an, während `docker rm` einen gestoppten Container entfernt.",
      "Beide Befehle sind Aliase und haben exakt die gleiche Funktion.",
      "`docker rm` startet einen gestoppten Container neu, während `docker stop` einen neuen erstellt."
    ],
    "loesung": 1,
    "erklaerung": "Mit `docker stop` wird ein Container ordnungsgemäß beendet, sein Zustand bleibt aber erhalten. `docker rm` löscht den Container und alle damit verbundenen Daten (außer in Volumes) endgültig.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Beenden vs. Entfernen",
      "schritte": [
        "`stop` sendet SIGTERM/SIGKILL und hält die Instanz an.",
        "`rm` entfernt Metadaten und den beschreibbaren Layer des Containers.",
        "Persistente Daten gehören in Volumes und überleben `rm`."
      ]
    },
    "mini_glossary": {
      "**SIGTERM/SIGKILL**": "Signale zum kontrollierten bzw. harten Beenden eines Prozesses.",
      "**Volume-Persistenz**": "Daten in Volumes bleiben über Containerlebenszyklen erhalten."
    }
  },
  {
    "frage": "24. Welcher Fehlertyp ist spezifisch für die falsche Verwendung von Streamlit-Widgets?",
    "optionen": [
      "`StreamlitAPIException`",
      "`KeyError`",
      "`ValueError`",
      "`ModuleNotFoundError`"
    ],
    "loesung": 0,
    "erklaerung": "Eine `StreamlitAPIException` wird oft ausgelöst, wenn die Regeln der Streamlit-API verletzt werden, z.B. durch doppelte Widget-Keys oder das Platzieren von Elementen an der falschen Stelle.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Fehlerursachen bei Widgets",
      "schritte": [
        "Jeder Widget-Key muss eindeutig sein; Duplikate triggern Exceptions.",
        "Widgets müssen im Scriptfluss konsistent erzeugt werden (keine bedingte Strukturbrüche).",
        "Fehlermeldung lesen: Sie verweist meist direkt auf den problematischen Aufruf."
      ]
    },
    "mini_glossary": {
      "**`StreamlitAPIException`**": "Spezifische Ausnahme bei API-Fehlgebrauch in Streamlit.",
      "**Widget-Key**": "Eindeutige Kennung zur Zustandsverwaltung eines Widgets."
    }
  },
  {
    "frage": "25. Welchen Zweck erfüllt das `st.metric`-Widget in Streamlit?",
    "optionen": [
      "Es dient zur Anzeige einer einzelnen Kennzahl (KPI) mit optionalem Vergleichswert.",
      "Es wandelt eine gegebene Metrik von imperialen in metrische Einheiten um.",
      "Es definiert die Metrik für die Bewertung eines Machine-Learning-Modells.",
      "Es misst die Performance und Ladezeit der Streamlit-Anwendung."
    ],
    "loesung": 0,
    "erklaerung": "Mit `st.metric` können wichtige Kennzahlen wie Umsätze, Fehlerraten oder Zuwächse prominent und übersichtlich in einem Dashboard dargestellt werden.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "KPI knackig präsentieren",
      "schritte": [
        "`st.metric(label, value, delta)` zeigt Wert plus Veränderung.",
        "Mehrere Metriken in `st.columns()` strukturieren Dashboards.",
        "Konsistente Einheiten/Zeiträume sichern Vergleichbarkeit."
      ]
    },
    "mini_glossary": {
      "**KPI**": "Key Performance Indicator; zentrale Kennzahl zur Zielverfolgung.",
      "**Delta**": "Differenz zum Referenzwert (z. B. Vorwoche/Monat)."
    }
  },
  {
    "frage": "26. Was ist ein Hauptvorteil der Verwendung von Markdown in Jupyter Notebooks?",
    "optionen": [
      "Es ermöglicht die Ausführung von Shell-Befehlen direkt im Browser.",
      "Es erlaubt die reichhaltige Formatierung von Text zur Dokumentation des Analyseprozesses.",
      "Es ersetzt die Notwendigkeit von Python-Code für die Datenmanipulation.",
      "Es wird automatisch in eine interaktive Streamlit-Anwendung umgewandelt."
    ],
    "loesung": 1,
    "erklaerung": "Markdown-Zellen sind essenziell, um den Gedankengang, die Methodik und die Schlussfolgerungen einer Datenanalyse direkt neben dem ausführenden Code zu dokumentieren.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Wissen direkt neben Code festhalten",
      "schritte": [
        "Überschriften, Listen, Formeln und Links strukturieren Argumente.",
        "Kontext reduziert kognitive Last beim späteren Lesen/Review.",
        "Reproduzierbare Forschung lebt von verständlicher Begleitdokumentation."
      ]
    },
    "mini_glossary": {
      "**Markdown**": "Leichtgewichtige Auszeichnungssprache für formatierte Texte.",
      "**Notebook-Dokumentation**": "Narrative Beschreibung von Motivation, Methode, Ergebnis."
    }
  },
  {
    "frage": "27. Was ist der Unterschied zwischen `import pandas as pd` und `from pandas import *`?",
    "optionen": [
      "Es gibt keinen Unterschied, beide haben das gleiche Ergebnis.",
      "`import pandas as pd` importiert die Bibliothek unter einem Alias, während `from pandas import *` alle Namen in den globalen Namensraum importiert.",
      "`from pandas import *` ist die empfohlene Standardmethode für bessere Performance.",
      "`import pandas as pd` importiert nur die DataFrame-Klasse, nicht die restlichen Funktionen."
    ],
    "loesung": 1,
    "erklaerung": "Die Verwendung eines Alias wie `pd` ist eine weit verbreitete Konvention, die den Code lesbar hält. Ein `import *` (Wildcard-Import) ist fehleranfällig, da er den Namensraum unübersichtlich macht, und sollte vermieden werden.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Namensräume sauber halten",
      "schritte": [
        "Alias-Import (`as pd`) schafft kurze, eindeutige Präfixe.",
        "`from … import *` verschleiert Herkunft und überschreibt Symbole.",
        "Explizite Importe erleichtern Lesbarkeit, Autocomplete und Linting."
      ]
    },
    "mini_glossary": {
      "**Alias**": "Alternativer Kurzname für ein Modul (z. B. `pd`).",
      "**Namensraum**": "Menge benannter Bezeichner; Kollisionen führen zu schwer findbaren Fehlern."
    }
  },
  {
    "frage": "28. Was ist der typische Anwendungsfall für `st.file_uploader` in einer Streamlit-App?",
    "optionen": [
      "Das Hochladen von Dateien durch den Benutzer zur interaktiven Analyse in der App.",
      "Das automatische Hochladen der fertigen App in die Streamlit Cloud.",
      "Die Visualisierung der Dateistruktur des Servers in der App.",
      "Der Download von Ergebnis-Dateien aus der App heraus."
    ],
    "loesung": 0,
    "erklaerung": "Das `st.file_uploader`-Widget ermöglicht es Benutzern, ihre eigenen Datendateien (z.B. CSV, Bilder) hochzuladen, die dann serverseitig von der Streamlit-App verarbeitet werden können.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Benutzerdaten sicher einspeisen",
      "schritte": [
        "`st.file_uploader` liefert Dateihandles/Bytes für serverseitige Verarbeitung.",
        "Typprüfung/Parsing (z. B. CSV, XLSX, Bild) verhindert Fehler früh.",
        "Optionales Caching (`@st.cache_data`) beschleunigt wiederholte Ladevorgänge."
      ]
    },
    "mini_glossary": {
      "**File-Handle**": "Objekt, das Leseoperationen auf einer hochgeladenen Datei erlaubt.",
      "**Parsing**": "Strukturiertes Interpretieren roher Bytes in Datenstrukturen."
    }
  },
  {
    "frage": "29. Wofür steht das Schlüsselwort `def` in Python?",
    "optionen": [
      "Für 'default', um einen Standardwert für eine Variable festzulegen.",
      "Für 'define', um eine neue Funktion oder Methode zu definieren.",
      "Für 'defer', um die Ausführung eines Code-Blocks zu verzögern.",
      "Für 'delete file', um eine Datei aus dem System zu entfernen."
    ],
    "loesung": 1,
    "erklaerung": "Das Schlüsselwort `def` leitet in Python eine Funktionsdefinition ein. Es ist eines der grundlegendsten Konstrukte der Sprache für die Erstellung von wiederverwendbarem Code.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "`def` korrekt einsetzen",
      "schritte": [
        "Syntax: `def name(params):` gefolgt von eingerücktem Body.",
        "Rückgabe explizit mit `return`; ohne `return` ergibt die Funktion `None`.",
        "Typannotationen erhöhen Wartbarkeit und Tooling-Unterstützung."
      ]
    },
    "mini_glossary": {
      "**Funktion**": "Wiederverwendbarer Block mit Eingaben (Parameter) und optionaler Ausgabe.",
      "**`None`**": "Spezialwert für „kein Ergebnis“/leere Rückgabe in Python."
    }
  },
  {
    "frage": "30. Welchen Vorteil bietet Docker für die Zusammenarbeit in einem Data-Science-Team?",
    "optionen": [
      "Jeder Entwickler kann seine bevorzugten, inkompatiblen Bibliotheksversionen verwenden.",
      "Alle Teammitglieder arbeiten in einer identischen, versionierten und portablen Umgebung.",
      "Docker erzwingt die Verwendung der Programmiersprache Java anstelle von Python.",
      "Die Notwendigkeit einer Versionskontrolle wie Git wird durch Docker ersetzt."
    ],
    "loesung": 1,
    "erklaerung": "Docker eliminiert das 'works on my machine'-Problem, indem es sicherstellt, dass die gesamte Entwicklungsumgebung (Betriebssystem, Bibliotheken, Konfiguration) für alle Teammitglieder exakt gleich ist.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Teamproduktivität durch identische Umgebungen",
      "schritte": [
        "Ein gemeinsames Image bildet die Referenz; Builds sind versioniert/tagged.",
        "CI/CD nutzt dasselbe Image für Tests/Deployment und vermeidet Drift.",
        "Onboarding verkürzt sich, da Setup-Schritte im Image kodiert sind."
      ]
    },
    "mini_glossary": {
      "**Environment Drift**": "Schleichende Abweichung zwischen Dev/Test/Prod-Umgebungen.",
      "**Tag**": "Versionierungslabel eines Images (z. B. `v1.2.0`, `latest`)."
    }
  },
  {
    "frage": "31. Wie kann die Performance von Filteroperationen in `pandas` verbessert werden?",
    "optionen": [
      "Durch die Konvertierung aller Spalten in den `object`-Datentyp.",
      "Durch das Setzen einer relevanten Spalte als Index mit `set_index()`.",
      "Indem man statt Vektoroperationen explizite `for`-Schleifen verwendet.",
      "Durch das Speichern von Daten im JSON-Format anstelle von Parquet."
    ],
    "loesung": 1,
    "erklaerung": "Das Setzen eines Indexes auf eine häufig gefilterte Spalte ermöglicht `pandas` den Einsatz optimierter, hash-basierter Zugriffsmechanismen, was die Filterung erheblich beschleunigt.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Filter beschleunigen durch geeigneten Index",
      "schritte": [
        "Wählen Sie ein selektionsrelevantes, eindeutiges oder häufig gefiltertes Feld als Index (`df.set_index('key')`).",
        "Nutzen Sie labelbasierten Zugriff (`.loc`) statt boolescher Vollscans, um O(1)-ähnliche Lookups zu erhalten.",
        "Vermeiden Sie `object`-Typen für numerische Vergleiche; korrekte dtypes verbessern Vektorisierung und Speicherlayout."
      ]
    },
    "mini_glossary": {
      "**Index**": "Spezielle Achse für schnellen, labelbasierten Zugriff und Ausrichtung in `pandas`.",
      "**`.loc`**": "Labelbasierter Selektor für Zeilen/Spalten; spielt optimal mit Indexen zusammen.",
      "**dtype**": "Datentyp einer Spalte; beeinflusst Performance und Speicherverbrauch."
    }
  },
  {
    "frage": "32. Was ist der Hauptvorteil eines Multi-Stage-Builds in einem `Dockerfile`?",
    "optionen": [
      "Automatisches Erstellen einer `.dockerignore`-Datei zur besseren Cache-Nutzung.",
      "Paralleles Starten von dev-, test- und prod-Containern mit einem einzigen Befehl.",
      "Reduzierung der finalen Image-Größe durch Trennung von Build- und Runtime-Abhängigkeiten.",
      "Sichere Verwaltung von Umgebungsvariablen durch Integration in den Docker-Daemon."
    ],
    "loesung": 2,
    "erklaerung": "Multi-Stage-Builds erlauben es, in einer ersten 'Build'-Stufe Pakete zu kompilieren oder zu installieren und in einer zweiten 'Final'-Stufe nur die notwendigen Artefakte zu übernehmen, was zu schlankeren und sichereren Images führt.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Schlanke Images mit Build- und Runtime-Phasen",
      "schritte": [
        "Kompilieren/Installieren in einer `builder`-Stage; nur Artefakte werden exportiert.",
        "Die finale Stage basiert auf einem minimalen Runtime-Image ohne Build-Tools.",
        "Ergebnis sind kleinere, sicherere Images mit weniger Angriffsfläche."
      ]
    },
    "mini_glossary": {
      "**Multi-Stage-Build**": "Mehrere FROM-Abschnitte im Dockerfile, die Artefakte zwischen Stages kopieren.",
      "**Artefakt**": "Erzeugtes Ergebnis des Builds (z. B. Binärdatei, Wheels), das zur Laufzeit benötigt wird."
    }
  },
  {
    "frage": "33. Wie wird die A³-Phase des QUA³CK-Modells im Kurs modern interpretiert?",
    "optionen": [
      "Durch interaktive Datenexploration in einer Streamlit-App.",
      "Durch das Deployment des finalen Modells in die Streamlit Cloud.",
      "Durch die Erstellung eines automatisierten MLOps-Dashboards.",
      "Durch systematisches Experiment-Tracking mit `MLflow`."
    ],
    "loesung": 3,
    "erklaerung": "Die A³-Phase (Algorithm, Adapting, Adjusting) wird modern durch MLOps-Praktiken wie das Experiment-Tracking mit `MLflow` umgesetzt, um Modellvarianten systematisch zu verwalten und zu vergleichen.",
    "gewichtung": 2,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "A³ als experimentelle Iteration operativ machen",
      "schritte": [
        "Variantenbildung durch Hyperparameter/Features erzeugt viele Modellstände.",
        "`MLflow` protokolliert Parameter, Metriken und Artefakte je Lauf reproduzierbar.",
        "Vergleich/Selektionsentscheidungen werden datengetrieben und auditierbar."
      ]
    },
    "mini_glossary": {
      "**A³-Phase**": "Algorithm/Adapting/Adjusting: Modellwahl, -anpassung und -feinschliff.",
      "**Experiment-Tracking**": "Systematische Erfassung von Läufen, Parametern, Metriken und Artefakten."
    }
  },
  {
    "frage": "34. Wofür wird der `stratify`-Parameter in `train_test_split` verwendet?",
    "optionen": [
      "Um sicherzustellen, dass die Daten vor dem Splitten zufällig gemischt werden.",
      "Um die relative Häufigkeit der Klassen in Trainings- und Test-Set beizubehalten.",
      "Um eine exakte Aufteilung von 80% Trainings- und 20% Testdaten zu garantieren.",
      "Um den `random_state` für reproduzierbare Splits zu ersetzen."
    ],
    "loesung": 1,
    "erklaerung": "Bei Klassifikationsproblemen, besonders mit unbalancierten Klassen, sorgt `stratify` dafür, dass die prozentuale Verteilung der Zielklassen in Trainings- und Testdaten identisch ist.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Faire Aufteilung bei unausgeglichenen Klassen",
      "schritte": [
        "Geben Sie die Zielvariable an `stratify=y`, damit Klassenverhältnisse erhalten bleiben.",
        "So vermeiden Sie, dass seltene Klassen zufällig nur im Train- oder Test-Set landen.",
        "Resultat sind stabilere, besser vergleichbare Leistungskennzahlen."
      ]
    },
    "mini_glossary": {
      "**Stratifizierung**": "Aufteilung, die die Klassenverteilung in Teilmengen konstant hält.",
      "**Klassendrift**": "Verzerrung der Klassenhäufigkeiten zwischen Datensplits."
    }
  },
  {
    "frage": "35. Was ist der primäre Zweck des Dekorators `@st.cache_data` in Streamlit?",
    "optionen": [
      "Das Zwischenspeichern von UI-Elementen wie Buttons oder Slidern.",
      "Die Beschleunigung der App durch Caching von datenintensiven Funktionsergebnissen.",
      "Das Speichern des `session_state` über verschiedene Browser-Tabs hinweg.",
      "Die automatische Konvertierung von CSV-Dateien in das Parquet-Format."
    ],
    "loesung": 1,
    "erklaerung": "`@st.cache_data` speichert das Ergebnis einer Funktion. Wird die Funktion erneut mit den gleichen Argumenten aufgerufen, wird das Ergebnis aus dem Cache geholt, was Operationen wie das Laden von Daten beschleunigt.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Idempotente Datenfunktionen beschleunigen",
      "schritte": [
        "Dekorieren Sie reine Datenfunktionen (z. B. Lesen/Parsen) mit `@st.cache_data`.",
        "Streamlit bildet Argumente auf Cache-Keys ab und liefert Memoized-Ergebnisse.",
        "Bei Datenänderung (Argumentwechsel) wird der Cache gezielt invalidiert."
      ]
    },
    "mini_glossary": {
      "**Caching**": "Zwischenspeichern von Ergebnissen, um erneute Berechnungen zu vermeiden.",
      "**Memoization**": "Cache-Strategie basierend auf Funktionsargumenten als Schlüssel."
    }
  },
  {
    "frage": "36. Welches Problem wird durch die Verwendung von Docker-Volumes gelöst?",
    "optionen": [
      "Die initiale Build-Zeit des Docker-Images wird signifikant reduziert.",
      "Daten bleiben persistent erhalten, auch wenn der zugehörige Container gelöscht wird.",
      "Die Netzwerkkommunikation zwischen voneinander isolierten Containern wird ermöglicht.",
      "Python-Abhängigkeiten können ohne `pip` direkt im Container installiert werden."
    ],
    "loesung": 1,
    "erklaerung": "Volumes entkoppeln die Lebensdauer von Daten von der Lebensdauer eines Containers. Sie mappen ein Verzeichnis auf dem Host-System in den Container und sorgen so für Persistenz.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Zustandsdaten vom Container entkoppeln",
      "schritte": [
        "Erstellen/Mounten eines Volumes speichert Daten außerhalb des Container-Lebenszyklus.",
        "Container können gefahrlos ersetzt werden, ohne Daten zu verlieren.",
        "Backup/Sharing der Volumes ermöglicht saubere Betriebsprozesse."
      ]
    },
    "mini_glossary": {
      "**Volume**": "Von Docker verwalteter, persistenter Speicher, der in Container gemountet wird.",
      "**Bind Mount**": "Direktes Mounten eines Host-Verzeichnisses in den Container."
    }
  },
  {
    "frage": "37. In welcher Phase des QUA³CK-Modells findet die Aktivität 'Conclude and Compare' statt?",
    "optionen": [
      "Phase Q (Question)",
      "Phase U (Understanding)",
      "Phase A³ (Algorithm, Adapting, Adjusting)",
      "Phase C (Conclude and Compare)"
    ],
    "loesung": 3,
    "erklaerung": "Die C-Phase (Conclude and Compare) ist explizit dem Vergleich der Modellergebnisse, der Ziehung von Schlussfolgerungen und der finalen Bewertung des Projekterfolgs gewidmet.",
    "gewichtung": 2,
    "thema": "QUA³CK & MLOps",
    "extended_explanation": {
      "title": "Modellvergleich strukturiert abschließen",
      "schritte": [
        "Konsolidieren Sie Metriken/Validierungsergebnisse über alle Kandidatenmodelle.",
        "Vergleichen Sie Modelle entlang fachlicher KPIs und nicht-funktionaler Kriterien (Interpretierbarkeit, Latenz).",
        "Dokumentieren Sie die Entscheidung inkl. Trade-offs und Limitationen."
      ]
    },
    "mini_glossary": {
      "**C-Phase**": "Conclude & Compare: Ergebnisbewertung und Auswahl.",
      "**Trade-off**": "Bewusster Kompromiss zwischen Zielgrößen (z. B. Genauigkeit vs. Laufzeit)."
    }
  },
  {
    "frage": "38. Was unterscheidet `st.session_state` von einer normalen globalen Variable in Streamlit?",
    "optionen": [
      "`st.session_state` ist in der Ausführung schneller als eine globale Variable.",
      "`st.session_state` kann komplexe Objekte speichern, globale Variablen nicht.",
      "`st.session_state` behält seinen Wert über die Reruns einer Benutzersession hinweg, eine Variable nicht.",
      "`st.session_state` ist nur innerhalb der Funktion sichtbar, in der es definiert wurde."
    ],
    "loesung": 2,
    "erklaerung": "Da Streamlit das Skript bei jeder Interaktion neu ausführt, werden normale Variablen zurückgesetzt. `st.session_state` ist ein Mechanismus, um Zustandsinformationen über diese Reruns hinweg zu speichern.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Zustand stabil über Reruns halten",
      "schritte": [
        "Legen Sie Schlüssel/Werte in `st.session_state` an (z. B. `st.session_state['counter']`).",
        "State bleibt pro Browser-Session konsistent, obwohl das Skript rerunnt.",
        "Eindeutige Widget-Keys koppeln UI-Status an `session_state`."
      ]
    },
    "mini_glossary": {
      "**Rerun**": "Neu-Ausführung des Streamlit-Skripts nach Interaktionen.",
      "**Widget-Key**": "Eindeutiges Kennzeichen zur Zuordnung von UI-Elementen zum Zustand."
    }
  },
  {
    "frage": "39. Welches Datenformat ist oft performanter als CSV für die Speicherung großer DataFrames?",
    "optionen": [
      "JSON",
      "Excel (.xlsx)",
      "Parquet",
      "Text (.txt)"
    ],
    "loesung": 2,
    "erklaerung": "`Parquet` ist ein spaltenbasiertes Speicherformat, das eine hohe Kompression und schnelle Lesezeiten ermöglicht, da nur die tatsächlich benötigten Spalten geladen werden müssen.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Warum Parquet bei Scale überzeugt",
      "schritte": [
        "Spaltenorientierung erlaubt selektives Laden benötigter Spalten (I/O-Reduktion).",
        "Effiziente Kompression/Encoding senkt Speicherbedarf und Lesezeit.",
        "Schemata bewahren Typinformationen, vermeiden fehleranfälliges Parsen."
      ]
    },
    "mini_glossary": {
      "**Spaltenorientiert**": "Daten werden spaltenweise gespeichert; ideal für analytische Workloads.",
      "**Schema**": "Explizite Typ-/Strukturinformation, die mit den Daten gespeichert wird."
    }
  },
  {
    "frage": "40. Was beschreibt der Begriff `Vektorisierung` im Kontext von `pandas`?",
    "optionen": [
      "Die Umwandlung von Textdaten in numerische Vektoren mittels Word Embeddings.",
      "Das Anwenden von Operationen auf ganze Spalten (Arrays) statt auf einzelne Elemente in einer Schleife.",
      "Das Speichern eines DataFrames in einer speziellen Vektor-Datenbank.",
      "Die Visualisierung von Daten als Vektorgrafiken anstelle von Rastergrafiken."
    ],
    "loesung": 1,
    "erklaerung": "`Vektorisierung` nutzt optimierte, in C oder Cython implementierte Routinen, um Operationen auf ganzen Daten-Arrays gleichzeitig auszuführen. Dies ist fundamental schneller als eine manuelle Iteration in Python.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Schleifen eliminieren, Throughput erhöhen",
      "schritte": [
        "Verwenden Sie Spaltenarithmetik und integrierte `pandas`/NumPy-Funktionen statt `for`-Schleifen.",
        "Unter der Haube arbeiten C-optimierte Kernels; Python-Overhead entfällt.",
        "Ergebnis sind kürzerer Code und signifikante Laufzeitgewinne."
      ]
    },
    "mini_glossary": {
      "**Broadcasting**": "Automatisches Angleichen von Array-Shapes für Vektoroperationen.",
      "**Cython/C**": "Sprachen/Erweiterungen, in denen Performance-kritische Routinen implementiert sind."
    }
  },
  {
    "frage": "41. Welche MLOps-Praxis wird mit der K-Phase (Knowledge Transfer) des QUA³CK-Modells assoziiert?",
    "optionen": [
      "Das Tracking von Experimenten mit `MLflow`.",
      "Die interaktive Datenexploration mit `Streamlit`.",
      "Das Deployment in der Cloud und die Erstellung eines Portfolios.",
      "Die automatisierte Gegenüberstellung von Modellen in einem Dashboard."
    ],
    "loesung": 2,
    "erklaerung": "Die K-Phase fokussiert sich auf den Transfer des erarbeiteten Wissens. Dazu gehören das Deployment des Modells für Endanwender und die Dokumentation der Ergebnisse in einem Portfolio.",
    "gewichtung": 2,
    "thema": "QUA³CK & MLOps",
    "extended_explanation": {
      "title": "Vom Ergebnis zur Nutzung",
      "schritte": [
        "Modell über APIs/Apps bereitstellen, damit Stakeholder es produktiv nutzen können.",
        "Dokumentation und Portfolio sichern Wiederverwendung und Wissensweitergabe.",
        "Monitoring/Feedback-Schleifen initiieren, um den Nutzen zu verifizieren."
      ]
    },
    "mini_glossary": {
      "**Deployment**": "Bereitstellung eines Modells als Service oder App für Nutzer/Prozesse.",
      "**Portfolio**": "Sammlung veröffentlichter Ergebnisse/Apps zur Kommunikation des Nutzens."
    }
  },
  {
    "frage": "42. Welchen Zweck erfüllt der Befehl `docker-compose down`?",
    "optionen": [
      "Er lädt die neuesten Versionen der Images aus der Docker-Registry herunter.",
      "Er zeigt die aggregierten Logs aller laufenden Services in Echtzeit an.",
      "Er stoppt und entfernt die von `docker-compose up` erstellten Container, Netzwerke und Volumes.",
      "Er baut alle Docker-Images neu, ohne die zugehörigen Container zu starten."
    ],
    "loesung": 2,
    "erklaerung": "`docker-compose down` ist der komplementäre Befehl zu `up` und dient dazu, die gesamte Anwendungslandschaft sauber zu beenden und die erstellten Ressourcen freizugeben.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Ressourcen kontrolliert abbauen",
      "schritte": [
        "`down` stoppt alle durch das Compose-Projekt gestarteten Services.",
        "Es entfernt Container/Netzwerke/optionale Volumes, um sauber aufzuräumen.",
        "Dadurch werden Ressourcenkonflikte bei erneuten `up`-Läufen vermieden."
      ]
    },
    "mini_glossary": {
      "**Compose-Projekt**": "Menge von Ressourcen, die durch eine `docker-compose.yml` definiert sind.",
      "**Teardown**": "Geordneter Abbau von laufenden Diensten und Infrastruktur."
    }
  },
  {
    "frage": "43. Was ist der Hauptunterschied zwischen Supervised und Unsupervised Learning?",
    "optionen": [
      "Supervised Learning benötigt gelabelte Zieldaten, Unsupervised Learning nicht.",
      "Unsupervised Learning liefert immer genauere Ergebnisse als Supervised Learning.",
      "Supervised Learning wird für Clustering verwendet, Unsupervised für Klassifikation.",
      "Unsupervised Learning kann nur mit numerischen Daten umgehen."
    ],
    "loesung": 0,
    "erklaerung": "Der fundamentale Unterschied liegt in den Daten: Supervised Learning lernt eine Abbildung von Eingabedaten auf bekannte Ausgabedaten (Labels). Unsupervised Learning sucht nach Mustern in Daten ohne vordefinierte Labels.",
    "gewichtung": 1,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Labels entscheiden den Lernmodus",
      "schritte": [
        "Supervised: Lernen einer Funktion f(X)→y aus Beispielen mit Zielwerten.",
        "Unsupervised: Struktur in X identifizieren (Cluster, Dichten, Projektionen) ohne y.",
        "Auswahl richtet sich nach Verfügbarkeit/Qualität der Labels."
      ]
    },
    "mini_glossary": {
      "**Label**": "Zielwert/Annotation für ein Beispiel, z. B. Klasse oder Zahl.",
      "**Clustering**": "Gruppierung ähnlicher Punkte ohne vorgegebene Klassen."
    }
  },
  {
    "frage": "44. Welches Streamlit-Element wird zur Darstellung von Inhalten in nebeneinanderliegenden Spalten verwendet?",
    "optionen": [
      "`st.expander()`",
      "`st.tabs()`",
      "`st.columns()`",
      "`st.container()`"
    ],
    "loesung": 2,
    "erklaerung": "Mit `st.columns()` kann das Layout in mehrere vertikale Spalten aufgeteilt werden, um beispielsweise Kennzahlen, Diagramme oder Steuerelemente nebeneinander zu positionieren.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Horizontale Layouts strukturieren",
      "schritte": [
        "Erstellen Sie Spalten via `cols = st.columns(n)` und platzieren Inhalte kontextbezogen.",
        "Kombinieren Sie Spalten mit `st.metric`, `st.chart` für kompakte Dashboards.",
        "Achten Sie auf Responsivität: zu viele Spalten auf kleinen Displays vermeiden."
      ]
    },
    "mini_glossary": {
      "**`st.columns`**": "API zur Erstellung paralleler Spaltenbereiche.",
      "**Responsivität**": "Anpassungsfähigkeit des Layouts an unterschiedliche Bildschirmbreiten."
    }
  },
  {
    "frage": "45. Warum wird im `Dockerfile` `COPY requirements.txt .` vor `COPY . .` ausgeführt?",
    "optionen": [
      "Um die Lesbarkeit durch die Priorisierung von Konfigurationsdateien zu verbessern.",
      "Weil `COPY . .` eine sehr langsame Operation ist, die man ans Ende stellen sollte.",
      "Um Docker's Layer-Caching zu nutzen, sodass Abhängigkeiten nur bei Änderung der `requirements.txt` neu installiert werden.",
      "Um sicherzustellen, dass die Abhängigkeiten als separate, wiederverwendbare Schicht im Image existieren."
    ],
    "loesung": 2,
    "erklaerung": "Docker baut Images in Schichten (Layers). Ändert sich eine Zeile, werden alle nachfolgenden Schichten neu gebaut. Da sich die `requirements.txt` seltener ändert als der App-Code, wird durch diese Reihenfolge der zeitintensive `pip install`-Schritt oft aus dem Cache geladen.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Build-Cache optimal ausnutzen",
      "schritte": [
        "Zuerst `requirements.txt` kopieren und installieren, um eine stabile Cache-Schicht zu erzeugen.",
        "Danach App-Code kopieren; Codeänderungen invalidieren nicht den teuren Installationsschritt.",
        "Das reduziert Build-Zeiten signifikant bei iterativer Entwicklung."
      ]
    },
    "mini_glossary": {
      "**Layer-Cache**": "Mechanismus, der unveränderte Bauschichten wiederverwendet.",
      "**Invalidierung**": "Neubau nachfolgender Schichten durch Änderungen in vorherigen Schritten."
    }
  },
  {
    "frage": "46. Was ist eine `Chained Assignment`-Warnung in `pandas`?",
    "optionen": [
      "Eine Warnung, die bei der Verkettung von zu vielen Methodenaufrufen (`.pipe()...`) auftritt.",
      "Ein Hinweis auf eine potenziell fehlerhafte Zuweisung auf eine Kopie statt auf die Originaldaten; `.loc` sollte verwendet werden.",
      "Eine reine Information, dass eine Zuweisung auf eine Kette von Indizes erfolgreich war.",
      "Ein Fehler, der auftritt, wenn der DataFrame-Index nicht korrekt sortiert ist."
    ],
    "loesung": 1,
    "erklaerung": "Die 'SettingWithCopyWarning' deutet darauf hin, dass eine Operation möglicherweise auf einer temporären Kopie eines DataFrames stattfindet. Um sicherzustellen, dass die Zuweisung im Original-DataFrame ankommt, sollte man `.loc` für den gleichzeitigen Zugriff und die Zuweisung verwenden.",
    "gewichtung": 1,
    "thema": "Pandas & Python Basics",
    "extended_explanation": {
      "title": "Sicher zuweisen ohne Kopierfallen",
      "schritte": [
        "Vermeiden Sie Ausdrücke wie `df[df.x>0]['y']=…`; das kann auf Kopien arbeiten.",
        "Nutzen Sie `df.loc[df.x>0, 'y'] = …` für eindeutige Zieladressierung.",
        "Prüfen Sie Warnungen ernsthaft; sie deuten auf stillschweigend wirkungslose Updates hin."
      ]
    },
    "mini_glossary": {
      "**SettingWithCopyWarning**": "Warnung bei möglicher Zuweisung auf eine Kopie statt Original.",
      "**`.loc`**": "Expliziter Selektor für gleichzeitiges Filtern und Zuweisen."
    }
  },
  {
    "frage": "47. Welches Werkzeug wird im Kurs-Setup für das Experiment-Tracking eingesetzt?",
    "optionen": [
      "`Streamlit`",
      "`Jupyter Lab`",
      "`MLflow`",
      "`Docker Hub`"
    ],
    "loesung": 2,
    "erklaerung": "`MLflow` ist eine Open-Source-Plattform zur Verwaltung des gesamten Machine-Learning-Lebenszyklus, einschließlich des Trackings von Experimenten, Parametern, Metriken und Modellen.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Experimente nachvollziehbar machen",
      "schritte": [
        "Loggen Sie Hyperparameter, Metriken, Artefakte und Modelle pro Run.",
        "Vergleichen Sie Runs in der UI und wählen Sie systematisch den Favoriten aus.",
        "Registrieren/Versionieren Sie Modelle für reproduzierbares Deployment."
      ]
    },
    "mini_glossary": {
      "**Run**": "Einzelner Trainings-/Evaluationsdurchlauf mit geloggten Informationen.",
      "**Model Registry**": "Komponente zur Versionierung und Freigabe von Modellen."
    }
  },
  {
    "frage": "48. Was bewirkt die Taste `M` im Command Mode eines Jupyter Notebooks?",
    "optionen": [
      "Sie verschiebt die aktuelle Zelle eine Position nach unten (Move).",
      "Sie fügt eine neue Zelle oberhalb der aktuellen ein (More).",
      "Sie wandelt die aktuelle Zelle in eine Markdown-Zelle um.",
      "Sie startet den Kernel der Anwendung neu (Master Reset)."
    ],
    "loesung": 2,
    "erklaerung": "Im Command Mode wandelt die Taste `M` eine Zelle in eine `Markdown`-Zelle um, was für die Dokumentation innerhalb eines Notebooks essenziell ist. `Y` wandelt sie zurück in eine Code-Zelle.",
    "gewichtung": 1,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Zelltyp-Shortcuts sicher nutzen",
      "schritte": [
        "`Esc` für Command Mode; `M` setzt Zelle auf Markdown, `Y` auf Code.",
        "Dokumentation und Code sauber trennen erhöht Lesbarkeit und Review-Fähigkeit.",
        "Weitere nützliche Shortcuts: `A`/`B` (Zelle über/unter einfügen), `D,D` (löschen)."
      ]
    },
    "mini_glossary": {
      "**Command Mode**": "Notebook-Modus für Struktur-Operationen an Zellen.",
      "**Kernel**": "Ausführungsumgebung des Notebooks für Codezellen."
    }
  },
  {
    "frage": "49. Was ist der primäre Zweck der `EXPOSE`-Anweisung in einem `Dockerfile`?",
    "optionen": [
      "Den Container aktiv mit dem Internet zu verbinden.",
      "Einen Port vom Host-System auf einen Port im Container zu mappen.",
      "Den Port zu dokumentieren, auf dem der Service im Container lauscht.",
      "Die interne Firewall des Containers für einen bestimmten Port zu öffnen."
    ],
    "loesung": 2,
    "erklaerung": "`EXPOSE` hat rein dokumentarischen Charakter. Es teilt dem Benutzer des Images mit, auf welchem Port die Anwendung im Container standardmäßig lauscht. Die tatsächliche Port-Veröffentlichung geschieht erst beim `docker run` mit dem `-p`-Flag.",
    "gewichtung": 1,
    "thema": "Docker & Infrastruktur",
    "extended_explanation": {
      "title": "Dokumentation statt Portfreigabe",
      "schritte": [
        "`EXPOSE 8501` signalisiert, dass der Dienst intern auf Port 8501 lauscht.",
        "Konkretes Mapping erfolgt zur Laufzeit via `-p HOST:CONTAINER`.",
        "Klare Port-Doku erleichtert Betrieb/Orchestrierung (Compose/K8s)."
      ]
    },
    "mini_glossary": {
      "**Port-Mapping**": "Bindung eines Host-Ports an einen Container-Port beim Start.",
      "**K8s Service**": "Kubernetes-Ressource, die Pods unter Ports/IPs erreichbar macht."
    }
  },
  {
    "frage": "50. Welche Metrik eignet sich zur Bewertung eines K-Means-Clustering-Modells, wenn die wahren Labels bekannt sind?",
    "optionen": [
      "`Accuracy`",
      "`Adjusted Rand Score`",
      "`F1-Score`",
      "`Root Mean Squared Error (RMSE)`"
    ],
    "loesung": 1,
    "erklaerung": "Der `Adjusted Rand Score` ist eine Metrik, die die Ähnlichkeit zwischen den gefundenen Clustern und den wahren Klassen misst. Im Gegensatz zur `Accuracy` ist sie unabhängig von der absoluten Benennung der Cluster.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Cluster-Güte gegen Ground Truth prüfen",
      "schritte": [
        "K-Means liefert Clusterlabels, deren Namen beliebig permutiert sein können.",
        "`Adjusted Rand Score` vergleicht Paarzugehörigkeiten und korrigiert Zufallstreffer.",
        "Dadurch ist er invariant gegenüber Label-Permutationen und aussagekräftiger als Accuracy."
      ]
    },
    "mini_glossary": {
      "**Adjusted Rand Score**": "Ähnlichkeitsmaß der Clusterzuweisung, zufallskorrigiert (Bereich −1 bis 1).",
      "**Label-Permutation**": "Neubenennung von Clustern ohne Änderung der Gruppierung."
    }
  },
  {
    "frage": "51. Was ist ein wesentlicher Vorteil von Entscheidungsbäumen?",
    "optionen": [
      "Ihre hohe Interpretierbarkeit durch visualisierbare Regeln.",
      "Ihre angeborene Robustheit gegenüber Overfitting.",
      "Ihre Unempfindlichkeit gegenüber der Wahl der Hyperparameter.",
      "Ihre Fähigkeit, ohne gelabelte Daten zu lernen."
    ],
    "loesung": 0,
    "erklaerung": "Entscheidungsbäume sind 'White-Box'-Modelle. Die gelernten Wenn-Dann-Regeln können leicht visualisiert und von Menschen nachvollzogen werden, was in vielen Anwendungsbereichen eine Anforderung ist.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Regeln statt Blackbox",
      "schritte": [
        "Bäume teilen den Raum über klare, achsenparallele Regeln (z. B. `feat ≤ t`).",
        "Pfade zu Blättern lassen sich als Entscheidungsregeln formulieren und visualisieren.",
        "Fachbereiche können Entscheidungen prüfen, erklären und regulatorisch auditieren."
      ]
    },
    "mini_glossary": {
      "**White-Box**": "Modell, dessen Entscheidungslogik nachvollziehbar ist.",
      "**Blatt**": "Terminalknoten, der Vorhersagen/Klassen liefert."
    }
  },
  {
    "frage": "52. Was ist ein bekannter Nachteil von einzelnen Entscheidungsbäumen?",
    "optionen": [
      "Sie sind rechnerisch extrem aufwendig im Training.",
      "Sie neigen dazu, die Trainingsdaten zu überanpassen (Overfitting).",
      "Sie können ausschließlich lineare Zusammenhänge modellieren.",
      "Sie erfordern eine aufwendige Normalisierung der Eingabedaten."
    ],
    "loesung": 1,
    "erklaerung": "Einzelne, ungestutzte Entscheidungsbäume neigen dazu, sehr komplexe Strukturen zu bilden, die die Trainingsdaten perfekt lernen, aber schlecht auf neue, unbekannte Daten generalisieren.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Tiefe Bäume, hohes Overfitting-Risiko",
      "schritte": [
        "Viele Splits passen sich Rauschen/Outliern an und verringern Generalisierung.",
        "Gegenmaßnahmen: Pruning, Mindestblattgrößen, max. Tiefe begrenzen.",
        "Ensembling (Random Forest, Boosting) stabilisiert und reduziert Varianz."
      ]
    },
    "mini_glossary": {
      "**Pruning**": "Beschneiden überkomplexer Baumteile zur Generalisierungsverbesserung.",
      "**Varianz**": "Empfindlichkeit eines Modells gegenüber Trainingsdatenfluktuationen."
    }
  },
  {
    "frage": "53. Wie funktioniert der K-Nearest-Neighbors (KNN) Algorithmus?",
    "optionen": [
      "Er klassifiziert einen Datenpunkt basierend auf der Mehrheitsklasse seiner k nächsten Nachbarn.",
      "Er erstellt einen hierarchischen Baum von Entscheidungsregeln.",
      "Er teilt den Datenraum in k Cluster basierend auf deren Mittelpunkten auf.",
      "Er passt eine lineare Funktion an die Daten an, um den Fehler zu minimieren."
    ],
    "loesung": 0,
    "erklaerung": "KNN ist ein 'lazy learner'. Für eine Vorhersage sucht er die k ähnlichsten Datenpunkte aus dem Trainingsdatensatz und lässt sie über die Klasse 'abstimmen'.",
    "gewichtung": 1,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Mehrheit der Nachbarn entscheidet",
      "schritte": [
        "Definieren Sie eine Distanzmetrik (z. B. euklidisch) und wählen `k`.",
        "Finden Sie die k nächsten Trainingspunkte zum neuen Beispiel.",
        "Klassifikation per Mehrheitsvotum; Regression per Durchschnitt/Median."
      ]
    },
    "mini_glossary": {
      "**Lazy Learning**": "Kein explizites Trainingsmodell; Arbeit erfolgt bei der Vorhersage.",
      "**Distanzmetrik**": "Funktion zur Quantifizierung der Ähnlichkeit zwischen Punkten."
    }
  },
  {
    "frage": "54. Warum sollte der Hyperparameter `k` bei KNN für eine binäre Klassifikation ungerade gewählt werden?",
    "optionen": [
      "Um die Berechnungszeit des Algorithmus zu halbieren.",
      "Um eine eindeutige Mehrheitsentscheidung zu ermöglichen und Unentschieden zu vermeiden.",
      "Um die Anfälligkeit des Modells für Overfitting zu reduzieren.",
      "Weil der Algorithmus mit geraden Zahlen einen Fehler auslöst."
    ],
    "loesung": 1,
    "erklaerung": "Wenn `k` bei einer binären Klassifikation (z.B. Klasse A vs. B) gerade ist, könnte es zu einem Patt kommen (z.B. 2 Nachbarn für A, 2 für B). Ein ungerades `k` stellt eine klare Mehrheit sicher.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Patt-Situationen vermeiden",
      "schritte": [
        "Bei binären Labels kann ein gerades `k` zu Stimmengleichheit führen.",
        "Ungerades `k` erzwingt eine Mehrheitsklasse unter den k Nachbarn.",
        "Bei Mehrklassenproblemen hilft ein ausreichend großes `k` gegen Zufallspatts."
      ]
    },
    "mini_glossary": {
      "**Hyperparameter**": "Nicht gelernter Parameter, der das Lernverhalten steuert.",
      "**Mehrheitsvotum**": "Entscheidung nach der häufigsten Klasse unter Nachbarn."
    }
  },
  {
    "frage": "55. Was ist ein typischer Anwendungsfall für K-Means Clustering?",
    "optionen": [
      "Die Vorhersage von numerischen Zielwerten wie Aktienkursen.",
      "Die Gruppierung von Datenpunkten in Cluster ohne vordefinierte Labels.",
      "Die Klassifikation von Bildern in vordefinierte Kategorien.",
      "Die Reduzierung der Dimensionalität eines Datensatzes."
    ],
    "loesung": 1,
    "erklaerung": "K-Means ist ein Unsupervised-Learning-Algorithmus, der verwendet wird, um inhärente Gruppen oder Cluster in einem Datensatz zu entdecken, z.B. für die Kundensegmentierung.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "K-Means als Segmentierungswerkzeug",
      "schritte": [
        "Initialisieren Sie k Zentroiden und ordnen Sie Punkte dem nächsten Zentrum zu.",
        "Aktualisieren Sie Zentroiden als Mittelwerte der zugeordneten Punkte; wiederholen bis Konvergenz.",
        "Nutzen Sie resultierende Cluster für Personas, Targeting oder Anomaliescreening."
      ]
    },
    "mini_glossary": {
      "**Zentroid**": "Mittelpunkt eines Clusters; arithmetisches Mittel seiner Punkte.",
      "**Konvergenz**": "Zustand, in dem Zuordnungen/Zentren sich nicht mehr wesentlich ändern."
    }
  },
  {
    "frage": "56. Was unterscheidet Supervised von Unsupervised Learning?",
    "optionen": [
      "Supervised Learning wird für Regression, Unsupervised für Klassifikation verwendet.",
      "Supervised Learning erfordert mehr Rechenleistung, ist aber immer genauer.",
      "Supervised Learning lernt von Daten mit Zielwerten (Labels), Unsupervised Learning von Daten ohne.",
      "Unsupervised Learning ist ein Teilgebiet von Deep Learning, Supervised Learning nicht."
    ],
    "loesung": 2,
    "erklaerung": "Der fundamentale Unterschied liegt in den Daten: Supervised Learning lernt eine Abbildung von Eingabedaten auf bekannte Ausgabedaten (Labels). Unsupervised Learning sucht nach Mustern in Daten ohne vordefinierte Labels.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Zielwerte vorhanden oder nicht",
      "schritte": [
        "Supervised: Lernen mit Zielwerten (Klassifikation/Regression).",
        "Unsupervised: Strukturfindung ohne Zielwerte (Cluster, Dichte, Dimensionen).",
        "Semi-Supervised/Weakly-Supervised nutzen Mischformen je nach Datenlage."
      ]
    },
    "mini_glossary": {
      "**Regression**": "Vorhersage kontinuierlicher Zielgrößen.",
      "**Semi-Supervised**": "Ansatz mit wenigen Labels und vielen unlabeled Beispielen."
    }
  },
  {
    "frage": "57. Was ist ein Vorteil des KNN-Algorithmus?",
    "optionen": [
      "Er ist nicht-parametrisch und macht keine Annahmen über die Datenverteilung.",
      "Die Vorhersage ist bei sehr großen Trainingsdatensätzen extrem schnell.",
      "Er ist unempfindlich gegenüber der Skalierung der Features.",
      "Er funktioniert am besten mit hochdimensionalen Datensätzen."
    ],
    "loesung": 0,
    "erklaerung": "KNN ist ein nicht-parametrischer Algorithmus, das heißt, er macht keine Annahmen über die funktionale Form der zugrundeliegenden Datenverteilung, was ihn sehr flexibel macht.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Flexibel ohne Modellannahmen",
      "schritte": [
        "KNN passt sich lokal der Datenstruktur an statt globale Form anzunehmen.",
        "Dadurch kann es komplexe, nichtlineare Grenzen approximieren.",
        "Voraussetzung sind sinnvolle Distanzen (Skalierung/Feature-Auswahl)."
      ]
    },
    "mini_glossary": {
      "**Nicht-parametrisch**": "Kein fester Funktionsansatz; Komplexität wächst mit Daten.",
      "**Entscheidungsgrenze**": "Grenzlinie/-fläche, die Klassen im Merkmalsraum trennt."
    }
  },
  {
    "frage": "58. Was ist ein bekannter Nachteil des KNN-Algorithmus?",
    "optionen": [
      "Er ist empfindlich gegenüber der Skalierung der Features.",
      "Er kann keine nicht-linearen Entscheidungsgrenzen lernen.",
      "Die Trainingsphase ist sehr rechenintensiv.",
      "Er ist nur für binäre Klassifikationsprobleme geeignet."
    ],
    "loesung": 0,
    "erklaerung": "Da KNN auf Distanzmaßen basiert, können Features mit großen Wertebereichen die Distanzberechnung dominieren. Daher ist eine Skalierung (z.B. Normalisierung) der Daten meist erforderlich.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Skalierung ist Pflicht",
      "schritte": [
        "Standardisieren/Normalisieren Sie Features, damit keine Dimension dominiert.",
        "Prüfen Sie Distanzmetriken (Manhattan, Minkowski) je nach Dateneigenschaft.",
        "Nutzen Sie Feature-Selektion, um Rauschen in hohen Dimensionen zu reduzieren."
      ]
    },
    "mini_glossary": {
      "**Standardisierung**": "Transformation auf Mittelwert 0 und Varianz 1.",
      "**Manhattan-Distanz**": "L₁-Distanz; Summe der Absolutdifferenzen je Dimension."
    }
  },
  {
    "frage": "59. Wozu dient die 'Ellbogenmethode' (Elbow Method) im Kontext von K-Means?",
    "optionen": [
      "Sie hilft bei der Bestimmung einer geeigneten Anzahl von Clustern (k).",
      "Sie ist eine Technik zur Skalierung der Daten vor dem Clustering.",
      "Sie dient der Visualisierung der Cluster in einem zweidimensionalen Raum.",
      "Sie beschleunigt die Konvergenz des Algorithmus erheblich."
    ],
    "loesung": 0,
    "erklaerung": "Bei der Ellbogenmethode wird die Summe der quadrierten Abstände zu den Cluster-Zentren für verschiedene Werte von `k` aufgetragen. Das 'Knie' oder der 'Ellbogen' der Kurve deutet auf einen guten Kompromiss für `k` hin.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "K mittels Knickpunkt wählen",
      "schritte": [
        "Berechnen Sie WCSS (Within-Cluster Sum of Squares) für mehrere k.",
        "Plotten Sie k vs. WCSS und suchen Sie den Knick (abnehmender Grenznutzen).",
        "Wählen Sie k am Ellbogen als Balance aus Einfachheit und Güte."
      ]
    },
    "mini_glossary": {
      "**WCSS**": "Summe der quadrierten Distanzen der Punkte zu ihren Zentroiden.",
      "**Grenznutzen**": "Abnehmender Zusatzgewinn bei Erhöhung von k um 1."
    }
  },
  {
    "frage": "60. Was repräsentiert ein interner Knoten in einem Entscheidungsbaum?",
    "optionen": [
      "Eine finale Klassenzuweisung (ein Blatt).",
      "Eine Entscheidungsregel basierend auf einem Feature-Wert.",
      "Den Mittelwert aller Features in den Daten.",
      "Die Wahrscheinlichkeit für das Auftreten einer Klasse."
    ],
    "loesung": 1,
    "erklaerung": "Jeder interne Knoten in einem Entscheidungsbaum repräsentiert einen 'Test' für ein bestimmtes Feature (z.B. 'Alter > 30?'), der die Daten in die entsprechenden Kind-Knoten aufteilt.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Knoten als Test auf Feature-Schwelle",
      "schritte": [
        "Ein interner Knoten prüft eine Bedingung (z. B. `x_j ≤ t`) und verzweigt.",
        "Diese rekursiven Tests partitionieren den Raum in homogene Regionen.",
        "Blätter enthalten am Ende Vorhersagen (Klasse oder Wert)."
      ]
    },
    "mini_glossary": {
      "**Split**": "Aufteilung der Daten anhand einer Regel in zwei (oder mehr) Teilmengen.",
      "**Impurity**": "Unreinheit eines Knotens (z. B. Gini/Entropie) zur Split-Bewertung."
    }
  },
  {
    "frage": "61. Was ist ein typischer Vorteil von K-Means?",
    "optionen": [
      "Die Fähigkeit, verborgene Gruppen oder Muster in Daten zu entdecken.",
      "Die Garantie, immer die global optimale Cluster-Lösung zu finden.",
      "Die Robustheit gegenüber nicht-sphärischen Cluster-Formen.",
      "Die Notwendigkeit, keine Hyperparameter wie `k` festlegen zu müssen."
    ],
    "loesung": 0,
    "erklaerung": "K-Means ist ein leistungsfähiges Werkzeug für die explorative Datenanalyse, um ohne Vorwissen über Klassen eine erste Segmentierung der Daten vorzunehmen und Hypothesen zu generieren.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Warum K-Means Strukturen sichtbar macht",
      "schritte": [
        "K-Means gruppiert Punkte um Zentren, sodass innerhalb eines Clusters die Ähnlichkeit maximal ist.",
        "Diese Selbstorganisation deckt verborgene Segmente auf (z. B. Kundengruppen).",
        "Die Cluster dienen als Hypothesenbasis für weitere Analysen oder A/B-Tests."
      ]
    },
    "mini_glossary": {
      "**Cluster**": "Gruppe ähnlicher Datenpunkte basierend auf einer Distanzmetrik.",
      "**Zentroid**": "Mittelwertvektor eines Clusters, um den K-Means gruppiert."
    }
  },
  {
    "frage": "62. Was ist ein typischer Nachteil von K-Means?",
    "optionen": [
      "Die Anzahl der Cluster (k) muss a priori festgelegt werden.",
      "Der Algorithmus ist nur für kategorische Daten geeignet.",
      "Er benötigt gelabelte Daten für das Training.",
      "Er ist im Vergleich zu anderen Algorithmen sehr rechenintensiv."
    ],
    "loesung": 0,
    "erklaerung": "Die Notwendigkeit, die Anzahl der Cluster `k` im Voraus zu bestimmen, ist eine der größten Herausforderungen bei der Anwendung von K-Means, da die Wahl von `k` das Ergebnis stark beeinflusst.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Warum die Wahl von k schwierig ist",
      "schritte": [
        "Unterschiedliche k-Werte führen zu unterschiedlichen Segmentierungen und Interpretationen.",
        "Heuristiken wie Ellbogen- oder Silhouettenmethode geben Anhaltspunkte, sind aber nicht eindeutig.",
        "Domänenwissen und Validierung im Downstream-Use-Case stabilisieren die Wahl."
      ]
    },
    "mini_glossary": {
      "**k**": "Hyperparameter, der die Anzahl der zu findenden Cluster festlegt.",
      "**Silhouettenwert**": "Maß für Clustertrennschärfe (zwischen −1 und 1)."
    }
  },
  {
    "frage": "63. In welchen Anwendungsbereichen werden Entscheidungsbäume oft bevorzugt?",
    "optionen": [
      "In regulierten Branchen wie Finanzen oder Medizin, wo Interpretierbarkeit gefordert ist.",
      "Bei hochdimensionalen Bilderkennungsaufgaben mit Millionen von Pixeln.",
      "Für die Verarbeitung von sequenziellen Daten wie natürlicher Sprache.",
      "Wenn die zugrundeliegenden Zusammenhänge stark nicht-linear sind."
    ],
    "loesung": 0,
    "erklaerung": "Aufgrund ihrer 'White-Box'-Natur werden Entscheidungsbäume oft dort eingesetzt, wo die Gründe für eine Entscheidung nachvollziehbar sein müssen, z.B. bei der Kreditwürdigkeitsprüfung.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Nachvollziehbarkeit als Compliance-Faktor",
      "schritte": [
        "Bäume liefern klare Wenn-Dann-Regeln, die fachlich geprüft werden können.",
        "Audits/Erklärbarkeit sind in regulierten Domänen essenziell.",
        "Visualisierungen der Pfade erleichtern Risiko- und Bias-Bewertung."
      ]
    },
    "mini_glossary": {
      "**White-Box**": "Modellklasse mit transparenten, erklärbaren Entscheidungen.",
      "**Auditierbarkeit**": "Fähigkeit, Modelle/Entscheidungen formell zu prüfen und zu begründen."
    }
  },
  {
    "frage": "64. Was ist ein typischer Einsatzbereich für den KNN-Algorithmus?",
    "optionen": [
      "Empfehlungssysteme, die 'ähnliche' Benutzer oder Artikel finden.",
      "Die Analyse von Zeitreihendaten zur Vorhersage zukünftiger Werte.",
      "Die Segmentierung von Kunden in verschiedene Kaufverhaltensgruppen.",
      "Das Training von tiefen neuronalen Netzen zur Objekterkennung."
    ],
    "loesung": 0,
    "erklaerung": "KNN eignet sich gut für Empfehlungssysteme (Collaborative Filtering), da das Konzept der 'Ähnlichkeit' zwischen Datenpunkten (z.B. Benutzern mit ähnlichem Geschmack) zentral ist.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Nachbarschaft als Empfehlung",
      "schritte": [
        "KNN sucht nächste Nachbarn im Merkmalsraum (z. B. Nutzerprofile).",
        "Ähnliche Nachbarn liefern implizite Hinweise auf Präferenzen.",
        "Aggregation der Nachbarn erzeugt personalisierte Empfehlungen."
      ]
    },
    "mini_glossary": {
      "**Ähnlichkeitsmaß**": "Metrik (z. B. Kosinus, euklidisch) zur Bestimmung von Nähe.",
      "**Collaborative Filtering**": "Empfehlungen aus Nutzer-zu-Nutzer- oder Item-Ähnlichkeiten."
    }
  },
  {
    "frage": "65. Welches Geschäftsproblem lässt sich gut mit K-Means lösen?",
    "optionen": [
      "Die Segmentierung eines Kundenstamms in verschiedene Marketing-Gruppen.",
      "Die Vorhersage des genauen Umsatzes für das nächste Quartal.",
      "Die Erkennung von betrügerischen Transaktionen in Echtzeit.",
      "Die automatische Übersetzung von Kundenrezensionen."
    ],
    "loesung": 0,
    "erklaerung": "K-Means ist ideal für die Kundensegmentierung, da es basierend auf Merkmalen wie Kaufverhalten oder demografischen Daten automatisch Gruppen von ähnlichen Kunden identifizieren kann.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Kundensegmente datengetrieben finden",
      "schritte": [
        "Features (RFM, Demografie) standardisieren und K-Means anwenden.",
        "Clusterprofile interpretieren und mit Business-Zielen abgleichen.",
        "Maßnahmen (Targeting, Pricing) je Segment testen und messen."
      ]
    },
    "mini_glossary": {
      "**RFM**": "Recency, Frequency, Monetary – Kaufverhaltensmerkmale.",
      "**Segmentierung**": "Aufteilung der Kundenbasis in homogene Gruppen."
    }
  },
  {
    "frage": "66. Was ist ein Vorteil von Entscheidungsbäumen gegenüber KNN?",
    "optionen": [
      "Sie erfordern keine Skalierung der Features.",
      "Sie können besser mit hochdimensionalen Daten umgehen.",
      "Die Vorhersage ist bei großen Datensätzen schneller.",
      "Sie sind unempfindlicher gegenüber irrelevanten Features."
    ],
    "loesung": 0,
    "erklaerung": "Da Entscheidungsbäume auf regelbasierten Aufteilungen (z.B. 'Feature X > 5') basieren, sind sie unempfindlich gegenüber der Skalierung der Eingabedaten, im Gegensatz zum distanzbasierten KNN.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Regeln statt Distanzen",
      "schritte": [
        "Bäume vergleichen Merkmale gegen Schwellen; Maßeinheiten sind egal.",
        "KNN nutzt Distanzen, die ohne Skalierung verzerrt werden.",
        "Damit sind Bäume out-of-the-box robuster bzgl. Feature-Skalen."
      ]
    },
    "mini_glossary": {
      "**Skalierung**": "Transformation von Merkmalen auf vergleichbare Größenordnungen.",
      "**Schwellwertsplit**": "Aufteilung eines Knotens bei `Feature ≤/ > t`."
    }
  },
  {
    "frage": "67. Was ist ein Vorteil von KNN gegenüber Entscheidungsbäumen?",
    "optionen": [
      "Er macht keine Annahmen über die zugrundeliegende Datenverteilung.",
      "Er ist leichter zu interpretieren und zu visualisieren.",
      "Er benötigt keine Einstellung von Hyperparametern wie `k`.",
      "Die Trainingsphase ist rechenintensiver, aber genauer."
    ],
    "loesung": 0,
    "erklaerung": "Als nicht-parametrischer Algorithmus kann KNN komplexe Entscheidungsgrenzen lernen, ohne eine spezifische Form (wie achsenparallele Splits bei Bäumen) anzunehmen.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Flexible Grenzen ohne Funktionsansatz",
      "schritte": [
        "KNN bildet lokale Strukturen ab und passt sich der Datengeometrie an.",
        "Dadurch können geschwungene, komplexe Entscheidungsflächen entstehen.",
        "Voraussetzung: sinnvolles Distanzmaß und Feature-Skalierung."
      ]
    },
    "mini_glossary": {
      "**Nicht-parametrisch**": "Komplexität wächst mit Daten; kein fixer Modellansatz.",
      "**Entscheidungsgrenze**": "Trennfläche zwischen Klassen im Merkmalsraum."
    }
  },
  {
    "frage": "68. Was ist ein Vorteil von K-Means gegenüber KNN?",
    "optionen": [
      "Er kann Gruppen in Daten ohne bekannte Labels finden (Unsupervised).",
      "Er ist ein Supervised-Learning-Algorithmus für Klassifikation.",
      "Er ist robuster gegenüber der Wahl des Hyperparameters `k`.",
      "Er kann Cluster beliebiger, nicht-sphärischer Formen erkennen."
    ],
    "loesung": 0,
    "erklaerung": "K-Means ist ein Unsupervised-Algorithmus, der für die Entdeckung von Strukturen in ungelabelten Daten konzipiert ist, während KNN ein Supervised-Algorithmus ist, der gelabelte Trainingsdaten benötigt.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Unsupervised statt labelbasiert",
      "schritte": [
        "K-Means benötigt keine Zielwerte und entdeckt Muster autonom.",
        "KNN setzt gelabelte Beispiele voraus, um Mehrheitsklassen zu bilden.",
        "K-Means eignet sich daher für Explorationsphasen und kalte Starts."
      ]
    },
    "mini_glossary": {
      "**Unsupervised**": "Lernen ohne Zielwerte/Labels.",
      "**Kalter Start**": "Situation ohne oder mit wenigen Labels/Historie."
    }
  },
  {
    "frage": "69. Was ist eine zentrale 'Best Practice' im Machine Learning?",
    "optionen": [
      "Immer mehrere Algorithmen evaluieren, um das beste Modell für das spezifische Problem zu finden.",
      "Sich immer für den komplexesten verfügbaren Algorithmus wie Deep Learning entscheiden.",
      "Die Aufteilung in Trainings- und Testdaten vermeiden, um alle Daten zum Training zu nutzen.",
      "Das Feature Engineering ignorieren und die Rohdaten direkt verwenden."
    ],
    "loesung": 0,
    "erklaerung": "Es gibt keinen 'einen besten' Algorithmus. Eine gute Praxis ist es, mehrere Modelle (von einfach bis komplex) zu testen und ihre Leistung systematisch zu vergleichen, um die beste Lösung zu finden.",
    "gewichtung": 2,
    "thema": "Werkzeuge & Ökosystem",
    "extended_explanation": {
      "title": "Breit starten, dann fokussieren",
      "schritte": [
        "Baselines (z. B. lineare Modelle, Bäume) schaffen Vergleichsmaßstäbe.",
        "Systematisches Experiment-Tracking verhindert Cherry-Picking.",
        "Auswahl entlang Metrik, Robustheit, Kosten und Betriebsrestriktionen."
      ]
    },
    "mini_glossary": {
      "**Baseline**": "Einfaches Referenzmodell als Untergrenze der Performance.",
      "**Cherry-Picking**": "Selektive Ergebniswahl ohne vollständige Evidenzbasis."
    }
  },
  {
    "frage": "70. Was besagt die 'AMALEA-Weisheit' zu den 'Big 3' Algorithmen (DT, KNN, K-Means)?",
    "optionen": [
      "Wer ihre Prinzipien versteht, kann etwa 80% aller klassischen ML-Projekte konzeptionell einordnen.",
      "Diese drei Algorithmen sind veraltet und sollten nicht mehr verwendet werden.",
      "Alle modernen Deep-Learning-Architekturen basieren direkt auf diesen drei Algorithmen.",
      "Sie sind nur für sehr kleine Datensätze mit weniger als 1000 Zeilen geeignet."
    ],
    "loesung": 0,
    "erklaerung": "Die 'Big 3' repräsentieren drei grundlegende Paradigmen des maschinellen Lernens (regelbasiert, distanzbasiert, clusterbasiert). Ihr Verständnis bildet eine solide Basis für das Verständnis der meisten anderen ML-Methoden.",
    "gewichtung": 2,
    "thema": "Klassische ML-Algorithmen",
    "extended_explanation": {
      "title": "Drei Paradigmen als Landkarte",
      "schritte": [
        "Entscheidungsbäume: regelbasiertes, interpretierbares Lernen.",
        "KNN: distanzbasiertes, nicht-parametrisches Lernen.",
        "K-Means: clusterbasiertes Unsupervised-Lernen zur Musterentdeckung."
      ]
    },
    "mini_glossary": {
      "**Paradigma**": "Grundlegendes Modellierungsprinzip mit typischen Stärken/Schwächen.",
      "**Konzeptionelle Einordnung**": "Schnelles Zuordnen eines Problems zu passenden Methodenfamilien."
    }
  },
  {
    "frage": "71. Was ist ein grundlegendes Merkmal eines neuronalen Netzes?",
    "optionen": [
      "Es besteht aus Schichten von Neuronen, die durch gewichtete Verbindungen miteinander verknüpft sind.",
      "Es verwendet einen einzelnen Entscheidungsbaum, um Vorhersagen zu treffen.",
      "Es benötigt grundsätzlich keine Trainingsdaten, um zu funktionieren.",
      "Es kann ausschließlich lineare Regressionen durchführen."
    ],
    "loesung": 0,
    "erklaerung": "Neuronale Netze sind von der Struktur des Gehirns inspiriert und bestehen aus miteinander verbundenen Knoten (Neuronen), die in Schichten angeordnet sind. Die Stärke der Verbindungen (Gewichte) wird im Training gelernt.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Schichten, Gewichte, Vorwärts- und Rückwärtslauf",
      "schritte": [
        "Eingaben propagieren vorwärts durch verbundene Schichten.",
        "Gewichte bestimmen die Transformation; Aktivierungen fügen Nichtlinearität hinzu.",
        "Im Training werden Gewichte per Gradientenabstieg angepasst."
      ]
    },
    "mini_glossary": {
      "**Gewicht**": "Skalar, der die Stärke einer Verbindung zwischen Neuronen bestimmt.",
      "**Aktivierung**": "Nichtlineare Funktion wie ReLU oder Sigmoid."
    }
  },
  {
    "frage": "72. Was ist die Hauptfunktion einer Aktivierungsfunktion in einem neuronalen Netz?",
    "optionen": [
      "Sie führt Nichtlinearität ein, was dem Netz ermöglicht, komplexe Muster zu lernen.",
      "Sie berechnet den Gradienten für die Backpropagation.",
      "Sie initialisiert die Gewichte der Neuronen vor dem Training.",
      "Sie normalisiert die Eingabedaten auf einen Bereich zwischen 0 und 1."
    ],
    "loesung": 0,
    "erklaerung": "Ohne nichtlineare Aktivierungsfunktionen wäre ein neuronales Netz, egal wie viele Schichten es hat, nur in der Lage, lineare Zusammenhänge zu modellieren. Die Nichtlinearität ist entscheidend für seine Mächtigkeit.",
    "gewichtung": 1,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Nichtlinearität als Schlüssel zur Ausdruckskraft",
      "schritte": [
        "Aktivierungen brechen lineare Kombinatorik auf und erlauben komplexe Abbildungen.",
        "Unterschiedliche Funktionen (ReLU, Tanh, Sigmoid) haben unterschiedliche Gradienteneigenschaften.",
        "Wahl der Aktivierung beeinflusst Trainingstabilität und Konvergenz."
      ]
    },
    "mini_glossary": {
      "**ReLU**": "Rectified Linear Unit, `f(x)=max(0,x)`; schnell und gradientenstark.",
      "**Tanh/Sigmoid**": "S-förmige Aktivierungen mit Sättigungszonen."
    }
  },
  {
    "frage": "73. Was ist ein wesentlicher Vorteil der `ReLU`-Aktivierungsfunktion gegenüber `Sigmoid`?",
    "optionen": [
      "`ReLU` leidet weniger unter dem 'Vanishing Gradient'-Problem.",
      "`ReLU` ist über den gesamten Definitionsbereich stetig differenzierbar.",
      "`ReLU` eignet sich besser für die Ausgabeschicht bei binärer Klassifikation.",
      "`ReLU` ist rechenintensiver, aber genauer."
    ],
    "loesung": 0,
    "erklaerung": "Die Ableitung der `Sigmoid`-Funktion ist in vielen Bereichen nahe null, was bei tiefen Netzen zum 'Verschwinden' der Gradienten führen kann. `ReLU` hat für positive Eingaben eine konstante Ableitung von 1, was den Gradientenfluss erleichtert.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Stärkerer Gradientenfluss mit ReLU",
      "schritte": [
        "ReLU lässt positive Signale linear passieren, vermeidet Sättigung.",
        "Damit bleiben Gradienten in vielen Schichten groß genug zum Lernen.",
        "Zusätze wie Leaky-ReLU mildern das 'Dead-Neuron'-Problem."
      ]
    },
    "mini_glossary": {
      "**Vanishing Gradient**": "Gradienten werden in tiefen Netzen zu klein; Lernen stockt.",
      "**Leaky-ReLU**": "Variante mit kleiner Steigung für negative x-Werte."
    }
  },
  {
    "frage": "74. Was beschreibt das Backpropagation-Verfahren?",
    "optionen": [
      "Die effiziente Berechnung der Gradienten des Fehlers bezüglich der Gewichte.",
      "Die zufällige Initialisierung der Gewichte vor dem ersten Trainingsschritt.",
      "Die Auswahl der optimalen Anzahl von Neuronen für ein Hidden Layer.",
      "Die schichtweise Vorwärtsausbreitung der Eingabedaten durch das Netz."
    ],
    "loesung": 0,
    "erklaerung": "`Backpropagation` ist der Algorithmus, mit dem die Gewichte eines neuronalen Netzes trainiert werden. Er propagiert den Fehler von der Ausgabeschicht rückwärts durch das Netz, um die Gradienten für die Gewichtsaktualisierung zu berechnen.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Fehler rückwärts, Gewichte vorwärts",
      "schritte": [
        "Vorwärtslauf berechnet Vorhersagen und Verlust.",
        "Rückwärtslauf bestimmt mittels Kettenregel die Gradienten je Gewicht.",
        "Optimierer aktualisiert Gewichte entlang des negativen Gradienten."
      ]
    },
    "mini_glossary": {
      "**Kettenregel**": "Regel zur Ableitung verschachtelter Funktionen.",
      "**Optimierer**": "Verfahren zur Aktualisierung der Gewichte (z. B. SGD, Adam)."
    }
  },
  {
    "frage": "75. Was ist eine typische Ursache für Overfitting bei neuronalen Netzen?",
    "optionen": [
      "Ein zu komplexes Modell (zu viele Parameter) im Verhältnis zu wenigen Trainingsdaten.",
      "Eine zu kleine Lernrate während des Trainings.",
      "Die Verwendung von Regularisierungstechniken wie `Dropout`.",
      "Das Fehlen einer nichtlinearen Aktivierungsfunktion in den Hidden Layers."
    ],
    "loesung": 0,
    "erklaerung": "`Overfitting` tritt auf, wenn ein Modell so flexibel ist, dass es beginnt, das Rauschen in den Trainingsdaten auswendig zu lernen, anstatt das zugrundeliegende Muster zu generalisieren.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Zu viele Parameter, zu wenig Signal",
      "schritte": [
        "Hohe Modellkapazität memoriert Rauschen und Ausreißer.",
        "Val-Leistung fällt ab, obwohl Train-Fehler weiter sinkt.",
        "Gegenmittel: Regularisierung, Datenaugmentierung, Early Stopping."
      ]
    },
    "mini_glossary": {
      "**Kapazität**": "Ausdrucksfähigkeit/Komplexität eines Modells.",
      "**Generalisation Gap**": "Differenz zwischen Trainings- und Validierungsleistung."
    }
  },
  {
    "frage": "76. Was ist `Dropout` im Kontext von Deep Learning?",
    "optionen": [
      "Eine Regularisierungstechnik, bei der während des Trainings zufällig Neuronen 'ausgeschaltet' werden.",
      "Eine Methode zur Beschleunigung des Trainings durch Reduzierung der Batch-Größe.",
      "Eine spezielle Aktivierungsfunktion für die Ausgabeschicht.",
      "Ein Optimierungsalgorithmus, der die Lernrate adaptiv anpasst."
    ],
    "loesung": 0,
    "erklaerung": "Durch das zufällige Deaktivieren von Neuronen in jedem Trainingsschritt zwingt `Dropout` das Netzwerk, robustere und weniger voneinander abhängige Features zu lernen, was Overfitting entgegenwirkt.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Ensembling-Effekt durch Zufallsausfälle",
      "schritte": [
        "Während des Trainings werden Knoten mit einer Rate p deaktiviert.",
        "Das verhindert Ko-Adaptationen und wirkt wie ein Ensemble vieler Teilnetze.",
        "Inference nutzt alle Neuronen, ggf. mit Skalierung der Gewichte."
      ]
    },
    "mini_glossary": {
      "**Ko-Adaptation**": "Ungewollte wechselseitige Abhängigkeit von Neuronen.",
      "**Dropout-Rate p**": "Wahrscheinlichkeit, mit der ein Neuron deaktiviert wird."
    }
  },
  {
    "frage": "77. Welchen Zweck erfüllt eine `Loss Function` (Verlustfunktion)?",
    "optionen": [
      "Sie quantifiziert den Fehler zwischen der Modellvorhersage und dem wahren Zielwert.",
      "Sie berechnet die optimale Anzahl der Neuronen für die gegebene Aufgabe.",
      "Sie legt die Lernrate für den Optimierungsalgorithmus fest.",
      "Sie bestimmt die maximale Anzahl der Trainingsepochen."
    ],
    "loesung": 0,
    "erklaerung": "Die Verlustfunktion ist das Signal, das der Optimierungsalgorithmus (z.B. SGD) zu minimieren versucht. Ein kleinerer Verlustwert bedeutet eine bessere Anpassung des Modells an die Trainingsdaten.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Verlust als Trainingslenker",
      "schritte": [
        "Loss misst Abweichung (z. B. Kreuzentropie, MSE) zwischen Vorhersage und Ziel.",
        "Gradienten des Loss treiben die Gewichtsaktualisierung.",
        "Passende Loss-Wahl hängt von Aufgabe und Outputverteilung ab."
      ]
    },
    "mini_glossary": {
      "**Kreuzentropie**": "Loss für Klassifikation mit probabilistischen Ausgaben.",
      "**MSE**": "Mean Squared Error, Standard-Loss für Regression."
    }
  },
  {
    "frage": "78. Was ist ein Hauptvorteil von `Batch Normalization`?",
    "optionen": [
      "Sie beschleunigt und stabilisiert den Trainingsprozess.",
      "Sie erhöht die Anzahl der lernbaren Parameter im Modell.",
      "Sie ersetzt die Notwendigkeit von Aktivierungsfunktionen.",
      "Sie funktioniert nur in der ersten Schicht eines Netzwerks."
    ],
    "loesung": 0,
    "erklaerung": "`Batch Normalization` normalisiert die Aktivierungen zwischen den Schichten. Dies wirkt dem Problem des 'Internal Covariate Shift' entgegen, erlaubt höhere Lernraten und macht das Training insgesamt stabiler.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Stabilere Verteilungen, schnellere Konvergenz",
      "schritte": [
        "BN zentriert und skaliert Zwischenausgaben je Mini-Batch.",
        "Konstante Aktivierungsverteilungen erlauben größere Lernraten.",
        "Zusätzliche Scale/Shift-Parameter bewahren Modellflexibilität."
      ]
    },
    "mini_glossary": {
      "**Internal Covariate Shift**": "Schwankende Aktivierungsverteilungen zwischen Schichten.",
      "**Mini-Batch**": "Teilmenge der Trainingsdaten pro Update-Schritt."
    }
  },
  {
    "frage": "79. Was ist ein potenzielles Problem bei sehr tiefen neuronalen Netzen?",
    "optionen": [
      "Das Auftreten von 'Vanishing' oder 'Exploding Gradients'.",
      "Sie sind prinzipiell schneller zu trainieren als flache Netze.",
      "Sie können keine nichtlinearen Aktivierungsfunktionen verwenden.",
      "Sie sind von Natur aus robuster gegen Overfitting."
    ],
    "loesung": 0,
    "erklaerung": "Bei der `Backpropagation` in sehr tiefen Netzen kann der Gradient, der rückwärts propagiert wird, exponentiell klein ('vanishing') oder groß ('exploding') werden, was das Lernen verhindert oder destabilisiert.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Warum Tiefe Training erschweren kann",
      "schritte": [
        "Wiederholte Kettenregel kann Gradienten abschwächen oder verstärken.",
        "Resultat sind Stillstand (vanishing) oder instabile Updates (exploding).",
        "Gegenmittel: geeignete Initialisierung, ReLUs, Residual-Verbindungen, BN."
      ]
    },
    "mini_glossary": {
      "**Exploding Gradient**": "Zu große Gradienten verursachen instabile Gewichtsupdates.",
      "**Residual-Netz**": "Netz mit Skip-Connections zur Stabilisierung tiefer Modelle."
    }
  },
  {
    "frage": "80. Für welche Art von Aufgaben sind `Convolutional Neural Networks (CNNs)` besonders gut geeignet?",
    "optionen": [
      "Aufgaben mit gitterartigen Daten wie Bildklassifikation und Objekterkennung.",
      "Die Verarbeitung von sequenziellen Daten wie Zeitreihen oder Text.",
      "Das Finden von Clustern in ungelabelten Datensätzen.",
      "Probleme des Reinforcement Learning in Spielumgebungen."
    ],
    "loesung": 0,
    "erklaerung": "`CNNs` sind darauf spezialisiert, lokale räumliche Muster in Daten wie Bildern durch Faltungsoperationen (Convolutions) zu erkennen, was sie für Computer-Vision-Aufgaben prädestiniert.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Gitterdaten und lokale Muster",
      "schritte": [
        "Faltungen extrahieren Kanten/Texturen unabhängig von Position.",
        "Pooling reduziert Dimensionen und erhöht Robustheit gegen Verschiebungen.",
        "Tiefere Stufen kombinieren lokale Muster zu Objekten."
      ]
    },
    "mini_glossary": {
      "**Faltung (Convolution)**": "Lineare Operation mit lokalem Filter auf Nachbarschaften.",
      "**Pooling**": "Downsampling-Operation (z. B. Max-Pooling) zur Verdichtung."
    }
  },
  {
    "frage": "81. Was ist ein Vorteil von `Stochastic Gradient Descent (SGD)`?",
    "optionen": [
      "Es ist speichereffizient und ermöglicht das Training mit sehr großen Datensätzen.",
      "Es konvergiert garantiert immer zum globalen Minimum der Verlustfunktion.",
      "Es benötigt keine manuelle Einstellung der Lernrate.",
      "Es führt im Vergleich zu anderen Methoden zu einer schnelleren Konvergenz."
    ],
    "loesung": 0,
    "erklaerung": "Da `SGD` die Gewichte nach jedem einzelnen Datenpunkt (oder einem kleinen Batch) aktualisiert, muss nicht der gesamte Datensatz im Speicher gehalten werden. Die 'rauschhaften' Updates können auch helfen, lokalen Minima zu entkommen.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Große Daten, kleine Batches",
      "schritte": [
        "SGD berechnet Gradienten auf Mini-Batches statt auf dem gesamten Datensatz.",
        "Das spart Speicher und erhöht Update-Frequenz (schnellere Iterationen).",
        "Stochastisches Rauschen kann aus flachen lokalen Minima herausführen."
      ]
    },
    "mini_glossary": {
      "**Lernrate (η)**": "Schrittweite der Gewichtsaktualisierung.",
      "**Lokales Minimum**": "Punkt, der nur gegenüber Nachbarschaft minimal ist."
    }
  },
  {
    "frage": "82. Welche Funktion hat ein `Hidden Layer` in einem neuronalen Netz?",
    "optionen": [
      "Es lernt hierarchische und zunehmend komplexe Merkmale aus den Eingabedaten.",
      "Es dient ausschließlich dazu, die finale Vorhersage des Netzes auszugeben.",
      "Es initialisiert die Gewichte für die Eingabeschicht.",
      "Es normalisiert die Ausgabewerte auf einen Bereich zwischen 0 und 1."
    ],
    "loesung": 0,
    "erklaerung": "Versteckte Schichten (Hidden Layers) sind die Kernkomponenten, in denen das Netzwerk lernt, aus den Rohdaten der vorherigen Schicht abstraktere und nützlichere Repräsentationen zu extrahieren.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Feature-Hierarchien aufbauen",
      "schritte": [
        "Frühere Layer lernen einfache Muster (z. B. Kanten), spätere kombinieren sie.",
        "Nichtlinearitäten ermöglichen komplexe Repräsentationen.",
        "Tiefe bestimmt die Abstraktionstiefe der gelernten Merkmale."
      ]
    },
    "mini_glossary": {
      "**Repräsentation**": "Interne Merkmalsdarstellung, auf der die Vorhersage beruht.",
      "**Abstraktionsstufe**": "Grad der Entfernung vom Rohsignal (niedrig → hoch)."
    }
  },
  {
    "frage": "83. Was ist ein Nachteil der `Sigmoid`-Aktivierungsfunktion in tiefen Netzen?",
    "optionen": [
      "Ihre Ableitung ist oft nahe null, was zum 'Vanishing Gradient'-Problem führt.",
      "Sie ist nicht stetig differenzierbar und kann nicht für `Backpropagation` verwendet werden.",
      "Sie kann keine nichtlinearen Zusammenhänge im Netzwerk abbilden.",
      "Sie ist ausschließlich für Regressionsprobleme mit positiven Zielwerten geeignet."
    ],
    "loesung": 0,
    "erklaerung": "Die `Sigmoid`-Funktion 'sättigt' bei großen positiven oder negativen Eingaben, was bedeutet, dass ihre Ableitung nahe null wird. Dies verlangsamt oder stoppt den Lernprozess in den unteren Schichten tiefer Netzwerke.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Sättigung bremst Gradienten",
      "schritte": [
        "In den Sättigungsbereichen flacht Sigmoid ab; Ableitung ≈ 0.",
        "Kettenregel multipliziert kleine Ableitungen → Gradient verschwindet.",
        "ReLU/Tanh oder BN entschärfen das Problem in der Praxis."
      ]
    },
    "mini_glossary": {
      "**Sättigung**": "Bereich, in dem Funktionswerte kaum auf Eingaben reagieren.",
      "**Gradientenfluss**": "Transport der Ableitungen durch die Schichten."
    }
  },
  {
    "frage": "84. Was ist ein Vorteil des `Adam`-Optimierers gegenüber einfachem `SGD`?",
    "optionen": [
      "Er passt die Lernrate für jeden Parameter individuell und adaptiv an.",
      "Er benötigt keine `Backpropagation` zur Berechnung der Gradienten.",
      "Er ist speziell für sehr kleine Netzwerke mit wenigen Parametern optimiert.",
      "Er verwendet keine Gradienten, sondern einen genetischen Algorithmus."
    ],
    "loesung": 0,
    "erklaerung": "`Adam` (Adaptive Moment Estimation) kombiniert die Ideen von Momentum und RMSprop. Er pflegt eine adaptive Lernrate für jedes Gewicht, was oft zu einer schnelleren und stabileren Konvergenz führt als Standard-SGD.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Adaptive Schritte pro Parameter",
      "schritte": [
        "Adam schätzt Mittelwert und Varianz der Gradienten (Momente).",
        "Parameter mit hohem Rauschen erhalten kleinere, stabile Updates.",
        "Ergebnis: oft schnellere, robustere Konvergenz als Plain-SGD."
      ]
    },
    "mini_glossary": {
      "**Momentum**": "Glättung der Updates über vergangene Gradienten.",
      "**RMSprop**": "Skalierung der Lernrate anhand der Gradientenvarianz."
    }
  },
  {
    "frage": "85. Warum wird die Technik des `Early Stopping` beim Training eingesetzt?",
    "optionen": [
      "Um Overfitting zu vermeiden, indem das Training beendet wird, wenn sich der Validierungsfehler nicht mehr verbessert.",
      "Um die anfängliche Lernrate dynamisch während der ersten Epochen zu erhöhen.",
      "Um die Gewichte des Netzwerks auf einen bekannten, guten Zustand zurückzusetzen.",
      "Um das Training zu stoppen, sobald eine Genauigkeit von 100% auf den Trainingsdaten erreicht ist."
    ],
    "loesung": 0,
    "erklaerung": "`Early Stopping` ist eine Form der Regularisierung, bei der die Leistung des Modells auf einem separaten Validierungsdatensatz überwacht wird. Das Training wird abgebrochen, sobald diese Leistung stagniert oder schlechter wird, um Overfitting zu verhindern.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Wenn Val-Metrik stagniert: stoppen",
      "schritte": [
        "Überwachen einer Validierungsmetrik pro Epoche mit Geduld (Patience).",
        "Bei ausbleibender Verbesserung wird das beste Modell gesichert.",
        "So wird unnötiges Überanpassen an Trainingsdaten verhindert."
      ]
    },
    "mini_glossary": {
      "**Patience**": "Anzahl tolerierter schlechterer Epochen vor Abbruch.",
      "**Checkpointing**": "Speichern des jeweils besten Modellzustands."
    }
  },
  {
    "frage": "86. Welche Rolle spielt die `Softmax`-Funktion typischerweise in einem neuronalen Netz?",
    "optionen": [
      "Sie wandelt die Logits der Ausgabeschicht in eine Wahrscheinlichkeitsverteilung über die Klassen um.",
      "Sie dient als Regularisierungstechnik, um Overfitting in den Hidden Layers zu reduzieren.",
      "Sie ersetzt die Verlustfunktion bei Regressionsproblemen.",
      "Sie wird als Aktivierungsfunktion in den Hidden Layers von CNNs verwendet."
    ],
    "loesung": 0,
    "erklaerung": "Die `Softmax`-Funktion ist ideal für die Ausgabeschicht bei Multi-Klassen-Klassifikationsproblemen, da sie sicherstellt, dass die Summe der Ausgaben 1 beträgt und jeder Wert als die Wahrscheinlichkeit für die jeweilige Klasse interpretiert werden kann.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Von Logits zu Wahrscheinlichkeiten",
      "schritte": [
        "Softmax exponentiert Logits und normalisiert sie zur Verteilung.",
        "So sind Ausgaben vergleichbar und als Klassenwahrscheinlichkeiten interpretierbar.",
        "Kombiniert mit Kreuzentropie bildet sie ein konsistentes Lernziel."
      ]
    },
    "mini_glossary": {
      "**Logit**": "Unskalierter Score einer Klasse vor Softmax.",
      "**Kreuzentropie**": "Loss, der Softmax-Wahrscheinlichkeiten gegen One-Hot-Labels optimiert."
    }
  },
  {
    "frage": "87. Was ist ein potenzieller Nachteil einer zu großen Lernrate?",
    "optionen": [
      "Das Training kann instabil werden, da das Optimum 'übersprungen' wird.",
      "Das Training konvergiert extrem langsam gegen ein lokales Minimum.",
      "Die Gewichte des Netzwerks werden während des Trainings nicht aktualisiert.",
      "Die Aktivierungsfunktionen in den Hidden Layers werden deaktiviert."
    ],
    "loesung": 0,
    "erklaerung": "Eine zu große Lernrate kann dazu führen, dass die Gewichtsaktualisierungen so groß sind, dass der Optimierungsprozess über das Minimum der Verlustfunktion hinwegschießt und der Fehler wieder ansteigt (Divergenz).",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Zu große Schritte → Zickzack oder Divergenz",
      "schritte": [
        "Große Updates können das Tal des Loss überfliegen statt hineinzusinken.",
        "Sichtbar als oszillierende oder wachsende Validierungsverluste.",
        "Abhilfe: Lernraten-Schedules oder adaptive Optimierer."
      ]
    },
    "mini_glossary": {
      "**Divergenz**": "Auseinanderlaufen der Optimierung ohne Annäherung an ein Minimum.",
      "**LR-Schedule**": "Zeitplan zur Anpassung der Lernrate über Epochen."
    }
  },
  {
    "frage": "88. Warum ist eine gute Gewichtsinitialisierung ('Weight Initialization') wichtig?",
    "optionen": [
      "Sie hilft, Probleme wie 'Vanishing/Exploding Gradients' zu vermeiden und beschleunigt die Konvergenz.",
      "Sie bestimmt die endgültige Anzahl der Hidden Layers im Netzwerk.",
      "Sie ersetzt die Notwendigkeit einer nichtlinearen Aktivierungsfunktion.",
      "Sie verhindert die Verwendung von Regularisierungstechniken wie `Dropout`."
    ],
    "loesung": 0,
    "erklaerung": "Eine schlechte Initialisierung (z.B. alle Gewichte auf null) kann den Lernprozess verhindern. Techniken wie Xavier/Glorot-Initialisierung sorgen für eine gute Varianz der Aktivierungen und einen stabilen Gradientenfluss zu Beginn des Trainings.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Startwerte bestimmen Lernbahnen",
      "schritte": [
        "Geeignete Varianz der Gewichte hält Aktivierungen/Gradienten im Arbeitsbereich.",
        "Glorot/He-Init passen Varianz an Fan-In/Fan-Out und Aktivierung an.",
        "Bessere Initialwerte → schnellere, stabilere Konvergenz."
      ]
    },
    "mini_glossary": {
      "**Glorot/Xavier**": "Initialisierung mit Varianz ∝ 2/(fan_in+fan_out).",
      "**He-Initialisierung**": "Für ReLU-Netze; Varianz ∝ 2/fan_in."
    }
  },
  {
    "frage": "89. Für welche Art von Daten sind `Recurrent Neural Networks (RNNs)` besonders geeignet?",
    "optionen": [
      "Für Sequenzdaten, bei denen die Reihenfolge der Elemente von Bedeutung ist (z.B. Text, Zeitreihen).",
      "Für gitterartige Daten ohne zeitliche Komponente wie statische Bilder.",
      "Für tabellarische Daten mit unabhängigen Zeilen wie in einer CSV-Datei.",
      "Für das Finden von Clustern in ungelabelten Datensätzen."
    ],
    "loesung": 0,
    "erklaerung": "`RNNs` besitzen interne Schleifen, die es ihnen ermöglichen, einen 'Gedächtnis'-Zustand zu pflegen. Dies macht sie ideal für Aufgaben, bei denen der Kontext aus vorherigen Schritten in einer Sequenz wichtig ist.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Reihenfolge als Informationsträger",
      "schritte": [
        "RNNs verarbeiten Eingaben schrittweise und aktualisieren einen Zustandsvektor.",
        "So fließt Kontext aus der Vergangenheit in die aktuelle Entscheidung ein.",
        "LSTM/GRU mildern Langzeit-Abhängigkeitsprobleme."
      ]
    },
    "mini_glossary": {
      "**Zustand (State)**": "Vektor, der Sequenzkontext zusammenfasst.",
      "**LSTM/GRU**": "RNN-Varianten mit Toren für stabile Langzeitabhängigkeiten."
    }
  },
  {
    "frage": "90. Was ist ein entscheidender Vorteil von Deep Learning gegenüber klassischen ML-Algorithmen?",
    "optionen": [
      "Die Fähigkeit zum automatischen 'Feature Learning' direkt aus Rohdaten.",
      "Die Garantie, dass kein Overfitting auf den Trainingsdaten stattfindet.",
      "Die hohe Interpretierbarkeit der gelernten Modelle ('White-Box').",
      "Der geringere Bedarf an Trainingsdaten für komplexe Aufgaben."
    ],
    "loesung": 0,
    "erklaerung": "Während bei klassischen ML-Ansätzen oft aufwendiges, manuelles Feature Engineering nötig ist, können tiefe neuronale Netze eine Hierarchie von Merkmalen direkt aus den Rohdaten (z.B. Pixeln eines Bildes) lernen.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Merkmale aus Rohdaten extrahieren",
      "schritte": [
        "Tiefe Netze lernen End-to-End Repräsentationen ohne manuelle Features.",
        "Frühere Layer lernen generische Muster, spätere aufgabenspezifische.",
        "Ergebnis: starke Performance bei komplexen, hochdimensionalen Aufgaben."
      ]
    },
    "mini_glossary": {
      "**Feature Learning**": "Automatisches Lernen relevanter Merkmale aus Daten.",
      "**End-to-End**": "Direktes Mapping von Rohinput zu Ziel ohne handgefertigte Stufen."
    }
  },
  {
    "frage": "91. Was ist der Hauptvorteil von CNNs gegenüber Fully-Connected Networks bei der Bildverarbeitung?",
    "optionen": [
      "Sie sind unempfindlich gegenüber der Farbe der Pixel.",
      "Sie reduzieren die Anzahl der Parameter drastisch durch lokale Verbindungen und Gewichtsteilung.",
      "Sie können ausschließlich Graustufenbilder effizient verarbeiten.",
      "Sie konvergieren beim Training grundsätzlich schneller."
    ],
    "loesung": 1,
    "erklaerung": "Ein Fully-Connected Network für ein Bild hätte für jedes Pixel eine Verbindung zu jedem Neuron der nächsten Schicht, was zu Millionen von Parametern führt. CNNs nutzen kleine Filter, deren Gewichte über das gesamte Bild geteilt werden, was rechnerisch effizienter ist.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Lokale Rezeptive Felder + Weight Sharing",
      "schritte": [
        "Filter koppeln nur lokale Nachbarschaften, nicht jeden Pixel mit jedem Neuron.",
        "Gewichtsteilung nutzt denselben Filter über das ganze Bild.",
        "Ergebnis: deutlich weniger Parameter und bessere Generalisierung."
      ]
    },
    "mini_glossary": {
      "**Rezeptives Feld**": "Bereich des Inputs, den ein Neuron „sieht“. ",
      "**Weight Sharing**": "Gleiche Filtergewichte an allen Positionen anwenden."
    }
  },
  {
    "frage": "92. Welche Eigenschaft von CNNs ermöglicht die Erkennung von Objekten unabhängig von ihrer Position im Bild?",
    "optionen": [
      "`Dropout`",
      "`Translation Invariance`",
      "`Dense Layer`",
      "`Batch Normalization`"
    ],
    "loesung": 1,
    "erklaerung": "Durch die Anwendung desselben Filters (Gewichtsteilung) über das gesamte Bild und die anschließende Abstraktion durch Pooling-Layer lernt ein CNN, ein Merkmal (z.B. ein Auge) zu erkennen, egal ob es links oben oder rechts unten im Bild erscheint.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Positionsunabhängigkeit durch Faltung und Pooling",
      "schritte": [
        "Geteilte Filter detektieren identische Muster an verschiedenen Stellen.",
        "Pooling reduziert Sensitivität für kleine Verschiebungen.",
        "Höhere Schichten kombinieren Muster zu positionsrobusten Objektmerkmalen."
      ]
    },
    "mini_glossary": {
      "**Translationsinvarianz**": "Merkmale bleiben bei Verschiebungen erkennbar.",
      "**Pooling**": "Verdichtung, die Lagevariationen glättet."
    }
  },
  {
    "frage": "93. Was ist ein `Filter` (oder Kernel) in einem Convolutional Layer?",
    "optionen": [
      "Eine Methode zur Umwandlung eines Farbbildes in ein Graustufenbild.",
      "Eine kleine Matrix von Gewichten, die über das Eingabebild gefaltet wird, um Merkmale zu extrahieren.",
      "Ein Algorithmus zur Kompression der Bilddaten vor der Verarbeitung.",
      "Ein Verfahren zur künstlichen Vergrößerung des Trainingsdatensatzes (Data Augmentation)."
    ],
    "loesung": 1,
    "erklaerung": "Ein Filter ist der zentrale Baustein eines Convolutional Layers. Er fungiert als Merkmalsdetektor (z.B. für Kanten, Ecken, Texturen), dessen Gewichte während des Trainings gelernt werden.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Kleine Gewichte, große Wirkung",
      "schritte": [
        "Der Kernel gleitet über das Bild und bildet lokale Skalarprodukte.",
        "Hohe Aktivierung signalisiert starkes Vorhandensein des gesuchten Merkmals.",
        "Training passt die Kerngewichte an, um relevante Muster zu verstärken."
      ]
    },
    "mini_glossary": {
      "**Kernelgröße**": "Breite × Höhe des Filters (z. B. 3×3).",
      "**Stride/Padding**": "Schrittweite des Kernels bzw. Randauffüllung."
    }
  },
  {
    "frage": "94. Welche Funktion hat ein `Mean-Filter` in der klassischen Bildverarbeitung?",
    "optionen": [
      "Er schärft die Kanten in einem Bild durch die Betonung von Gradienten.",
      "Er glättet das Bild und reduziert Rauschen, indem er Pixel durch den Mittelwert ihrer Nachbarschaft ersetzt.",
      "Er erhöht die Farbsättigung, um das Bild lebendiger erscheinen zu lassen.",
      "Er erkennt und markiert die hellsten und dunkelsten Bereiche im Bild."
    ],
    "loesung": 1,
    "erklaerung": "Der `Mean-Filter` ist ein einfacher Weichzeichner (Blurring-Filter). Er wird oft zur Rauschunterdrückung eingesetzt, führt aber auch zu einem Verlust von Bilddetails.",
    "gewichtung": 1,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Rauschen reduzieren, Details opfern",
      "schritte": [
        "Jeder Pixel wird durch den Durchschnitt seiner Nachbarschaft ersetzt.",
        "Zufällige Intensitätsschwankungen werden geglättet.",
        "Feine Kanten/Strukturen werden dabei mit verwischt."
      ]
    },
    "mini_glossary": {
      "**Blurring**": "Weichzeichnung durch Mittelung oder Faltung.",
      "**Rauschen**": "Zufällige, unerwünschte Intensitätsschwankungen im Bild."
    }
  },
  {
    "frage": "95. Welche Filter werden typischerweise zur Kantendetektion eingesetzt?",
    "optionen": [
      "Der Median-Filter zur Rauschunterdrückung.",
      "Der Prewitt- oder Sobel-Filter zur Berechnung von Gradienten.",
      "Der Gauß-Filter zur Weichzeichnung des Bildes.",
      "Der Box-Filter als einfache Form des Mittelwertfilters."
    ],
    "loesung": 1,
    "erklaerung": "Filter wie `Sobel` und `Prewitt` sind darauf ausgelegt, die Ableitung (den Gradienten) der Bildintensität zu approximieren. Starke Gradienten deuten auf Kanten hin und werden durch diese Filter hervorgehoben.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Kanten = starke Gradienten",
      "schritte": [
        "Sobel/Prewitt approximieren partielle Ableitungen entlang x/y.",
        "Hohe Gradientmagnituden markieren Übergänge (Kanten).",
        "Kombination beider Richtungen liefert Kantenkarten."
      ]
    },
    "mini_glossary": {
      "**Gradient**": "Richtungsableitung der Intensität; Stärke und Richtung der Veränderung.",
      "**Kantenbild**": "Darstellung der Kantenstärke über das Bild."
    }
  },
  {
    "frage": "96. Was passiert, wenn ein Prewitt-Filter in x-Richtung auf ein Bild angewendet wird?",
    "optionen": [
      "Vertikale Kanten werden stark hervorgehoben.",
      "Horizontale Kanten werden stark hervorgehoben.",
      "Das Bild wird in der x-Richtung unscharf.",
      "Die Farben werden in ihr Komplementär invertiert."
    ],
    "loesung": 0,
    "erklaerung": "Ein Filter in x-Richtung misst die Intensitätsänderungen entlang der horizontalen Achse. Eine starke Änderung tritt bei einer vertikalen Kante auf, weshalb diese detektiert wird.",
    "gewichtung": 2,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Richtungsableitung entlang x",
      "schritte": [
        "Der x-Prewitt schätzt ∂I/∂x, also horizontale Intensitätsänderungen.",
        "Vertikale Kanten erzeugen hohe Antworten, horizontale geringe.",
        "Zusammen mit y-Prewitt ergibt sich die Gesamtgradientenkarte."
      ]
    },
    "mini_glossary": {
      "**∂I/∂x**": "Partielle Ableitung der Intensität in x-Richtung.",
      "**Richtungsfilter**": "Filter, der auf Veränderungen in einer bestimmten Achse reagiert."
    }
  },
  {
    "frage": "97. Warum ist `Weight Sharing` (Gewichtsteilung) in CNNs so wichtig?",
    "optionen": [
      "Es erhöht die Komplexität des Modells, um Overfitting zu vermeiden.",
      "Es reduziert die Anzahl der zu lernenden Parameter drastisch.",
      "Es stellt sicher, dass alle Gewichte im Netzwerk positiv bleiben.",
      "Es wird ausschließlich bei der Verarbeitung von Textdaten angewendet."
    ],
    "loesung": 1,
    "erklaerung": "Beim `Weight Sharing` wird derselbe Filter (derselbe Satz von Gewichten) an jeder Position des Bildes angewendet. Dadurch muss das Netzwerk nicht für jede Position separate Merkmalsdetektoren lernen, was die Anzahl der Parameter massiv reduziert.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Weniger Parameter, mehr Datenökonomie",
      "schritte": [
        "Ein Filter arbeitet überall gleich, statt positionsspezifische Gewichte zu lernen.",
        "Das senkt Parameterzahl und Overfitting-Risiko.",
        "Gleichzeitig entsteht Translationsinvarianz bei Mustererkennung."
      ]
    },
    "mini_glossary": {
      "**Parameterbudget**": "Gesamtanzahl trainierbarer Gewichte.",
      "**Overfitting-Risiko**": "Tendenz, Trainingsrauschen statt Muster zu lernen."
    }
  },
  {
    "frage": "98. Was ist eine `Feature Map` in einem CNN?",
    "optionen": [
      "Eine grafische Darstellung der wichtigsten Features im Trainingsdatensatz.",
      "Die Ausgabe eines Filters nach der Faltungsoperation, die die Aktivierung eines Merkmals anzeigt.",
      "Ein spezieller Datensatz, der ausschließlich zur Bewertung der Merkmalsextraktion verwendet wird.",
      "Ein Layer, der neue Features durch die Kombination bestehender Features erzeugt."
    ],
    "loesung": 1,
    "erklaerung": "Eine `Feature Map` ist das Ergebnis der Anwendung eines Filters auf eine Eingabe. Sie ist eine 2D-Karte, deren Werte anzeigen, wie stark das vom Filter gesuchte Merkmal an der jeweiligen Position der Eingabe vorhanden ist.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Aktivierungskarten verstehen",
      "schritte": [
        "Jeder Filter erzeugt eine Map mit hohen Werten an „Merkmals-Hotspots“.",
        "Stapelt man mehrere Filter, erhält man ein 3D-Aktivierungsvolumen.",
        "Nachfolgende Schichten nutzen diese Maps als Eingabe."
      ]
    },
    "mini_glossary": {
      "**Aktivierung**": "Ausgabe eines Neurons nach Filterung und Nichtlinearität.",
      "**Aktivierungsvolumen**": "Stack mehrerer Feature-Maps (Kanäle)."
    }
  },
  {
    "frage": "99. Aus welchen Layern besteht eine typische CNN-Architektur?",
    "optionen": [
      "`Convolutional`, `Pooling`, `Flatten` und `Dense` Layers.",
      "Ausschließlich aus `Dense` (Fully-Connected) Layers.",
      "Nur aus `Pooling`- und `Dropout`-Layern.",
      "Einer Abfolge von `RNN`- und `LSTM`-Layern."
    ],
    "loesung": 0,
    "erklaerung": "Eine klassische CNN-Architektur besteht aus einer Abfolge von `Convolutional`-Layern (zur Merkmalsextraktion) und `Pooling`-Layern (zur Dimensionsreduktion), gefolgt von einem `Flatten`-Layer und `Dense`-Layern für die finale Klassifikation.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Von Pixeln zu Klassen",
      "schritte": [
        "Conv/Pooling-Blöcke extrahieren und verdichten Merkmale.",
        "`Flatten` wandelt Feature-Maps in Vektoren um.",
        "`Dense`-Schichten klassifizieren auf Basis der extrahierten Repräsentation."
      ]
    },
    "mini_glossary": {
      "**Flatten**": "Operation zum Umformen von 3D-Tensors in 1D-Vektor.",
      "**Dense Layer**": "Vollständig verbundene Schicht zur Klassifikation/Regression."
    }
  },
  {
    "frage": "100. Was unterscheidet die Filter in CNNs von klassischen Bildverarbeitungsfiltern?",
    "optionen": [
      "Klassische Filter wie Sobel sind fest definiert, während die Filter in CNNs während des Trainings gelernt werden.",
      "CNN-Filter sind immer signifikant größer als klassische Filter, um globale Merkmale zu erfassen.",
      "Klassische Filter können keine Kanten oder Texturen im Bild erkennen.",
      "CNN-Filter benötigen keine nichtlineare Aktivierungsfunktion nach der Anwendung."
    ],
    "loesung": 0,
    "erklaerung": "Der entscheidende Unterschied ist, dass die Werte der Filtermatrizen in einem CNN nicht von einem Menschen entworfen, sondern durch `Backpropagation` gelernt werden. Das Netzwerk findet so selbst die optimalen Filter für die gegebene Aufgabe.",
    "gewichtung": 3,
    "thema": "Deep Learning",
    "extended_explanation": {
      "title": "Vom Hand-Design zum Lernen der Filter",
      "schritte": [
        "Klassische Filter haben feste Koeffizienten (z. B. Sobel).",
        "CNNs optimieren Filtergewichte datengesteuert für die Zielaufgabe.",
        "Dadurch passen sich Merkmalsdetektoren an Domäne und Daten an."
      ]
    },
    "mini_glossary": {
      "**Gelerntes Filter**": "Kernmatrix, deren Werte durch Training optimiert werden.",
      "**Sobel-Filter**": "Klassischer, fest definierter Gradientfilter zur Kantendetektion."
    }
  }
]