Im Kontext des Themas **Big Data** soll dir dieses Fragenset helfen, die folgenden Lernziele zu erreichen:

### Reproduktion

**Du kannst …**

1. Big Data anhand von Umfang, Vielfalt und skalierbarer Verarbeitung definieren.
2. Data Lake und Data Warehouse anhand von Schema-on-read und Schema-on-write unterscheiden.

### Anwendung

**Du kannst …**

1. eine geeignete Ablagestrategie für Logdaten im Object Storage mit Parquet auswählen.
2. Apache Kafka so partitionieren, dass Ordering pro Nutzer über den Key erhalten bleibt.
3. Apache Spark für iterative Workloads gegenüber MapReduce auswählen.
4. Join-Skew in verteilten Jobs durch Salting oder Broadcast Join reduzieren.
5. Time-Series-Daten in Cassandra mit geeignetem Partition Key und TTL modellieren.
6. Parquet als Spaltenformat zur Beschleunigung selektiver Analysen einsetzen.
7. Exactly-once-Verarbeitung in Streaming-Pipelines durch Offset-Management und Idempotenz umsetzen.
8. verspätete Events mit Event Time und Watermarks in Windowing-Logik berücksichtigen.
9. Pseudonymisierung und Anonymisierung im DSGVO-Kontext unterscheiden.
10. Data Quality durch Schema-Validierung und Constraints in einer Pipeline absichern.
11. Elasticsearch-Mappings für effiziente Aggregationen modellieren.
12. Erasure Coding zur kosteneffizienten Erhöhung der Haltbarkeit gegenüber Replikation auswählen.

### Strukturelle Analyse

**Du kannst …**

1. Kappa- und Lambda-Architektur hinsichtlich Komplexität und Datenpfaden vergleichen.
2. CAP-Tradeoffs unter Partitionen zwischen Konsistenz und Verfügbarkeit begründen.
3. Lakehouse-Ansätze mit ACID und Metadatenkatalog hinsichtlich Governance und Analysen einordnen.
4. Data Drift und Concept Drift als Ursachen für Performanceabfall in ML-Systemen analysieren.
5. Hash-Sharding und Range-Sharding hinsichtlich Hotspots und Lastverteilung abwägen.
6. Differential Privacy über Rauschen und Privacy Budget hinsichtlich Utility und Schutzwirkung einordnen.
