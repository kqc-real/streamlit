{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n","!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/images/images.zip\" && unzip -q images.zip"]},{"cell_type":"markdown","metadata":{},"source":["# Abschauen ist erwünscht!"]},{"cell_type":"markdown","metadata":{},"source":["## Autoencoder für semantische Segmentierung\n","\n","### Semantische Segmentierung\n","\n","Auf CNN basierende Modelle wurden in großer Vielfalt aufgebaut, um verschiedene Aufgaben zu lösen. Allgemein lassen sich die Herausforderungen der Klassifizierung, der semantischen Segmentierung, Objekterkennung und Instanzensegmentierung unter komplexeren neueren wie Keypoint Detection oder DensePose etc benennen.\n","Die Zuweisung einer Objektklasse, die in einem Bild als Ganzes eine Objektklasse zuzuordnen ist, wird als Klassifizierung bezeichnet. Während bei der semantischen Segmentierung alle Pixel durch die Objektklassen, auf die sie sich beziehen, identifiziert werden müssen. Im Gegensatz zur Klassifizierung können mehrere Objektklassen in einem Bild vorkommen.\n","\n","\n","\n","### Segnet - Ein Autoencoder für semantische Segmentierung\n","\n","Basierend auf [Kitti Road dataset](http://www.cvlibs.net/datasets/kitti/eval_road.php). Ein Segmentierungsdatensatz für autonomes Fahren, der vom __Karlsruher Institut für Technologie (KIT)__, dem MPI Tübingen und der University of Toronto erstellt wurde.\n","\n","\n","![CNN Autoencoder](images/segnet.png \"CNN Autoencoder\")\n","\n","\n","                                    Quelle: http://mi.eng.cam.ac.uk/projects/segnet/\n","\n","Es ist möglich, diese Klassifizierungsaufgabe zu lösen, indem man am Ende eine Softmax-Schicht verwendet oder ein gegebenes RGB-Bild regressiert. Im letzteren Fall sind die RGB-Werte möglicherweise nicht genau gleich und es gibt eine intrinsische Ordnung in den Klassen. Auch wenn anschließend ein Schwellenwert verwendet wird, ist der Dimensionsraum allerdings viel kleiner. Im Allgemeinen sollte man sich für den ersten Ansatz entscheiden, da er das Problem als reguläre Klassifikationsaufgabe löst und gängige Praxis ist. Es ist nicht empfehlenswert, dies in einer Regression anzuwenden. Der zweite Ansatz dient nur dazu, alternative Wege zu zeigen, wie man ein Problem angehen kann (und zum Spaß).\n","\n","Die Netzwerkarchitektur eines Autoencoders verwendet eine Struktur, die oft vorher auf einigen Daten wie [ImageNet](http://www.image-net.org/) trainiert wurde. Die Idee ist, dass diese Gewichte bereits etwas mit der späteren Aufgabe gemeinsam haben, so dass das Training schneller und möglicherweise besser konvergiert, als wenn man mit zufälligen Gewichten anfängt. In der obigen SegNet-Architektur wird die Standard-Klassifikationsnetzarchitektur `VGG-16` verwendet, um das Inputbild in einen höheren abstrakten Raum zu kodieren. Anschließend projizieren Upsampling und Faltungen die extrahierten Features zurück in den ursprünglichen Inputraum."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.1:</b> In welcher Stadt wurden die Bilder dieses Datensatzes erstellt?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Kurze Wierderholung\n","\n","Bevor wir beginnen, lassen Sie uns eine kleine Rekapitulation anhand der Bilder aus dem Video vornehmen, beginnend mit dem folgenden Bild:\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","img = Image.open('images/original.png')\n","\n","plt.figure(figsize=(18, 72))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Als erstes wurde die Objekterkennung eingeführt. Ein Objektdetektor versucht, verschiedene, vordefinierte Objekte im Bild zu lokalisieren und zu klassifizieren. Dabei werden Bounding Boxes verwendet. Sie geben die Position des erkannten Objekts im Bild an. Zusätzlich wurde der Bounding Box ein Klassenlabel zugeordnet. "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["img = Image.open('images/bounding_boxes.png')\n","\n","plt.figure(figsize=(18, 72))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Des Weiteren wurde die Sematische Segmentierung eingeführt. Hierbei wird, im Vergleich zur Objektdetektion, das Bild eingefärbt statt Bounding Boxes zu berechnen. Das eingefärbte Bild wird im Folgenden auch als Maske bezeichnet. Ein weiterer Unterschied ist, dass sich bei der Objektdetektion Bounding Boxen überschneiden können - dies ist im Falle der Semantischen Segmentierung nicht möglich. Hierbei wird ein Pixel eindeutig zugeordnet."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["img = Image.open('images/semantic_segmentation.png')\n","\n","plt.figure(figsize=(18, 72))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Zuletzt wurde die Instanz Segmentierung, als Erweiterung der Semantischen Segmentierung, vorgestellt. Diese kann zwischen verschiedenen Objekten der gleichen Klasse in einem Bild unterscheiden."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["img = Image.open('images/instance_segmentation.png')\n","\n","plt.figure(figsize=(18, 72))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Die Daten\n","\n","Zuerst werden die Daten geladen. Der Datensatz befindet sich im Ordner data und muss aus dem Zip-Archiv entpackt werden. Bitte entpacken Sie den Datensatz im data-Ordner. Praktischerweise sind die Daten bereits in Training und Validierung aufgeteilt.\n","Die Variable `class_or_regr` dient zur Steuerung, ob eine Regression oder Klassifikation durchgeführt wird.\n","Im Falle, dass die Variable 1 ist, erfolgt eine Klassifikation; falls diese 0 ist, wird eine Regression durchgeführt."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import utils"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# As regression or classification?\n","# 0-regre, 1-classification\n","\n","# Set class_or_regr 1 first.\n","class_or_regr = 1\n","root = 'data/dataset/'\n","\n","path_train_img = root+'training/image_2'\n","path_train_gt_img = root+'training/semantic_rgb'\n","\n","path_test_img = root+'testing/image_2'\n","\n","x_train_semseg = np.load(root+'x_train.npy')\n","y_train_semseg = np.load(root+'y_train.npy')\n","\n","x_val_semseg = np.load(root+'x_val.npy')\n","y_val_semseg = np.load(root+'y_val.npy')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","plt.subplots(figsize=(15, 15))\n","num_columns = 2\n","num_rows = 1\n","\n","for i in range(0,2):\n","    plt.subplot(num_rows, num_columns, i+1)\n","    if i == 0:\n","        plt.title('Input Image')\n","        plt.imshow(x_train_semseg[0,:,:,:])  # Visualizes the input data\n","    else:\n","        plt.title('Ground Truth')\n","        plt.imshow(y_train_semseg[0,:,:,:])  # Visualizes the ground truth\n","    plt.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["### Die Labels"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","# Load shortened form of labels with referring rgb values\n","rgb_array = np.load(root+'rgb_array.npy')\n","\n","# Create bitmaps ... this will take some time\n","if class_or_regr == 1:\n","    y_train_bitmap = utils.transform_into_bitmap(y_train_semseg, rgb_array.tolist())\n","    y_val_bitmap = utils.transform_into_bitmap(y_val_semseg, rgb_array.tolist())"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Visualize a bitmap of one class out of 29\n","if class_or_regr == 1:\n","    plt.title('Bitmap of one class')\n","    plt.imshow(y_train_bitmap[0,:,:,16])  # Visualize a bitmap of your desire\n","    plt.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.2:</b> Wie viele Datenpunkte gibt es für Training und Validierung?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.3:</b> Erläutern Sie die Dimensionen der Bitmaps! (z. B.: y_train_bitmap[?,?,?,?]) \n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Data Augmentation mit numpy\n","\n","Zuvor haben wir gelernt, dass wir die Anzahl unserer Trainingsdaten durch Datenvergrößerung erhöhen können.\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 5.4.4:</b> Im Folgenden werden die Bilder von uns selbst erweitert. Verwenden Sie \"numpy\"-Funktionen zum Erweitern der Bilder, wie in den Kommentaren beschrieben.\n","</div>"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","plt.title('Orignal image')\n","plt.imshow(x_train_semseg[0,:,:,:])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Use a numpy function to flip the image horizontally\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Use a numpy function to rotate the image\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Use a numpy function to shift the image\n","\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","# Execute this block to augment the data\n","# You can define which augmentation methods you would like to include\n","# in default all three methods are applied to the images in the training set\n","\n","x_train_aug_semseg = utils.augment_images(x_train_semseg, h_flip=True, rotate180=True, shift_random=True)\n","\n","if class_or_regr == 1:\n","    #Use the function to augment the ground_truth_bitmaps in the training set\n","    y_train_aug_bitmap = utils.augment_images(y_train_bitmap, h_flip=True, rotate180=True, shift_random=True)\n","\n","elif class_or_regr == 0:\n","    # Use the function to augment the ground_truth_images in the training set\n","    y_train_aug_semseg = utils.augment_images(y_train_semseg, h_flip=True, rotate180=True, shift_random=True)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.5:</b> Erklären Sie in einigen Worten, warum wir eine Datenerweiterung durchführen wollen, insbesondere bei einem Datensatz wie dem Kitti.\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.6:</b> Wie viele Datenpunkte gibt es nun (unter Verwendung aller angegebenen Augmentierungsmethoden)?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.7:</b> Warum wird das Bild um 180 Grad gedreht und nicht in 90-Grad-Schritten?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","# Visualize all possible augmentations of one image\n","\n","plt.subplots(figsize=(15, 15))\n","num_columns = 2\n","num_rows = 4\n","nb_augments = int(x_train_aug_semseg.shape[0]/160)\n","\n","for i in range(0, nb_augments):\n","    \n","    plt.subplot(num_rows, num_columns, i+1)\n","    plt.imshow(x_train_aug_semseg[i*160,:,:,:])\n","    plt.axis('off')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","# Visualize all possible augmentations of reffering ground truth bitmap of one class\n","\n","plt.subplots(figsize=(15, 15))\n","num_columns = 2\n","num_rows = 4\n","\n","for i in range(0,nb_augments):\n","    \n","    plt.subplot(num_rows, num_columns, i+1)\n","    \n","    if class_or_regr == 0:\n","        plt.imshow(y_train_aug_semseg[i*160,:,:])\n","    elif class_or_regr == 1:\n","        plt.imshow(y_train_aug_bitmap[i*160,:,:,16])\n","    plt.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["### Daten normalisieren"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","x_train_aug_semseg.astype('float32')\n","x_val_semseg.astype('float32')\n","\n","x_train_aug_semseg = x_train_aug_semseg / 255\n","x_val_semseg = x_val_semseg / 255\n","\n","if class_or_regr == 0: \n","    # only divide in regression task, bitmaps are already between 0 and 1\n","    y_train_aug_semseg.astype('float32')\n","    y_val_semseg.astype('float32')\n","    y_train_aug_semseg = y_train_aug_semseg / 255\n","    y_val_semseg = y_val_semseg / 255"]},{"cell_type":"markdown","metadata":{},"source":["### Transfer Lernen mit dem VGG-16 Kodierer"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Import the VGG-16 model and name it VGG16\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#### DO NOT EDIT\n","vgg16_encoder = VGG16(weights='imagenet', include_top=False) # this might take some time to download\n","# vgg16_encoder.summary() "]},{"cell_type":"markdown","metadata":{},"source":["### Autoencoder"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Use this to definitely change the name before training in combination with the next cell\n","from ipywidgets import widgets\n","from IPython.display import display\n","ae_specification = widgets.Text()\n","old_spec = 'None'\n","\n","display(ae_specification)\n","\n","def printer(sender):\n","    print(ae_specification.value)\n","\n","ae_specification.on_submit(printer)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Check if the name changed.\n","print(\"The current training specification is referred to as\", ae_specification.value)\n","if old_spec == ae_specification.value:\n","    print(\"There were no changes made to the previous training name!\")\n","\n","\n","# Callbacks for tensorboard and save weights for the best performing period.\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n","Acc_Logger = utils.LossGraph('acc')\n","tensorboard = TensorBoard(log_dir='logs/autoencoder_logs/'+ae_specification.value+'/')\n","Checkpoint = ModelCheckpoint('logs/autoencoder_logs/'+ae_specification.value+'/weights.hdf5'\n","                             , monitor='val_loss', save_best_only=True, save_weights_only=True, mode='auto',\n","                            save_freq = 1)\n","\n","# Build the Autoencoder\n","autoencoder = utils.build_ae(vgg16_encoder, x_train_semseg.shape[1:], class_or_regr)\n","\n","# Compile the models depending on the task\n","if class_or_regr == 0:\n","    \n","    autoencoder.compile(loss='mean_squared_error', metrics = ['accuracy'], optimizer='Adam')\n","    \n","    x_train_ae = x_train_aug_semseg\n","    y_train_ae = y_train_aug_semseg\n","    \n","    x_val_ae = x_val_semseg\n","    y_val_ae = y_val_semseg\n","    \n","    autoencoder.fit(x_train_ae, y_train_ae, batch_size = 4,#4\n","                epochs=1, validation_data=(x_val_ae, y_val_ae),\n","                callbacks=[Loss_Logger,tensorboard, Checkpoint], verbose=1)\n","    \n","elif class_or_regr == 1:\n","    \n","    autoencoder.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='Adam')\n","    \n","    x_train_ae = x_train_aug_semseg\n","    y_train_ae = y_train_aug_bitmap\n","    \n","    x_val_ae = x_val_semseg\n","    y_val_ae = y_val_bitmap\n","    \n","    autoencoder.fit(x_train_ae, y_train_ae, batch_size = 4,#4\n","                epochs=1, validation_data=(x_val_ae, y_val_ae),\n","                callbacks=[Acc_Logger,tensorboard, Checkpoint], verbose=1)\n","\n","# If training was successfull, do not use the same name again\n","old_spec = ae_specification.value"]},{"cell_type":"markdown","metadata":{},"source":["### Mit Ihrem Autoencoder vorhersagen\n","\n","Es ist möglich, bereits vortrainierte Gewichte zu laden, um einige Vorhersagen zu erhalten.\n","Verwenden Sie dazu: \n","autoencoder.load_weights(path_to_weights)\n","\n","Mögliche Gewichte:\n","- Der beste MSE trainiert 200 Epochen (/logs/autoencoder_logs/Regression200/weights.hdf5)\n","- Die beste Klassifikation trainiert 200 Epochen (/logs/autoencoder_logs/Classifier200/weights.hdf5)\n","\n","Schauen Sie sich auch Ihre Tensorboard-Ergebnisse an.\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-info\">\n","<b>Hinweis:</b> Um die Regressionsergebnisse zu betrachten, ändern Sie <code>class_or_regr</code> im Unterabschnitt der Daten auf 0. Führen Sie alle nachfolgenden Blöcke aus. Es könnte einfacher sein, das Training zu überspringen, wenn nur die Ergebnisse der bereits trainierten Modelle interessant sind.\n","\n","\n","\n","</div>"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Loop through training images, and visualize those between lower and upper bound\n","# Validation images are index from 160 up to 200\n","\n","# model.load_weights('path') #uncommend this if you want to use the pre-trained model weights, set the path by yourself\n","\n","lower_bound = 160\n","upper_bound = 165\n","\n","utils.ae_predict(autoencoder, path_train_img, path_train_gt_img, lower_bound, upper_bound,\n","                 ae_specification.value, class_or_regr)"]},{"cell_type":"markdown","metadata":{},"source":["### Abschließende Fragen:\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.8:</b> Erklären Sie die Unterschiede zwischen den Varianten Regression und Klassifikation.\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.4.9:</b> Was würden Sie vorschlagen, um Ihr Segmentierungsmodell zu verbessern?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Weitere Informationen\n","\n","[SegmentationForAutonomousDriving](https://blog.playment.io/semantic-segmentation-models-autonomous-vehicles/#U-Net)\n","\n","[Dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n","\n","[BatchNormalization](https://arxiv.org/pdf/1502.03167.pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python-amalea"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
