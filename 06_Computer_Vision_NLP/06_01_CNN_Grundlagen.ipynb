{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Setup: Installation und Importe\n",
    "# Führen Sie diese Zelle aus, um alle notwendigen Libraries zu installieren\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from scipy import signal\n",
    "import skimage\n",
    "from skimage import filters, feature\n",
    "\n",
    "# Streamlit (für interaktive Apps)\n",
    "import streamlit as st\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "# Konfiguration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"✅ Alle Libraries erfolgreich importiert!\")\n",
    "print(f\"📊 TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"🖼️ OpenCV Version: {cv2.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🚀 GPU verfügbar!\")\n",
    "else:\n",
    "    print(\"💻 CPU wird verwendet\")\n",
    "\n",
    "# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/images/images.zip\" && unzip -q images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👁️ 06.1 CNN Grundlagen - Convolutional Neural Networks verstehen\n",
    "\n",
    "**Data Analytics & Big Data - Woche 6.1**  \n",
    "*IU Internationale Hochschule*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Lernziele\n",
    "\n",
    "Nach diesem Notebook können Sie:\n",
    "- ✅ **CNN-Architektur** verstehen (Convolutional, Pooling, Dense Layers)\n",
    "- ✅ **Bildfilter** anwenden und deren Wirkung verstehen  \n",
    "- ✅ **Feature Maps** visualisieren und interpretieren\n",
    "- ✅ **Streamlit-App** für Computer Vision entwickeln\n",
    "- ✅ **TensorFlow/Keras** für CNN-Implementierung nutzen\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Was sind CNNs?\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** sind spezialisierte neuronale Netze für die **Bildverarbeitung**. \n",
    "\n",
    "### 🔑 Kernkonzepte:\n",
    "1. **Convolution (Faltung)** - Erkennt lokale Features wie Kanten\n",
    "2. **Pooling** - Reduziert Bildgröße, behält wichtige Information  \n",
    "3. **Feature Maps** - Zeigen welche Merkmale das Netz erkannt hat\n",
    "4. **Hierarchical Learning** - Von einfachen Kanten zu komplexen Objekten\n",
    "\n",
    "### 🏗️ CNN-Architektur:\n",
    "```\n",
    "Input Image → Conv2D → ReLU → MaxPool → Conv2D → ReLU → MaxPool → Flatten → Dense → Output\n",
    "```\n",
    "\n",
    "![CNN feedforward](images/stanford_cnn.jpg \"CNN_feedforward\")\n",
    "\n",
    "*Quelle: Stanford CS231n*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Warum CNNs für Computer Vision?\n",
    "\n",
    "### 🤔 Das Problem mit normalen Neural Networks\n",
    "\n",
    "**Normale Fully-Connected Networks** für Bilder haben massive Probleme:\n",
    "\n",
    "```python\n",
    "# Beispiel: 28x28 Pixel Bild (MNIST)\n",
    "input_size = 28 * 28 * 1  # = 784 Parameter pro Bild\n",
    "hidden_layer = 128        # = 784 * 128 = 100,352 Gewichte nur für erste Schicht!\n",
    "\n",
    "# Bei 224x224 RGB Bildern (ImageNet):\n",
    "input_size = 224 * 224 * 3  # = 150,528 Parameter\n",
    "# → Millionen von Gewichten schon in der ersten Schicht!\n",
    "```\n",
    "\n",
    "### 💡 Die CNN-Lösung\n",
    "\n",
    "**CNNs nutzen 3 Schlüsselkonzepte:**\n",
    "\n",
    "#### 1. 🔍 **Local Connectivity (Lokale Verbindungen)**\n",
    "- Jedes Neuron schaut nur auf einen kleinen Bildbereich (z.B. 3x3 Pixel)\n",
    "- Reduziert Parameter drastisch\n",
    "\n",
    "#### 2. 🔄 **Weight Sharing (Gewichtsteilen)**  \n",
    "- Derselbe Filter wird über das gesamte Bild angewendet\n",
    "- Ein 3x3 Filter hat nur 9 Parameter (statt Millionen!)\n",
    "\n",
    "#### 3. 📐 **Translation Invariance (Positionsunabhängigkeit)**\n",
    "- Erkennt Objekte unabhängig von ihrer Position im Bild\n",
    "- Katze oben links = Katze unten rechts\n",
    "\n",
    "### 🏗️ CNN-Architektur im Detail\n",
    "\n",
    "```\n",
    "Input (28x28x1) \n",
    "    ↓ Conv2D(32 filters, 3x3) + ReLU\n",
    "Feature Maps (26x26x32)\n",
    "    ↓ MaxPooling(2x2) \n",
    "Reduced Maps (13x13x32)\n",
    "    ↓ Conv2D(64 filters, 3x3) + ReLU\n",
    "Feature Maps (11x11x64)\n",
    "    ↓ MaxPooling(2x2)\n",
    "Final Maps (5x5x64)\n",
    "    ↓ Flatten\n",
    "Vector (1600)\n",
    "    ↓ Dense(128) + ReLU\n",
    "    ↓ Dense(10) + Softmax\n",
    "Output (10 classes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Bildfilter verstehen - Von klassisch zu CNN\n",
    "\n",
    "### 🎯 Lernziel\n",
    "Bevor wir CNNs trainieren, verstehen wir **wie Filter funktionieren** durch klassische Computer Vision Filter.\n",
    "\n",
    "### 📊 Filter-Konzept\n",
    "Ein **Filter (Kernel)** ist eine kleine Matrix (z.B. 3x3), die über ein Bild \"gefaltet\" wird:\n",
    "\n",
    "```python\n",
    "# Beispiel 3x3 Filter\n",
    "filter_3x3 = [\n",
    "    [a, b, c],\n",
    "    [d, e, f], \n",
    "    [g, h, i]\n",
    "]\n",
    "\n",
    "# Convolution Operation:\n",
    "output_pixel = (pixel_area * filter).sum()\n",
    "```\n",
    "\n",
    "### 🛠️ Filter-Typen die wir testen werden:\n",
    "\n",
    "1. **🌫️ Blur Filter (Mean)** - Bild unscharf machen\n",
    "2. **⚡ Edge Detection (Prewitt/Sobel)** - Kanten finden  \n",
    "3. **🔄 Sharpening (Laplace)** - Bild schärfen\n",
    "4. **🎨 Custom Filters** - Eigene Effekte\n",
    "\n",
    "**💡 Der Clou:** CNNs lernen diese Filter automatisch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 Spezielle Importe für Bildverarbeitung\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "from scipy import misc, signal\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Matplotlib Konfiguration für bessere Plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ Bildverarbeitungs-Libraries importiert!\")\n",
    "\n",
    "# Test: Verfügbare Beispielbilder\n",
    "try:\n",
    "    # Scipy Beispielbild\n",
    "    ascent = misc.ascent()\n",
    "    print(f\"📸 Beispielbild geladen: {ascent.shape}\")\n",
    "    \n",
    "    # OpenCV Test\n",
    "    print(f\"🔧 OpenCV Version: {cv2.__version__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Fehler beim Laden: {e}\")\n",
    "    print(\"💡 Tipp: Installieren Sie scipy und opencv-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Ascent image from scipy\n",
    "ascent = misc.ascent()\n",
    "\n",
    "# Moderne Visualisierung\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original Bild\n",
    "axes[0].imshow(ascent, cmap='gray', interpolation='nearest')\n",
    "axes[0].set_title('🏔️ Original Bild (Ascent)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Histogram\n",
    "axes[1].hist(ascent.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].set_title('📊 Pixel-Verteilung', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Pixel-Intensität (0-255)')\n",
    "axes[1].set_ylabel('Anzahl Pixel')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bildinformationen\n",
    "print(f\"📏 Bildgröße: {ascent.shape}\")\n",
    "print(f\"📊 Datentyp: {ascent.dtype}\")\n",
    "print(f\"📈 Min/Max Werte: {ascent.min()} / {ascent.max()}\")\n",
    "print(f\"🎯 Durchschnitt: {ascent.mean():.1f}\")\n",
    "\n",
    "# Fun Fact\n",
    "print(f\"\\n💡 Fun Fact: Dieses {ascent.shape[0]}x{ascent.shape[1]} Bild hat {ascent.size:,} Pixel!\")\n",
    "print(f\"💾 Speicherbedarf: {ascent.nbytes:,} Bytes ({ascent.nbytes/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean-Filter\n",
    "\n",
    "Der erste Filter, den wir anwenden werden, heißt \"Mean-Filter\". Der Mean-Filter ersetzt einen Pixelwert durch den Mittelwert der Werte, die sich in der Nachbarschaft des Pixels $9\\times 9$ befinden. Das heißt, wir verwenden alle benachbarten Pixel sowie die Werte des zu ersetzenden Pixels, dann berechnen wir den Mittelwert dieser neun Werte und verwenden das Ergebnis als neuen Pixelwert.\n",
    "\n",
    "## 🎮 Interaktive Filter-Experimente\n",
    "\n",
    "### 💻 Jupyter Widgets für Live-Experimente\n",
    "\n",
    "Wir können mit **ipywidgets** interaktiv verschiedene Filter testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Interaktive Filter-Demo\n",
    "\n",
    "def apply_filter_interactive(filter_type='Original', blur_size=3):\n",
    "    \"\"\"\n",
    "    Interaktive Funktion zum Testen verschiedener Filter\n",
    "    \"\"\"\n",
    "    # Filter auswählen\n",
    "    if filter_type == 'Original':\n",
    "        result = ascent\n",
    "    elif filter_type == 'Blur':\n",
    "        kernel_size = max(3, blur_size)\n",
    "        if kernel_size % 2 == 0:  # Kernel muss ungerade sein\n",
    "            kernel_size += 1\n",
    "        blur_kernel = np.ones((kernel_size, kernel_size)) / (kernel_size**2)\n",
    "        result = signal.convolve2d(ascent, blur_kernel, boundary='symm', mode='same')\n",
    "    elif filter_type == 'Edge X':\n",
    "        result = np.abs(signal.convolve2d(ascent, sobel_x, boundary='symm', mode='same'))\n",
    "    elif filter_type == 'Edge Y':\n",
    "        result = np.abs(signal.convolve2d(ascent, sobel_y, boundary='symm', mode='same'))\n",
    "    elif filter_type == 'Sharpen':\n",
    "        result = signal.convolve2d(ascent, sharpen, boundary='symm', mode='same')\n",
    "        result = np.clip(result, 0, 255)  # Werte begrenzen\n",
    "    elif filter_type == 'Laplace':\n",
    "        result = signal.convolve2d(ascent, laplace, boundary='symm', mode='same')\n",
    "        result = np.abs(result)  # Absolutwerte für bessere Sichtbarkeit\n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(ascent, cmap='gray')\n",
    "    plt.title('🏔️ Original', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(result, cmap='gray')\n",
    "    plt.title(f'{filter_type} Filter', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken\n",
    "    print(f\"📊 {filter_type} Filter Statistiken:\")\n",
    "    print(f\"   Min: {result.min():.1f}, Max: {result.max():.1f}\")\n",
    "    print(f\"   Mean: {result.mean():.1f}, Std: {result.std():.1f}\")\n",
    "\n",
    "# Widget erstellen\n",
    "filter_widget = interact(\n",
    "    apply_filter_interactive,\n",
    "    filter_type=widgets.Dropdown(\n",
    "        options=['Original', 'Blur', 'Edge X', 'Edge Y', 'Sharpen', 'Laplace'],\n",
    "        value='Original',\n",
    "        description='🔧 Filter:'\n",
    "    ),\n",
    "    blur_size=widgets.IntSlider(\n",
    "        value=3, min=3, max=15, step=2,\n",
    "        description='Blur Size:'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"🎮 Interaktive Filter-Demo geladen!\")\n",
    "print(\"💡 Tipp: Probieren Sie verschiedene Filter und Blur-Größen aus!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some 3x3 filter matrices used in computer vision\n",
    "\n",
    "# Mean-filter\n",
    "mean_3x3 = (1/9) * np.ones([3, 3])\n",
    "mean_9x9 = (1/81) * np.ones([9, 9])  # Stärkerer Blur-Effekt\n",
    "\n",
    "# Approximation of the gradient\n",
    "# Prewitt Filter - findet Kanten in x/y Richtung\n",
    "prewitt_x = np.array([[-1, 0, 1], \n",
    "                      [-1, 0, 1], \n",
    "                      [-1, 0, 1]])\n",
    "\n",
    "prewitt_y = np.array([[-1, -1, -1], \n",
    "                      [ 0,  0,  0], \n",
    "                      [ 1,  1,  1]])\n",
    "\n",
    "# Sobel Filter - bessere Kantendetection (gewichtet)\n",
    "sobel_x = np.array([[-1, 0, 1], \n",
    "                    [-2, 0, 2], \n",
    "                    [-1, 0, 1]])\n",
    "\n",
    "sobel_y = np.array([[-1, -2, -1], \n",
    "                    [ 0,  0,  0], \n",
    "                    [ 1,  2,  1]])\n",
    "\n",
    "# Second-order derivation \n",
    "# Laplace Filter - hebt Details hervor\n",
    "laplace = np.array([[ 0, -1,  0], \n",
    "                    [-1,  4, -1], \n",
    "                    [ 0, -1,  0]])\n",
    "\n",
    "# Enhanced Sharpening\n",
    "sharpen = np.array([[ 0, -1,  0], \n",
    "                    [-1,  5, -1], \n",
    "                    [ 0, -1,  0]])\n",
    "\n",
    "# 📊 Filter visualisieren\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "filters = [\n",
    "    (mean_3x3, \"🌫️ Blur (3x3)\"),\n",
    "    (prewitt_x, \"⚡ Prewitt X\"),\n",
    "    (prewitt_y, \"⚡ Prewitt Y\"), \n",
    "    (sobel_x, \"⚡ Sobel X\"),\n",
    "    (sobel_y, \"⚡ Sobel Y\"),\n",
    "    (laplace, \"🔍 Laplace\"),\n",
    "    (sharpen, \"✨ Sharpen\"),\n",
    "    (mean_9x9[:3,:3], \"🌫️ Blur (9x9)\")  # Zeige nur 3x3 Ausschnitt\n",
    "]\n",
    "\n",
    "for i, (filter_matrix, title) in enumerate(filters):\n",
    "    row, col = i // 4, i % 4\n",
    "    im = axes[row, col].imshow(filter_matrix, cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[row, col].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Werte in die Zellen schreiben\n",
    "    for (j, k), val in np.ndenumerate(filter_matrix):\n",
    "        axes[row, col].text(k, j, f'{val:.1f}', ha='center', va='center', \n",
    "                           color='white' if abs(val) > 1 else 'black', fontweight='bold')\n",
    "    \n",
    "    axes[row, col].set_xticks([])\n",
    "    axes[row, col].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Filter erfolgreich definiert!\")\n",
    "print(\"\\n💡 Wie Filter funktionieren:\")\n",
    "print(\"1. Filter-Matrix wird über Bild 'gefaltet' (convolved)\")  \n",
    "print(\"2. Jeder Pixel wird durch gewichtete Summe seiner Nachbarn ersetzt\")\n",
    "print(\"3. Verschiedene Filter erkennen verschiedene Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌫️ BLUR FILTER - Mean Filter anwenden\n",
    "\n",
    "# Verschiedene Blur-Stärken testen\n",
    "blur_light = signal.convolve2d(ascent, mean_3x3, boundary='symm', mode='same')\n",
    "blur_heavy = signal.convolve2d(ascent, mean_9x9, boundary='symm', mode='same')\n",
    "\n",
    "# Moderne Vergleichsvisualisierung\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "images = [\n",
    "    (ascent, \"🏔️ Original\", \"gray\"),\n",
    "    (blur_light, \"🌫️ Blur (3x3)\", \"gray\"),  \n",
    "    (blur_heavy, \"🌫️ Blur (9x9)\", \"gray\")\n",
    "]\n",
    "\n",
    "for i, (img, title, cmap) in enumerate(images):\n",
    "    axes[i].imshow(img, cmap=cmap, interpolation='nearest')\n",
    "    axes[i].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Info-Text hinzufügen\n",
    "    axes[i].text(10, 30, f\"Min: {img.min():.0f}\\nMax: {img.max():.0f}\\nMean: {img.mean():.1f}\", \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 📊 Detailvergleich - Bildbereich vergrößern\n",
    "region = slice(200, 300), slice(200, 300)  # 100x100 Pixel Ausschnitt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (img, title, cmap) in enumerate(images):\n",
    "    axes[i].imshow(img[region], cmap=cmap, interpolation='nearest')\n",
    "    axes[i].set_title(f\"{title} - Detail\", fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"🔍 Detailvergleich: Blur-Effekt\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📝 Mean Filter Analyse:\")\n",
    "print(\"• 3x3 Filter: Leichter Blur-Effekt, Details bleiben erkennbar\")\n",
    "print(\"• 9x9 Filter: Starker Blur-Effekt, viele Details gehen verloren\")\n",
    "print(\"• Anwendung: Rauschunterdrückung, künstlerische Effekte\")\n",
    "print(\"\\n💡 In CNNs: Das Netzwerk lernt automatisch, welche 'Blur-Stärke' optimal ist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, wird dadurch das Bild unscharf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prewitt-Filter\n",
    "\n",
    "Das Ziel des Prewitt-Filters ist, den Pixel-Wert durch seine Ableitung, d.h. die Änderung im Farbwert, zu ersetzen. Auch hier kommt eine $9\\times 9$-Nachbarschaft zum Einsatz. Da Bilder i.d.R. 2-dimensional sind (x- und y-Achse), kann die Ableitung sowohl der einen als auch der anderen Dimension berechnet werden. Praktisch bedeutet dies, dass es zwei Prewitt-Filter gibt, einen in x- und einen in y-Richtung. Außerdem korrespondieren größere Änderungen im Pixelwert (d.h. der Wert der Ableitung ist größer) mit Kanten im Bild, weshalb die Prewitt-Filter auch als Kantendetektoren bezeichnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascent_x_prew = signal.convolve2d(ascent, prewitt_x, boundary='symm', mode='same')\n",
    "ascent_x_prew = np.absolute(ascent_x_prew)\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x-Richtung', fontsize = 15)\n",
    "ax.imshow(ascent_x_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascent_y_prew = signal.convolve2d(ascent, prewitt_y, boundary='symm', mode='same')\n",
    "ascent_y_prew = np.absolute(ascent_y_prew)\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in y-Richtung', fontsize = 15)\n",
    "ax.imshow(ascent_y_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf den beiden Bildern sieht man, dass die entsprechenden Kanten in entweder x- oder y-Richtung extrahiert wurden. In der praktischen Anwendung, möchte man aber meist alle Kanten extrahieren - unabhängig von der Richtung. In diesem Fall werden schlichtweg die beiden Bilder addiert und anschließend durch 2 geteilt. Das Teilen durch 2 ist notwendig, damit die Pixelwert im Bereich 0 bis 255 verbleiben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_prew = (ascent_y_prew + ascent_x_prew) / 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x- & y-Richtung', fontsize = 15)\n",
    "ax.imshow(x_y_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.1.1:</b> Welcher lineare Prewitt-Filter wird benötigt, um horizontale Kanten hervorzuheben? \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gesichtserkennung\n",
    "\n",
    "Im folgenden wird die im Video erwähnte Gesichtserkennung skizziert.\n",
    "\n",
    "Zu Beginn erstmal ein Bild mit Gesichtern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('images/Gesichter.jpg')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann mithilfe einer Gesichtserkennung, die ensprechenden Bounding Boxen extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Gesichtserkennung_1_Bounding_Boxes.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und abschließend die Landmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/Gesichtserkennung_2_Landmarks.png')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können die Gesichter extrahiert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/face/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können die Landmarken extrahiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/landmarks/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zuletzt werden die Gesichter transformiert, sodass alle gleich große (96x96 Pixel) sind und mittig liegen. Somit können die Landmarken (z.B. Augen) immer an der gleichen Stelle im Bild auftauchen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = './images/landmarks/transformed/'\n",
    "for file in os.listdir(path):\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        plt.figure()\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Von klassischen Filtern zu modernen CNNs\n",
    "\n",
    "### 💡 Der Durchbruch: Lernbare Filter\n",
    "\n",
    "Wir haben gesehen, wie **handgeschriebene Filter** funktionieren. CNNs machen dasselbe - nur **automatisch**!\n",
    "\n",
    "### 🏗️ Modernes CNN mit TensorFlow/Keras\n",
    "\n",
    "Lassen Sie uns ein einfaches CNN für MNIST-Ziffenerkennung erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Modernes CNN mit TensorFlow/Keras\n",
    "\n",
    "# MNIST Daten laden\n",
    "print(\"📥 MNIST Dataset laden...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Daten vorbereiten\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"✅ Training: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "# CNN Modell erstellen\n",
    "model = keras.Sequential([\n",
    "    # 1. Convolutional Block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    \n",
    "    # 2. Convolutional Block  \n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    \n",
    "    # 3. Dense Layers\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(128, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.2, name='dropout'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Modell-Architektur anzeigen\n",
    "print(\"\\n🏗️ CNN Architektur:\")\n",
    "model.summary()\n",
    "\n",
    "# Visualisierung der Architektur\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, \n",
    "                          to_file='cnn_architecture.png', dpi=150)\n",
    "\n",
    "# Plot anzeigen\n",
    "try:\n",
    "    from IPython.display import Image as IPImage\n",
    "    display(IPImage('cnn_architecture.png'))\n",
    "except:\n",
    "    print(\"📊 Architektur in 'cnn_architecture.png' gespeichert\")\n",
    "\n",
    "print(\"\\n💡 CNN vs. Fully-Connected Vergleich:\")\n",
    "print(\"🔹 CNN Parameter: {:,}\".format(model.count_params()))\n",
    "\n",
    "# Vergleich mit Fully-Connected\n",
    "fc_params = (28*28) * 128 + 128 * 10  # Nur erste und letzte Schicht\n",
    "print(f\"🔹 Equivalent FC Parameter: {fc_params:,}\")\n",
    "print(f\"🚀 Parameter-Reduktion: {(1 - model.count_params()/fc_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 CNN Training (Quick Demo)\n",
    "\n",
    "print(\"🏃‍♂️ Schnelles Training (1 Epoche für Demo)...\")\n",
    "\n",
    "# Kleine Stichprobe für schnelles Training\n",
    "x_sample = x_train[:1000]\n",
    "y_sample = y_train[:1000]\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    x_sample, y_sample,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Test auf paar Beispielen\n",
    "test_loss, test_acc = model.evaluate(x_test[:100], y_test[:100], verbose=0)\n",
    "print(f\"\\n📊 Test Accuracy (100 Samples): {test_acc:.3f}\")\n",
    "\n",
    "print(\"✅ CNN erfolgreich trainiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Feature Maps Visualisierung - Das Herz von CNNs!\n",
    "\n",
    "def visualize_feature_maps(model, input_image, layer_names=['conv1', 'conv2']):\n",
    "    \"\"\"\n",
    "    Visualisiert Feature Maps von CNN Layern\n",
    "    \"\"\"\n",
    "    # Model für Feature Extraction erstellen\n",
    "    layer_outputs = [model.get_layer(name).output for name in layer_names]\n",
    "    activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # Feature Maps berechnen\n",
    "    activations = activation_model.predict(input_image[np.newaxis, ...])\n",
    "    \n",
    "    for layer_idx, (layer_name, activation) in enumerate(zip(layer_names, activations)):\n",
    "        # Anzahl Feature Maps\n",
    "        n_features = activation.shape[-1]\n",
    "        n_cols = 8\n",
    "        n_rows = n_features // n_cols + (1 if n_features % n_cols else 0)\n",
    "        \n",
    "        # Plot erstellen\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 2*n_rows))\n",
    "        fig.suptitle(f'🔍 Feature Maps - {layer_name.upper()} Layer', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            if n_rows == 1:\n",
    "                ax = axes[col] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "            # Feature Map anzeigen\n",
    "            feature_map = activation[0, :, :, i]\n",
    "            im = ax.imshow(feature_map, cmap='viridis', interpolation='nearest')\n",
    "            ax.set_title(f'Filter {i+1}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Leere Subplots verstecken\n",
    "        for i in range(n_features, n_rows * n_cols):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            if n_rows == 1:\n",
    "                ax = axes[col] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col]\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"📊 {layer_name} Layer: {activation.shape} -> {n_features} Feature Maps\")\n",
    "\n",
    "# Beispielbild auswählen\n",
    "test_image = x_test[0]  # Erste Testziffer\n",
    "\n",
    "# Original Bild anzeigen\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image.squeeze(), cmap='gray')\n",
    "plt.title('🔢 Original MNIST Ziffer', fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Vorhersage\n",
    "prediction = model.predict(test_image[np.newaxis, ...])\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = np.max(prediction) * 100\n",
    "\n",
    "print(f\"🎯 Vorhersage: Ziffer {predicted_class} (Confidence: {confidence:.1f}%)\")\n",
    "\n",
    "# Feature Maps visualisieren\n",
    "print(\"\\n🔍 Feature Maps Analyse:\")\n",
    "print(\"Diese zeigen, was das CNN 'sieht' in jedem Layer...\")\n",
    "visualize_feature_maps(model, test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 Streamlit Integration\n",
    "\n",
    "### 💻 Interaktive CNN Filter App\n",
    "\n",
    "Sie können die **Streamlit App** parallel zu diesem Notebook ausführen:\n",
    "\n",
    "```bash\n",
    "# Im Terminal ausführen:\n",
    "cd 06_Computer_Vision_NLP\n",
    "streamlit run 06_01_streamlit_cnn_filter.py\n",
    "```\n",
    "\n",
    "### 🌟 Features der App:\n",
    "- ✅ **Interaktive Filter-Tests** mit eigenen Bildern\n",
    "- ✅ **Live Parameter-Anpassung** \n",
    "- ✅ **Filter-Kernel Visualisierung**\n",
    "- ✅ **Bildstatistiken** in Echtzeit\n",
    "- ✅ **Download-Funktion** für gefilterte Bilder\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Zusammenfassung & Key Takeaways\n",
    "\n",
    "### ✅ Was Sie gelernt haben:\n",
    "\n",
    "#### 1. 🔍 **Klassische Bildfilter**\n",
    "- **Mean Filter:** Blur-Effekte durch Mittelwertbildung\n",
    "- **Sobel/Prewitt:** Kanten-Detection durch Gradient-Berechnung  \n",
    "- **Laplace:** Detail-Enhancement durch 2. Ableitung\n",
    "- **Custom Filter:** Eigene 3x3 Kernel erstellen\n",
    "\n",
    "#### 2. 🧠 **CNN Grundkonzepte**\n",
    "- **Local Connectivity:** Nur lokale Pixel-Verbindungen\n",
    "- **Weight Sharing:** Gleicher Filter über ganzes Bild\n",
    "- **Translation Invariance:** Positionsunabhängige Erkennung\n",
    "- **Hierarchical Learning:** Von einfach zu komplex\n",
    "\n",
    "#### 3. 🚀 **Praktische CNN Implementation**\n",
    "- **TensorFlow/Keras** für moderne CNNs\n",
    "- **Feature Maps** visualisieren und verstehen\n",
    "- **Parameter-Effizienz** vs. Fully-Connected Networks\n",
    "- **MNIST Klassifikation** als Proof-of-Concept\n",
    "\n",
    "### 💡 **Verbindung Filter ↔ CNN:**\n",
    "\n",
    "```python\n",
    "# Klassischer Filter (handgeschrieben)\n",
    "sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "filtered = signal.convolve2d(image, sobel_x)\n",
    "\n",
    "# CNN Filter (automatisch gelernt)\n",
    "conv_layer = layers.Conv2D(32, (3, 3), activation='relu')\n",
    "# Das Netzwerk lernt 32 verschiedene 3x3 Filter automatisch!\n",
    "```\n",
    "\n",
    "### 🎯 **Nächste Schritte:**\n",
    "\n",
    "In den kommenden Notebooks vertiefen wir:\n",
    "- **06.2:** Computer Vision Anwendungen (Objekterkennung, Segmentierung)\n",
    "- **06.3:** Data Augmentation für bessere Modelle\n",
    "- **06.4:** Transfer Learning mit vortrainierten Modellen\n",
    "\n",
    "---\n",
    "\n",
    "### 🏆 **Challenge für Sie:**\n",
    "\n",
    "1. **📊 Experimentieren Sie** mit der Streamlit App - probieren Sie verschiedene Filter!\n",
    "2. **🔧 Modifizieren Sie** das CNN-Modell (mehr Layer, andere Aktivierungen)\n",
    "3. **📸 Testen Sie** eigene Bilder mit den klassischen Filtern\n",
    "4. **🤔 Überlegen Sie:** Welche Filter würde ein CNN für Ihr Anwendungsgebiet lernen?\n",
    "\n",
    "**💪 Advanced:** Implementieren Sie einen eigenen Filter in der Streamlit App!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-amalea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
