{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n",
    "\n",
    "# 🔧 Setup: Computer Vision Libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Computer Vision  \n",
    "import cv2\n",
    "from scipy import signal, ndimage\n",
    "import skimage\n",
    "from skimage import feature, filters, segmentation, measure\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import applications, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Interactive Widgets\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Streamlit (für Apps)\n",
    "import streamlit as st\n",
    "\n",
    "# Konfiguration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "figure_inches = 10\n",
    "\n",
    "print(\"✅ Computer Vision Libraries geladen!\")\n",
    "print(f\"📊 TensorFlow: {tf.__version__}\")\n",
    "print(f\"🖼️ OpenCV: {cv2.__version__}\")\n",
    "print(f\"🔬 Scikit-Image: {skimage.__version__}\")\n",
    "\n",
    "# GPU Check\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"🚀 GPU verfügbar für Deep Learning!\")\n",
    "else:\n",
    "    print(\"💻 CPU wird verwendet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🖼️ 06.2 Computer Vision Anwendungen - Von Theorie zur Praxis\n",
    "\n",
    "**Data Analytics & Big Data - Woche 6.2**  \n",
    "*IU Internationale Hochschule*\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Lernziele\n",
    "\n",
    "Nach diesem Notebook können Sie:\n",
    "- ✅ **Faltungsoperationen** mathematisch verstehen und implementieren\n",
    "- ✅ **Verschiedene Filter** (Roberts, Sobel, etc.) anwenden  \n",
    "- ✅ **Praktische CV-Anwendungen** entwickeln (Objekterkennung, Segmentierung)\n",
    "- ✅ **OpenCV** für professionelle Computer Vision nutzen\n",
    "- ✅ **Streamlit-App** für Bildklassifikation erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Faltungsoperationen verstehen\n",
    "\n",
    "**Convolution** ist das Herzstück von Computer Vision! Lassen Sie uns verstehen, wie es funktioniert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧮 Faltungsoperation (Convolution) verstehen\n",
    "\n",
    "### 🤔 Was ist Convolution?\n",
    "\n",
    "**Convolution** ist eine mathematische Operation, die zwei Funktionen miteinander \"faltet\". In Computer Vision:\n",
    "- **Bild** (Input) ⊛ **Filter** (Kernel) = **Feature Map** (Output)\n",
    "\n",
    "### 📐 Mathematische Definition\n",
    "\n",
    "**Kontinuierlich:** \n",
    "$$x(t) \\ast y(t) = \\int_{-\\infty}^{+\\infty} x(t -\\tau) y(\\tau) d\\tau$$\n",
    "\n",
    "**Diskret (für Pixel):**\n",
    "$$x_n \\ast y_n = \\sum_{i = -\\infty}^{\\infty} x_i \\cdot y_{n-i}$$\n",
    "\n",
    "### 🎯 Warum ist das wichtig?\n",
    "\n",
    "1. **Feature Detection:** Filter erkennen Muster (Kanten, Texturen, etc.)\n",
    "2. **Parameter Sharing:** Ein Filter wird über das ganze Bild angewendet  \n",
    "3. **Translation Invariance:** Objekte werden überall im Bild erkannt\n",
    "\n",
    "### 🎮 Interaktive Convolution Demonstration\n",
    "\n",
    "Verstehen Sie Convolution durch interaktive Rechteck-Funktionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "figure_inches = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_demo(tau: float, width1: float, width2: float):\n",
    "    \"\"\"\n",
    "    📊 Interaktive Convolution-Demonstration\n",
    "    \n",
    "    Parameter:\n",
    "    - tau: Verschiebung der zweiten Funktion\n",
    "    - width1: Breite der ersten Rechteck-Funktion\n",
    "    - width2: Breite der zweiten Rechteck-Funktion\n",
    "    \"\"\"\n",
    "    # Definiere x-Achse\n",
    "    x1 = np.linspace(-3.5, 3.5, num=1000)\n",
    "    dX = x1[1] - x1[0]    \n",
    "    \n",
    "    # Erstelle Rechteck-Funktionen\n",
    "    rect1 = np.where(abs(x1) <= width1/2, 1, 0)  # rect₁(t)\n",
    "    rect2 = np.where(abs(x1 - tau) <= width2/2, 1, 0)  # rect₂(t-τ)\n",
    "    \n",
    "    # Convolution berechnen\n",
    "    conv = np.convolve(rect1, rect2, 'same') * dX \n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(16.5, 4))\n",
    "    \n",
    "    # Plots\n",
    "    plt.plot(x1, rect1, 'b', linewidth=2, label='📊 rect₁(t)')\n",
    "    plt.plot(x1, rect2, 'r', linewidth=2, label='📊 rect₂(t-τ)')\n",
    "    \n",
    "    # Convolution bis zu aktueller Position\n",
    "    x_gr = x1 - tau\n",
    "    if tau <= 0:\n",
    "        index = np.where((np.absolute(x_gr) - np.absolute(tau)) <= 0.004)\n",
    "        index = index[0][0] if len(index[0]) > 0 else 0\n",
    "    else:\n",
    "        index = np.where(np.absolute(x_gr - tau) <= 0.004)\n",
    "        index = index[0][0] if len(index[0]) > 0 else 999\n",
    "        \n",
    "    plt.plot(x_gr[:index], conv[:index], 'g', linewidth=3, \n",
    "             label='🎯 rect₁ ⊛ rect₂ (Convolution)')\n",
    "    \n",
    "    # Vertikale Linie bei aktueller Position\n",
    "    plt.axvline(x=tau, color='r', linestyle='--', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    # Styling\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 13})\n",
    "    plt.ylim(0, np.maximum(np.max(conv), np.max(rect1)) + 0.1)\n",
    "    plt.xlim(-2.5, 2.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.title(f'🎮 Convolution Demo: τ = {tau:.2f}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Zeit t', fontsize=12)\n",
    "    plt.ylabel('Amplitude', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Info-Text\n",
    "    overlap_area = np.trapz(conv[:index], x_gr[:index]) if index > 0 else 0\n",
    "    print(f\"📊 Convolution Wert bei τ={tau:.2f}: {overlap_area:.3f}\")\n",
    "    print(f\"💡 Interpretation: Überlappungsbereich der beiden Rechtecke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Interaktive Convolution Demo\n",
    "\n",
    "print(\"🎯 Convolution verstehen durch interaktive Rechteck-Funktionen:\")\n",
    "print(\"• Bewegen Sie τ (tau) um die zweite Funktion zu verschieben\")\n",
    "print(\"• Ändern Sie width1/width2 um die Rechteck-Breiten anzupassen\")\n",
    "print(\"• Beobachten Sie, wie sich die Convolution (grün) entwickelt\")\n",
    "\n",
    "# Widget für interaktive Convolution\n",
    "interactive_plot = interactive(\n",
    "    convolution_demo, \n",
    "    tau=widgets.FloatSlider(\n",
    "        value=0, min=-2.0, max=2.0, step=0.1,\n",
    "        description='τ (Verschiebung):',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    width1=widgets.FloatSlider(\n",
    "        value=1.0, min=0.25, max=1.75, step=0.25,\n",
    "        description='Width 1:',\n",
    "        style={'description_width': 'initial'}  \n",
    "    ),\n",
    "    width2=widgets.FloatSlider(\n",
    "        value=1.0, min=0.25, max=1.75, step=0.25,\n",
    "        description='Width 2:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Widget anzeigen\n",
    "display(interactive_plot)\n",
    "\n",
    "print(\"\\n💡 Key Insight: Convolution misst die 'Ähnlichkeit' zwischen zwei Funktionen!\")\n",
    "print(\"🔗 In CNNs: Bild ⊛ Filter = Feature Map (zeigt wo Features gefunden wurden)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natürlich erfolgt die Berechnung der Faltung nicht im kontinuierlichen Bereich. Daher verwendet numpy die folgende Formel für diskrete Faltung im 1-dimensionalen Raum:\n",
    "\n",
    "\\begin{equation*}\n",
    "x_n \\ast y_n = \\sum_{i = -\\infty}^{\\infty} x_i  \\  y_{n-i} = \\sum_{i = -\\infty}^{\\infty} y_{i}  \\ x_{n-i}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🖼️ 2D-Convolution für Bilder\n",
    "\n",
    "### 📐 Von 1D zu 2D\n",
    "\n",
    "**2D-Convolution** für Bilder:\n",
    "$$x_{mn} \\star y_{mn} = \\sum_{i} \\sum_{j} x_{ij} \\cdot y_{m-i, n-j}$$\n",
    "\n",
    "### 🔧 Praktische Parameter\n",
    "\n",
    "**Bildformat:** Height × Width × Channels (H × W × C)\n",
    "- **Graustufenbild:** 28 × 28 × 1  \n",
    "- **RGB-Bild:** 224 × 224 × 3\n",
    "\n",
    "**Filter-Parameter:**\n",
    "- **Kernel Size (K):** z.B. 3×3, 5×5  \n",
    "- **Stride (S):** Schrittweite (meist 1 oder 2)\n",
    "- **Padding (P):** Randbehandlung (SAME, VALID)\n",
    "\n",
    "### 📏 Ausgabegröße berechnen\n",
    "\n",
    "$$W_{out} = \\frac{W_{in} - K + 2P}{S} + 1$$\n",
    "$$H_{out} = \\frac{H_{in} - K + 2P}{S} + 1$$\n",
    "\n",
    "### 🎯 Beispiel-Berechnung\n",
    "\n",
    "```python\n",
    "# Input: 28×28×1 (MNIST)\n",
    "# Filter: 3×3, Stride=1, Padding=0\n",
    "W_out = (28 - 3 + 0) / 1 + 1 = 26\n",
    "H_out = (28 - 3 + 0) / 1 + 1 = 26  \n",
    "# Output: 26×26×1\n",
    "```\n",
    "\n",
    "### 🔍 Convolution Visualisierung\n",
    "\n",
    "Die Animation zeigt, wie ein 3×3 Filter über ein Bild \"gleitet\":\n",
    "\n",
    "![Convolution Animation](images/Faltung1.png)\n",
    "\n",
    "**Schritt-für-Schritt:**\n",
    "1. **Filter positionieren** auf Bildbereich\n",
    "2. **Element-wise Multiplikation** zwischen Filter und Pixeln  \n",
    "3. **Summe bilden** → ein Output-Pixel\n",
    "4. **Filter verschieben** (Stride) und wiederholen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.2.1:</b> \n",
    "Implementieren Sie die Funktion <code>conv</code> welche ein gegebenes Bild <code>image_data</code> mit einem gegebenenen Filter <code>filter_kern</code> filtert. Nehmen Sie an:\n",
    "\n",
    "* Das Bild liegt entsprechend dem Beispiel (in der folgenden Zelle) als eine Liste von Listen vor\n",
    "* Die Tiefe des Bildes ist 1\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 🔧 Convolution-Funktion implementieren\n",
    "\n",
    "def conv(image_data: List[List[int]], filter_kern: List[List[int]]) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    🎯 2D-Convolution Implementation (Educational)\n",
    "    \n",
    "    Parameter:\n",
    "    - image_data: Bild als 2D-Liste [[pixel, pixel, ...], [row2, ...], ...]\n",
    "    - filter_kern: Filter als 2D-Liste [[w1, w2], [w3, w4], ...]\n",
    "    \n",
    "    Returns:\n",
    "    - Gefilterte Bilddata als 2D-Liste\n",
    "    \n",
    "    💡 Hinweis: Das ist eine vereinfachte Educational-Version.\n",
    "       In der Praxis nutzen wir OpenCV oder TensorFlow!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dimensionen ermitteln\n",
    "    image_height = len(image_data)\n",
    "    image_width = len(image_data[0])\n",
    "    filter_height = len(filter_kern)\n",
    "    filter_width = len(filter_kern[0])\n",
    "    \n",
    "    # Output-Dimensionen berechnen (ohne Padding)\n",
    "    output_height = image_height - filter_height + 1\n",
    "    output_width = image_width - filter_width + 1\n",
    "    \n",
    "    # Initialisiere Output\n",
    "    result = []\n",
    "    \n",
    "    # Convolution durchführen\n",
    "    for i in range(output_height):\n",
    "        row = []\n",
    "        for j in range(output_width):\n",
    "            # Berechne Convolution für aktuelle Position\n",
    "            conv_sum = 0\n",
    "            for fi in range(filter_height):\n",
    "                for fj in range(filter_width):\n",
    "                    # Element-wise Multiplikation und Summation\n",
    "                    pixel_value = image_data[i + fi][j + fj]\n",
    "                    filter_value = filter_kern[fi][fj]\n",
    "                    conv_sum += pixel_value * filter_value\n",
    "            \n",
    "            row.append(conv_sum)\n",
    "        result.append(row)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ Convolution-Funktion implementiert!\")\n",
    "print(\"\\n📚 Lernziel: Verstehen Sie, wie Convolution 'unter der Haube' funktioniert\")\n",
    "print(\"🚀 In der Praxis: Nutzen Sie cv2.filter2D() oder TensorFlow für Performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Test der Convolution-Implementierung\n",
    "\n",
    "print(\"🧪 Test-Case: Convolution mit Edge-Detection Filter\")\n",
    "\n",
    "# Test-Daten\n",
    "test_input_data = [\n",
    "    [0, 0, 0, 0, 0], \n",
    "    [0, 1, 1, 1, 0], \n",
    "    [0, 0, 2, 0, 0], \n",
    "    [0, 3, 3, 3, 0], \n",
    "    [0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "test_filter = [\n",
    "    [0, 0], \n",
    "    [-1, 1]\n",
    "]\n",
    "\n",
    "expected_result = [\n",
    "    [1, 0, 0, -1],\n",
    "    [0, 2, -2, 0],\n",
    "    [3, 0, 0, -3], \n",
    "    [0, 0, 0, 0], \n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "print(\"📊 Input Bild:\")\n",
    "for i, row in enumerate(test_input_data):\n",
    "    print(f\"Row {i}: {row}\")\n",
    "\n",
    "print(f\"\\n🔧 Filter (Edge Detection):\")\n",
    "for i, row in enumerate(test_filter):\n",
    "    print(f\"Filter Row {i}: {row}\")\n",
    "\n",
    "# Test ausführen\n",
    "found = conv(test_input_data, test_filter)\n",
    "\n",
    "print(f\"\\n🎯 Erwartetes Ergebnis:\")\n",
    "for i, row in enumerate(expected_result):\n",
    "    print(f\"Expected Row {i}: {row}\")\n",
    "\n",
    "print(f\"\\n✅ Gefundenes Ergebnis:\")\n",
    "for i, row in enumerate(found):\n",
    "    print(f\"Found Row {i}: {row}\")\n",
    "\n",
    "# Verification\n",
    "try:\n",
    "    assert found == expected_result\n",
    "    print(\"\\n🎉 Test erfolgreich! Ihre Convolution-Implementation ist korrekt!\")\n",
    "except AssertionError:\n",
    "    print(\"\\n❌ Test fehlgeschlagen. Überprüfen Sie Ihre Implementation.\")\n",
    "    print(\"💡 Tipp: Stellen Sie sicher, dass Sie Element-wise Multiplikation und Summation korrekt implementiert haben.\")\n",
    "\n",
    "print(\"\\n💡 Was passiert hier?\")\n",
    "print(\"• Filter [-1, 1] erkennt horizontale Übergänge (Links-Rechts Unterschiede)\")\n",
    "print(\"• Positive Werte: Übergang von dunkel zu hell\")  \n",
    "print(\"• Negative Werte: Übergang von hell zu dunkel\")\n",
    "print(\"• Zero Werte: Keine Änderung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtertypen\n",
    "\n",
    "Bevor wir nun in Richtung praktische Anwendung gehen, schauen wir uns grundlegende Filter an. Außerdem werden wir uns die Effekte der Filter anschauen - hierzu verwenden wir das folgende Bild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_as_array(image_path: str, new_size: Tuple[int, int] = (500, 500)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    📸 Bild laden und als NumPy Array zurückgeben\n",
    "    \n",
    "    Parameter:\n",
    "    - image_path: Pfad zur Bilddatei\n",
    "    - new_size: Neue Bildgröße (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    - NumPy Array mit Graustufenwerten\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('L')  # Graustufen\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)  # Moderne Resampling-Methode\n",
    "        return np.array(img)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Bild nicht gefunden: {image_path}\")\n",
    "        # Fallback: Erstelle Beispielbild\n",
    "        return create_sample_image(new_size)\n",
    "\n",
    "def create_sample_image(size: Tuple[int, int] = (500, 500)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    🎨 Erstellt ein Beispielbild mit geometrischen Formen\n",
    "    \"\"\"\n",
    "    img = np.zeros(size)\n",
    "    h, w = size\n",
    "    \n",
    "    # Rechteck\n",
    "    img[h//4:3*h//4, w//4:w//2] = 255\n",
    "    \n",
    "    # Kreis  \n",
    "    center = (3*w//4, h//2)\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    mask = (x - center[0])**2 + (y - center[1])**2 <= (w//8)**2\n",
    "    img[mask] = 128\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Lade Testbild\n",
    "print(\"📸 Lade Testbild...\")\n",
    "\n",
    "try:\n",
    "    # Versuche Lama-Bild zu laden\n",
    "    if os.path.exists('images/lama.png'):\n",
    "        lama_array = read_image_as_array('images/lama.png', (500, 500))\n",
    "        print(\"✅ Lama-Bild geladen!\")\n",
    "    else:\n",
    "        print(\"⚠️  Lama-Bild nicht gefunden, erstelle Beispielbild...\")\n",
    "        lama_array = create_sample_image((500, 500))\n",
    "        \n",
    "    # Visualisierung\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original Bild\n",
    "    axes[0].imshow(lama_array, cmap='gray', interpolation='nearest')\n",
    "    axes[0].set_title('🖼️ Testbild für Filter-Experimente', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1].hist(lama_array.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1].set_title('📊 Pixel-Intensitäts-Verteilung', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Grauwert (0-255)')\n",
    "    axes[1].set_ylabel('Anzahl Pixel')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📏 Bildgröße: {lama_array.shape}\")\n",
    "    print(f\"📊 Grauwert-Bereich: {lama_array.min()} - {lama_array.max()}\")\n",
    "    print(f\"🎯 Durchschnitt: {lama_array.mean():.1f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Bildladen: {e}\")\n",
    "    lama_array = create_sample_image()\n",
    "    print(\"🎨 Verwende generiertes Beispielbild\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identitätsfilter\n",
    "\n",
    "Der erste Filter entspricht der Identität, d.h. der Wert eines Pixel wird auf genau diesen abgebildet. Um dies zu erreichen wird ein quadratischer Filterkernel benötigt, dessen Größe ungerade ist. Außerdem ist der mittlere Eintrag 1 und alle anderen 0. Ein $3\\times 3$-Filterkernel hat somit die Form:\n",
    "\n",
    "$\\left\\lbrack\\begin{array}{ccc} 0&0&0\\\\ 0&1&0\\\\ 0&0&0\\end{array}\\right\\rbrack$\n",
    "\n",
    "Und nun die angekündigte Anwendung auf das Bild:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "# Identitätsfilter definieren\n",
    "identity_filter = [\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0], \n",
    "    [0, 0, 0]\n",
    "]\n",
    "\n",
    "# Filter anwenden (mit unserer Implementation)\n",
    "filtered_identity = conv(data, identity_filter)\n",
    "\n",
    "# Visualisierung\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "images_data = [\n",
    "    (data, \"🖼️ Original\", \"Original Bild\"),\n",
    "    (np.array(filtered_identity), \"🔄 Identitätsfilter\", \"Sollte identisch sein\"),\n",
    "    (np.abs(data[1:-1, 1:-1] - np.array(filtered_identity)), \"📊 Differenz\", \"Unterschied (sollte ≈0 sein)\")\n",
    "]\n",
    "\n",
    "for i, (img, title, subtitle) in enumerate(images_data):\n",
    "    axes[i].imshow(img, cmap='gray', interpolation='nearest')\n",
    "    axes[i].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[i].text(0.02, 0.98, subtitle, transform=axes[i].transAxes, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n",
    "                verticalalignment='top', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verifikation\n",
    "original_roi = data[1:-1, 1:-1]  # Region of Interest (ohne Rand)\n",
    "filtered_array = np.array(filtered_identity)\n",
    "difference = np.abs(original_roi - filtered_array)\n",
    "max_diff = np.max(difference)\n",
    "\n",
    "print(f\"\\n📊 Verifikation:\")\n",
    "print(f\"   Original ROI Shape: {original_roi.shape}\")\n",
    "print(f\"   Filtered Shape: {filtered_array.shape}\")\n",
    "print(f\"   Max Differenz: {max_diff}\")\n",
    "\n",
    "if max_diff < 1e-10:\n",
    "    print(\"   ✅ Perfect! Identitätsfilter verändert nichts\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Kleine Differenz gefunden (erwartet bei Integer-Implementierung)\")\n",
    "\n",
    "print(\"\\n💡 Key Insight:\")\n",
    "print(\"• Identitätsfilter = Kernel mit 1 in der Mitte, 0 überall sonst\")\n",
    "print(\"• Wichtig für CNN-Architekturen (Skip Connections, ResNet)\")\n",
    "print(\"• Baseline um andere Filter zu verstehen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eckendetektoren\n",
    "\n",
    "Die nächsten drei Filter ziehlen darauf ab, Ecken im Bild zu finden. Ziel hierbei ist es flächige Bereiche voneinander zu trennen. Die Filter sind oft nach deren Erfinder bzw. Entdecker benannt. In diesem Fall stellt der Sobel2 eine Verbesserung des Sobel1 dar - dieser kann zusätzlich zum horizontalen sowie vertikalen auch im $45^\\circ$ Bereich messen.\n",
    "\n",
    "Roberts: $\\left\\lbrack\\begin{array}{ccc} 1&0&-1\\\\ 0&0&0\\\\ -1&0&1 \\end{array}\\right\\rbrack$\n",
    "\n",
    "Sobel1: $\\left\\lbrack\\begin{array}{ccc} 0&-1&0\\\\ -1&4&-1\\\\ 0&-1&0\\end{array}\\right\\rbrack$\n",
    "\n",
    "Sobel2: $\\left\\lbrack\\begin{array}{ccc} -1&-1&-1\\\\-1&8&-1\\\\ -1&-1&-1\\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚡ Edge Detection Filter - Finde Kanten und Konturen!\n",
    "\n",
    "print(\"🎯 Edge Detection: Findet Übergänge zwischen verschiedenen Bereichen\")\n",
    "print(\"💡 Anwendung: Objekterkennung, Contouring, Feature Extraction\")\n",
    "\n",
    "# Edge Detection Filter definieren\n",
    "filters_edge = {\n",
    "    \"Roberts\": {\n",
    "        \"kernel\": [[1, 0, -1], [0, 0, 0], [-1, 0, 1]],\n",
    "        \"description\": \"Einfache Diagonal-Edge Detection\"\n",
    "    },\n",
    "    \"Sobel (Light)\": {\n",
    "        \"kernel\": [[0, -1, 0], [-1, 4, -1], [0, -1, 0]], \n",
    "        \"description\": \"Kantenverstärkung in 4 Richtungen\"\n",
    "    },\n",
    "    \"Sobel (Strong)\": {\n",
    "        \"kernel\": [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]],\n",
    "        \"description\": \"Starke Kantenverstärkung in 8 Richtungen\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Moderne OpenCV Vergleiche\n",
    "sobel_x_cv = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "sobel_y_cv = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "\n",
    "print(\"\\n🔧 Filter-Übersicht:\")\n",
    "fig, axes = plt.subplots(1, len(filters_edge), figsize=(15, 4))\n",
    "\n",
    "for i, (name, filter_info) in enumerate(filters_edge.items()):\n",
    "    kernel = np.array(filter_info[\"kernel\"])\n",
    "    \n",
    "    # Filter visualisieren\n",
    "    im = axes[i].imshow(kernel, cmap='RdBu', vmin=-8, vmax=8)\n",
    "    axes[i].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Werte in Zellen schreiben\n",
    "    for (j, k), val in np.ndenumerate(kernel):\n",
    "        axes[i].text(k, j, f'{val}', ha='center', va='center', \n",
    "                    color='white' if abs(val) > 2 else 'black', fontweight='bold')\n",
    "    \n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "    \n",
    "    # Beschreibung\n",
    "    axes[i].text(0.5, -0.15, filter_info[\"description\"], \n",
    "                transform=axes[i].transAxes, ha='center', \n",
    "                fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filter anwenden und vergleichen\n",
    "print(\"\\n⚙️  Anwenden der Edge-Detection Filter...\")\n",
    "\n",
    "# Roberts Filter anwenden\n",
    "roberts_result = conv(lama_array.tolist(), filters_edge[\"Roberts\"][\"kernel\"])\n",
    "\n",
    "# Visualisierung der Ergebnisse\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(lama_array, cmap='gray')\n",
    "axes[0, 0].set_title('🖼️ Original Bild', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Roberts Result\n",
    "roberts_array = np.array(roberts_result)\n",
    "axes[0, 1].imshow(np.abs(roberts_array), cmap='gray')  # Abs für bessere Sichtbarkeit\n",
    "axes[0, 1].set_title('⚡ Roberts Edge Detection', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# OpenCV Sobel X\n",
    "sobel_x_result = cv2.filter2D(lama_array.astype(np.float32), -1, sobel_x_cv)\n",
    "axes[1, 0].imshow(np.abs(sobel_x_result), cmap='gray')\n",
    "axes[1, 0].set_title('🔍 Sobel X (OpenCV)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# OpenCV Sobel Y  \n",
    "sobel_y_result = cv2.filter2D(lama_array.astype(np.float32), -1, sobel_y_cv)\n",
    "axes[1, 1].imshow(np.abs(sobel_y_result), cmap='gray')\n",
    "axes[1, 1].set_title('🔍 Sobel Y (OpenCV)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiken\n",
    "print(f\"\\n📊 Edge Detection Statistiken:\")\n",
    "print(f\"   Roberts - Min: {roberts_array.min():.1f}, Max: {roberts_array.max():.1f}\")\n",
    "print(f\"   Sobel X - Min: {sobel_x_result.min():.1f}, Max: {sobel_x_result.max():.1f}\")\n",
    "print(f\"   Sobel Y - Min: {sobel_y_result.min():.1f}, Max: {sobel_y_result.max():.1f}\")\n",
    "\n",
    "print(\"\\n💡 Edge Detection Insights:\")\n",
    "print(\"• Negative Werte = Übergang von hell zu dunkel\")\n",
    "print(\"• Positive Werte = Übergang von dunkel zu hell\") \n",
    "print(\"• Absolutwerte = Kantenintensität (unabhängig von Richtung)\")\n",
    "print(\"• Sobel X = Vertikale Kanten, Sobel Y = Horizontale Kanten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_sobel1 = [[0,-1,0], [-1,4,-1], [0,-1,0]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sobel1)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 Moderne Computer Vision mit OpenCV\n",
    "\n",
    "### 💡 Von manuellen Filtern zu intelligenten Algorithmen\n",
    "\n",
    "Bisher haben wir **manuelle Filter** kennengelernt. Jetzt schauen wir uns **intelligente CV-Algorithmen** an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Moderne Computer Vision Anwendungen mit OpenCV\n",
    "\n",
    "print(\"🚀 Von manuellen Filtern zu intelligenten Algorithmen\")\n",
    "\n",
    "# 1. 📊 CANNY EDGE DETECTION - State-of-the-Art\n",
    "def canny_edge_detection(image, low_threshold=50, high_threshold=150):\n",
    "    \"\"\"\n",
    "    🔍 Canny Edge Detection - Beste Edge Detection Methode\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    return edges\n",
    "\n",
    "# 2. 🎯 CONTOUR DETECTION - Objektumrisse finden\n",
    "def detect_contours(image):\n",
    "    \"\"\"\n",
    "    🎯 Findet Objekt-Konturen im Bild\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # Canny edges\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    # Contours finden\n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Konturen auf Original zeichnen\n",
    "    result = image.copy() if len(image.shape) == 3 else cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.drawContours(result, contours, -1, (0, 255, 0), 2)\n",
    "    \n",
    "    return result, contours\n",
    "\n",
    "# 3. 🧩 IMAGE SEGMENTATION - K-Means Clustering\n",
    "def kmeans_segmentation(image, k=4):\n",
    "    \"\"\"\n",
    "    🧩 K-Means Bildsegmentierung\n",
    "    \"\"\"\n",
    "    # Reshape für K-Means\n",
    "    data = image.reshape((-1, 3))\n",
    "    data = np.float32(data)\n",
    "    \n",
    "    # K-Means Parameter\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n",
    "    \n",
    "    # K-Means ausführen\n",
    "    _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Ergebnis rekonstruieren\n",
    "    centers = np.uint8(centers)\n",
    "    segmented_data = centers[labels.flatten()]\n",
    "    segmented_image = segmented_data.reshape(image.shape)\n",
    "    \n",
    "    return segmented_image\n",
    "\n",
    "# 4. 🔍 FEATURE DETECTION - SIFT Features\n",
    "def detect_sift_features(image):\n",
    "    \"\"\"\n",
    "    🔍 SIFT (Scale-Invariant Feature Transform) Features\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # SIFT Detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    \n",
    "    # Keypoints zeichnen\n",
    "    result = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return result, keypoints, descriptors\n",
    "\n",
    "print(\"✅ Computer Vision Funktionen definiert!\")\n",
    "\n",
    "# Teste mit unserem Bild\n",
    "if 'lama_array' in locals():\n",
    "    # Konvertiere zu RGB falls nötig\n",
    "    if len(lama_array.shape) == 2:\n",
    "        test_image_rgb = cv2.cvtColor(lama_array, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        test_image_rgb = lama_array\n",
    "        \n",
    "    print(f\"📸 Test-Bild bereit: {test_image_rgb.shape}\")\n",
    "else:\n",
    "    print(\"⚠️  Laden Sie zuerst das Test-Bild!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Computer Vision Applications Demo\n",
    "\n",
    "print(\"🎯 Anwendung aller CV-Algorithmen auf unser Test-Bild:\")\n",
    "\n",
    "# Stelle sicher, dass wir ein RGB-Bild haben\n",
    "if len(lama_array.shape) == 2:\n",
    "    demo_image = cv2.cvtColor(lama_array, cv2.COLOR_GRAY2RGB)\n",
    "else:\n",
    "    demo_image = lama_array.copy()\n",
    "\n",
    "# 1. Canny Edge Detection\n",
    "edges = canny_edge_detection(demo_image)\n",
    "\n",
    "# 2. Contour Detection\n",
    "contour_result, contours = detect_contours(demo_image)\n",
    "\n",
    "# 3. K-Means Segmentation\n",
    "segmented = kmeans_segmentation(demo_image, k=4)\n",
    "\n",
    "# 4. SIFT Feature Detection\n",
    "sift_result, keypoints, descriptors = detect_sift_features(demo_image)\n",
    "\n",
    "# Große Visualisierung aller Ergebnisse\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(demo_image)\n",
    "axes[0, 0].set_title('🖼️ Original Bild', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Canny Edges\n",
    "axes[0, 1].imshow(edges, cmap='gray')\n",
    "axes[0, 1].set_title('⚡ Canny Edge Detection', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Contours\n",
    "axes[0, 2].imshow(contour_result)\n",
    "axes[0, 2].set_title(f'🎯 Contours ({len(contours)} gefunden)', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# K-Means Segmentation\n",
    "axes[1, 0].imshow(segmented)\n",
    "axes[1, 0].set_title('🧩 K-Means Segmentation', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# SIFT Features\n",
    "axes[1, 1].imshow(sift_result)\n",
    "axes[1, 1].set_title(f'🔍 SIFT Features ({len(keypoints)} gefunden)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Kombiniertes Ergebnis\n",
    "combined = demo_image.copy()\n",
    "# Canny Edges als rote Overlay\n",
    "combined[edges > 0] = [255, 0, 0]\n",
    "axes[1, 2].imshow(combined)\n",
    "axes[1, 2].set_title('🎨 Kombiniert: Original + Edges', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiken ausgeben\n",
    "print(f\"\\n📊 Computer Vision Analyse-Ergebnisse:\")\n",
    "print(f\"   🖼️  Bildgröße: {demo_image.shape}\")\n",
    "print(f\"   ⚡ Canny Edges: {np.sum(edges > 0):,} Pixel ({np.sum(edges > 0)/edges.size*100:.1f}%)\")\n",
    "print(f\"   🎯 Konturen: {len(contours)} Objekte erkannt\")\n",
    "print(f\"   🔍 SIFT Features: {len(keypoints)} charakteristische Punkte\")\n",
    "print(f\"   🧩 Segmentierung: 4 Bereiche unterschieden\")\n",
    "\n",
    "# Größte Kontur analysieren\n",
    "if contours:\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    contour_area = cv2.contourArea(largest_contour)\n",
    "    image_area = demo_image.shape[0] * demo_image.shape[1]\n",
    "    print(f\"   📐 Größtes Objekt: {contour_area:.0f} Pixel ({contour_area/image_area*100:.1f}% des Bildes)\")\n",
    "\n",
    "print(f\"\\n💡 SIFT Descriptor Info:\")\n",
    "if descriptors is not None:\n",
    "    print(f\"   📊 Descriptor Matrix: {descriptors.shape}\")\n",
    "    print(f\"   🔢 Pro Feature: {descriptors.shape[1]} Dimensionen\")\n",
    "    print(f\"   💾 Speicher: {descriptors.nbytes:,} Bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 Interaktive Computer Vision Experimente\n",
    "\n",
    "### 💻 Jupyter Widgets für Live-Demos\n",
    "\n",
    "Testen Sie verschiedene CV-Parameter interaktiv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎮 Interaktive Canny Edge Detection\n",
    "\n",
    "def interactive_canny(low_threshold=50, high_threshold=150, blur_kernel=1):\n",
    "    \"\"\"\n",
    "    🎯 Interaktive Canny Edge Detection mit Parameter-Tuning\n",
    "    \"\"\"\n",
    "    # Bild vorbereiten\n",
    "    img = demo_image.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Optional: Blur anwenden\n",
    "    if blur_kernel > 1:\n",
    "        gray = cv2.GaussianBlur(gray, (blur_kernel, blur_kernel), 0)\n",
    "    \n",
    "    # Canny Edge Detection\n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    \n",
    "    # Visualisierung\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('🖼️ Original', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Graustufen (mit Blur)\n",
    "    axes[1].imshow(gray, cmap='gray')\n",
    "    title = f'📊 Graustufen' + (f' + Blur({blur_kernel}x{blur_kernel})' if blur_kernel > 1 else '')\n",
    "    axes[1].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Canny Edges\n",
    "    axes[2].imshow(edges, cmap='gray')\n",
    "    axes[2].set_title(f'⚡ Canny (L:{low_threshold}, H:{high_threshold})', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiken\n",
    "    edge_count = np.sum(edges > 0)\n",
    "    edge_percentage = (edge_count / edges.size) * 100\n",
    "    \n",
    "    print(f\"📊 Canny Statistiken:\")\n",
    "    print(f\"   Threshold: Low={low_threshold}, High={high_threshold}\")\n",
    "    print(f\"   Edge Pixel: {edge_count:,} ({edge_percentage:.2f}%)\")\n",
    "    print(f\"   Blur Kernel: {blur_kernel}x{blur_kernel}\")\n",
    "\n",
    "# Widget erstellen\n",
    "print(\"🎮 Interaktive Canny Edge Detection:\")\n",
    "print(\"• Low Threshold: Minimaler Gradient für Edge-Kandidaten\")\n",
    "print(\"• High Threshold: Minimaler Gradient für sichere Edges\")  \n",
    "print(\"• Blur Kernel: Rauschunterdrückung (ungerade Zahlen)\")\n",
    "\n",
    "interactive_canny_widget = interact(\n",
    "    interactive_canny,\n",
    "    low_threshold=widgets.IntSlider(\n",
    "        value=50, min=10, max=200, step=10,\n",
    "        description='Low Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    high_threshold=widgets.IntSlider(\n",
    "        value=150, min=50, max=300, step=10,\n",
    "        description='High Threshold:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    blur_kernel=widgets.IntSlider(\n",
    "        value=1, min=1, max=15, step=2,\n",
    "        description='Blur Kernel:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n💡 Tuning-Tipps:\")\n",
    "print(\"• Niedrige Thresholds = mehr Edges (mehr Rauschen)\")\n",
    "print(\"• Hohe Thresholds = weniger Edges (nur starke Kanten)\")\n",
    "print(\"• Blur reduziert Rauschen, kann aber Details verwischen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 Streamlit Computer Vision App\n",
    "\n",
    "### 💻 Professionelle CV-Anwendung erstellen\n",
    "\n",
    "Sie können eine **vollständige Computer Vision App** mit Streamlit erstellen:\n",
    "\n",
    "```bash\n",
    "# Im Terminal ausführen:\n",
    "cd 06_Computer_Vision_NLP\n",
    "streamlit run 06_02_streamlit_cv_apps.py\n",
    "```\n",
    "\n",
    "### 🌟 Features der CV-App:\n",
    "- ✅ **Multiple CV-Algorithmen:** Edge Detection, Object Detection, Segmentation\n",
    "- ✅ **Eigene Bilder hochladen** und analysieren\n",
    "- ✅ **Parameter-Tuning** in Echtzeit\n",
    "- ✅ **Feature Detection** (SIFT, ORB, Harris Corners)\n",
    "- ✅ **Filter-Vergleiche** side-by-side\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Zusammenfassung & Key Takeaways\n",
    "\n",
    "### ✅ Was Sie gelernt haben:\n",
    "\n",
    "#### 1. 🧮 **Mathematische Grundlagen**\n",
    "- **Convolution Operation:** Wie Filter mathematisch funktionieren\n",
    "- **2D-Convolution:** Anwendung auf Bilder (H×W×C)\n",
    "- **Parameter-Berechnung:** Kernel Size, Stride, Padding\n",
    "- **Eigene Implementation:** Convolution von Grund auf verstehen\n",
    "\n",
    "#### 2. 🔧 **Klassische Filter**\n",
    "- **Identitätsfilter:** Baseline und Verständnis\n",
    "- **Edge Detection:** Roberts, Sobel (Light/Strong)\n",
    "- **Filter-Comparison:** Manuelle vs. OpenCV Implementation\n",
    "- **Parameter-Tuning:** Optimale Einstellungen finden\n",
    "\n",
    "#### 3. 🚀 **Moderne Computer Vision**\n",
    "- **Canny Edge Detection:** State-of-the-Art Kantendetection\n",
    "- **Contour Detection:** Objektumrisse automatisch finden\n",
    "- **Image Segmentation:** K-Means Clustering für Bereiche\n",
    "- **Feature Detection:** SIFT für charakteristische Punkte\n",
    "\n",
    "#### 4. 🎮 **Praktische Anwendungen**\n",
    "- **OpenCV Integration:** Professionelle CV-Library nutzen\n",
    "- **Interactive Widgets:** Parameter live anpassen\n",
    "- **Streamlit Apps:** Portfolio-ready CV-Anwendungen\n",
    "- **Real-world Workflows:** Von Preprocessing bis Analyse\n",
    "\n",
    "### 💡 **Von Theorie zur Praxis:**\n",
    "\n",
    "```python\n",
    "# Klassische Implementierung (Educational)\n",
    "def conv(image, filter_kernel):\n",
    "    # Manual convolution for understanding\n",
    "    \n",
    "# Moderne Implementierung (Production)\n",
    "edges = cv2.Canny(image, 50, 150)\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "\n",
    "### 🎯 **Nächste Schritte:**\n",
    "\n",
    "In den kommenden Notebooks vertiefen wir:\n",
    "- **06.3:** Data Augmentation für robuste Modelle\n",
    "- **06.4:** Transfer Learning mit vortrainierten CNNs\n",
    "- **Portfolio:** Eigene CV-Projekte für Bewerbungen\n",
    "\n",
    "---\n",
    "\n",
    "### 🏆 **Portfolio-Projekt Challenge:**\n",
    "\n",
    "1. **📸 Eigenes Bild-Dataset** sammeln (Selfies, Objekte, etc.)\n",
    "2. **🔧 CV-Pipeline** entwickeln (Preprocessing → Detection → Analysis)\n",
    "3. **🎮 Streamlit-App** erstellen mit Upload-Funktionalität\n",
    "4. **📊 Analyse-Dashboard** mit Statistiken und Visualisierungen\n",
    "5. **🚀 GitHub Repository** für Ihr Portfolio\n",
    "\n",
    "**💪 Advanced Challenge:** Kombinieren Sie Edge Detection + Contour Detection + Feature Detection für ein intelligentes Objekt-Analyse-Tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_sobel2 = [[-1,-1,-1], [-1,8,-1], [-1,-1,-1]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sobel2)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bildschärfen\n",
    "\n",
    "Der nächste Filter dient, wie der Name bereits vermuten lässt, dazu, dass Konturen im Bild schärfer werden.\n",
    "\n",
    "$\\left\\lbrack\\begin{array}{ccc} 0&-1&0\\\\ -1&5&-1\\\\ 0&-1&0 \\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_kern_sharp = [[0,-1,0], [-1,5,-1], [0,-1,0]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_sharp)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blur / Unschärfe\n",
    "\n",
    "Die letzen beiden Filter dienen dazu, das Bild zu glätten. Der erste Filter wird auch als Box-Linear-Filter bezeichnet und ist verhätlinismäßig relativ simple aufgebaut. Der zweite Filter basiert auf einer Gaußverteilung und wird daher als Gauß-Filter bezeichnet.\n",
    "\n",
    "Box-Linear-Filter: $\\frac{1}{9} \\left\\lbrack\\begin{array}{ccc}1&1&1\\\\ 1&1&1\\\\ 1&1&1\\end{array}\\right\\rbrack$\n",
    "\n",
    "Gauß-Filter: $\\frac{1}{16} \\left\\lbrack\\begin{array}{ccc}1&2&1\\\\ 2&4&2\\\\ 1&2&1\\end{array}\\right\\rbrack$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_blf = [[1/9, 1/9, 1/9], [1/9, 1/9, 1/9], [1/9, 1/9, 1/9]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_blf)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kern_gauss = [[1/16, 2/16, 1/16], [2/16, 4/16, 2/16], [1/16, 2/16, 1/16]]\n",
    "\n",
    "plt.figure(figsize=(figure_inches, figure_inches))\n",
    "data = read_image_as_array('images/lama.png', (500,500))\n",
    "filtered_data = conv(data, filter_kern_gauss)\n",
    "\n",
    "plt.imshow(filtered_data, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB-Bilder\n",
    "\n",
    "Farbige Bilder können in der Regel durch RGB-Bilder dargestellt werden, wobei $d$ gleich 3 ist und enthält:\n",
    "\n",
    "- R (rot), \n",
    "- G (grün),\n",
    "- B (blau)\n",
    "\n",
    "Werte für alle Pixel in einem Bild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lama = Image.open('images/lama.png')\n",
    "lama = np.array(lama)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Lama image 768x1024', fontsize = 15)\n",
    "ax.imshow(lama, interpolation='nearest')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general deep learning (and in tensorflow) Conv-layers will \n",
    "# regard all channels and therefore use \"cubic\" filter\n",
    "\n",
    "# The filter used here in the example down below is only using d=1 (two - dimensional) of the \n",
    "# rgb image (therefore red), you can change [:,:,0] to [:,:,1] (green) and [:,:,2] (blue)!\n",
    "# Try it! :)\n",
    "\n",
    "prewitt_x =  np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "\n",
    "lama_x_prew = signal.convolve2d(lama[:,:,0], prewitt_x, boundary='symm', mode='same')\n",
    "lama_x_prew = np.absolute(lama_x_prew)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n",
    "ax.set_title('Horizontale Ableitung des Lama Bildes', fontsize = 15)\n",
    "ax.imshow(lama_x_prew, interpolation='nearest', cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die Faltungsschicht (engl. Convolutional Layer)\n",
    "\n",
    "<img src=\"images/featuremaps.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Eine Faltungsschicht, welche die erste Schicht im Netzwerk sein könnte, ist im Bild oben dargestellt. Ihr Kernel oder Filter mit den Dimensionen $K_x \\times K_y \\times d$ enthält Gewichte, die während des Trainings aktualisiert werden und auch die Darstellung der Bilder verändern. Eine Aktivierungskarte (engl. activation map) entspricht einer Faltungsoperation mit einem bestimmten Filter und dem zugehörigen Eingangsbild oder den räumlichen Daten der vorherigen Schicht. In den meisten Fällen werden nicht nur ein, sondern mehrere Filter in einer Faltungsschicht gelernt, so dass es mehrere Aktivierungskarten gibt. In diesem speziellen Fall scheint die Ausgabegröße dieser Faltungsschicht im Vergleich zur Eingabegröße größer geworden zu sein. Infolgedessen werden häufig Pooling-Operationen angehängt, um die Daten innerhalb des Netzwerks zu reduzieren. Die nächste Schicht erhält dann wieder räumliche Informationen und verwendet Filter, um die räumlichen Informationen zu extrahieren und zu verändern.\n",
    "\n",
    "\n",
    "**Idea**: _`Spärliche Verbindungen (engl. Sparse Connections)` (nicht vollständig verbundene Schichten wie bei einem MLP) sind als Kernel für große Datenstrukturen gegeben. Die Anzahl der lernbaren Gewichte sinkt!_\n",
    "\n",
    "Vergleichen wir eine standardmäßige voll verbundene Schicht (engl. fully connected layer) eines MLP mit einer Faltungsschicht für ein reguläres farbiges Bild der Größe $256\\times256\\times3$:\n",
    "- Erstes Hidden Layer in einer voll verbundenen Schicht:\n",
    "    - Input Neuronen $\\rightarrow$ $256*256*3$\n",
    "    - Beginnen Sie z. B. mit der Hälfte der Neuronen im ersten Hidden Layer $\\rightarrow$ $128*256*3$\n",
    "    - Ergebnisse in Gewichte und Biases $\\rightarrow$ $256*256*3*128*256*3 + 128*256*3 = 19.327.451.136$ Parameters\n",
    "\n",
    "        \n",
    "- Erste Faltungsschicht in einem faltigen neuronalen Netz: Standard 256 Filter (vernünftige Größe) der Größe $3\\times3\\times3$ \n",
    "    - Gewichte und Biases $\\rightarrow$ $256 * 3 * 3 *3 + 256 = 7.168 $ Parameters\n",
    "    \n",
    "Trotzdem brauchen Faltungen mit räumlichen Blöcken wie in der obigen Abbildung noch Zeit, um verarbeitet zu werden.\n",
    "Lokale Informationen werden nur nicht wie globale Abhängigkeiten in Hidden Layers verwendet!\n",
    "\n",
    "Die **Vorteile** einer Faltungsschicht (`CONV`) gegenüber einer vollverknüpften Schicht sind die folgenden\n",
    " - Weniger Parameter für das Training\n",
    " - Nutzung der lokalen Strukturen des Bildes\n",
    " - Unabhängig von der Position des Merkmals im Bild\n",
    " \n",
    "**Nachteile** von Faltungsschichten (`CONV`):\n",
    " - Informationen müssen räumliche Abhängigkeiten haben (wie bei einem menschlich erkennbaren Bild)\n",
    "\n",
    "Beim Stapeln mehrerer Faltungsschichten hat ein Kernel der folgenden Faltungsschicht die Form $K_x \\times K_y \\times d$, wobei $d$ die Anzahl der Kanäle der vorherigen Schicht ist. Die Anzahl der Kanäle ist gegeben durch die Anzahl der verschiedenen Filter, die in der Faltungsschicht verwendet werden. Definiert man also eine Faltungsschicht mit z. B. $nb\\_filters=64$, so legt man die dritte Dimension eines Filters in der nächsten Schicht fest. Denn im zweidimensionalen Fall expandiert der Filter immer auf die vorherige Kanaldimension. Betrachtet man CNNs für die Videoanalyse oder für Zeitreihen, so stößt man auf 3-dimensionale Faltungsschichten, die sich nicht nur in den Bilddimensionen bewegen, sondern in einer dritten Dimension (in diesem Fall: Zeit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die Poolingsschicht (engl. pooling layer)\n",
    "\n",
    "<img src=\"images/maxpool.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "                                     Quelle: http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "\n",
    "Die Pooling Schicht ist ein Filter wie alle anderen Filter im neuronalen Faltungsnetzwerk. Allerdings mit der Ausnahme, dass sie ihre Gewichte nicht aktualisiert und eine feste Funktionsoperation durchführt. Die häufigste Pooling-Operation ist das Max-Pooling. Wie der Name schon sagt, wird im Bereich des Kerns nur der Maximalwert weitergegeben. Normalerweise entspricht der Stride den Dimensionen des Kernels. Das Max-Pooling wird nur auf die Höhe und Breite des Bildes angewendet, so dass die Kanaldimensionen nicht betroffen sind. Es wird verwendet, um räumliche Informationen zu reduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 5.2.2:</b> Implementieren Sie die Funktion <code>max_pool</code> die Maxpooling durchführt. Gegeben ist wieder ein Grauwertbild <code>image_data</code>, d.h. es besitzt nur einen Kanal und Sie können annehmen, dass das Bilder wieder als eine Liste von Listen übergeben wird. Außerdem ist die Größe des Filters <code>filter_size</code> als Tupel und die <code>stride</code> als <code>int</code> gegeben.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(image_data:list, filter_size:tuple, stride:int)->list:\n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = [[0,0,0,0,0], [0,1,1,1,0], [0,0,2,0,0], [0,3,3,3,0], [0,0,0,0,0], [0,0,0,0,0]]\n",
    "test_filter_size = (2,2)\n",
    "stride = 2\n",
    "test_result = [[1,1],[3,3], [0,0]]\n",
    "\n",
    "# The folgende Zeile erzeugt einen Fehler, wenn die Ausgabe der Methode nicht mit der erwarteten übereinstimmt \n",
    "found = max_pool(test_input_data, test_filter_size, stride)\n",
    "assert found == test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU - Schicht oder Aktivierung\n",
    "Die \"RELU\"-Schicht oder Aktivierung verwendet eine elementweise Aktivierungsfunktion auf das Raumvolumen an, wie auf jeden Knoten in einer Hidden Layer. Die Funktion kann als $max(0,x)$ angegeben werden und ist unten dargestellt. Betrachten Sie $\\sigma(x)$ als die Aktivierungsfunktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x:float)->float:\n",
    "    return np.maximum(0,x)\n",
    "x = np.linspace(-10, 10, num = 1000)\n",
    "\n",
    "plt.figure(2, figsize=(10,3.5))\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.title('The ReLU activation')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\sigma(x)$')\n",
    "plt.tight_layout()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "\n",
    "Die folgende Animation zeigt recht gut, wie ein Faltungsnetzwerk (engl. convolutional network) anhand des `MNIST`-Datensatzes funktioniert.\n",
    "Nachdem die Faltungsschichten die Repräsentation der Bilder verändert haben, werden die endgültigen mehrdimensionalen Blöcke in ein langes Array gelegt (die Operation wird \"Flattening\" genannt) und an voll verbundene Schichten eines neuronalen Netzes weitergeleitet.\n",
    "\n",
    "[MNIST-CLassification](http://scs.ryerson.ca/~aharley/vis/conv/flat.html)\n",
    "\n",
    "#### Receptive Field\n",
    "\n",
    "In der Animation bzw. Simulation von MNIST werden Abhängigkeiten, die als Linien zwischen mehr als zwei Schichten dargestellt werden, nicht abgebildet.\n",
    "Dennoch ist es möglich, Beziehungen zwischen beliebigen Schichten innerhalb des Netzes darzustellen. Dadurch ist es möglich, ein gewisses Wissen oder eine Idee über die Anzahl der Faltungsschichten zu erhalten, die für eine Anwendung oder Aufgabe verwendet werden sollten. Betrachten Sie drei übereinander gestapelte Faltungsschichten wie im Bild unten. Ein Wert in der grünen Schicht bezieht sich auf 9 Eingangswerte. Folglich summiert sich ein Wert in der gelben Schicht auf 9 in der grünen Schicht. Ein Eintrag in der gelben Schicht wird also von mehr Werten beeinflusst als die grünen Aktivierungseinträge in Bezug auf das Eingangsbild. Dieser Bereich ist gelb dargestellt und deckt 49 Werte des Eingangsbildes ab. Um die Dimensionen während der Faltungen wie in üblichen CNNs beizubehalten, wurde ein Padding verwendet, um die Dimensionen der Matrix gleich zu halten. Die `Initialmatrix` ist dann von der Größe $7 \\times 7$.\n",
    "\n",
    "<img src=\"images/ReceptiveField.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "    Quelle:https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 5.2.3:</b> Was ist der Hauptunterschied zwischen einer Faltungsschicht (engl. convolutional layer) und einer vollverknüpften Schicht (engl. fully-connected layer) und warum werden überhaupt Filter verwendet?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Ihre Antwort:</b></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python-amalea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
