{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7708c7cb",
   "metadata": {},
   "source": [
    "# üöÄ MLOps und Model Deployment\n",
    "### Woche 7, Notebook 1: Von der Entwicklung zur Produktion\n",
    "\n",
    "**Lernziele:**\n",
    "- üîÑ **MLOps-Pipeline**: Verstehen moderner ML-Deployment-Praktiken\n",
    "- üê≥ **Docker & Containerisierung**: Reproduzierbare ML-Umgebungen erstellen\n",
    "- üåê **API-Deployment**: ML-Modelle √ºber REST APIs bereitstellen\n",
    "- üìä **Model Monitoring**: Performance-√úberwachung in Produktion\n",
    "- ‚ö° **CI/CD f√ºr ML**: Automatisierte Deployment-Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**Was Sie nach diesem Notebook k√∂nnen:**\n",
    "- ML-Modelle f√ºr Produktionsumgebungen vorbereiten\n",
    "- Docker-Container f√ºr ML-Services erstellen\n",
    "- REST APIs f√ºr Model Serving implementieren\n",
    "- Model Performance √ºberwachen und loggen\n",
    "- Deployment-Strategien f√ºr verschiedene Umgebungen anwenden\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Praxis-Tipp**: In diesem Notebook erstellen wir ein vollst√§ndiges Deployment-Setup f√ºr Ihr Abschlussprojekt!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837f8c7",
   "metadata": {},
   "source": [
    "## üîÑ Was ist MLOps?\n",
    "\n",
    "**MLOps** (Machine Learning Operations) kombiniert Machine Learning, DevOps und Data Engineering, um ML-Systeme zuverl√§ssig und effizient in Produktion zu bringen.\n",
    "\n",
    "### üéØ Kernprinzipien von MLOps\n",
    "\n",
    "1. **üîÅ Automatisierung**: Von Training bis Deployment\n",
    "2. **üîç Versionierung**: Code, Daten und Modelle\n",
    "3. **üìä Monitoring**: Performance und Data Drift\n",
    "4. **üß™ Testing**: Validierung und A/B-Tests\n",
    "5. **üöÄ Continuous Deployment**: Schnelle, sichere Releases\n",
    "\n",
    "### üìà MLOps vs. Traditional DevOps\n",
    "\n",
    "| Aspekt | Traditional DevOps | MLOps |\n",
    "|--------|-------------------|-------|\n",
    "| **Artefakte** | Code, Configs | Code + Daten + Modelle |\n",
    "| **Testing** | Unit/Integration Tests | Data/Model Validation |\n",
    "| **Deployment** | Statische Anwendungen | Dynamische ML-Services |\n",
    "| **Monitoring** | System Metrics | Model Performance + Drift |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è MLOps Toolchain (2025)\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data] --> B[Preprocessing]\n",
    "    B --> C[Training]\n",
    "    C --> D[Validation]\n",
    "    D --> E[Containerization]\n",
    "    E --> F[Deployment]\n",
    "    F --> G[Monitoring]\n",
    "    G --> A\n",
    "```\n",
    "\n",
    "**Beispiel-Tools:**\n",
    "- **Experiment Tracking**: MLflow, Weights & Biases\n",
    "- **Model Registry**: MLflow, DVC\n",
    "- **Containerization**: Docker, Kubernetes\n",
    "- **Deployment**: FastAPI, Flask, Streamlit\n",
    "- **Monitoring**: Prometheus, Grafana, Evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup und Imports f√ºr MLOps\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# ML und Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# API und Web Framework\n",
    "try:\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "    import uvicorn\n",
    "    print(\"‚úÖ FastAPI verf√ºgbar\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è FastAPI nicht installiert - kann √ºber pip install fastapi uvicorn installiert werden\")\n",
    "\n",
    "# Monitoring und Logging\n",
    "try:\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "    print(\"‚úÖ MLflow verf√ºgbar\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è MLflow nicht installiert - kann √ºber pip install mlflow installiert werden\")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Basis MLOps-Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"üìÖ Setup-Zeit: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Logging Setup f√ºr Produktionsumgebung\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f1481",
   "metadata": {},
   "source": [
    "## üîÑ MLOps-Lifecycle in der Praxis\n",
    "\n",
    "### 1Ô∏è‚É£ Model Development Pipeline\n",
    "\n",
    "Wir implementieren eine vollst√§ndige MLOps-Pipeline mit einem **Iris Classification Modell** als Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Model Development mit Tracking\n",
    "\n",
    "class MLOpsModelPipeline:\n",
    "    \"\"\"\n",
    "    Vollst√§ndige MLOps-Pipeline f√ºr Model Development und Deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"iris_classification\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model = None\n",
    "        self.model_version = None\n",
    "        self.model_path = None\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Daten laden und vorbereiten\"\"\"\n",
    "        logger.info(\"üìä Lade und bereite Daten vor...\")\n",
    "        \n",
    "        # Iris Dataset laden\n",
    "        iris = load_iris()\n",
    "        X, y = iris.data, iris.target\n",
    "        \n",
    "        # Train/Test Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        self.X_train, self.X_test = X_train, X_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "        self.feature_names = iris.feature_names\n",
    "        self.target_names = iris.target_names\n",
    "        \n",
    "        logger.info(f\"‚úÖ Daten geladen: {len(X_train)} Training, {len(X_test)} Test Samples\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_model(self, n_estimators=100):\n",
    "        \"\"\"Model Training mit Tracking\"\"\"\n",
    "        logger.info(\"üèãÔ∏è Starte Model Training...\")\n",
    "        \n",
    "        # Model Training\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        \n",
    "        # Model speichern\n",
    "        self.model = model\n",
    "        self.model_path = f\"models/iris_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        joblib.dump(model, self.model_path)\n",
    "        \n",
    "        # Logging (falls MLflow verf√ºgbar)\n",
    "        training_info = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"algorithm\": \"RandomForest\",\n",
    "            \"accuracy\": accuracy,\n",
    "            \"train_size\": len(self.X_train),\n",
    "            \"test_size\": len(self.X_test),\n",
    "            \"model_path\": self.model_path\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"‚úÖ Model trainiert! Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"üíæ Model gespeichert: {self.model_path}\")\n",
    "        \n",
    "        return model, accuracy, training_info\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Detaillierte Model Evaluation\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model muss zuerst trainiert werden!\")\n",
    "        \n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(\n",
    "            self.y_test, y_pred, \n",
    "            target_names=self.target_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        print(\"üìä Model Evaluation Report:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(classification_report(self.y_test, y_pred, target_names=self.target_names))\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Pipeline initialisieren und ausf√ºhren\n",
    "pipeline = MLOpsModelPipeline()\n",
    "X_train, X_test, y_train, y_test = pipeline.load_and_prepare_data()\n",
    "model, accuracy, training_info = pipeline.train_model()\n",
    "\n",
    "print(f\"üéØ Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üî¢ Feature Names: {pipeline.feature_names}\")\n",
    "print(f\"üè∑Ô∏è  Target Classes: {pipeline.target_names}\")\n",
    "\n",
    "# Detaillierte Evaluation\n",
    "evaluation_report = pipeline.evaluate_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46251443",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Model Validation und Testing\n",
    "\n",
    "Bevor wir ein Modell in Produktion bringen, m√ºssen wir es gr√ºndlich validieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72690cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Model Validation und Testing\n",
    "\n",
    "class ModelValidator:\n",
    "    \"\"\"\n",
    "    Klasse f√ºr umfassende Model Validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_test, y_test, feature_names, target_names):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.feature_names = feature_names\n",
    "        self.target_names = target_names\n",
    "    \n",
    "    def validate_input_schema(self, X_input):\n",
    "        \"\"\"Validiert das Input-Schema\"\"\"\n",
    "        expected_features = len(self.feature_names)\n",
    "        \n",
    "        if X_input.shape[1] != expected_features:\n",
    "            raise ValueError(\n",
    "                f\"Input hat {X_input.shape[1]} Features, \"\n",
    "                f\"erwartet werden {expected_features}\"\n",
    "            )\n",
    "        \n",
    "        logger.info(\"‚úÖ Input Schema Validation erfolgreich\")\n",
    "        return True\n",
    "    \n",
    "    def validate_prediction_range(self, predictions):\n",
    "        \"\"\"Validiert den Vorhersagebereich\"\"\"\n",
    "        valid_classes = set(range(len(self.target_names)))\n",
    "        pred_classes = set(predictions)\n",
    "        \n",
    "        if not pred_classes.issubset(valid_classes):\n",
    "            invalid_classes = pred_classes - valid_classes\n",
    "            raise ValueError(f\"Ung√ºltige Klassen vorhergesagt: {invalid_classes}\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Prediction Range Validation erfolgreich\")\n",
    "        return True\n",
    "    \n",
    "    def performance_regression_test(self, min_accuracy=0.85):\n",
    "        \"\"\"Performance Regression Test\"\"\"\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        \n",
    "        if accuracy < min_accuracy:\n",
    "            raise ValueError(\n",
    "                f\"Model Performance zu niedrig: {accuracy:.4f} < {min_accuracy}\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"‚úÖ Performance Test bestanden: {accuracy:.4f} >= {min_accuracy}\")\n",
    "        return accuracy\n",
    "    \n",
    "    def run_all_validations(self, min_accuracy=0.85):\n",
    "        \"\"\"F√ºhrt alle Validierungen durch\"\"\"\n",
    "        print(\"üß™ Starte umfassende Model Validation...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Schema Validation\n",
    "            self.validate_input_schema(self.X_test)\n",
    "            \n",
    "            # Predictions generieren\n",
    "            predictions = self.model.predict(self.X_test)\n",
    "            \n",
    "            # Range Validation\n",
    "            self.validate_prediction_range(predictions)\n",
    "            \n",
    "            # Performance Test\n",
    "            accuracy = self.performance_regression_test(min_accuracy)\n",
    "            \n",
    "            print(f\"üéâ Alle Validierungen erfolgreich! Model ist produktionsbereit.\")\n",
    "            print(f\"üéØ Finale Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            return True, accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation fehlgeschlagen: {str(e)}\")\n",
    "            return False, None\n",
    "\n",
    "# Model Validation durchf√ºhren\n",
    "validator = ModelValidator(\n",
    "    model=pipeline.model,\n",
    "    X_test=pipeline.X_test,\n",
    "    y_test=pipeline.y_test,\n",
    "    feature_names=pipeline.feature_names,\n",
    "    target_names=pipeline.target_names\n",
    ")\n",
    "\n",
    "validation_passed, final_accuracy = validator.run_all_validations(min_accuracy=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c66657",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Model Serving mit FastAPI\n",
    "\n",
    "Jetzt erstellen wir eine REST API f√ºr unser Modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Model Serving mit FastAPI\n",
    "\n",
    "# Datenmodelle f√ºr API\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Eingabe-Schema f√ºr Iris Prediction\"\"\"\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"sepal_length\": 5.1,\n",
    "                \"sepal_width\": 3.5,\n",
    "                \"petal_length\": 1.4,\n",
    "                \"petal_width\": 0.2\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Ausgabe-Schema f√ºr Prediction\"\"\"\n",
    "    prediction: int\n",
    "    prediction_label: str\n",
    "    confidence: float\n",
    "    model_version: str\n",
    "    timestamp: str\n",
    "\n",
    "# Model Serving Klasse\n",
    "class IrisModelServer:\n",
    "    \"\"\"\n",
    "    Production-ready Model Server\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, target_names: List[str]):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.target_names = target_names\n",
    "        self.model_version = os.path.basename(model_path)\n",
    "        logger.info(f\"‚úÖ Model geladen: {model_path}\")\n",
    "    \n",
    "    def predict(self, features: IrisFeatures) -> PredictionResponse:\n",
    "        \"\"\"Einzelne Prediction\"\"\"\n",
    "        try:\n",
    "            # Features zu Array konvertieren\n",
    "            X = np.array([[\n",
    "                features.sepal_length,\n",
    "                features.sepal_width,\n",
    "                features.petal_length,\n",
    "                features.petal_width\n",
    "            ]])\n",
    "            \n",
    "            # Prediction\n",
    "            prediction = self.model.predict(X)[0]\n",
    "            probabilities = self.model.predict_proba(X)[0]\n",
    "            confidence = float(np.max(probabilities))\n",
    "            \n",
    "            # Response erstellen\n",
    "            response = PredictionResponse(\n",
    "                prediction=int(prediction),\n",
    "                prediction_label=self.target_names[prediction],\n",
    "                confidence=confidence,\n",
    "                model_version=self.model_version,\n",
    "                timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Prediction: {prediction} ({self.target_names[prediction]}) - Confidence: {confidence:.4f}\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction Fehler: {str(e)}\")\n",
    "            raise HTTPException(status_code=500, detail=f\"Prediction fehlgeschlagen: {str(e)}\")\n",
    "\n",
    "# Model Server initialisieren\n",
    "model_server = IrisModelServer(\n",
    "    model_path=pipeline.model_path,\n",
    "    target_names=pipeline.target_names.tolist()\n",
    ")\n",
    "\n",
    "# Test Prediction\n",
    "test_features = IrisFeatures(\n",
    "    sepal_length=5.1,\n",
    "    sepal_width=3.5,\n",
    "    petal_length=1.4,\n",
    "    petal_width=0.2\n",
    ")\n",
    "\n",
    "test_prediction = model_server.predict(test_features)\n",
    "print(\"üß™ Test Prediction:\")\n",
    "print(f\"   Klasse: {test_prediction.prediction_label}\")\n",
    "print(f\"   Confidence: {test_prediction.confidence:.4f}\")\n",
    "print(f\"   Model Version: {test_prediction.model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb2245",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ FastAPI Application erstellen\n",
    "\n",
    "Jetzt erstellen wir die vollst√§ndige FastAPI Anwendung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d89de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ FastAPI Application f√ºr Model Serving\n",
    "\n",
    "# FastAPI App erstellen\n",
    "app = FastAPI(\n",
    "    title=\"Iris Classification API\",\n",
    "    description=\"Production-ready ML API f√ºr Iris Klassifikation\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global model server\n",
    "global_model_server = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Initialisierung beim App-Start\"\"\"\n",
    "    global global_model_server\n",
    "    global_model_server = model_server\n",
    "    logger.info(\"üöÄ Iris Classification API gestartet\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Health Check Endpoint\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Iris Classification API\",\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_version\": global_model_server.model_version if global_model_server else None,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Detaillierter Health Check\"\"\"\n",
    "    if global_model_server is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model Server nicht verf√ºgbar\")\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": True,\n",
    "        \"model_version\": global_model_server.model_version,\n",
    "        \"target_classes\": global_model_server.target_names,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(features: IrisFeatures):\n",
    "    \"\"\"Iris Klassifikation Endpoint\"\"\"\n",
    "    if global_model_server is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model Server nicht verf√ºgbar\")\n",
    "    \n",
    "    return global_model_server.predict(features)\n",
    "\n",
    "@app.post(\"/predict/batch\")\n",
    "async def predict_batch(features_list: List[IrisFeatures]):\n",
    "    \"\"\"Batch Prediction Endpoint\"\"\"\n",
    "    if global_model_server is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model Server nicht verf√ºgbar\")\n",
    "    \n",
    "    predictions = []\n",
    "    for features in features_list:\n",
    "        prediction = global_model_server.predict(features)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"batch_size\": len(predictions),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ FastAPI Application erstellt!\")\n",
    "print(\"üìù Verf√ºgbare Endpoints:\")\n",
    "print(\"   GET  /              - Root endpoint\")\n",
    "print(\"   GET  /health        - Health check\")\n",
    "print(\"   POST /predict       - Einzelne Prediction\")\n",
    "print(\"   POST /predict/batch - Batch Predictions\")\n",
    "\n",
    "# API Dokumentation anzeigen\n",
    "print(\"\\nüìö API Dokumentation verf√ºgbar unter:\")\n",
    "print(\"   http://localhost:8000/docs (Swagger UI)\")\n",
    "print(\"   http://localhost:8000/redoc (ReDoc)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95bbac",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Dockerization\n",
    "\n",
    "Jetzt erstellen wir Docker-Container f√ºr reproduzierbare Deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0657843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üê≥ Dockerfile und Docker Deployment\n",
    "\n",
    "dockerfile_content = \"\"\"\n",
    "# Production Dockerfile f√ºr ML API\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# System Dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Working Directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Requirements kopieren und installieren\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Anwendungscode kopieren\n",
    "COPY . .\n",
    "\n",
    "# Model Directory erstellen\n",
    "RUN mkdir -p models\n",
    "\n",
    "# Port exposition\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health Check\n",
    "HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n",
    "    CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Anwendung starten\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\"\n",
    "\n",
    "# Docker-Compose f√ºr Development\n",
    "docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  iris-api:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - ENV=development\n",
    "      - LOG_LEVEL=info\n",
    "    volumes:\n",
    "      - ./models:/app/models\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "  # Optional: Monitoring mit Prometheus\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "      - '--storage.tsdb.path=/prometheus'\n",
    "      - '--web.console.libraries=/etc/prometheus/console_libraries'\n",
    "      - '--web.console.templates=/etc/prometheus/consoles'\n",
    "\n",
    "  # Optional: Grafana f√ºr Visualisierung\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    volumes:\n",
    "      - grafana-storage:/var/lib/grafana\n",
    "\n",
    "volumes:\n",
    "  grafana-storage:\n",
    "\"\"\"\n",
    "\n",
    "# Requirements f√ºr Production\n",
    "requirements_content = \"\"\"\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "scikit-learn==1.3.2\n",
    "pandas==2.1.4\n",
    "numpy==1.24.3\n",
    "joblib==1.3.2\n",
    "pydantic==2.5.0\n",
    "python-multipart==0.0.6\n",
    "\"\"\"\n",
    "\n",
    "# Dateien erstellen\n",
    "os.makedirs('docker_deployment', exist_ok=True)\n",
    "\n",
    "with open('docker_deployment/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "with open('docker_deployment/docker-compose.yml', 'w') as f:\n",
    "    f.write(docker_compose_content)\n",
    "\n",
    "with open('docker_deployment/requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"üê≥ Docker-Dateien erstellt:\")\n",
    "print(\"   üìÑ docker_deployment/Dockerfile\")\n",
    "print(\"   üìÑ docker_deployment/docker-compose.yml\")\n",
    "print(\"   üìÑ docker_deployment/requirements.txt\")\n",
    "\n",
    "print(\"\\nüöÄ Deployment Commands:\")\n",
    "print(\"   docker build -t iris-api .\")\n",
    "print(\"   docker run -p 8000:8000 iris-api\")\n",
    "print(\"   docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d1af0",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ Model Monitoring und Logging\n",
    "\n",
    "F√ºr Produktionsumgebungen ist Monitoring essentiell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ffe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Model Monitoring und Performance Tracking\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"\n",
    "    Model Performance Monitor f√ºr Production\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prediction_count = 0\n",
    "        self.prediction_times = []\n",
    "        self.prediction_confidences = []\n",
    "        self.class_distribution = defaultdict(int)\n",
    "        self.errors = []\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def log_prediction(self, prediction_time: float, confidence: float, predicted_class: str):\n",
    "        \"\"\"Prediction Metriken loggen\"\"\"\n",
    "        with self.lock:\n",
    "            self.prediction_count += 1\n",
    "            self.prediction_times.append(prediction_time)\n",
    "            self.prediction_confidences.append(confidence)\n",
    "            self.class_distribution[predicted_class] += 1\n",
    "    \n",
    "    def log_error(self, error_message: str):\n",
    "        \"\"\"Fehler loggen\"\"\"\n",
    "        with self.lock:\n",
    "            self.errors.append({\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'error': error_message\n",
    "            })\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Aktuelle Metriken abrufen\"\"\"\n",
    "        with self.lock:\n",
    "            if not self.prediction_times:\n",
    "                return {\n",
    "                    'total_predictions': 0,\n",
    "                    'avg_prediction_time': 0,\n",
    "                    'avg_confidence': 0,\n",
    "                    'class_distribution': {},\n",
    "                    'error_count': len(self.errors)\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'total_predictions': self.prediction_count,\n",
    "                'avg_prediction_time': np.mean(self.prediction_times),\n",
    "                'max_prediction_time': np.max(self.prediction_times),\n",
    "                'min_prediction_time': np.min(self.prediction_times),\n",
    "                'avg_confidence': np.mean(self.prediction_confidences),\n",
    "                'min_confidence': np.min(self.prediction_confidences),\n",
    "                'class_distribution': dict(self.class_distribution),\n",
    "                'error_count': len(self.errors),\n",
    "                'recent_errors': self.errors[-5:] if self.errors else []\n",
    "            }\n",
    "\n",
    "# Enhanced Model Server mit Monitoring\n",
    "class MonitoredIrisModelServer(IrisModelServer):\n",
    "    \"\"\"\n",
    "    Model Server mit integriertem Monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, target_names: List[str]):\n",
    "        super().__init__(model_path, target_names)\n",
    "        self.monitor = ModelMonitor()\n",
    "    \n",
    "    def predict(self, features: IrisFeatures) -> PredictionResponse:\n",
    "        \"\"\"Prediction mit Monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = super().predict(features)\n",
    "            \n",
    "            # Monitoring Daten loggen\n",
    "            prediction_time = time.time() - start_time\n",
    "            self.monitor.log_prediction(\n",
    "                prediction_time=prediction_time,\n",
    "                confidence=response.confidence,\n",
    "                predicted_class=response.prediction_label\n",
    "            )\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.monitor.log_error(str(e))\n",
    "            raise\n",
    "    \n",
    "    def get_monitoring_data(self):\n",
    "        \"\"\"Monitoring Daten abrufen\"\"\"\n",
    "        return self.monitor.get_metrics()\n",
    "\n",
    "# Monitored Model Server erstellen\n",
    "monitored_server = MonitoredIrisModelServer(\n",
    "    model_path=pipeline.model_path,\n",
    "    target_names=pipeline.target_names.tolist()\n",
    ")\n",
    "\n",
    "# Test mit mehreren Predictions\n",
    "print(\"üß™ Testing mit Monitoring...\")\n",
    "test_cases = [\n",
    "    IrisFeatures(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2),\n",
    "    IrisFeatures(sepal_length=7.0, sepal_width=3.2, petal_length=4.7, petal_width=1.4),\n",
    "    IrisFeatures(sepal_length=6.3, sepal_width=3.3, petal_length=6.0, petal_width=2.5)\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    prediction = monitored_server.predict(test_case)\n",
    "    print(f\"   Test {i+1}: {prediction.prediction_label} (Confidence: {prediction.confidence:.4f})\")\n",
    "\n",
    "# Monitoring Metriken anzeigen\n",
    "metrics = monitored_server.get_monitoring_data()\n",
    "print(\"\\nüìä Monitoring Metriken:\")\n",
    "print(f\"   Total Predictions: {metrics['total_predictions']}\")\n",
    "print(f\"   Avg Prediction Time: {metrics['avg_prediction_time']:.4f}s\")\n",
    "print(f\"   Avg Confidence: {metrics['avg_confidence']:.4f}\")\n",
    "print(f\"   Class Distribution: {metrics['class_distribution']}\")\n",
    "print(f\"   Errors: {metrics['error_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ad633",
   "metadata": {},
   "source": [
    "### 7Ô∏è‚É£ Streamlit Dashboard f√ºr Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d119051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Streamlit Monitoring Dashboard erstellen\n",
    "\n",
    "streamlit_dashboard = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"MLOps Monitoring Dashboard\",\n",
    "    page_icon=\"üìä\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "st.title(\"üöÄ MLOps Monitoring Dashboard\")\n",
    "st.markdown(\"**Real-time Monitoring f√ºr Iris Classification API**\")\n",
    "\n",
    "# Sidebar f√ºr Konfiguration\n",
    "st.sidebar.header(\"‚öôÔ∏è Konfiguration\")\n",
    "api_url = st.sidebar.text_input(\"API URL\", \"http://localhost:8000\")\n",
    "refresh_interval = st.sidebar.slider(\"Refresh Interval (s)\", 5, 60, 10)\n",
    "\n",
    "# Auto-refresh\n",
    "if st.sidebar.button(\"üîÑ Refresh\"):\n",
    "    st.rerun()\n",
    "\n",
    "# API Health Check\n",
    "try:\n",
    "    health_response = requests.get(f\"{api_url}/health\", timeout=5)\n",
    "    if health_response.status_code == 200:\n",
    "        health_data = health_response.json()\n",
    "        st.success(f\"‚úÖ API Status: {health_data['status']}\")\n",
    "        \n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        with col1:\n",
    "            st.metric(\"Model Version\", health_data.get('model_version', 'N/A'))\n",
    "        with col2:\n",
    "            st.metric(\"Model Loaded\", \"‚úÖ\" if health_data.get('model_loaded') else \"‚ùå\")\n",
    "        with col3:\n",
    "            st.metric(\"Target Classes\", len(health_data.get('target_classes', [])))\n",
    "    else:\n",
    "        st.error(f\"‚ùå API nicht erreichbar (Status: {health_response.status_code})\")\n",
    "except Exception as e:\n",
    "    st.error(f\"‚ùå Verbindung zur API fehlgeschlagen: {str(e)}\")\n",
    "\n",
    "st.divider()\n",
    "\n",
    "# Prediction Interface\n",
    "st.header(\"üß™ Model Testing\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.subheader(\"Input Features\")\n",
    "    sepal_length = st.slider(\"Sepal Length\", 4.0, 8.0, 5.1, 0.1)\n",
    "    sepal_width = st.slider(\"Sepal Width\", 2.0, 4.5, 3.5, 0.1)\n",
    "    petal_length = st.slider(\"Petal Length\", 1.0, 7.0, 1.4, 0.1)\n",
    "    petal_width = st.slider(\"Petal Width\", 0.1, 2.5, 0.2, 0.1)\n",
    "    \n",
    "    if st.button(\"üîÆ Predict\", type=\"primary\"):\n",
    "        try:\n",
    "            prediction_data = {\n",
    "                \"sepal_length\": sepal_length,\n",
    "                \"sepal_width\": sepal_width,\n",
    "                \"petal_length\": petal_length,\n",
    "                \"petal_width\": petal_width\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{api_url}/predict\",\n",
    "                json=prediction_data,\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                st.success(f\"Prediction: **{result['prediction_label']}**\")\n",
    "                st.info(f\"Confidence: {result['confidence']:.4f}\")\n",
    "                st.caption(f\"Timestamp: {result['timestamp']}\")\n",
    "            else:\n",
    "                st.error(f\"Prediction fehlgeschlagen: {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Fehler bei Prediction: {str(e)}\")\n",
    "\n",
    "with col2:\n",
    "    st.subheader(\"Iris Species\")\n",
    "    st.image(\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\", \n",
    "             caption=\"Iris Versicolor\", width=300)\n",
    "\n",
    "st.divider()\n",
    "\n",
    "# Performance Metriken (Simulation)\n",
    "st.header(\"üìà Performance Metriken\")\n",
    "\n",
    "# Simulierte Daten f√ºr Demo\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2024-01-01', periods=30, freq='D')\n",
    "predictions_per_day = np.random.poisson(100, 30)\n",
    "avg_response_time = np.random.normal(0.05, 0.01, 30)\n",
    "avg_confidence = np.random.normal(0.92, 0.05, 30)\n",
    "\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "with col1:\n",
    "    st.metric(\n",
    "        \"Total Predictions (30d)\",\n",
    "        f\"{predictions_per_day.sum():,}\",\n",
    "        delta=f\"+{predictions_per_day[-1] - predictions_per_day[-2]}\"\n",
    "    )\n",
    "\n",
    "with col2:\n",
    "    st.metric(\n",
    "        \"Avg Response Time\",\n",
    "        f\"{avg_response_time[-1]:.3f}s\",\n",
    "        delta=f\"{avg_response_time[-1] - avg_response_time[-2]:+.3f}s\"\n",
    "    )\n",
    "\n",
    "with col3:\n",
    "    st.metric(\n",
    "        \"Avg Confidence\",\n",
    "        f\"{avg_confidence[-1]:.3f}\",\n",
    "        delta=f\"{avg_confidence[-1] - avg_confidence[-2]:+.3f}\"\n",
    "    )\n",
    "\n",
    "with col4:\n",
    "    error_rate = np.random.uniform(0.001, 0.005)\n",
    "    st.metric(\n",
    "        \"Error Rate\",\n",
    "        f\"{error_rate:.3%}\",\n",
    "        delta=\"-0.001%\"\n",
    "    )\n",
    "\n",
    "# Charts\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    # Predictions over time\n",
    "    fig_predictions = px.line(\n",
    "        x=dates, y=predictions_per_day,\n",
    "        title=\"üìä Predictions pro Tag\",\n",
    "        labels={'x': 'Datum', 'y': 'Anzahl Predictions'}\n",
    "    )\n",
    "    st.plotly_chart(fig_predictions, use_container_width=True)\n",
    "\n",
    "with col2:\n",
    "    # Response time over time\n",
    "    fig_response = px.line(\n",
    "        x=dates, y=avg_response_time,\n",
    "        title=\"‚ö° Response Time Trend\",\n",
    "        labels={'x': 'Datum', 'y': 'Response Time (s)'}\n",
    "    )\n",
    "    st.plotly_chart(fig_response, use_container_width=True)\n",
    "\n",
    "# Class Distribution\n",
    "st.subheader(\"üéØ Prediction Class Distribution\")\n",
    "class_counts = {\n",
    "    'setosa': np.random.poisson(200),\n",
    "    'versicolor': np.random.poisson(180),\n",
    "    'virginica': np.random.poisson(220)\n",
    "}\n",
    "\n",
    "fig_pie = px.pie(\n",
    "    values=list(class_counts.values()),\n",
    "    names=list(class_counts.keys()),\n",
    "    title=\"Class Distribution (Last 30 Days)\"\n",
    ")\n",
    "st.plotly_chart(fig_pie, use_container_width=True)\n",
    "\n",
    "# Footer\n",
    "st.divider()\n",
    "st.caption(f\"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\"\"\"\n",
    "\n",
    "# Streamlit Dashboard Datei erstellen\n",
    "with open('07_01_streamlit_mlops_dashboard.py', 'w') as f:\n",
    "    f.write(streamlit_dashboard)\n",
    "\n",
    "print(\"üìä Streamlit Monitoring Dashboard erstellt!\")\n",
    "print(\"üìÑ Datei: 07_01_streamlit_mlops_dashboard.py\")\n",
    "print(\"\\nüöÄ Dashboard starten mit:\")\n",
    "print(\"   streamlit run 07_01_streamlit_mlops_dashboard.py\")\n",
    "print(\"\\nüì± Dashboard Features:\")\n",
    "print(\"   ‚úÖ API Health Monitoring\")\n",
    "print(\"   üß™ Interactive Model Testing\")\n",
    "print(\"   üìà Performance Metriken\")\n",
    "print(\"   üéØ Prediction Analytics\")\n",
    "print(\"   üîÑ Real-time Updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00ebd7",
   "metadata": {},
   "source": [
    "## üéØ Portfolio-Zusammenfassung: MLOps & Deployment\n",
    "\n",
    "### üèÜ Was Sie gelernt haben\n",
    "\n",
    "1. **üîÑ MLOps-Pipeline**: Vollst√§ndigen ML-Lifecycle von Training bis Deployment\n",
    "2. **üê≥ Containerisierung**: Docker-Setup f√ºr reproduzierbare ML-Services\n",
    "3. **üåê API-Development**: FastAPI f√ºr robuste ML-Endpoints\n",
    "4. **üìä Model Monitoring**: Performance-Tracking in Produktionsumgebungen\n",
    "5. **üì± Dashboard-Entwicklung**: Streamlit f√ºr ML-Monitoring\n",
    "\n",
    "### üõ†Ô∏è Praktische F√§higkeiten\n",
    "\n",
    "- ‚úÖ **Model Validation** - Automatisierte Tests f√ºr ML-Modelle\n",
    "- ‚úÖ **API Design** - RESTful Services f√ºr ML-Predictions\n",
    "- ‚úÖ **Docker Deployment** - Containerisierte ML-Anwendungen\n",
    "- ‚úÖ **Monitoring Setup** - Performance- und Error-Tracking\n",
    "- ‚úÖ **Production Readiness** - Robuste, skalierbare ML-Services\n",
    "\n",
    "### üìÅ Generierte Artefakte\n",
    "\n",
    "1. **ü§ñ Trainiertes Model** - `models/iris_classifier_*.joblib`\n",
    "2. **üåê FastAPI Application** - REST API f√ºr Model Serving\n",
    "3. **üê≥ Docker Setup** - `docker_deployment/` Ordner\n",
    "4. **üìä Monitoring Dashboard** - `07_01_streamlit_mlops_dashboard.py`\n",
    "5. **üìù Deployment Documentation** - Production-ready Setup\n",
    "\n",
    "### üöÄ N√§chste Schritte f√ºr Ihr Projekt\n",
    "\n",
    "1. **üîß Anpassung**: MLOps-Pipeline f√ºr Ihr eigenes Modell adaptieren\n",
    "2. **‚òÅÔ∏è Cloud Deployment**: AWS/Azure/GCP Integration\n",
    "3. **üìà Advanced Monitoring**: Drift Detection, A/B Testing\n",
    "4. **üîÑ CI/CD Integration**: GitHub Actions f√ºr automatisches Deployment\n",
    "5. **üîí Security**: Authentication, Rate Limiting, Encryption\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Portfolio-Tipp**: Dokumentieren Sie Ihre MLOps-Pipeline ausf√ºhrlich - das zeigt Arbeitgebern Ihre Production-Ready Skills!\n",
    "\n",
    "### üìö Weiterf√ºhrende Ressourcen\n",
    "\n",
    "- **MLOps Best Practices**: [Google MLOps Guide](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n",
    "- **FastAPI Documentation**: [fastapi.tiangolo.com](https://fastapi.tiangolo.com/)\n",
    "- **Docker Best Practices**: [Docker Documentation](https://docs.docker.com/develop/dev-best-practices/)\n",
    "- **Model Monitoring**: [Evidently AI](https://evidentlyai.com/)\n",
    "- **Kubernetes f√ºr ML**: [Kubeflow](https://www.kubeflow.org/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
