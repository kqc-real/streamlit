{"cells":[{"cell_type":"code","execution_count":null,"source":["# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n","!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["# Wie geht's eigentlich weiter?"],"metadata":{}},{"cell_type":"markdown","source":["## Einführung und Überblick\n","\n","In dieser Aufgabe lernen Sie die Grundlagen zur Entwicklung rekurrenter neuronaler Netze (RNN), speziell Long Short-Term Memory Networks (LSTM). Diese Art von Algorithmen sind wegen ihrer Fähigkeit, zeitliche Abhängigkeiten in Daten zu beschreiben, sehr bekannt. Wenn Sie versuchen, ein Signal oder die Reaktion eines kausalen Systems zu modellieren, sind RNNs normalerweise eine der ersten Optionen, die Sie in Betracht ziehen (zusammen mit typischen statistischen Methoden wie linearen Regressionen, exponentieller Glättung, ARIMA-Modellen usw., die in dieser Aufgabe nicht berücksichtigt werden). Durch die rekursiven Verbindungen in diesem neuronalen Netz entsteht eine Art Gedächtnis, das Zeitreihen gut beschreibt. Dies ist besonders hilfreich im Hinblick auf Prognosen. Prognosen werden von vielen Unternehmen verwendet, um bei der Budgetierung, der Planung und der Abschätzung des zukünftigen Wachstums zu helfen. Mit anderen Worten, es ist der Versuch, zukünftige Ergebnisse auf der Grundlage vergangener Ereignisse vorherzusagen.\n","\n","Diese Arbeit ist wie folgt aufgebaut: Zunächst werden einige theoretische Hintergründe zur Zeitreihenanalyse und -prognose gegeben. Dann wird auf die Datenvorbereitung für Zeitreihen eingegangen, was der erste Schritt ist, den man vor Beginn eines jeden Prognoseprojekts machen muss. Danach wird der interessanteste Teil erklärt: LSTMs. Wir werden zunächst ein einfaches Vanilla-LSTM betrachten, mit einem Hidden Layer. Danach werden wir mehrere Layers stapeln und die Ergebnisse mit der Vanilla-Lösung vergleichen. Am Ende werden Sie einen TV-Skript-Generator erforschen und dabei Ihr erworbenes Wissen über Zeitreihen und LSTM-Netzwerke anwenden."],"metadata":{}},{"cell_type":"markdown","source":["## Zeitreihenvorhersage\n","Eine Zeitreihe ist eine Folge von Beobachtungen, die zeitlich aufeinander folgen. Diese Beobachtungen sind normalerweise Messungen in Form von numerischen Werten. Zum Beispiel kann der alle fünf Minuten gemessene Batteriestand eines Elektroautos als Zeitreihe ausgedrückt werden. \n","\n","### Beschreiben vs. Vorhersagen\n","Das Verstehen eines Datensatzes, in diesem Fall Zeitreihenanalyse genannt, ist ein wichtiger Schritt, bevor man mit der Arbeit des Datensatzes beginnt. Dies kann helfen, bessere Vorhersagen zu treffen. Eine tiefe Zeitreihenanalyse ist jedoch nicht erforderlich, da sie zu einem großen technischen Aufwand, Zeit und Fachwissen führen kann, die nicht direkt mit dem gewünschten Ergebnis, nämlich der Vorhersage der Zukunft, übereinstimmt.\n","\n","Bei der deskriptiven Modellierung oder __Zeitreihenanalyse (engl. time series analysis)__ wird eine Zeitreihe modelliert, um ihre Komponenten in Bezug auf saisonale Muster, Trends, Beziehung zu externen Faktoren und dergleichen zu bestimmen. Im Gegensatz dazu nutzt die __Zeitreihenprognose (engl. time series forecasting)__ die Informationen in einer Zeitreihe (oft zusammen mit zusätzlichen Informationen), um zukünftige Werte der Reihe zu prognostizieren.[4]"],"metadata":{}},{"cell_type":"markdown","source":["### Problem Definition\n","Im folgenden Tutorial werden Sie mit einem einfachen Datensatz der elektrischen Last in Deutschland in MWh von Oktober 2017 bis Oktober 2019 arbeiten. Das Ziel dieser Aufgabe ist es, die Last für eine Woche mit LSTMs zu prognostizieren. \n","\n","Erkunden Sie zunächst den gegebenen Datensatz. Laden Sie die csv-Datei *Load_DE_2017_2019.csv*, geben Sie einige Datenelemente aus, prüfen Sie die Größe der Datei, verwenden Sie die Funktion __[describe()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)__ von Pandas, um Mittelwert, Standardabweichung, Median, Minimum und Maximum der Beobachtungen zu prüfen. Dies kann helfen, eine Vorstellung von der Verteilung und Streuung der Werte zu bekommen. Dies kann Ihnen auch einige Ideen zur Datenskalierung und sogar zur Datenbereinigung geben, die Sie später als Teil der Vorbereitung Ihres Datensatzes für die Modellierung durchführen können.\n","\n","In dieser Aufgabe werden wir Series von Pandas als Datenstruktur verwenden. Weitere Informationen finden Sie in der offiziellen Pandas-Dokumentation [Intro to Data Structures](http://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html)."],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.1:</b> Vervollständigen Sie die folgenden Code-Zellen entsprechend den darin enthaltenen Kommentaren.\n","\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["from pandas import read_csv\n","import pandas as pd\n","\n","# Load the dataset using read_csv() with \n","# squeeze=True to return a Series instead of a DataFrame\n","# parse_dates=True, dayfirst=True to convert the date to a datetime column\n","# index_col=0 to consider the date as index\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE\n","\n","# Use the function to_numeric with treating errors as 'coerce' from pandas in order to read the values of the dataset as floats\n","# And fill the NANs in the dataset using the method \"ffil\" with downcast as \"infer\"\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# Check that a Series data type (and not a DataFrame) was created\n","print(type(series))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# Print some rows of the dataset (Hint: use .head() for this)\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["# Check the size of the series (Hint: use .size for this)\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["# Check something else from the series if you are curious...\n","# For example print the data from October\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["# Calculate descriptive statistics on your time series using describe()\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Taxonomie von Zeitreihenprognosen-Problemen\n","\n","Um das Verständnis des Prognoseproblems zu verbessern, ist die Struktur des Modells erforderlich und es ist notwendig zu wissen, wie man es auswertet. Daher wird empfohlen, die folgenden Schlüsselpunkte zu berücksichtigen, bevor ein Projekt zur Zeitreihenprognose gestartet wird [2]:\n","\n","- __Eingabedaten vs. Ausgabedaten__: Die Eingabedaten sind die Werte, die zur Erstellung einer Prognose verwendet werden. Zum Beispiel die Verkaufsdaten der letzten sieben Tage, um den Umsatz des nächsten Tages zu prognostizieren. Die Eingabedaten sind nicht die Daten, die zum Trainieren des Modells, sondern zum Testen/Vorhersagen verwendet werden.<br>\n","    Die Ausgabedaten entsprechen der Vorhersage oder Prognose für einen zukünftigen Zeitschritt, der über die als Eingabe bereitgestellten Daten hinausgeht.<br>\n","    Die Definition der Ein- und Ausgabedaten des Modells zwingt Sie dazu, sich Gedanken darüber zu machen, was genau benötigt wird oder werden könnte, um eine Prognose zu erstellen. Möglicherweise können Sie bei den Eingabedaten nicht genau sein. Sie können z. B. nicht wissen, ob ein oder mehrere vorherige Zeitschritte erforderlich sind, um eine Prognose zu erstellen. Aber Sie werden in der Lage sein, die Variablen zu identifizieren, die einen Einfluss auf den Prognoseprozess haben könnten.\n","\n","- __Endogene vs. Exogene Eingangsvariablen__: Eine Eingangsvariable ist endogen, wenn sie von anderen Variablen im System beeinflusst wird und die Ausgangsvariable von ihr abhängt. In einer Zeitreihe zeigen die Beobachtungen einer Eingangsvariablen Abhängigkeiten in sich selbst. Zum Beispiel ist die Beobachtung zum Zeitpunkt *t* abhängig von der Beobachtung zum Zeitpunkt _t-1_; _t-1_ kann von _t-2_ abhängen, und so weiter. <br>\n","    Eine Eingangsvariable ist eine exogene Variable, wenn sie unabhängig von anderen Variablen im System ist und die Ausgangsvariable von ihr abhängt. Vereinfacht ausgedrückt, werden endogene Variablen von anderen Variablen im System (einschließlich ihrer selbst) beeinflusst, während exogene Variablen dies nicht sind und als außerhalb des Systems betrachtet werden.<br>\n","    Typischerweise hat ein Zeitreihenprognoseproblem endogene Variablen (z. B. ist die Ausgabe eine Funktion einer gewissen Anzahl von vorherigen Zeitschritten) und kann exogene Variablen haben oder auch nicht. Oft werden die exogenen Variablen ignoriert. Das explizite Nachdenken über beide Variablentypen kann helfen, leicht zu übersehende exogene Daten oder sogar technische Features zu identifizieren, die das Modell verbessern können.\n","    \n","- __Unstrukturiert vs. Strukturiert__: Es ist nützlich, jede Variable in einer Zeitreihe darzustellen und die Darstellung auf mögliche Muster zu untersuchen. Eine Zeitreihe für eine einzelne Variable weist möglicherweise kein offensichtliches Muster auf. Wir können eine Reihe ohne Muster als unstrukturiert betrachten, da es keine erkennbare zeitabhängige Struktur gibt. <br>\n","    Alternativ dazu kann eine Zeitreihe offensichtliche Muster aufweisen und in vier Bestandteile zerlegt werden: \n","    - Level: Der Basiswert der Reihe, wenn sie eine Gerade wäre.\n","    - Trend: Das optionale und oft linear ansteigende oder abfallende Verhalten der Reihe über die Zeit.\n","    - Saisonalität: Die optionalen sich wiederholenden Muster oder Zyklen des Verhaltens im Zeitverlauf.\n","    - Rauschen: Die optionale Variabilität in Beobachtungen, die nicht durch das Modell erklärt werden kann.<br>\n","    \n","- __Regression vs. Klassifikation__: Bei Regressionsvorhersagemodellen handelt es sich um Probleme, bei denen eine Quantität vorhergesagt wird. Eine Quantität ist ein numerischer Wert, z. B. ein Preis, eine Anzahl, ein Volumen usw. Ein Zeitreihen-Prognoseproblem, bei dem Sie einen oder mehrere zukünftige numerische Werte vorhersagen möchten, ist ein Regressions-Prognosemodellierungsproblem. <br>\n","    Klassifikationsprädiktive Modellierungsprobleme sind solche, bei denen eine Kategorie vorhergesagt wird. Eine Kategorie ist eine Bezeichnung aus einer kleinen, wohldefinierten Menge von Bezeichnungen. Zum Beispiel sind \"heiß\", \"kalt\", \"aufwärts\", \"abwärts\", \"kaufen\" und \"verkaufen\" Kategorien. Ein Zeitreihenprognoseproblem, bei dem Sie die eingegebenen Zeitreihendaten klassifizieren möchten, ist ein Prognosemodellierungsproblem vom Typ Klassifikation. <br>\n","    Zwischen diesen Typen gibt es eine gewisse Flexibilität. So kann z. B. ein Regressionsproblem in ein Klassifizierungsproblem und ein Klassifizierungsproblem in eine Regression umgewandelt werden. Einige Probleme, wie z. B. die Vorhersage eines Ordinalwerts, können sowohl eine Klassifizierung als auch eine Regression zugeordnet werden. Es ist möglich, dass eine Neuausrichtung Ihres Zeitreihen-Prognoseproblems dieses vereinfachen kann.\n","\n","- __Univariat vs. Multivariat__:  Eine einzelne Variable, die über die Zeit gemessen wird, wird als univariate Zeitreihe bezeichnet. Mehrere Variablen, die über die Zeit gemessen werden, werden als multivariate Zeitreihen bezeichnet. Die Betrachtung dieser Frage in Bezug auf Inputs und Outputs kann zu einer weiteren Unterscheidung führen. Die Anzahl der Variablen kann sich zwischen den Inputs und Outputs unterscheiden, d. h. die Daten sind möglicherweise nicht symmetrisch. Es kann z. B. sein, dass Sie mehrere Variablen als Input für das Modell haben und nur an der Vorhersage einer der Variablen als Output interessiert sind. In diesem Fall besteht im Modell die Annahme, dass die mehreren Eingabevariablen das Modell selbst verbessern und für die Vorhersage der einzelnen Ausgabevariablen erforderlich sind.\n","\n","- __Einschrittig vs. Mehrschrittig__: Ein Prognoseproblem, das eine Vorhersage des nächsten Zeitschritts erfordert, wird als einschrittiges Prognosemodell bezeichnet. Ein Prognoseproblem, das eine Vorhersage von mehr als einem Zeitschritt erfordert, wird hingegen als mehrschrittiges Prognosemodell bezeichnet. Je mehr Zeitschritte in die Zukunft projiziert werden müssen, desto schwieriger wird das Problem, da sich die Unbestimmtheit in jedem prognostizierten Zeitschritt erhöht.\n","\n","- __Statisch vs. Dynamisch__: Es ist möglich, ein Modell einmal zu entwickeln und es wiederholt für Vorhersagen zu verwenden. Da das Modell zwischen den Prognosen nicht aktualisiert oder geändert wird, kann man dieses Modell als statisch betrachten. Umgekehrt können wir neue Beobachtungen erhalten, bevor wir eine nachfolgende Vorhersage machen, die zur Erstellung eines neuen Modells oder zur Aktualisierung des vorhandenen Modells verwendet werden können. Wir können die Entwicklung eines neuen oder aktualisierten Modells vor jeder Prognose als ein dynamisches Problem betrachten.\n","\n","- __Kontinuierlich vs. Diskontinuierlich__: Eine Zeitreihe, bei der die Beobachtungen über die Zeit gleichmäßig sind, kann als kontinuierlich beschrieben werden. Viele Zeitreihenprobleme haben kontinuierlich Beobachtungen, z. B. eine Beobachtung pro Stunde, Tag, Monat oder Jahr. Eine Zeitreihe, bei der die Beobachtungen im Laufe der Zeit nicht einheitlich sind, kann als diskontinuierlich bezeichnet werden. Die fehlende Gleichmäßigkeit der Beobachtungen kann durch fehlende oder fehlerhafte Werte verursacht werden. Sie kann auch dadurch bedingt sein, dass Beobachtungen nur sporadisch oder in immer kürzeren Zeitabständen zur Verfügung gestellt werden. Im Falle von uneinheitlichen Beobachtungen kann bei der Anpassung einiger Modelle eine spezielle Datenformatierung erforderlich sein, um die Beobachtungen über die Zeit zu vereinheitlichen."],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe/Frage 6.3.2:</b> Jetzt sind Sie an der Reihe. Bewerten Sie die Taxonomie des vorgeschlagenen Vorhersageproblems der elektrischen Last in Deutschland. Beschreiben Sie kurz dieses Problems anhand der oben genannten Stichpunkte.\n","</div>\n","\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["## Datenaufbereitung"],"metadata":{}},{"cell_type":"markdown","source":["### Zerlegen von Zeitreihendaten\n","Man geht davon aus, dass eine gegebene Zeitreihe aus drei systematischen Komponenten besteht, einschließlich __Level__, __Trend__, __Saisonalität__, und einer nicht systematischen Komponente, die __Rauschen__ genannt wird. Man geht davon aus, dass eine Reihe ein Aggregat oder eine Kombination aus diesen vier Komponenten besteht. Alle Reihen haben ein Level und ein Rauschen. Die Komponenten Trend und Saisonalität sind optional. In diesem Abschnitt werden Methoden zur automatischen Zerlegung einer Zeitreihe erläutert.\n","\n","Die Bibliothek [Statsmodels](https://www.statsmodels.org/stable/index.html) bietet eine Implementierung der naiven oder klassischen Zerlegungsmethode in einer Funktion namens `seasonal_decompose()`. Sie erfordert die Angabe, ob das Modell additiv oder multiplikativ ist. \n","\n","__Wichtig:__ Diese Funktion erzeugt eine naive Dekomposition. Für weitergehende Analysen sollten anspruchsvollere Methoden bevorzugt werden.<br>\n","Das additive Modell wird beschrieben als Y[t] = T[t] + S[t] + e[t]<br>\n","Das multiplikative Modell ist Y[t] = T[t] * S[t] * e[t]<br>\n","Die saisonale Komponente wird zunächst durch Anwendung eines Faltungsfilters auf die Daten entfernt. Der Durchschnitt dieser geglätteten Reihe für jede Periode ist die zurückgegebene saisonale Komponente.<br>\n","\n","Wenn die Art des Zerlegungsmodells unbekannt ist, kann eine Überprüfung eines Plots der Zeitreihe und einiger zusammenfassender Statistiken oft ein guter Anfang sein. Diese geben eine Vorstellung davon, ob das Zeitreihenproblem additiv oder multiplikativ aussieht.\n","\n","Abbildung 1 zeigt zwei verschiedene Zeitreihen, die mit der additiven (links) und multiplikativen (rechts) Methode zerlegt werden können. Betrachten Sie die Form der Reihen und wie unterschiedlich sie sind. \n","\n","<img src=\"images/Additive-Multiplicative-Decomposition-time-series.png\" alt=\"drawing\" style=\"width:700px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 1 - Beispiel für eine additive (links, [10]) und eine multiplikative Zerlegung von Zeitreihen (rechts, [1]).\n","</p>\n"],"metadata":{}},{"cell_type":"markdown","source":["Im folgenden Schnipsel sehen Sie den Code für eine additive saisonale Zerlegung."],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.3:</b> Vervollständigen Sie den Code, um alle Komponenten in einem Graph darzustellen. Schreiben Sie auch einen Code, um eine einzelne Komponente in einem Diagramm darzustellen, z. B. den Trend.\n","<ul>\n","<li>Hinweis: <code>sm.tsa.seasonal_decompose</code> liefert ein DecomposeResult. Dieses hat die Attribute <i>observed, trend, seasonal</i> und <i>resid</i>, die Pandas-Reihen sind. Sie können jede von ihnen mit der Pandas-Plot-Funktionalität darstellen.\n","\n","\n","</ul>\n","\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["from statsmodels.tsa.seasonal import seasonal_decompose\n","from matplotlib import pyplot\n","\n","result_add = seasonal_decompose(series[:2000], model='additive', period=96)\n","trend_add = result_add.trend\n","seasonality_add = result_add.seasonal\n","residual_add = result_add.resid\n","original_data = result_add.observed"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["# Plot all components of the time series\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["# Plot a single component. For example the seasonality\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe/Frage 6.3.4:</b> Zerlegen Sie das gleiche Signal mit dem multiplikativen Modell und vergleichen Sie. Was können Sie aus beiden Zerlegungsmodellen schließen?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["# Multiplicative Decomposition\n","# Plot all components of the time series\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["# Plot a single component\n","# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Datennormalisierung\n","Die Normalisierung ist eine Neuskalierung der Daten aus ihrem ursprünglichen Bereich, so dass alle Werte im Bereich von 0 und 1 liegen. Dies hilft dem neuronalen Netz, die Trends der Daten leichter zu lernen. \n","\n","Die Bibliothek _Scikit-learn_ verfügt über ein Tool namens *MinMaxScaler*, das zum Skalieren der Daten verwendet werden kann. Die nächsten Schritte sind die __allgemeinen Schritte__, die zu befolgen sind:\n","- Passen Sie den Skalierer anhand der verfügbaren Trainingsdaten an. Für die Normalisierung bedeutet dies, dass die Trainingsdaten verwendet werden, um die minimalen und maximalen beobachtbaren Werte zu schätzen. Dies geschieht durch den Aufruf der Methode `fit()`.\n","- Wenden Sie die Skalierung auf die Trainingsdaten an, um die normalisierten Daten zum Trainieren Ihres Modells zu verwenden. Dies geschieht durch den Aufruf der Methode `transform()`.\n","- Wenden Sie die Skalierung auf die Daten an, die für den Vorwärtsdurchlauf verwendet werden. Dies bereitet die Daten vor, die Sie für die Vorhersagen verwenden werden."],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Prepare data for normalization\n","values = series.values\n","values = values.reshape((len(values), 1))\n","\n","# Train the normalization\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaler = scaler.fit(values)\n","print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n","\n","# Normalize the dataset and print\n","normalized = scaler.transform(values)\n","print(\"Normalized values:\\n %s\" %(normalized))\n","\n","# Inverse transform and print\n","inversed = scaler.inverse_transform(normalized)\n","print(\"Denormalized values:\\n %s\" %(inversed))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Datenstandardisierung\n","Bei der Standardisierung eines Datensatzes wird die Verteilung der Werte so skaliert, dass der Mittelwert der beobachteten Werte gleich 0 und die Standardabweichung gleich 1 ist. Dies kann als Subtraktion des Mittelwerts oder als Zentrierung der Daten betrachtet werden. Wie die Normalisierung kann auch die Standardisierung nützlich und in einigen Algorithmen für maschinelles Lernen sogar erforderlich sein, wenn Ihre Daten Eingabewerte mit unterschiedlichen Skalen aufweisen. Bei der Standardisierung wird davon ausgegangen, dass Ihre Beobachtungen einer Gauß-Verteilung (Glockenkurve) mit einem gut verhaltenen Mittelwert und einer Standardabweichung entsprechen. Sie können Ihre Zeitreihendaten auch dann standardisieren, wenn diese Erwartung nicht erfüllt ist, aber Sie erhalten möglicherweise keine zuverlässigen Ergebnisse.<br>\n","Sie können Ihren Datensatz mithilfe des *scikit-learn*-Objekts *StandardScaler* standardisieren."],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["from sklearn.preprocessing import StandardScaler\n","from math import sqrt\n","\n","# Train the standardization\n","scaler = StandardScaler()\n","scaler = scaler.fit(values)\n","print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, sqrt(scaler.var_)))\n","\n","# Normalize the dataset and print\n","standardized = scaler.transform(values)\n","print(\"Standardized values:\\n %s\" %(standardized))\n","\n","# Inverse transform and print\n","inversed = scaler.inverse_transform(standardized)\n","print(\"De-standardized values:\\n %s\" %(inversed))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Zusätzliche Ressourcen zur Datenaufbereitung und Zeitreihenprognose\n","\n","Die folgenden Artikel helfen Ihnen weiter, falls Sie tiefer in die Zeitreihenprognose einsteigen möchten:\n","\n","- [Open Machine Learning Course - Time series analysis in Python](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic09_time_series/topic9_part1_time_series_python.ipynb) von Dmitriy Sergeyev\n","- [7 Ways Time Series Forecasting Differs from Machine Learning](https://www.datascience.com/blog/time-series-forecasting-machine-learning-differences) verfasst von Roman de las Heras\n","- [Data Science for Business - Time Series Forecasting Part 1: EDA & Data Preparation](https://shiring.github.io/forecasting/2017/05/28/retail_forcasting_part1) "],"metadata":{}},{"cell_type":"markdown","source":["## Limitationen von mehrschichtigen Perzeptronen (engl. Multilayer Perceptrons)\n","Trotz ihrer Flexibilität und Leistungsfähigkeit funktionieren Feed-Forward-Neuronale Netze nur für Probleme gut, deren Eingaben und Targets sinnvoll mit Vektoren fester Dimensionalität kodiert werden können. Dies ist eine erhebliche Einschränkung, da viele wichtige Probleme am besten mit Sequenzen ausgedrückt werden, deren Länge nicht von vornherein bekannt ist. Zum Beispiel sind Spracherkennung und maschinelle Übersetzung sequenzielle Probleme. Ebenso kann das Beantworten von Fragen auch als das Abbilden einer Folge von Wörtern angesehen werden, die die Frage darstellen zu einer Folge von Wörtern, die die Antwort darstellen. [5]\n","\n","Aus dem Stand der Technik ist auch bekannt, dass MLPs schlecht abschneiden, wenn das zu lösende Problem zeitliche Abhängigkeiten aufweist, wie es bei Zeitreihen-Prognoseproblemen häufig der Fall ist. Dies ist auf den Mangel an Langzeitspeicher im Netzwerk zurückzuführen. Für diese Art von Anwendungen haben sich rekurrente neuronale Netze als leistungsfähiger erwiesen als MLPs. Diese Art von Netzwerken wird in diesem Abschnitt beschrieben."],"metadata":{}},{"cell_type":"markdown","source":["## Rekurrente Neurale Netzwerke\n","Rekurrente neuronale Netze (RNNs) enthalten Zyklen, die die Aktivierungen der Neuronen aus einem vorherigen Zeitschritt als Eingaben in das Netz einspeisen. Dies geschieht, um Vorhersagen zum aktuellen Zeitschritt zu beeinflussen. Diese Aktivierungen werden in den internen Zuständen des Netzes gespeichert, die prinzipiell langfristige zeitliche Kontextinformationen enthalten können. Dieser Mechanismus erlaubt es RNNs, ein sich dynamisch veränderndes Kontextfenster über die Eingangssequenzhistorie auszunutzen. [6]\n","\n","Um zu verstehen, wie RNNs den inneren Zustand der Zellen als rekurrenten Input einspeisen, zeigt Abbildung 2 [11] die möglichen Architekturen, die RNNs in Abhängigkeit von verschiedenen Anwendungen haben können. In dieser Abbildung ist das Netzwerk in der Zeit abgerollt dargestellt, d. h. die horizontale Achse stellt die Zeit und die vertikale Achse die Tiefe des Netzwerks dar. Zu Lehrzwecken gibt es in allen Architekturdarstellungen in Abbildung 2 nur ein Hidden Layer mit einer Zelle. Im Folgenden sind einige Anwendungsbeispiele für jede Architektur aufgeführt: \n","\n","- 1:1 : Diese Architektur beschreibt, wie Vanilla Feed Forward neuronale Netzwerke arbeiten. Sie haben eine Eingabe, alle Neuronen sind ohne Rekursion mit dem nächsten Neuron verbunden und es wird nur eine Ausgabe erwartet. \n","- 1:n : Dies ist z. B. der Fall bei Bildbeschriftungen. In diesem Fall kommt ein einzelnes Bild herein und als Ausgabe wird eine Phrase (eine Menge von Wörtern) erwartet. Die rekursiven Verbindungen von Zelle zu Zelle geben dem Netz die Möglichkeit, das nächste Wort in Abhängigkeit von den letzten Ausgaben genauer zu bestimmen, um einen (prinzipiell) sinnvollen Text zu erzeugen. \n","- n:1 : Eine solche Architektur kann für die Sentiment-Analyse verwendet werden. In diesem Fall wird ein Text in Form von Wörtern oder Zeichen in das Netzwerk eingegeben und das entsprechend ausgedrückte Sentiment als Ausgabe erwartet. \n","- n:n : Zwei Anwendungsbeispiele für diese Architektur sind zum einen die Sprachübersetzung (Text als Eingabe, Text als Ausgabe) auf Zeichen- oder Wortebene und zum anderen die Videoklassifikation auf Frame-Ebene, bei der eine Frame-Beschriftung in Abhängigkeit von vergangenen Frames erfolgt. \n","\n","<img src=\"images/RNN-architectures.png\" alt=\"drawing\" style=\"width:1000px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 2 - Architekturen neuronaler Netze: Die 1:1-Architektur entspricht einem Vanilla Neural Network, während die Fälle 1:n, n:1 und n:n die Strukturen rekurrenter neuronaler Netze beschreiben [11].\n","</p>\n"],"metadata":{}},{"cell_type":"markdown","source":["## Long Short-Term Memory Netzwerke\n","Long short-term memory Netzwerke (LSTM) sind eine Untergruppe der rekurrenten neuronalen Netze, aber im Gegensatz zu allgemeinen RNNs haben LSTMs eine einzigartige Formulierung, die es ihnen ermöglicht, die Probleme der verschwindenden und ausnutzenden Gradienten (Gewichtsänderungen, die schnell so klein werden, dass sie keine Wirkung mehr haben, oder so groß, dass sie zu einem Überlauf führen) zu vermeiden. [3]  \n","\n","In den folgenden Teilen dieser Arbeit wird das LSTM zunächst als Vanilla-Version (einzelnes Hidden Layer) behandelt, um die Grundlagen seiner Entwicklung zu verstehen. Danach werden mehrere Schichten übereinander gelegt, um kompliziertere Probleme zu lösen.  \n","\n","### LSTM Zelle \n","Eine LSTM-Zelle (auch Speicherzelle genannt) hat Gewichtungsparameter für den Eingang, den Ausgang und für den internen Zustand. Sie werden durch die Exposition gegenüber den Eingaben in jedem Zeitschritt aufgebaut und bei der Berechnung der Ausgabe(n) verwendet.\n","Der Schlüssel zur Speicherzelle sind die Gates. Auch diese sind gewichtete Funktionen, die den Informationsfluss in der Zelle weiter steuern. <br>\n","In einer LSTM-Zelle gibt es drei Gates, wie in Abbildung 3 zu sehen ist:\n","- Forget-Gate: Entscheidet, welche Informationen in der Zelle verworfen werden.\n","- Input-Gate: Entscheidet, welche Werte von der Eingabe verwendet werden, um den Speicherzustand zu aktualisieren.\n","- Ausgangs-Gate: Entscheidet, was basierend auf dem Eingang und dem Speicher der Zelle ausgegeben werden soll. [3]<br>\n","\n","<img src=\"images/LSTM-description-block.png\" alt=\"drawing\" style=\"width:1000px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 3 - Detailliertes Schema der Einheit eines einfachen rekurrenten Netzwerks (RNN) (links) und einer Speicherzelle (rechts), wie sie in den Hidden Layers eines LSTMs verwendet werden. [7]\n","</p>\n","\n","\n","### Limitationen von LSTM\n","Eine wichtige Einschränkung von LSTMs ist die Möglichkeit, den Speicher zu missbrauchen. Es ist möglich, ein LSTM-Modell dazu zu zwingen, sich eine einzelne Beobachtung über eine sehr lange Anzahl von Eingabezeitschritten zu merken. Dies ist eine schlechte Nutzung von LSTMs. Wenn man von einem LSTM-Modell verlangt, sich mehrere Beobachtungen zu merken, wird es scheitern. Das kann man sehen, wenn man LSTMs auf Zeitreihenvorhersagen anwendet, bei denen das Problem als Autoregression formuliert ist, die erfordert, dass die Ausgabe eine Funktion von mehreren entfernten Zeitschritten in der Eingabesequenz ist. Ein LSTM kann gezwungen werden, dieses Problem zu lösen, wird aber im Allgemeinen weniger effizient sein als ein sorgfältig entworfenes Autoregressionsmodell."],"metadata":{}},{"cell_type":"markdown","source":["### Vanilla LSTM\n","Das neuronale Netzwerk LSTM kann für univariate Zeitreihenprognosen verwendet werden. Wie ein RNN liest es jeden Zeitschritt einer Eingabesequenz schrittweise ein. Das LSTM hat einen internen Speicher, der es ihm ermöglicht, interne Zustände zu akkumulieren, während es die Schritte einer gegebenen Eingabesequenz liest. Am Ende der Sequenz gibt jeder Knoten in einer Schicht von versteckten LSTM-Einheiten (engl. hidden units) einen einzelnen Wert aus. Dieser Vektor von Werten fasst zusammen, was das LSTM gelernt oder aus der Eingabesequenz extrahiert hat. Dies kann von einer vollständig verbundenen Schicht (engl. fully connected layer) interpretiert werden, bevor eine endgültige Vorhersage getroffen wird. [3]\n","\n","Das folgende Modell umfasst ein einzelnes LSTM-Layer, gefolgt von einer vollständig verbundenen Outputlayer (Dense), wie in Abbildung 4 zu sehen. Dies ist die LSTM-Architektur, die im ursprünglichen LSTM Paper von 1997 [8] definiert wurde, und die Architektur, die bei den meisten kleinen Sequenzvorhersageproblemen gute Ergebnisse liefern wird.\n","\n","<img src=\"images/vanillalstm.png\" alt=\"drawing\" style=\"width:100px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 4 - Netzwerkstruktur für ein Vanilla-LSTM-Modell [3]\n","</p>\n","\n","Das Vanilla LSTM hat die folgenden 5 attraktiven Eigenschaften [3], von denen die meisten bereits im Originalpaper [8] demonstriert wurden:\n","- Sequenzklassifikation in Abhängigkeit von mehreren verteilten Eingabezeitschritten.\n","- Speicherung von präzisen Eingangsbeobachtungen über Tausende von Zeitschritten.\n","- Sequenzvorhersage als Funktion von vorherigen Zeitschritten.\n","- Robust gegenüber dem Einfügen von zufälligen Zeitschritten auf der Eingangssequenz.\n","- Robust gegenüber der Platzierung von Signaldaten auf der Eingangssequenz. "],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["# Import all libraries needed to implement a Vanilla LSTM\n","from math import sqrt\n","import numpy as np\n","from numpy import array\n","from numpy import mean\n","from numpy import std\n","import pandas as pd\n","from pandas import DataFrame\n","from pandas import concat\n","from pandas import read_csv\n","from sklearn.metrics import mean_squared_error\n","import tensorflow as tf\n","from tensorflow.python import keras\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dense\n","from tensorflow.python.keras.layers import LSTM\n","from matplotlib import pyplot\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Prepare data\n","series = read_csv('data/Load_DE_2017_2019.csv', header=0, index_col=0, parse_dates=True, squeeze=True, dayfirst=True)\n","series = pd.to_numeric(series, errors='coerce').fillna(method='ffill', downcast='infer')\n","values = series.values\n","values = values.reshape((len(values), 1))\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaler = scaler.fit(values)\n","normalized = scaler.transform(values)\n"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Nun werden wir die aus *Load_DE_2017_2019.csv* importierten Zeitreihen in ein geeignetes Format zur Modellierung von LSTM-Netzen konvertieren. "],"metadata":{}},{"cell_type":"markdown","source":["#### Train Test Split\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.5:</b> Schreiben Sie zunächst eine Funktion, die den univariaten Datensatz in Trainings-/Testsätze aufteilt. Verwenden Sie die Variable <i>n_test</i> (Anzahl der Datenpunkte im Testsatz) als Aufteilungsindex im Array. Der Eingabetyp ist ein Numpy-Array und die Ausgabe sollte ebenfalls ein aufgeteiltes Numpy-Array sein.\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["# Split a univariate dataset into train/test sets\n","def train_test_split(data:np.array, n_test:int):\n","    # Input:\n","        # data: ndarray \n","        # n_test: integer, splitting index in the array\n","    # Return: \n","        # train, test: ndarray \n","\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["# Test your function\n","dummy_array = np.arange(1,11,1)\n","# To keep format the same\n","dummy_array = np.expand_dims(dummy_array, axis=1)\n","# Print transposed arrays for better visualization\n","print('This is the (transposed) dummy_array', dummy_array.T)\n","print('This is the (transposed) train data:', train_test_split(dummy_array, 2)[0][:].T)\n","print('This is the (transposed) test data:', train_test_split(dummy_array, 2)[1][:].T)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Zeitreihen im überwachten Lernen\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.6:</b> Schreiben Sie eine Funktion, die eine Liste in das Format des überwachten Lernens umwandelt, d. h. in eine Eingabe- und Prognosesequenz. Betrachten Sie <i>n_in</i> als die Menge der Eingangsdatenelemente und <i>n_out</i> als die Menge der Elemente in der Prognosesequenz. Betrachten Sie die Sequenz als das gleiche Format wie das <i>dummy_array</i>.\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["Ein Beispiel ist in der nächsten Abbildung zu sehen: \n","<img src=\"images/series_to_supervised_example.png\" alt=\"drawing\" style=\"width:500px;\"/>"],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["# Transform list into supervised learning format\n","def series_to_supervised(data:np.array, n_in:int, n_out:int):\n","    # Input:\n","        # data: numpy ndarray \n","        # n_in: rnn input data elements\n","        # n_out: elements in the forecast sequence\n","    # Output:\n","        # numpy ndarray of size: (data_length - n_in - n_out + 1)x(n_in + n_out) \n","        \n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["# Test your function\n","dummy_array_sup = series_to_supervised(dummy_array, n_in=2, n_out=2)\n","print(type(dummy_array_sup))\n","print(dummy_array_sup)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Modellanpassung & Vorhersage\n","\n","Die Funktion zur Berechnung des mittleren quadratischen Fehlers der Vorhersage zu den tatsächlichen Datenwerten ist unten angegeben."],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["# Root mean squared error (rmse)\n","def measure_rmse(actual:list, predicted:list)->float:\n","    return sqrt(mean_squared_error(actual, predicted))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Mit der Funktion `model_fit()` können Sie die LSTM-Struktur aufbauen und das Modell trainieren. Diese Funktion hat als Eingabe eine *config*-Liste, die fünf Modell-Hyperparameter enthält. Diese sind:\n","\n","- n_input: Die Anzahl der Lag-Beobachtungen, die als Eingabe für das Modell verwendet werden sollen.\n","- n_Ausgang: Die Anzahl der Ausgänge des Netzwerks.\n","- n_Knoten: Die Anzahl der LSTM-Einheiten, die im Hidden Laye verwendet werden sollen.\n","- n_epochs: Die Anzahl, wie oft das Modell dem gesamten Trainingsdatensatz ausgesetzt werden soll.\n","- n_batch: Die Anzahl der Samples innerhalb einer Epoche, nach der die Gewichte aktualisiert werden.\n","- n_freq: Die Frequenz der Reihe, die für die saisonale Zerlegung verwendet wird.\n","\n","Die Eingabe für das LSTM-Netzwerk muss eine dreidimensionale Struktur haben, die aus (Samples, Timesteps, Features) besteht. *Timesteps* stellt die Anzahl der Elemente aus der Vergangenheit dar, die als Eingabe an das Netzwerk gegeben werden. *Features* sind die Variablen, die als Eingabeelemente verwendet werden, um etwas vorherzusagen, die sogenannten Prädiktoren. \n","\n","In unserem Fallbeispiel haben wir nur einen Prädiktor als Eingabe, nämlich die elektrische Lastreihe, daher _Features=1_. Die Anzahl der Zeitschritte in der Vergangenheit, die wir in unserem Eingabefenster berücksichtigen, entspricht *n_input*. Die Anzahl der Fenster, die wir für die Vorhersage verwenden, ist gleich _Samples_. Daher muss die Form der Eingangsvariablen für unser LSTM [samples, n_input, 1] sein.\n","\n","Außerdem sollten Sie den Trainingsdatensatz deseasonalisieren, bevor Sie mit dem Trainingsprozess beginnen. Im Gegensatz zu MLPs und CNNs, die die Sequenzdaten nicht schrittweise einlesen, ist die Leistung des LSTM besser, wenn die Daten stationär sind. Als Übung können Sie jedoch beide Fälle, saisonale und deseasonalisierte Daten als Eingabe, ausprobieren. "],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.7:</b> Vervollständigen Sie die Funktion <code>model_fit()</code> wie in den Kommentaren beschrieben.\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["# Fit a model\n","\n","def model_fit(train:list, config:list):\n","    \n","    # Input:\n","        # train: training data (seasonalized)\n","        # config: list with model hyperparameters\n","    # Output: \n","        # model: trained model\n","    \n","    # Unpack config\n","    n_input, n_output, n_nodes, n_epochs, n_batch, n_freq = config\n","    \n","    # Prepare data: deseasonalize the input data as preparation of the training dataset\n","    # Please check the train shape afterwards\n","    # train =  ??\n","    \n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE\n","    \n","    # Change the format of the dataset to be used in supervised learning  \n","    # data =  ??\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE\n","    \n","    # Devide the data in features and labels\n","    train_x, train_y = data[:, :-1], data[:, -1]\n","    \n","    # Reshape training features in the correct input format for LSTM networks\n","    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n","\n","    # Init SequentialModel\n","    model = Sequential()   \n","\n","    # Define one LSTM hidden layer followed by one dense hidden layer and \n","    # one output layer as a dense layer with one node using model.add() (f.ex. as in Fig. 7 later in this task)\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE\n","    \n","    # Compile Model\n","    model.compile(loss='mse', optimizer='adam')\n","    \n","    # Fit Model\n","    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=1)\n","    \n","    return model"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.8:</b> Die Funktion <code>model_predict()</code> erzeugt eine Vorhersage auf Basis eines vortrainierten Modells und Daten aus der Vergangenheit (Historie). Vervollständigen Sie den folgenden Code wie in den Kommentaren beschrieben. \n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["# Forecast with a pre-fit model\n","\n","def model_predict(model, history:list, config:list):\n","    # Input:\n","        # model: model returned by model_fit()\n","        # history: all available data from the past\n","        # config: list with model hyperparameters\n","    # Output: \n","        # prediction: corrected prediction of the model with the seasonal term back\n","        \n","    # Unpack config\n","    n_input, _, _, _, _, n_freq = config\n","    \n","    # Prepare data: Deseasonalize the input data as preparation of the test dataset  \n","    # The correction term has to be used after predicting, in order to give back the seasonality to the data \n","    # history =  ??\n","    # correction =  ??\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE    \n","    \n","    x_input = array(history[-n_input:]).reshape((1, n_input, 1))\n","    \n","    # Forecast\n","    yhat = model.predict(x_input, verbose=0)\n","    \n","    return correction[-n_freq] + yhat[0]"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Walk Forward Validierung\n","\n","Die `walk_forward_validation()` führt die Trainings- und Testprozesse durch, d. h. sie passt das Modell an die Trainingsdaten an, erzeugt dann Vorhersagen aus dem Testdatensatz und berechnet den Vorhersagefehler für eine Posterior-Analyse. \n","\n","Der hier zu programmierende Ansatz erzeugt Vorhersagen von genau einem Zeitschritt nach dem Ende des Trainingsdatensatzes. Daher wird die Historie-Variable für `model_predict()` mit Trainingsdaten initialisiert und mit dem Testdatensatz verkettet. Sie können einen anderen Ansatz versuchen, bei dem das Modell mit einem Historie-Array getestet wird, das nur Testdaten enthält. Beide werten das Modell auf unterschiedliche Weise aus."],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe/Frage 6.3.9:</b> Schreiben Sie bitte eine Analyse der Vor- und Nachteile beider Lösungen und erklären Sie, warum wir bei der Prognose von Zeitreihen den einen oder den anderen Ansatz verwenden sollten. \n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n","\n","\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.10:</b> Verwenden Sie die Funktionen, die Sie zuvor programmiert haben, um den folgenden Code für <code>walk_forward_validation()</code> zu vervollständigen. \n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["# walk-forward validation for univariate data\n","def walk_forward_validation(data:list, n_test:int, cfg:list):\n","    # Input:\n","        # data: time series in ndarray format\n","        # n_test: integer, splitting index in the array\n","        # cfg: list of model hyperparameters\n","    # Output:\n","        # error: rmse of predictions from the test set\n","        # predictions: predicted values\n","        # test_values: data values, ground-truth\n","    \n","    # Initialize the predictions array\n","    predictions = list()\n","    \n","    # Split dataset\n","    # train, test = ??\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE    \n","    \n","    # Fit the model\n","    # model = ??\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE \n","    \n","    # Initialize history array with training dataset as historical data\n","    history = [x for x in train]\n","    \n","    print('The model starts predicting...')\n","    \n","    # Step over each time-step in the test set\n","    for i in range(len(test)):\n","        \n","        # Make forecast for history\n","        # yhat = ??\n","        # STUDENT CODE HERE\n","\n","        # STUDENT CODE until HERE \n","                \n","        # Store forecast in list of predictions\n","        predictions.append(yhat)\n","        \n","        # Add actual observation to history for the next loop\n","        history.append(test[i])\n","        \n","    # Estimate prediction error\n","    error = measure_rmse(test, predictions)\n","    \n","    print('Prediction error:  %.3f' % error)\n","    return error, predictions, test"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Schlussfolgerung Vanilla LSTM\n","\n","Der folgende Code führt Ihr LSTM-Vanilla-Modell aus. Es steht Ihnen frei, die Parameter im *config*-Array zu ändern, die normalisierten Daten für das Training zu verwenden und auch die Art des für den Trainingsprozess verwendeten Optimierers zu ändern. \n","\n","Für die ersten Testausführungen empfehlen wir, nur einen Teil des Datensatzes zu verwenden. Ansonsten kann es eine Weile dauern. Nachdem Sie sicher sind, dass der Code wie erwartet funktioniert, führen Sie ihn für den gesamten Datensatz aus."],"metadata":{}},{"cell_type":"code","execution_count":23,"source":["# data = series.values  ## uncomment to test for the whole dataset\n","data = normalized[:2976] ## Executing first with 31 days * 24 hours * 60 mins/15 mins = 2976 \n","# data split: Forecasting the load in a day\n","# n_test = int(0.7*series.size) ## Testing with 30% of the dataset\n","n_test = 384 ## Testing with the last 4 days = 384 elements\n","# Define config, (n_input, n_output, n_nodes, n_epochs, n_batch, n_freq = config)\n","config = [48, 1, 50, 1, 10, 96]\n","# Fit and evaluate the model n_repeats times\n","score, predictions, y_test = walk_forward_validation(data, n_test, config)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 6.3.11:</b> Können Sie erklären, warum das Modell in der Lage ist, die nächste 15-minütige elektrische Last besser vorherzusagen, indem es nur einen einzigen Eingabeschritt aus der Vergangenheit verwendet? Hinweis: Was ist der Unterschied zu einem normalen feed forward neuronalen Netz?\n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.12:</b> Importieren Sie matplotlib und plotten Sie die Vorhersagen und den Testsatz in einem Plot, um eine Intuition über die Vorhersage und Ihr Modell zu bekommen.\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":24,"source":["# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE "],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Anfängerfehler\n","Nachdem Sie Ihr Modell einmal mit Hilfe des Datensatzes angepasst und bewertet haben, neigen AnfängerInnen zu denken, dass der für diese Modellkonfiguration erstellte Skill-Bericht fertig ist. Wenn jedoch dieselbe Modellarchitektur mehrmals trainiert wird, ist die erhaltene Modellfähigkeit unterschiedlich. Der Grund dafür ist die stochastische Eigenschaft des Deep Learnings. Modelle wie LSTMs verwenden bei der Anpassung Zufälligkeiten, wie z. B. zufällige Anfangsgewichte oder die Umverteilung der Daten nach jeder Trainingsepoche während des stochastischen Gradientenabstiegs."],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.13:</b> Bewerten Sie die Zufälligkeit des Trainingsergebnisses des Vanilla LSTM, das Sie bereits programmiert haben. Probieren Sie es aus, indem Sie die Funktion <code>repeat_evaluate()</code> schreiben, wobei die Funktion <code>walk_forward_validation()</code> n_repeats mal ausgeführt wird. Alle Ergebnisse sollen in einem Array namens <i>scores</i> gespeichert und zurückgegeben werden. \n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":25,"source":["# Repeat evaluation of a LSTM model\n","def repeat_evaluate(data, config, n_test, n_repeats=30):\n","    # Input:\n","        # data: time series in ndarray format\n","        # config: list of model hyperparameters\n","        # n_test: integer, splitting index in the array\n","        # n_repeats: number of times walk_forward_validation() has to be executed\n","    # Output:\n","        # scores: list object of errors returned by walk_forward_validation()\n","        \n","    # Fit and evaluate the model n times\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE \n","    return scores\n","    \n","# data = series.values  ## uncomment to test for the whole dataset\n","data = normalized[:2976] ## Executing first with 31 days * 24 hours * 60 mins/15 mins = 2976 \n","# data split\n","# n_test = int(0.7*series.size)  ## Testing with 30% of the dataset\n","n_test = 384 ## Testing with the last 4 days = 384 elements\n","# Define config, with config = [n_input, n_output, n_nodes, n_epochs, n_batch, n_freq]\n","config = [48, 1, 50, 10, 10, 96]\n","# Fit and evaluate the model n_repeats times\n","scores = repeat_evaluate(data, config, n_test, n_repeats=10)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["#### Fähigkeitsschätzung eines Stochastischen Modells\n","Da jedes Mal, wenn ein Modell angepasst wird, eine gewisse Zufälligkeit im Fähigkeitsscore zu beobachten ist, müssen mehrere Durchläufe durchgeführt werden, um eine Vorstellung von der Stabilität des Modells zu bekommen. Die endgültige Modellfähigkeit muss als Mittelwert und Varianz der Werte angegeben werden. Dies ergibt eine robuste Schätzung des Modells. <br>\n","Jedes Modelltraining muss auf dem gleichen Trainingsdatensatz und mit der gleichen Architektur durchgeführt werden, um nur die intrinsischen Trainingsveränderungen im Modell zu bewerten. Die Anzahl der Durchläufe hängt von der Zeit ab, die das Modell zum Trainieren benötigt. Mehr Wiederholungen ermöglichen ein besseres Verständnis der Variabilität der Modelllösung.<br>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.14:</b> Vervollständigen Sie die Funktion <code>summarize_scores()</code>, die als Eingabe das von <code>repeat_evaluate()</code> erzeugte Array mit den Ergebnissen erhält sowie Mittelwert und Standardabweichung der Ergebnisse ausgibt. Diese Funktion sollte auch in der Lage sein, ein Boxplot der Ergebnisse zu erstellen. \n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":26,"source":["# Summarize model performance\n","def summarize_scores(name, scores):\n","    # Input:\n","        # name: string, model name\n","        # scores: repeat_evaluate() output array\n","    # Output: \n","        # none\n","        \n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE \n","    \n","# Summarize scores\n","summarize_scores('lstm', scores)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["### Gestapeltes LSTM\n","\n","Das Vanilla-LSTM, das nur ein Hidden Layer enthält, kann einige einfache Probleme lösen. Um kompliziertere Probleme zu lösen, \"müssen wir tiefer gehen\" (falls der Begriff *Deep Learning* nicht klar war, jetzt ist er es). Durch das Stapeln mehrerer Hidden Layers kann das LSTM-Netz komplexere Merkmale des Systems (Zeitreihe, Bild, Text, Video usw.) lernen und eine bessere Ausgabe (Vorhersage, Text, Sequenz usw.) erzeugen.\n","\n","<img src=\"images\\we-have-to-go-deeper.png\" alt=\"drawing\" style=\"width:500px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 5 - Um komplexere Probleme zu lösen, ist es wichtig, das Netz vertikal zu erweitern, d.h. Schichten zu stapeln. [Bildreferenz: Film Inception (2010)]\n","</p>\n","\n","Das gestapelte LSTM ist ein Modell mit mehreren versteckten LSTM-Schichten, wobei jede Schicht mehrere Speicherzellen enthält. Da LSTMs mit Sequenzdaten arbeiten, bedeutet dies, dass das Hinzufügen von Schichten zusätzliche Abstraktionsebenen der Input Beobachtungen über die Zeit hinzufügt. [3] RNNs haben von Natur aus eine große zeitliche Tiefe, da ihr versteckter Zustand (engl. hidden state) eine Funktion aller vorherigen versteckten Zustände ist. Sie können auch von der räumlichen Tiefe profitieren, d. h. von der Stapelung mehrerer rekurrenter versteckter Schichten übereinander. [9] Abbildung 6 zeigt ein gestapeltes LSTM-Netz mit zwei Hidden Layers. \n","\n","<img src=\"images\\stackedlstm.png\" alt=\"drawing\" style=\"width:100px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 6 - Netzwerkstruktur für ein gestapeltes LSTM-Modell. Die Anzahl der LSTM-Schichten hängt von der Anwendung ab. [3]\n","</p>\n","\n","Das gestapelte LSTM-Modell erwartet eine Liste von sechs Modell-Hyperparametern, welche sind:\n","\n","- n_input: Die Anzahl der Verzögerungsbeobachtungen, die als Input für das Modell verwendet werden sollen.\n","- n_output: Die Anzahl der Ausgaben des Netzwerks.\n","- n_layers: Die Anzahl der versteckten LSTM-Schichten.\n","- n_nodes: Die Anzahl der LSTM-Einheiten in jeder versteckten Schicht. Dieser Parameter muss ein Array mit dem entsprechenden Wert pro Schicht sein.\n","- n_epochs: Die Anzahl, wie oft das Modell dem gesamten Trainingsdatensatz ausgesetzt werden soll.\n","- n_batch: Die Anzahl der Datenpunkte innerhalb einer Epoche, nach der die Gewichte aktualisiert werden.\n","- n_freq: Die Frequenz der Reihe, die für die saisonale Zerlegung verwendet wird.\n","\n","Jede LSTM-Speicherzelle benötigt eine 3D-Eingabe. Wenn ein LSTM eine Eingangssequenz von Zeitschritten verarbeitet, wird jede Speicherzelle\n","einen einzigen Wert für die gesamte Sequenz als 2D-Array aus. Um LSTM-Schichten zu stapeln, müssen wir die Konfiguration der vorherigen LSTM-Schicht ändern, um ein 3D-Array als Eingabe für die nachfolgende Schicht auszugeben. Dies ist möglich, indem man das Argument *return_sequences*\n","auf der Ebene auf True setzt (Standard ist False). Dadurch wird eine Ausgabe für jeden Eingabezeitschritt zurückgegeben und ein 3D-Array bereitgestellt. Abbildung 7 zeigt, wie der Code für zwei versteckte LSTM-Schichten aussehen sollte. \n","\n","<img src=\"images/stacking_lstm_code.png\" alt=\"drawing\" style=\"width:700px;\"/>\n","<p style=\"text-align: center;\">\n","    Abb. 7 - Code zur Definition eines gestapelten LSTM mit 2 Hidden Layers. [3]\n","</p>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.15:</b> Ändern Sie die Funktionen (<code>model_fit()</code>, <code>model_predict()</code>), die Sie zuvor geschrieben haben (für den Vanilla LSTM), um mehrere LSTM-Schichten im Modell zu haben. Füllen Sie die folgende Zelle mit Ihrem neuen Code.\n","\n","\n","\n","<ul>\n","<li> Hinweis 1: Seien Sie vorsichtig mit der Definition von n_nodes; in diesem Fall ist es ein Array mit der Anzahl der LSTM-Einheiten in jeder versteckten Schicht und nicht mehr ein einzelner Integer-Wert wie zuvor.\n","<li>Hinweis 2: <code>model_predict()</code> muss nur die Konfiguration richtig entpacken.\n","\n","\n","</li>\n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":27,"source":["# STUDENT CODE HERE\n","\n","# STUDENT CODE until HERE"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 6.3.16:</b> Nehmen wir an, Sie wollen mit demselben Netz mehrere Zeitschritte vorhersagen. Welche Variable oder Parameter des von Ihnen programmierten gestapelten LSTM-Netzes müssen Sie anpassen, um mehrere Ausgänge statt nur einem zu haben?  \n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["#### Diagnose von Under- und Overfitting\n","Eine der wichtigsten Prüfungen, die Sie an Ihrem Modell durchführen müssen, ist die Überprüfung auf Under- und Overfitting. Dazu benötigen Sie die Trainings- und Validierungsverlustfunktionen, die von der Keras-Methode `.fit()` Ihres Modells erzeugt werden. Diese Werte werden in dem *History*-Objekt gespeichert, das von dieser Funktion zurückgegeben wird. <br> \n","Normalerweise enthält das History-Objekt die Trainings-/Validierungsgenauigkeit und die Verlustfunktionen, aber dies kann sich von Lösung zu Lösung ändern. Um zu überprüfen, was das History-Objekt enthält, verwenden Sie\n","````python \n","# list all data in history\n","print(history.history.keys())\n",">> ['acc', 'loss', 'val_acc', 'val_loss']\n","```"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Aufgabe 6.3.17:</b> Ändern Sie die Funktion <code>model_fit()</code>, um die Verlustfunktion für den Trainings- und den Validierungsprozess zu sammeln und darzustellen.\n","<ul>\n","\n","<li>Die Ausgabe der Historie erhalten Sie, indem Sie einfach <code>model_metric = model.fit(...)</code> verwenden.\n","    \n","\n","<li>\n","    Da in der Funktion <code>walk_forward_validation()</code> bereits eine Variable namens <i>history</i> verwendet wird, um vergangene Daten zu speichern, soll hier der Name <b>model_metric</b> für das History-Ausgabeobjekt der Keras-Methode <code>.fit()</code> verwendet werden. Übergeben Sie einfach <b>model_metric</b> durch <code>walk_forward_validation()</code>.\n","\n","<li>Um den Trainingsdatensatz innerhalb der <code>model_fit()</code>-Funktion in einen Validierungssatz aufzuteilen, verwenden Sie den Parameter <i>validation_split</i> aus der Keras-Methode <code>.fit()</code>. Ein Splitting-Verhältnis von einem Drittel der Gesamtmenge sollte ausreichen.\n","\n","</ul>\n","\n","</div>"],"metadata":{}},{"cell_type":"code","execution_count":28,"source":["def model_fit(train, config):\n","    # Input:\n","        # train: time series data in ndarray format. Shape: (time series length x 1)\n","        # config: list of model hyperparameters\n","    # Output:\n","        # model: trained LSTM model\n","        # model_metric: History output object of the Keras function fit()\n","        \n","\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE \n"," \n","def walk_forward_validation(data, n_test, cfg):\n","    # Input:\n","        # data: time series in ndarray format\n","        # n_test: integer, splitting index in the array\n","        # cfg: list of model hyperparameters\n","    # Output:\n","        # error: rmse of predictions from the test set\n","        # model_metric: History output object of model_fit()\n","\n","    n_input, n_output, n_layers, n_nodes, n_epochs, n_batch, n_freq = cfg\n","\n","    # STUDENT CODE HERE\n","\n","    # STUDENT CODE until HERE\n","    \n","# data = series.values  ## uncomment to test for the whole dataset\n","data = normalized[:2976] ## Executing first with 31 days * 24 hours * 60 mins/15 mins = 2976 \n","\n","# Data split\n","# n_test = int(0.7*series.size)  ## Testing with 30% of the dataset\n","n_test = 384 ## Testing with the last 4 days = 384 elements\n","\n","# Define config: [n_input, n_output, n_layers, n_nodes, n_epochs, n_batch, n_freq]\n","config = [48, 1, 3, [50, 30, 50], 10, 10, 96]\n","\n","score, predictions, y_test, model_metric = walk_forward_validation(data, n_test, config)\n","\n","# Plot train and validation loss\n","pyplot.plot(model_metric.history['loss'])\n","pyplot.plot(model_metric.history['val_loss'])\n","pyplot.title('model train vs validation loss')\n","pyplot.ylabel('loss')\n","pyplot.xlabel('epoch')\n","pyplot.legend(['train', 'validation'], loc='upper right')\n","pyplot.show()"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 6.3.18:</b> Analysieren Sie die Graphen, die Sie bei der Analyse des Under-/Overfittings erhalten haben. Was können Sie über Ihr Modell sagen? Würden Sie etwas ändern, um Ihre Ergebnisse zu verbessern? Wenn ja, nehmen Sie die Änderungen vor und schreiben Sie hier eine kurze Dokumentation über Ihre Beobachtungen.\n","<ul>\n","\n","<li> Hinweis: Wenn Sie mehr über die Analyse der Trainings- und Validierungsverlustgraphen wissen möchten, können Sie sich das folgende <a href=\"https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\">Tutorial zu Over- und Underfitting von Tensorflow</a> ansehen.\n","</ul>\n","</div>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"],"metadata":{}},{"cell_type":"markdown","source":["#### Einige abschließende Hinweise \n","\n","Bevor wir die Einführung in LSTMs beenden, sollten noch einige Hinweise zu den Hyperparametern des Netzes beachtet werden:\n","\n","- Das Problem des explodierenden Gradienten kann dort bleiben: LSTMs haben nicht das Problem der verschwindenden Gradienten wie die konventionellen RNNs. Dennoch kann das Problem des explodierenden Gradienten auch bei LSTM-Modellen auftreten. Daher ist das Beschneiden der Gradienten eine gute Lösung für dieses Problem (weitere Informationen finden Sie in <a href=\"https://machinelearningmastery.com/exploding-gradients-in-neural-networks\">diesem Artikel</a>).\n","- Initialisieren Sie die Forget-Gates mit einem hohen Bias, um das Erinnern zu Beginn des Trainingsprozesses zu fördern. \n","- Bedenken Sie, dass die L2-Regularisierung bei der Arbeit mit LSTM-Netzen manchmal nicht hilfreich ist.\n","- Es ist immer gut, Dropout im rekurrenten Teil des Netzes zu implementieren (nicht in der Zeitachse des Netzes)."],"metadata":{}},{"cell_type":"markdown","source":["## Referenzen\n","[1] J. Brownlee, Introduction to time series forecasting with python: how to prepare data and develop models to predict the future. 2018.<br>\n","[2] J. Brownlee, Deep Learning for Time Series Forecasting: predict the Future with MLPs, CNNs and LSTMs in Python. 2018.<br>\n","[3] J. Brownlee, Long Short-Term Memory Networks With Python: Develop Sequence Prediction Models With Deep Learning. 2017.<br>\n","[4] G. Shmueli, Practical Time Series Forecasting with R: A Hands-On Guide, 2nd ed. Axelrod Schnall Publishers, 2016.<br>\n","[5] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to Sequence Learning with Neural Networks,” in Neural Information Processing Systems Conference, 2014.<br>\n","[6] H. Sak, A. Senior, and F. Beaufays, “Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling.”<br>\n","[7] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber, “LSTM: A Search Space Odyssey,” Trans. Neural Networks Learn. Syst., 2017.<br>\n","[8] S. Hochreiter and Ju. Schmidhuber, “Long Short-Term Memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997.<br>\n","[9] A. Graves, A. Mohamed and G. Hinton, \"Speech Recognition With Deep Recurrent Neural Networks\", 2013. <br>\n","[10] PennState Eberly College of Science, “5.1 Decomposition Models | STAT 510,” 2018. [Online]. Available: https://onlinecourses.science.psu.edu/stat510/node/69/. [Accessed: 14-Nov-2018].<br>\n","[11] A. Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks. http://karpathy.github.io/2015/05/21/rnn-effectiveness/. Version: 2015. [Last Checked: 28.09.2018]. <br>\n","[12] J. Brownlee: How to develop Deep Learning Models for univariate time series forecasting. https://machinelearningmastery.com/how-to-develop-deep-learning-models-for-univariate-time-series-forecasting/. Version: October, 2018. [Last Checked: 19.05.2019]."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python-amalea"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
