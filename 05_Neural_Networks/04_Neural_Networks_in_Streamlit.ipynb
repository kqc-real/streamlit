{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48417768",
   "metadata": {},
   "source": [
    "# üß† Woche 4: Neural Networks in Streamlit - AMALEA Deep Learning\n",
    "\n",
    "**Integration der urspr√ºnglichen AMALEA-Notebooks:**\n",
    "- \"Jetzt geht's in die Tiefe\" ‚Üí Neural Network Grundlagen\n",
    "- \"Wir trainieren nur bergab\" ‚Üí Backpropagation & Optimierung\n",
    "- \"Regression II\" ‚Üí Neural Networks f√ºr Regression\n",
    "- \"Classification Softmax\" ‚Üí Neural Networks f√ºr Klassifikation\n",
    "\n",
    "## üìö Was du heute lernst\n",
    "\n",
    "- **K√ºnstliche Neuronen** üß† - Grundbausteine des Deep Learning\n",
    "- **Neural Networks** üï∏Ô∏è - Mehrere Neuronen vernetzen\n",
    "- **Backpropagation** ‚ö° - Wie Neuronen \"lernen\"\n",
    "- **Activation Functions** üìà - ReLU, Sigmoid, Softmax verstehen\n",
    "- **Streamlit Neural Network Apps** üöÄ - Interaktive Demos erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Neural Network Grundlagen aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "\n",
    "### Das k√ºnstliche Neuron (aus \"Jetzt geht's in die Tiefe\")\n",
    "\n",
    "Ein **k√ºnstliches Neuron** ist eine mathematische Funktion, inspiriert von biologischen Nervenzellen:\n",
    "\n",
    "$$f_{neuron}(x) = œÜ(‚àë_{n=1}^m x_n ¬∑ w_n + b)$$\n",
    "\n",
    "**Komponenten:**\n",
    "- **Eingaben** (x_n): Numerische Werte (Daten oder andere Neuronen)\n",
    "- **Gewichte** (w_n): Werden mit Eingaben multipliziert\n",
    "- **Bias** (b): Konstanter Wert, wird zur Summe addiert\n",
    "- **Aktivierungsfunktion** (œÜ): Transformiert die Summe\n",
    "- **Ausgabe** (y): Aktivierung des Neurons\n",
    "\n",
    "### Einfachstes Neuron: Lineare Regression\n",
    "\n",
    "Ohne Aktivierungsfunktion: `f(x) = w * x + b`\n",
    "\n",
    "Das ist bereits **lineare Regression**! ü§Ø\n",
    "\n",
    "### Aktivierungsfunktionen (entscheidend f√ºr Deep Learning)\n",
    "\n",
    "| Funktion | Formel | Verwendung |\n",
    "|----------|--------|------------|\n",
    "| **Linear** | f(x) = x | Regression (Ausgabeschicht) |\n",
    "| **ReLU** | f(x) = max(0, x) | **Meistverwendet** in Hidden Layers |\n",
    "| **Sigmoid** | f(x) = 1/(1+e^(-x)) | Bin√§re Klassifikation |\n",
    "| **Softmax** | f(x_i) = e^(x_i)/Œ£e^(x_j) | Multi-Class Klassifikation |\n",
    "\n",
    "### Warum Neural Networks so m√§chtig sind\n",
    "\n",
    "> **Universal Approximation Theorem**: Ein ausreichend gro√ües neuronales Netz kann jede kontinuierliche Funktion beliebig genau approximieren!\n",
    "\n",
    "**Aber**: Mehr Parameter ‚â† automatisch bessere Performance (Overfitting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70013964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Neural Network Demo - Basierend auf urspr√ºnglichen AMALEA-Konzepten\n",
    "# Integration von \"Jetzt geht's in die Tiefe\" + \"Wir trainieren nur bergab\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "print(\"üß† Neural Networks - AMALEA Konzepte modernisiert\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1Ô∏è‚É£ Einfachstes Neuron implementieren (aus urspr√ºnglichem AMALEA)\n",
    "print(\"1Ô∏è‚É£ Einfachstes Neuron: f(x) = w*x + b\")\n",
    "\n",
    "def simple_neuron(x, w, b):\n",
    "    \"\"\"\n",
    "    Implementierung aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "    Aufgabe 4.1.1: Einfachstes Neuron ohne Aktivierungsfunktion\n",
    "    \"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "# Demo mit AMALEA-Beispiel (w=-1, b=1)\n",
    "w, b = -1, 1\n",
    "x_values = range(10)\n",
    "y_values = [simple_neuron(x, w, b) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_values, y_values, 'o-', linewidth=2, markersize=8)\n",
    "plt.title(\"Einfachstes Neuron (AMALEA Original: w=-1, b=1)\")\n",
    "plt.xlabel(\"x (Input)\")\n",
    "plt.ylabel(\"y (Output)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mit w={w}, b={b}: f(5) = {simple_neuron(5, w, b)}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Aktivierungsfunktionen visualisieren (aus AMALEA erweitert)\n",
    "print(f\"\\n2Ô∏è‚É£ Aktivierungsfunktionen (zentral f√ºr Deep Learning):\")\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, relu(x), 'b-', linewidth=2)\n",
    "plt.title(\"ReLU: max(0, x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, sigmoid(x), 'r-', linewidth=2)\n",
    "plt.title(\"Sigmoid: 1/(1+e^(-x))\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, tanh(x), 'g-', linewidth=2)\n",
    "plt.title(\"Tanh: tanh(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3Ô∏è‚É£ Regression mit Neural Network (aus \"Regression II\")\n",
    "print(f\"\\n3Ô∏è‚É£ Neural Network f√ºr Regression (aus AMALEA 'Regression II'):\")\n",
    "\n",
    "# K√ºnstliche Daten erstellen\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Neural Network f√ºr Regression\n",
    "nn_regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 5),  # 2 Hidden Layers\n",
    "    activation='relu',           # ReLU Aktivierung (AMALEA-Standard)\n",
    "    solver='adam',              # Adam Optimizer (aus \"Wir trainieren nur bergab\")\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_regressor.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = nn_regressor.predict(X_test_reg)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train_reg, y_train_reg, alpha=0.6, label='Training')\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Test', color='orange')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regressionsdaten\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "plt.xlabel(\"Tats√§chliche Werte\")\n",
    "plt.ylabel(\"Vorhergesagte Werte\")\n",
    "plt.title(\"Vorhersage vs. Realit√§t\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Klassifikation mit Neural Network (aus \"Classification Softmax\")\n",
    "print(f\"\\n4Ô∏è‚É£ Neural Network f√ºr Klassifikation (aus AMALEA 'Softmax'):\")\n",
    "\n",
    "# K√ºnstliche Klassifikationsdaten\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=300, n_features=2, n_redundant=0, \n",
    "    n_informative=2, n_clusters_per_class=1, \n",
    "    random_state=42\n",
    ")\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Neural Network f√ºr Klassifikation\n",
    "nn_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_classifier.fit(X_train_clf, y_train_clf)\n",
    "y_pred_clf = nn_classifier.predict(X_test_clf)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y_train_clf == i\n",
    "    plt.scatter(X_train_clf[mask, 0], X_train_clf[mask, 1], \n",
    "               c=color, alpha=0.6, label=f'Klasse {i}')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Klassifikationsdaten\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Vorhergesagte Klasse\")\n",
    "plt.ylabel(\"Tats√§chliche Klasse\")\n",
    "\n",
    "# Zahlen in der Matrix anzeigen\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Neural Network Grundlagen aus dem AMALEA-Kurs erfolgreich modernisiert!\")\n",
    "print(f\"üöÄ Jetzt erstellen wir daraus interaktive Streamlit-Apps...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f35adf",
   "metadata": {},
   "source": [
    "## üéÆ Interaktive Streamlit-App: Neural Network Playground\n",
    "\n",
    "Jetzt erstellen wir eine interaktive Streamlit-App, die alle Neural Network Konzepte aus dem urspr√ºnglichen AMALEA-Kurs vereint!\n",
    "\n",
    "### üìù Streamlit-App erstellen\n",
    "\n",
    "Erstelle eine neue Datei `neural_network_playground.py` im Ordner `Woche_4_Deep_Learning/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9169b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ Streamlit Neural Network App erstellen\n",
    "# Diese App integriert alle AMALEA Neural Network Konzepte interaktiv\n",
    "\n",
    "# Erstelle diese Datei als neural_network_playground.py\n",
    "\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import make_regression, make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# üéØ Streamlit App Configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"üß† Neural Network Playground - AMALEA 2025\",\n",
    "    page_icon=\"üß†\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# üìä Title and Introduction\n",
    "st.title(\"üß† Neural Network Playground\")\n",
    "st.markdown(\"**Interaktive Demo der AMALEA Neural Network Konzepte**\")\n",
    "\n",
    "# Sidebar f√ºr Navigation\n",
    "st.sidebar.title(\"üéõÔ∏è Navigation\")\n",
    "app_mode = st.sidebar.selectbox(\n",
    "    \"W√§hle einen Bereich:\",\n",
    "    [\"üß† Einfachstes Neuron\", \"üìà Aktivierungsfunktionen\", \"üéØ Regression Demo\", \n",
    "     \"üè∑Ô∏è Klassifikation Demo\", \"üéÆ Interaktiver Playground\"]\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# üß† BEREICH 1: Einfachstes Neuron (aus AMALEA \"Jetzt geht's in die Tiefe\")\n",
    "# ============================================================================\n",
    "\n",
    "if app_mode == \"üß† Einfachstes Neuron\":\n",
    "    st.header(\"üß† Einfachstes Neuron verstehen\")\n",
    "    st.markdown(\"**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Jetzt geht's in die Tiefe'**\")\n",
    "    \n",
    "    # Interaktive Parameter\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üéõÔ∏è Neuron-Parameter\")\n",
    "        w = st.slider(\"Gewicht (w)\", -5.0, 5.0, -1.0, 0.1)\n",
    "        b = st.slider(\"Bias (b)\", -5.0, 5.0, 1.0, 0.1)\n",
    "        \n",
    "        # Formel anzeigen\n",
    "        st.latex(f\"f(x) = w \\\\cdot x + b = {w} \\\\cdot x + {b}\")\n",
    "        \n",
    "    with col2:\n",
    "        st.subheader(\"üìä Neuron-Verhalten\")\n",
    "        x_values = np.linspace(-10, 10, 100)\n",
    "        y_values = w * x_values + b\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=x_values, y=y_values, mode='lines', name='Neuron Output'))\n",
    "        fig.update_layout(\n",
    "            title=\"Einfachstes Neuron: f(x) = w*x + b\",\n",
    "            xaxis_title=\"Input (x)\",\n",
    "            yaxis_title=\"Output f(x)\",\n",
    "            height=400\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Beispielberechnung\n",
    "    st.subheader(\"üßÆ Beispiel-Berechnung\")\n",
    "    test_input = st.number_input(\"Teste Input-Wert:\", value=5.0)\n",
    "    result = w * test_input + b\n",
    "    st.success(f\"f({test_input}) = {w} √ó {test_input} + {b} = **{result:.2f}**\")\n",
    "    \n",
    "    # AMALEA-Originale\n",
    "    st.info(\"üí° **AMALEA-Original**: Mit w=-1, b=1 ergibt f(5) = -4\")\n",
    "\n",
    "# ============================================================================\n",
    "# üìà BEREICH 2: Aktivierungsfunktionen (AMALEA Deep Learning Grundlagen)\n",
    "# ============================================================================\n",
    "\n",
    "elif app_mode == \"üìà Aktivierungsfunktionen\":\n",
    "    st.header(\"üìà Aktivierungsfunktionen verstehen\")\n",
    "    st.markdown(\"**Herzst√ºck des Deep Learning - aus den urspr√ºnglichen AMALEA-Notebooks**\")\n",
    "    \n",
    "    # Funktionen definieren\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip f√ºr numerische Stabilit√§t\n",
    "    \n",
    "    def tanh_func(x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def leaky_relu(x, alpha=0.01):\n",
    "        return np.where(x > 0, x, alpha * x)\n",
    "    \n",
    "    # Interaktive Auswahl\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    col1, col2 = st.columns([1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üéõÔ∏è Funktions-Parameter\")\n",
    "        \n",
    "        show_relu = st.checkbox(\"ReLU\", value=True)\n",
    "        show_sigmoid = st.checkbox(\"Sigmoid\", value=True)\n",
    "        show_tanh = st.checkbox(\"Tanh\", value=True)\n",
    "        show_leaky = st.checkbox(\"Leaky ReLU\", value=False)\n",
    "        \n",
    "        if show_leaky:\n",
    "            alpha = st.slider(\"Leaky ReLU Œ±\", 0.01, 0.3, 0.01, 0.01)\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"üìä Aktivierungsfunktionen\")\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        if show_relu:\n",
    "            fig.add_trace(go.Scatter(x=x, y=relu(x), mode='lines', name='ReLU: max(0,x)', line=dict(color='blue')))\n",
    "        if show_sigmoid:\n",
    "            fig.add_trace(go.Scatter(x=x, y=sigmoid(x), mode='lines', name='Sigmoid: 1/(1+e^(-x))', line=dict(color='red')))\n",
    "        if show_tanh:\n",
    "            fig.add_trace(go.Scatter(x=x, y=tanh_func(x), mode='lines', name='Tanh: tanh(x)', line=dict(color='green')))\n",
    "        if show_leaky:\n",
    "            fig.add_trace(go.Scatter(x=x, y=leaky_relu(x, alpha), mode='lines', name=f'Leaky ReLU (Œ±={alpha})', line=dict(color='orange')))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Aktivierungsfunktionen im Vergleich\",\n",
    "            xaxis_title=\"Input (x)\",\n",
    "            yaxis_title=\"Output f(x)\",\n",
    "            height=500,\n",
    "            hovermode='x unified'\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Verwendungszwecke\n",
    "    st.subheader(\"üéØ Wann welche Funktion verwenden?\")\n",
    "    \n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"\"\"\n",
    "        **üîµ ReLU**\n",
    "        - ‚úÖ Standard f√ºr Hidden Layers\n",
    "        - ‚úÖ Schnell zu berechnen\n",
    "        - ‚ùå \"Dying ReLU\" Problem\n",
    "        \"\"\")\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"\"\"\n",
    "        **üî¥ Sigmoid**\n",
    "        - ‚úÖ Bin√§re Klassifikation (Output)\n",
    "        - ‚úÖ Werte zwischen 0 und 1\n",
    "        - ‚ùå Vanishing Gradient Problem\n",
    "        \"\"\")\n",
    "    \n",
    "    with col3:\n",
    "        st.markdown(\"\"\"\n",
    "        **üü¢ Tanh**\n",
    "        - ‚úÖ Werte zwischen -1 und 1\n",
    "        - ‚úÖ Zero-centered\n",
    "        - ‚ùå Auch Vanishing Gradient\n",
    "        \"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# üéØ BEREICH 3: Regression Demo (aus AMALEA \"Regression II\")\n",
    "# ============================================================================\n",
    "\n",
    "elif app_mode == \"üéØ Regression Demo\":\n",
    "    st.header(\"üéØ Neural Network Regression\")\n",
    "    st.markdown(\"**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Regression II - K√ºnstliche Gehirne'**\")\n",
    "    \n",
    "    # Daten generieren\n",
    "    col1, col2 = st.columns([1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üéõÔ∏è Daten-Parameter\")\n",
    "        n_samples = st.slider(\"Anzahl Samples\", 50, 500, 200)\n",
    "        noise_level = st.slider(\"Noise Level\", 0.0, 50.0, 10.0)\n",
    "        \n",
    "        st.subheader(\"üß† Netzwerk-Parameter\")\n",
    "        hidden_layers = st.selectbox(\"Hidden Layer\", [(10,), (20,), (10, 5), (20, 10)])\n",
    "        activation = st.selectbox(\"Aktivierung\", ['relu', 'tanh', 'logistic'])\n",
    "        learning_rate = st.selectbox(\"Learning Rate\", [0.001, 0.01, 0.1])\n",
    "        \n",
    "        train_button = st.button(\"üöÄ Trainiere Neural Network!\")\n",
    "    \n",
    "    with col2:\n",
    "        if train_button:\n",
    "            # Daten erstellen\n",
    "            X, y = make_regression(n_samples=n_samples, n_features=1, noise=noise_level, random_state=42)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Skalierung\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "            X_test_scaled = scaler_X.transform(X_test)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "            \n",
    "            # Neural Network trainieren\n",
    "            with st.spinner(\"Training Neural Network...\"):\n",
    "                nn = MLPRegressor(\n",
    "                    hidden_layer_sizes=hidden_layers,\n",
    "                    activation=activation,\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    max_iter=1000,\n",
    "                    random_state=42\n",
    "                )\n",
    "                nn.fit(X_train_scaled, y_train_scaled)\n",
    "                \n",
    "                # Vorhersagen\n",
    "                y_pred_scaled = nn.predict(X_test_scaled)\n",
    "                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "            \n",
    "            # Visualisierung\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Trainingsdaten\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_train.ravel(), y=y_train, mode='markers', \n",
    "                name='Training Data', opacity=0.6, marker=dict(color='blue')\n",
    "            ))\n",
    "            \n",
    "            # Testdaten\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_test.ravel(), y=y_test, mode='markers', \n",
    "                name='Test Data (True)', marker=dict(color='red')\n",
    "            ))\n",
    "            \n",
    "            # Vorhersagen\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X_test.ravel(), y=y_pred, mode='markers', \n",
    "                name='Predictions', marker=dict(color='green', symbol='x', size=10)\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=\"Neural Network Regression Results\",\n",
    "                xaxis_title=\"X\",\n",
    "                yaxis_title=\"y\",\n",
    "                height=500\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Metriken\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            st.success(f\"üéØ Mean Squared Error: **{mse:.2f}**\")\n",
    "\n",
    "# ============================================================================\n",
    "# üè∑Ô∏è BEREICH 4: Klassifikation Demo (aus AMALEA \"Classification Softmax\")\n",
    "# ============================================================================\n",
    "\n",
    "elif app_mode == \"üè∑Ô∏è Klassifikation Demo\":\n",
    "    st.header(\"üè∑Ô∏è Neural Network Klassifikation\")\n",
    "    st.markdown(\"**Aus dem urspr√ºnglichen AMALEA-Kurs: 'Classification Softmax-Eis'**\")\n",
    "    \n",
    "    # Dataset Auswahl\n",
    "    dataset_choice = st.selectbox(\n",
    "        \"üìä Dataset w√§hlen:\",\n",
    "        [\"Iris (AMALEA-Klassiker)\", \"K√ºnstliche Daten\", \"Kreise (Non-linear)\"]\n",
    "    )\n",
    "    \n",
    "    col1, col2 = st.columns([1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üß† Netzwerk-Konfiguration\")\n",
    "        \n",
    "        if dataset_choice == \"Iris (AMALEA-Klassiker)\":\n",
    "            # Iris Dataset\n",
    "            iris = load_iris()\n",
    "            X, y = iris.data, iris.target\n",
    "            feature_names = iris.feature_names\n",
    "            target_names = iris.target_names\n",
    "            \n",
    "            # Feature Auswahl\n",
    "            feature_1 = st.selectbox(\"Feature 1\", feature_names, index=0)\n",
    "            feature_2 = st.selectbox(\"Feature 2\", feature_names, index=1)\n",
    "            \n",
    "            X_viz = X[:, [feature_names.index(feature_1), feature_names.index(feature_2)]]\n",
    "            \n",
    "        elif dataset_choice == \"K√ºnstliche Daten\":\n",
    "            n_samples = st.slider(\"Anzahl Samples\", 100, 1000, 300)\n",
    "            n_classes = st.slider(\"Anzahl Klassen\", 2, 4, 3)\n",
    "            X_viz, y = make_classification(\n",
    "                n_samples=n_samples, n_features=2, n_redundant=0, \n",
    "                n_informative=2, n_clusters_per_class=1, n_classes=n_classes,\n",
    "                random_state=42\n",
    "            )\n",
    "            target_names = [f\"Klasse {i}\" for i in range(n_classes)]\n",
    "            \n",
    "        else:  # Kreise\n",
    "            from sklearn.datasets import make_circles\n",
    "            n_samples = st.slider(\"Anzahl Samples\", 100, 1000, 300)\n",
    "            noise = st.slider(\"Noise\", 0.0, 0.3, 0.1)\n",
    "            X_viz, y = make_circles(n_samples=n_samples, noise=noise, random_state=42)\n",
    "            target_names = [\"Innen\", \"Au√üen\"]\n",
    "        \n",
    "        # Netzwerk Parameter\n",
    "        hidden_layers = st.selectbox(\"Hidden Layers\", [(10,), (20,), (10, 5), (50, 20)])\n",
    "        activation = st.selectbox(\"Aktivierung\", ['relu', 'tanh', 'logistic'])\n",
    "        solver = st.selectbox(\"Optimizer\", ['adam', 'sgd', 'lbfgs'])\n",
    "        \n",
    "        train_classification = st.button(\"üöÄ Trainiere Klassifikator!\")\n",
    "    \n",
    "    with col2:\n",
    "        if train_classification:\n",
    "            # Daten aufteilen\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_viz, y, test_size=0.2, stratify=y, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Skalierung\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Neural Network trainieren\n",
    "            with st.spinner(\"Training Classifier...\"):\n",
    "                nn_clf = MLPClassifier(\n",
    "                    hidden_layer_sizes=hidden_layers,\n",
    "                    activation=activation,\n",
    "                    solver=solver,\n",
    "                    max_iter=1000,\n",
    "                    random_state=42\n",
    "                )\n",
    "                nn_clf.fit(X_train_scaled, y_train)\n",
    "                y_pred = nn_clf.predict(X_test_scaled)\n",
    "            \n",
    "            # Entscheidungsgrenze visualisieren\n",
    "            h = 0.02\n",
    "            x_min, x_max = X_viz[:, 0].min() - 1, X_viz[:, 0].max() + 1\n",
    "            y_min, y_max = X_viz[:, 1].min() - 1, X_viz[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                               np.arange(y_min, y_max, h))\n",
    "            \n",
    "            mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "            mesh_points_scaled = scaler.transform(mesh_points)\n",
    "            Z = nn_clf.predict(mesh_points_scaled)\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            \n",
    "            # Plot erstellen\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Entscheidungsgrenze\n",
    "            fig.add_trace(go.Contour(\n",
    "                x=np.arange(x_min, x_max, h),\n",
    "                y=np.arange(y_min, y_max, h),\n",
    "                z=Z,\n",
    "                showscale=False,\n",
    "                opacity=0.3,\n",
    "                colorscale='Viridis'\n",
    "            ))\n",
    "            \n",
    "            # Datenpunkte\n",
    "            colors = ['red', 'blue', 'green', 'orange']\n",
    "            for i, target_name in enumerate(target_names):\n",
    "                mask = y == i\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=X_viz[mask, 0], y=X_viz[mask, 1],\n",
    "                    mode='markers', name=target_name,\n",
    "                    marker=dict(color=colors[i % len(colors)], size=8)\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=\"Neural Network Classification + Decision Boundary\",\n",
    "                xaxis_title=\"Feature 1\",\n",
    "                yaxis_title=\"Feature 2\",\n",
    "                height=500\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            st.success(f\"üéØ Test Accuracy: **{accuracy:.2%}**\")\n",
    "            \n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            fig_cm = px.imshow(cm, text_auto=True, aspect=\"auto\",\n",
    "                             x=target_names[:len(np.unique(y))], \n",
    "                             y=target_names[:len(np.unique(y))],\n",
    "                             title=\"Confusion Matrix\")\n",
    "            st.plotly_chart(fig_cm, use_container_width=True)\n",
    "\n",
    "# ============================================================================\n",
    "# üéÆ BEREICH 5: Interaktiver Playground\n",
    "# ============================================================================\n",
    "\n",
    "else:  # Interaktiver Playground\n",
    "    st.header(\"üéÆ Neural Network Playground\")\n",
    "    st.markdown(\"**Experimentiere mit verschiedenen Architekturen!**\")\n",
    "    \n",
    "    # Zwei-Spalten Layout\n",
    "    col1, col2 = st.columns([1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"üèóÔ∏è Netzwerk-Architektur\")\n",
    "        \n",
    "        # Layer Konfiguration\n",
    "        n_layers = st.slider(\"Anzahl Hidden Layers\", 1, 5, 2)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            neurons = st.slider(f\"Layer {i+1} Neuronen\", 1, 100, 10*(i+1))\n",
    "            layers.append(neurons)\n",
    "        \n",
    "        # Aktivierungsfunktion\n",
    "        activation = st.selectbox(\"Aktivierungsfunktion\", ['relu', 'tanh', 'logistic'])\n",
    "        \n",
    "        # Trainings-Parameter\n",
    "        st.subheader(\"üéØ Training\")\n",
    "        learning_rate = st.select_slider(\"Learning Rate\", [0.001, 0.01, 0.1, 1.0])\n",
    "        max_iter = st.slider(\"Max Iterations\", 100, 2000, 500)\n",
    "        \n",
    "        # Problem Type\n",
    "        problem_type = st.radio(\"Problem Type\", [\"Regression\", \"Classification\"])\n",
    "        \n",
    "        st.info(f\"üß† **Architektur**: {layers}\\\\nüìä **Aktivierung**: {activation}\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"üéØ Live Training\")\n",
    "        \n",
    "        if st.button(\"üöÄ Starte Training!\"):\n",
    "            if problem_type == \"Regression\":\n",
    "                # Regression Problem\n",
    "                X, y = make_regression(n_samples=300, n_features=1, noise=15, random_state=42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                \n",
    "                scaler_X = StandardScaler()\n",
    "                scaler_y = StandardScaler()\n",
    "                X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "                X_test_scaled = scaler_X.transform(X_test)\n",
    "                y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                # Model\n",
    "                model = MLPRegressor(\n",
    "                    hidden_layer_sizes=tuple(layers),\n",
    "                    activation=activation,\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    max_iter=max_iter,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Training mit Progress\n",
    "                progress_bar = st.progress(0)\n",
    "                status_text = st.empty()\n",
    "                \n",
    "                model.fit(X_train_scaled, y_train_scaled)\n",
    "                progress_bar.progress(100)\n",
    "                status_text.text(\"Training completed!\")\n",
    "                \n",
    "                # Vorhersagen\n",
    "                y_pred_scaled = model.predict(X_test_scaled)\n",
    "                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "                \n",
    "                # Plot\n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Scatter(x=X_test.ravel(), y=y_test, mode='markers', name='True'))\n",
    "                fig.add_trace(go.Scatter(x=X_test.ravel(), y=y_pred, mode='markers', name='Predicted'))\n",
    "                fig.update_layout(title=\"Regression Results\", height=400)\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "                \n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                st.metric(\"MSE\", f\"{mse:.2f}\")\n",
    "                \n",
    "            else:\n",
    "                # Classification Problem\n",
    "                X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                         n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Model\n",
    "                model = MLPClassifier(\n",
    "                    hidden_layer_sizes=tuple(layers),\n",
    "                    activation=activation,\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    max_iter=max_iter,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Training\n",
    "                progress_bar = st.progress(0)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                progress_bar.progress(100)\n",
    "                \n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Plot\n",
    "                fig = go.Figure()\n",
    "                colors = ['red', 'blue']\n",
    "                for i in [0, 1]:\n",
    "                    mask = y_test == i\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=X_test[mask, 0], y=X_test[mask, 1],\n",
    "                        mode='markers', name=f'Class {i} (True)',\n",
    "                        marker=dict(color=colors[i])\n",
    "                    ))\n",
    "                    \n",
    "                    mask_pred = y_pred == i\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=X_test[mask_pred, 0], y=X_test[mask_pred, 1],\n",
    "                        mode='markers', name=f'Class {i} (Pred)',\n",
    "                        marker=dict(color=colors[i], symbol='x', size=10)\n",
    "                    ))\n",
    "                \n",
    "                fig.update_layout(title=\"Classification Results\", height=400)\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                st.metric(\"Accuracy\", f\"{accuracy:.2%}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Footer\n",
    "# ============================================================================\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\"**üéì AMALEA 2025 - Neural Networks modernisiert mit Streamlit**\")\n",
    "st.markdown(\"Alle urspr√ºnglichen AMALEA-Konzepte interaktiv erlebbar!\")\n",
    "'''\n",
    "\n",
    "# Speichere die Streamlit-App\n",
    "with open(\"neural_network_playground.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"‚úÖ Streamlit App 'neural_network_playground.py' erstellt!\")\n",
    "print(\"\\nüöÄ Starte die App mit:\")\n",
    "print(\"streamlit run neural_network_playground.py\")\n",
    "\n",
    "# Test der wichtigsten Komponenten\n",
    "print(\"\\nüß™ Teste Neural Network Komponenten:\")\n",
    "\n",
    "# Test 1: Einfachstes Neuron (AMALEA Original)\n",
    "def simple_neuron(x, w=-1, b=1):\n",
    "    return w * x + b\n",
    "\n",
    "test_result = simple_neuron(5)\n",
    "print(f\"‚úÖ Einfachstes Neuron: f(5) = {test_result} (AMALEA-Original: -4)\")\n",
    "\n",
    "# Test 2: Aktivierungsfunktionen\n",
    "def relu_test(x):\n",
    "    return max(0, x)\n",
    "\n",
    "print(f\"‚úÖ ReLU(-2) = {relu_test(-2)}, ReLU(3) = {relu_test(3)}\")\n",
    "\n",
    "print(\"\\nüéØ Die Streamlit-App integriert alle AMALEA Neural Network Konzepte:\")\n",
    "print(\"- Einfachstes Neuron (aus 'Jetzt geht's in die Tiefe')\")\n",
    "print(\"- Aktivierungsfunktionen (ReLU, Sigmoid, Tanh)\")\n",
    "print(\"- Regression (aus 'Regression II')\")\n",
    "print(\"- Klassifikation (aus 'Classification Softmax')\")\n",
    "print(\"- Interaktiver Playground f√ºr Experimente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89661cd",
   "metadata": {},
   "source": [
    "## üéì Backpropagation & Gradient Descent (aus \"Wir trainieren nur bergab\")\n",
    "\n",
    "Ein Neural Network ist nur so gut wie sein Training! Hier verstehen wir die wichtigsten Konzepte aus dem urspr√ºnglichen AMALEA-Kurs \"Wir trainieren nur bergab\".\n",
    "\n",
    "### üèîÔ∏è Warum \"bergab\"?\n",
    "\n",
    "Das Training eines Neural Networks ist wie das Finden des tiefsten Punkts in einer bergigen Landschaft:\n",
    "\n",
    "- **üèîÔ∏è Berglandschaft** = Loss Function (Fehlerfunktion)\n",
    "- **üìç Position** = Aktuelle Gewichte des Networks\n",
    "- **‚¨áÔ∏è Bergab gehen** = Gradient Descent (Gewichte verbessern)\n",
    "- **üéØ Tal** = Minimum der Loss Function (beste Gewichte)\n",
    "\n",
    "### üìä Loss Functions verstehen\n",
    "\n",
    "| Problem | Loss Function | Formel | Verwendung |\n",
    "|---------|---------------|--------|------------|\n",
    "| **Regression** | Mean Squared Error | MSE = 1/n Œ£(y - ≈∑)¬≤ | Kontinuierliche Werte |\n",
    "| **Binary Classification** | Binary Cross-Entropy | BCE = -1/n Œ£[y log(≈∑) + (1-y)log(1-≈∑)] | 2 Klassen |\n",
    "| **Multi-Class** | Categorical Cross-Entropy | CCE = -1/n Œ£ Œ£ y_ij log(≈∑_ij) | Mehrere Klassen |\n",
    "\n",
    "### ‚ö° Gradient Descent Algorithmus\n",
    "\n",
    "1. **Forward Pass**: Input ‚Üí Hidden Layers ‚Üí Output\n",
    "2. **Loss berechnen**: Vergleiche Output mit Ground Truth\n",
    "3. **Backward Pass**: Berechne Gradienten (‚àÇLoss/‚àÇWeight)\n",
    "4. **Weight Update**: w_new = w_old - learning_rate √ó gradient\n",
    "5. **Wiederholen** bis Konvergenz\n",
    "\n",
    "### üéõÔ∏è Hyperparameter aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "\n",
    "| Parameter | Beschreibung | Typische Werte | Auswirkung |\n",
    "|-----------|--------------|----------------|------------|\n",
    "| **Learning Rate** | Schrittgr√∂√üe beim \"Bergabgehen\" | 0.001 - 0.1 | Zu gro√ü: √úberspringen, Zu klein: Langsam |\n",
    "| **Batch Size** | Anzahl Samples pro Update | 32, 64, 128 | Gr√∂√üer: Stabilere Gradienten |\n",
    "| **Epochs** | Vollst√§ndige Durchl√§ufe durch Daten | 10 - 1000 | Mehr: Bessere Konvergenz (aber Overfitting!) |\n",
    "| **Hidden Layers** | Netzwerk-Architektur | 1-5 Layers | Mehr: Komplexere Funktionen |\n",
    "| **Neurons per Layer** | Kapazit√§t pro Layer | 10-1000 | Mehr: H√∂here Kapazit√§t |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèîÔ∏è Gradient Descent Visualisierung - AMALEA \"Wir trainieren nur bergab\"\n",
    "# Verstehe das \"Bergab\"-Konzept interaktiv!\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "print(\"üèîÔ∏è Gradient Descent - Das 'Bergab' verstehen\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1Ô∏è‚É£ Einfache 1D Loss Function (Parabel)\n",
    "print(\"1Ô∏è‚É£ Einfaches Beispiel: Loss = (w - 2)¬≤\")\n",
    "\n",
    "def loss_function_1d(w):\n",
    "    \"\"\"Einfache quadratische Loss Function\"\"\"\n",
    "    return (w - 2)**2\n",
    "\n",
    "def gradient_1d(w):\n",
    "    \"\"\"Gradient der Loss Function\"\"\"\n",
    "    return 2 * (w - 2)\n",
    "\n",
    "# Gradient Descent Simulation\n",
    "def gradient_descent_1d(start_w, learning_rate, iterations):\n",
    "    \"\"\"1D Gradient Descent Simulation\"\"\"\n",
    "    w_history = [start_w]\n",
    "    loss_history = [loss_function_1d(start_w)]\n",
    "    \n",
    "    w = start_w\n",
    "    for i in range(iterations):\n",
    "        grad = gradient_1d(w)\n",
    "        w = w - learning_rate * grad  # Das \"Bergab\"-Update!\n",
    "        \n",
    "        w_history.append(w)\n",
    "        loss_history.append(loss_function_1d(w))\n",
    "    \n",
    "    return w_history, loss_history\n",
    "\n",
    "# Verschiedene Learning Rates testen\n",
    "learning_rates = [0.1, 0.3, 0.8]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss Function\n",
    "plt.subplot(1, 3, 1)\n",
    "w_range = np.linspace(-1, 5, 100)\n",
    "loss_range = [loss_function_1d(w) for w in w_range]\n",
    "plt.plot(w_range, loss_range, 'k-', linewidth=2, label='Loss Function')\n",
    "plt.axvline(x=2, color='red', linestyle='--', alpha=0.7, label='Optimum (w=2)')\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Landscape')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Gradient Descent Pfade\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(w_range, loss_range, 'k-', linewidth=2, alpha=0.3)\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    w_hist, loss_hist = gradient_descent_1d(start_w=4.5, learning_rate=lr, iterations=10)\n",
    "    plt.plot(w_hist, loss_hist, 'o-', color=color, label=f'LR={lr}', markersize=4)\n",
    "\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gradient Descent Pfade')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Konvergenz √ºber Zeit\n",
    "plt.subplot(1, 3, 3)\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    w_hist, loss_hist = gradient_descent_1d(start_w=4.5, learning_rate=lr, iterations=20)\n",
    "    plt.plot(loss_hist, color=color, label=f'LR={lr}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss √ºber Zeit')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2Ô∏è‚É£ Learning Rate Auswirkungen demonstrieren\n",
    "print(f\"\\n2Ô∏è‚É£ Learning Rate Auswirkungen (AMALEA-Konzept):\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    w_hist, loss_hist = gradient_descent_1d(start_w=4.5, learning_rate=lr, iterations=10)\n",
    "    final_w = w_hist[-1]\n",
    "    final_loss = loss_hist[-1]\n",
    "    print(f\"LR={lr}: Finales w={final_w:.3f}, Loss={final_loss:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"- LR=0.1: Langsam aber stabil\")\n",
    "print(f\"- LR=0.3: Gute Balance\")\n",
    "print(f\"- LR=0.8: Schnell, aber m√∂glicherweise instabil\")\n",
    "\n",
    "# 3Ô∏è‚É£ 2D Loss Surface (f√ºr Multi-Parameter Networks)\n",
    "print(f\"\\n3Ô∏è‚É£ 2D Loss Surface (echte Neural Networks haben viele Parameter):\")\n",
    "\n",
    "def loss_function_2d(w1, w2):\n",
    "    \"\"\"2D Loss Function (Rosenbrock-√§hnlich)\"\"\"\n",
    "    return (1 - w1)**2 + 100 * (w2 - w1**2)**2\n",
    "\n",
    "# Grid f√ºr Visualisierung\n",
    "w1_range = np.linspace(-2, 2, 50)\n",
    "w2_range = np.linspace(-1, 3, 50)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "Z = loss_function_2d(W1, W2)\n",
    "\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "# 3D Surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('Weight 1')\n",
    "ax1.set_ylabel('Weight 2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('3D Loss Surface')\n",
    "\n",
    "# Contour Plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W1, W2, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('Weight 1')\n",
    "ax2.set_ylabel('Weight 2')\n",
    "ax2.set_title('Loss Contours')\n",
    "plt.colorbar(contour)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Optimizers Vergleich (aus urspr√ºnglichem AMALEA erweitert)\n",
    "print(f\"\\n4Ô∏è‚É£ Optimizer-Vergleich (moderne Erweiterung der AMALEA-Konzepte):\")\n",
    "\n",
    "optimizers_info = {\n",
    "    \"SGD\": {\n",
    "        \"Beschreibung\": \"Stochastic Gradient Descent\",\n",
    "        \"Update\": \"w = w - lr * gradient\",\n",
    "        \"Vorteile\": \"Einfach, wenig Speicher\",\n",
    "        \"Nachteile\": \"Langsam, kann steckenbleiben\"\n",
    "    },\n",
    "    \"SGD + Momentum\": {\n",
    "        \"Beschreibung\": \"SGD mit Schwung\",\n",
    "        \"Update\": \"v = Œ≤*v + lr*grad; w = w - v\",\n",
    "        \"Vorteile\": \"Schneller, √ºberwindet lokale Minima\",\n",
    "        \"Nachteile\": \"Ein zus√§tzlicher Hyperparameter\"\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"Beschreibung\": \"Adaptive Moment Estimation\",\n",
    "        \"Update\": \"Adaptiert lr per Parameter\",\n",
    "        \"Vorteile\": \"Robust, wenig Tuning n√∂tig\",\n",
    "        \"Nachteile\": \"Mehr Speicher, nicht immer optimal\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, info in optimizers_info.items():\n",
    "    print(f\"\\nüöÄ {name}:\")\n",
    "    print(f\"   üìù {info['Beschreibung']}\")\n",
    "    print(f\"   ‚ö° Update: {info['Update']}\")\n",
    "    print(f\"   ‚úÖ Vorteile: {info['Vorteile']}\")\n",
    "    print(f\"   ‚ùå Nachteile: {info['Nachteile']}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Praktische Tipps aus dem AMALEA-Kurs\n",
    "print(f\"\\n5Ô∏è‚É£ Praktische Tipps f√ºr das Training (AMALEA-Weisheiten):\")\n",
    "\n",
    "tips = [\n",
    "    \"üéØ Starte mit Learning Rate 0.01 oder 0.001\",\n",
    "    \"üìä Plotte die Loss Kurve - sie sollte monoton fallen\",\n",
    "    \"‚è∞ Fr√ºh stoppen wenn Validation Loss steigt (Overfitting!)\",\n",
    "    \"üîÑ Bei Oszillation: Learning Rate reduzieren\",\n",
    "    \"‚ö° Bei langsamer Konvergenz: Learning Rate erh√∂hen\",\n",
    "    \"üß† Mehr Hidden Units f√ºr komplexere Probleme\",\n",
    "    \"‚öñÔ∏è Regularization (Dropout) gegen Overfitting\",\n",
    "    \"üìà Batch Normalization f√ºr tiefere Networks\"\n",
    "]\n",
    "\n",
    "for tip in tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Das waren die wichtigsten Konzepte aus 'Wir trainieren nur bergab'!\")\n",
    "print(f\"üöÄ Jetzt verstehst du, wie Neural Networks wirklich lernen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c90f20",
   "metadata": {},
   "source": [
    "## üç¶ Softmax verstehen (aus \"Softmax-Eis f√ºr einen one-hot day\")\n",
    "\n",
    "Das **Softmax** ist die wichtigste Aktivierungsfunktion f√ºr Multi-Class Klassifikation!\n",
    "\n",
    "### üßÆ Softmax Mathematik\n",
    "\n",
    "F√ºr einen Vektor **z** = [z‚ÇÅ, z‚ÇÇ, ..., z‚Çñ]:\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}$$\n",
    "\n",
    "**Eigenschaften:**\n",
    "- ‚úÖ Ausgabewerte sind **Wahrscheinlichkeiten** (0 ‚â§ p ‚â§ 1)\n",
    "- ‚úÖ **Summe = 1** (alle Wahrscheinlichkeiten zusammen)\n",
    "- ‚úÖ **Differenzierbar** (wichtig f√ºr Backpropagation)\n",
    "\n",
    "### üéØ One-Hot Encoding\n",
    "\n",
    "**Problem**: Computer verstehen keine Kategorien wie \"Katze\", \"Hund\", \"Vogel\"\n",
    "\n",
    "**L√∂sung**: One-Hot Encoding\n",
    "\n",
    "| Tier | One-Hot Vector |\n",
    "|------|----------------|\n",
    "| Katze | [1, 0, 0] |\n",
    "| Hund | [0, 1, 0] |\n",
    "| Vogel | [0, 0, 1] |\n",
    "\n",
    "### üè∑Ô∏è Multi-Class Klassifikation Pipeline\n",
    "\n",
    "1. **Input** ‚Üí Neural Network ‚Üí **Raw Scores** (Logits)\n",
    "2. **Logits** ‚Üí Softmax ‚Üí **Wahrscheinlichkeiten**\n",
    "3. **Wahrscheinlichkeiten** ‚Üí argmax ‚Üí **Prediction**\n",
    "4. **Prediction** ‚Üî **One-Hot Ground Truth** ‚Üí **Loss**\n",
    "\n",
    "### üìä Cross-Entropy Loss verstehen\n",
    "\n",
    "```\n",
    "Loss = -log(p_correct_class)\n",
    "```\n",
    "\n",
    "- Wenn p_correct = 1.0 ‚Üí Loss = 0 (perfekt!)\n",
    "- Wenn p_correct = 0.5 ‚Üí Loss = 0.69 (okay)\n",
    "- Wenn p_correct = 0.1 ‚Üí Loss = 2.30 (schlecht!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üç¶ Softmax Demo - Verstehe \"Softmax-Eis f√ºr einen one-hot day\"\n",
    "# Integration des urspr√ºnglichen AMALEA-Konzepts\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üç¶ Softmax verstehen - AMALEA 'one-hot day'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1Ô∏è‚É£ Softmax Funktion implementieren\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax Aktivierungsfunktion\n",
    "    Aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "    \"\"\"\n",
    "    # Numerische Stabilit√§t: subtrahiere max(z)\n",
    "    z_stable = z - np.max(z)\n",
    "    exp_z = np.exp(z_stable)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "# 2Ô∏è‚É£ Beispiel aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "print(\"1Ô∏è‚É£ AMALEA-Beispiel: 3 Klassen (Katze, Hund, Vogel)\")\n",
    "\n",
    "# Raw Scores (Logits) vom Neural Network\n",
    "logits_examples = [\n",
    "    [2.0, 1.0, 0.1],    # Eindeutig Katze\n",
    "    [0.5, 0.6, 0.4],    # Unklare Entscheidung\n",
    "    [-1.0, 3.0, -0.5],  # Eindeutig Hund\n",
    "    [0.1, 0.2, 2.5]     # Eindeutig Vogel\n",
    "]\n",
    "\n",
    "class_names = [\"üê± Katze\", \"üê∂ Hund\", \"üê¶ Vogel\"]\n",
    "\n",
    "for i, logits in enumerate(logits_examples):\n",
    "    probs = softmax(np.array(logits))\n",
    "    predicted_class = np.argmax(probs)\n",
    "    confidence = probs[predicted_class]\n",
    "    \n",
    "    print(f\"\\nüìä Beispiel {i+1}:\")\n",
    "    print(f\"   Logits: {logits}\")\n",
    "    print(f\"   Softmax: [{probs[0]:.3f}, {probs[1]:.3f}, {probs[2]:.3f}]\")\n",
    "    print(f\"   Summe: {np.sum(probs):.3f} (sollte 1.0 sein!)\")\n",
    "    print(f\"   Vorhersage: {class_names[predicted_class]} (Konfidenz: {confidence:.1%})\")\n",
    "\n",
    "# 3Ô∏è‚É£ Softmax Visualisierung\n",
    "print(f\"\\n2Ô∏è‚É£ Softmax-Verhalten visualisieren:\")\n",
    "\n",
    "# Verschiedene \"Temperaturen\" testen\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "base_logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    \n",
    "    # \"Temperatur\" anwenden\n",
    "    scaled_logits = base_logits / temp\n",
    "    probs = softmax(scaled_logits)\n",
    "    \n",
    "    bars = plt.bar(class_names, probs, color=['orange', 'blue', 'green'])\n",
    "    plt.title(f'Temperatur = {temp}')\n",
    "    plt.ylabel('Wahrscheinlichkeit')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Werte anzeigen\n",
    "    for bar, prob in zip(bars, probs):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{prob:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üå°Ô∏è Temperatur-Effekt:\")\n",
    "print(f\"   - Niedrige Temperatur (0.5): Scharfe Entscheidungen\")\n",
    "print(f\"   - Hohe Temperatur (5.0): Weiche Entscheidungen\")\n",
    "\n",
    "# 4Ô∏è‚É£ One-Hot Encoding Demo\n",
    "print(f\"\\n3Ô∏è‚É£ One-Hot Encoding (essentiell f√ºr Training):\")\n",
    "\n",
    "def create_one_hot(class_index, num_classes):\n",
    "    \"\"\"One-Hot Vector erstellen\"\"\"\n",
    "    one_hot = np.zeros(num_classes)\n",
    "    one_hot[class_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Beispiele\n",
    "examples = [\n",
    "    (\"üê± Katze\", 0),\n",
    "    (\"üê∂ Hund\", 1), \n",
    "    (\"üê¶ Vogel\", 2)\n",
    "]\n",
    "\n",
    "print(\"One-Hot Encoding Beispiele:\")\n",
    "for name, idx in examples:\n",
    "    one_hot = create_one_hot(idx, 3)\n",
    "    print(f\"   {name}: {one_hot}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Cross-Entropy Loss berechnen\n",
    "print(f\"\\n4Ô∏è‚É£ Cross-Entropy Loss verstehen:\")\n",
    "\n",
    "def cross_entropy_loss(predicted_probs, true_one_hot):\n",
    "    \"\"\"Cross-Entropy Loss berechnen\"\"\"\n",
    "    # Numerische Stabilit√§t\n",
    "    predicted_probs = np.clip(predicted_probs, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(true_one_hot * np.log(predicted_probs))\n",
    "\n",
    "# Beispiele f√ºr verschiedene Vorhersagequalit√§ten\n",
    "scenarios = [\n",
    "    (\"Perfekte Vorhersage\", [0.99, 0.005, 0.005], [1, 0, 0]),\n",
    "    (\"Gute Vorhersage\", [0.8, 0.15, 0.05], [1, 0, 0]),\n",
    "    (\"Schlechte Vorhersage\", [0.4, 0.4, 0.2], [1, 0, 0]),\n",
    "    (\"Sehr schlechte Vorhersage\", [0.1, 0.8, 0.1], [1, 0, 0])\n",
    "]\n",
    "\n",
    "print(\"Cross-Entropy Loss f√ºr verschiedene Vorhersagen:\")\n",
    "for name, pred, true in scenarios:\n",
    "    loss = cross_entropy_loss(np.array(pred), np.array(true))\n",
    "    print(f\"   {name}: Loss = {loss:.3f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Praktisches Iris-Beispiel mit Softmax\n",
    "print(f\"\\n5Ô∏è‚É£ Praktisches Beispiel: Iris-Klassifikation mit Softmax:\")\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Daten laden\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Skalieren\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Neural Network mit Softmax\n",
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen (Wahrscheinlichkeiten!)\n",
    "y_pred_proba = nn.predict_proba(X_test_scaled)\n",
    "y_pred = nn.predict(X_test_scaled)\n",
    "\n",
    "# Erste 5 Beispiele zeigen\n",
    "print(\"Erste 5 Testbeispiele:\")\n",
    "print(\"True Class | Predicted Probabilities | Prediction\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i in range(5):\n",
    "    true_class = iris.target_names[y_test[i]]\n",
    "    pred_class = iris.target_names[y_pred[i]]\n",
    "    probs = y_pred_proba[i]\n",
    "    \n",
    "    print(f\"{true_class:10} | [{probs[0]:.3f}, {probs[1]:.3f}, {probs[2]:.3f}] | {pred_class}\")\n",
    "\n",
    "# Gesamtperformance\n",
    "accuracy = nn.score(X_test_scaled, y_test)\n",
    "print(f\"\\nüéØ Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Softmax Gradient (f√ºr Interessierte)\n",
    "print(f\"\\n6Ô∏è‚É£ Bonus: Softmax Gradient (f√ºr tieferes Verst√§ndnis):\")\n",
    "\n",
    "def softmax_gradient(softmax_output):\n",
    "    \"\"\"\n",
    "    Gradient der Softmax-Funktion\n",
    "    Wichtig f√ºr Backpropagation!\n",
    "    \"\"\"\n",
    "    # Softmax Gradient ist: s_i * (Œ¥_ij - s_j)\n",
    "    # wobei Œ¥_ij das Kronecker Delta ist\n",
    "    s = softmax_output.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "# Beispiel\n",
    "example_softmax = softmax(np.array([2.0, 1.0, 0.1]))\n",
    "gradient_matrix = softmax_gradient(example_softmax)\n",
    "\n",
    "print(\"Softmax Output:\", example_softmax)\n",
    "print(\"Gradient Matrix (3x3):\")\n",
    "print(gradient_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Softmax-Konzepte aus dem urspr√ºnglichen AMALEA erfolgreich integriert!\")\n",
    "print(f\"üéØ Du verstehst jetzt Multi-Class Klassifikation mit Neural Networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21d602",
   "metadata": {},
   "source": [
    "## üéØ Praktische √úbungen - Neural Networks meistern\n",
    "\n",
    "### üìù √úbung 1: Eigenes Neuron implementieren\n",
    "\n",
    "**Aufgabe**: Implementiere ein Neuron mit ReLU-Aktivierung f√ºr das XOR-Problem.\n",
    "\n",
    "```python\n",
    "def my_neuron(x1, x2, w1, w2, bias):\n",
    "    # Deine Implementierung hier\n",
    "    pass\n",
    "\n",
    "# XOR Daten: (0,0)‚Üí0, (0,1)‚Üí1, (1,0)‚Üí1, (1,1)‚Üí0\n",
    "# Teste verschiedene Gewichte!\n",
    "```\n",
    "\n",
    "### üìù √úbung 2: Aktivierungsfunktionen vergleichen\n",
    "\n",
    "**Aufgabe**: Trainiere das gleiche Network mit verschiedenen Aktivierungsfunktionen und vergleiche die Performance.\n",
    "\n",
    "### üìù √úbung 3: Learning Rate Experiment\n",
    "\n",
    "**Aufgabe**: Teste Learning Rates von 0.001 bis 1.0 und dokumentiere das Trainingsverhalten.\n",
    "\n",
    "### üìù √úbung 4: Softmax von Hand\n",
    "\n",
    "**Aufgabe**: Berechne Softmax f√ºr [3.0, 1.0, -1.0] ohne NumPy.\n",
    "\n",
    "### üöÄ Portfolio-Projekt: Deine erste Neural Network App\n",
    "\n",
    "**Ziel**: Erstelle eine Streamlit-App f√ºr ein selbstgew√§hltes Klassifikationsproblem.\n",
    "\n",
    "**Anforderungen**:\n",
    "1. ‚úÖ Eigener Datensatz (CSV Upload)\n",
    "2. ‚úÖ Interaktive Hyperparameter-Auswahl\n",
    "3. ‚úÖ Live-Training mit Progress Bar\n",
    "4. ‚úÖ Performance-Metriken visualisieren\n",
    "5. ‚úÖ Vorhersagen f√ºr neue Eingaben\n",
    "\n",
    "**Bewertungskriterien**:\n",
    "- üìä **Datenqualit√§t** (20%): Saubere, relevante Daten\n",
    "- üß† **Neural Network** (30%): Angemessene Architektur\n",
    "- üé® **UI/UX** (20%): Benutzerfreundliche Streamlit-App\n",
    "- üìà **Evaluation** (20%): Aussagekr√§ftige Metriken\n",
    "- üìù **Dokumentation** (10%): Code-Kommentare und README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ √úbungsl√∂sungen - Neural Networks praktisch anwenden\n",
    "\n",
    "print(\"üéØ √úbungsl√∂sungen - AMALEA Neural Networks\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ‚úÖ L√∂sung √úbung 1: Eigenes Neuron implementieren\n",
    "print(\"‚úÖ L√∂sung √úbung 1: XOR-Neuron\")\n",
    "\n",
    "def my_neuron(x1, x2, w1, w2, bias, activation='relu'):\n",
    "    \"\"\"\n",
    "    Eigenes Neuron mit verschiedenen Aktivierungsfunktionen\n",
    "    \"\"\"\n",
    "    # Lineare Kombination\n",
    "    z = w1 * x1 + w2 * x2 + bias\n",
    "    \n",
    "    # Aktivierungsfunktion anwenden\n",
    "    if activation == 'relu':\n",
    "        return max(0, z)\n",
    "    elif activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    elif activation == 'tanh':\n",
    "        return np.tanh(z)\n",
    "    else:\n",
    "        return z  # Linear\n",
    "\n",
    "# XOR-Problem testen (braucht mehrere Neuronen!)\n",
    "print(\"XOR-Problem: Ein einzelnes Neuron kann XOR nicht l√∂sen!\")\n",
    "print(\"(Das ist der Grund, warum wir mehrere Layer brauchen)\")\n",
    "\n",
    "xor_data = [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)]\n",
    "\n",
    "for x1, x2, expected in xor_data:\n",
    "    # Versuche mit verschiedenen Gewichten\n",
    "    result = my_neuron(x1, x2, w1=1, w2=1, bias=-0.5)\n",
    "    print(f\"Input: ({x1}, {x2}) ‚Üí Output: {result:.2f}, Expected: {expected}\")\n",
    "\n",
    "print(\"üí° XOR ist nicht linear separierbar ‚Üí braucht Hidden Layer!\")\n",
    "\n",
    "# ‚úÖ L√∂sung √úbung 4: Softmax von Hand\n",
    "print(f\"\\n‚úÖ L√∂sung √úbung 4: Softmax von Hand berechnen\")\n",
    "\n",
    "def softmax_by_hand(z_list):\n",
    "    \"\"\"Softmax Schritt f√ºr Schritt berechnen\"\"\"\n",
    "    print(f\"Eingabe: {z_list}\")\n",
    "    \n",
    "    # Schritt 1: Exponential\n",
    "    exp_values = []\n",
    "    for z in z_list:\n",
    "        exp_z = np.exp(z)\n",
    "        exp_values.append(exp_z)\n",
    "        print(f\"e^{z} = {exp_z:.3f}\")\n",
    "    \n",
    "    # Schritt 2: Summe\n",
    "    sum_exp = sum(exp_values)\n",
    "    print(f\"Summe: {sum_exp:.3f}\")\n",
    "    \n",
    "    # Schritt 3: Normalisierung\n",
    "    softmax_values = []\n",
    "    for i, exp_val in enumerate(exp_values):\n",
    "        softmax_val = exp_val / sum_exp\n",
    "        softmax_values.append(softmax_val)\n",
    "        print(f\"Softmax[{i}] = {exp_val:.3f} / {sum_exp:.3f} = {softmax_val:.3f}\")\n",
    "    \n",
    "    print(f\"Ergebnis: {softmax_values}\")\n",
    "    print(f\"Summe Check: {sum(softmax_values):.3f} (sollte 1.0 sein)\")\n",
    "    return softmax_values\n",
    "\n",
    "# Beispiel aus √úbung 4\n",
    "softmax_result = softmax_by_hand([3.0, 1.0, -1.0])\n",
    "\n",
    "# ‚úÖ Bonus: Neural Network Performance Metriken\n",
    "print(f\"\\nüèÜ Bonus: Wichtige Performance Metriken f√ºr Neural Networks\")\n",
    "\n",
    "# Simuliere Klassifikationsergebnisse\n",
    "np.random.seed(42)\n",
    "y_true = np.random.randint(0, 3, 100)  # 3 Klassen\n",
    "y_pred = y_true.copy()\n",
    "# F√ºge einige Fehler hinzu\n",
    "error_indices = np.random.choice(100, 20, replace=False)\n",
    "y_pred[error_indices] = np.random.randint(0, 3, 20)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Berechne Metriken\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"üìä Classification Metriken:\")\n",
    "print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall:    {recall:.3f}\")\n",
    "print(f\"   F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\nüéØ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# ‚úÖ Praktische Tipps f√ºr Neural Network Debugging\n",
    "print(f\"\\nüîß Neural Network Debugging Checklist:\")\n",
    "\n",
    "debugging_tips = [\n",
    "    \"üîç Check Input Data Shape: Stimmen die Dimensionen?\",\n",
    "    \"üìä Normalize Features: StandardScaler oder MinMaxScaler\",\n",
    "    \"üéØ Balance Classes: Gleiche Anzahl pro Klasse oder Class Weights\",\n",
    "    \"üìà Plot Loss Curve: Sollte monoton fallen\",\n",
    "    \"‚ö° Learning Rate: 0.001-0.1, bei Oszillation reduzieren\",\n",
    "    \"üß† Architecture: Start simple (1-2 Hidden Layers)\",\n",
    "    \"‚è∞ Early Stopping: Validation Loss √ºberwachen\",\n",
    "    \"üîÑ Regularization: Dropout (0.2-0.5) gegen Overfitting\",\n",
    "    \"üé≤ Random Seeds: F√ºr reproduzierbare Ergebnisse\",\n",
    "    \"üíæ Save Models: Beste Checkpoints speichern\"\n",
    "]\n",
    "\n",
    "for i, tip in enumerate(debugging_tips, 1):\n",
    "    print(f\"{i:2d}. {tip}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Neural Network Grundlagen aus AMALEA erfolgreich modernisiert!\")\n",
    "print(f\"üöÄ Du bist bereit f√ºr Woche 5: CNNs und Computer Vision!\")\n",
    "\n",
    "# N√§chste Schritte Preview\n",
    "print(f\"\\nüîÆ Vorschau Woche 5:\")\n",
    "print(f\"   üñºÔ∏è  Convolutional Neural Networks (CNNs)\")\n",
    "print(f\"   üëÅÔ∏è  Computer Vision Grundlagen\")\n",
    "print(f\"   üé®  Image Classification mit Streamlit\")\n",
    "print(f\"   üîÑ  Data Augmentation Techniken\")\n",
    "print(f\"   üì±  Transfer Learning f√ºr Praxis-Projekte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2bc37",
   "metadata": {},
   "source": [
    "## üéì Zusammenfassung: Neural Networks gemeistert!\n",
    "\n",
    "### üéØ Was du heute gelernt hast\n",
    "\n",
    "‚úÖ **Einfachstes Neuron** - Grundbaustein des Deep Learning verstanden  \n",
    "‚úÖ **Aktivierungsfunktionen** - ReLU, Sigmoid, Tanh richtig einsetzen  \n",
    "‚úÖ **Neural Network Architektur** - Hidden Layers, Gewichte, Bias  \n",
    "‚úÖ **Backpropagation** - Wie Networks \"lernen\" (bergab gehen!)  \n",
    "‚úÖ **Gradient Descent** - Optimierung verstehen  \n",
    "‚úÖ **Regression mit Neural Networks** - Kontinuierliche Vorhersagen  \n",
    "‚úÖ **Klassifikation mit Neural Networks** - Kategorien vorhersagen  \n",
    "‚úÖ **Softmax & One-Hot** - Multi-Class Klassifikation meistern  \n",
    "‚úÖ **Cross-Entropy Loss** - Loss Functions verstehen  \n",
    "‚úÖ **Hyperparameter Tuning** - Learning Rate, Architecture, etc.  \n",
    "‚úÖ **Streamlit Neural Network Apps** - Interaktive Demos erstellen\n",
    "\n",
    "### üîÑ AMALEA-Integration erfolgreich abgeschlossen\n",
    "\n",
    "Alle urspr√ºnglichen AMALEA-Konzepte wurden modernisiert integriert:\n",
    "\n",
    "| Original AMALEA | Modernisierte Integration |\n",
    "|----------------|---------------------------|\n",
    "| üß† \"Jetzt geht's in die Tiefe\" | ‚úÖ Neuron-Grundlagen + Streamlit Demo |\n",
    "| üèîÔ∏è \"Wir trainieren nur bergab\" | ‚úÖ Gradient Descent + Optimizer Vergleich |\n",
    "| üìä \"Regression II\" | ‚úÖ Neural Network Regression + Live Demo |\n",
    "| üç¶ \"Classification Softmax\" | ‚úÖ Softmax + One-Hot + Interactive Explorer |\n",
    "\n",
    "### üöÄ N√§chste Schritte - Woche 5 Preview\n",
    "\n",
    "**Woche 5: Convolutional Neural Networks (CNNs)**\n",
    "- üñºÔ∏è Computer Vision Grundlagen\n",
    "- üîç Convolution, Pooling, Feature Maps\n",
    "- üëÅÔ∏è Image Classification mit CNNs\n",
    "- üé® Data Augmentation f√ºr bessere Performance\n",
    "- üì± Transfer Learning f√ºr Praxis-Projekte\n",
    "- ü§ñ Object Detection Grundlagen\n",
    "\n",
    "### üìö Weiterf√ºhrende Ressourcen\n",
    "\n",
    "**F√ºr tieferes Verst√§ndnis:**\n",
    "- üìñ \"Deep Learning\" von Ian Goodfellow (Bibel des Deep Learning)\n",
    "- üéì CS231n Stanford Course (Convolutional Networks)\n",
    "- üß† Neural Network Playground (playground.tensorflow.org)\n",
    "- üì∫ 3Blue1Brown Neural Networks Serie (YouTube)\n",
    "\n",
    "**Praktische Tools:**\n",
    "- üîß TensorFlow/Keras f√ºr gr√∂√üere Projekte\n",
    "- ‚ö° PyTorch f√ºr Forschung\n",
    "- üéÆ Streamlit f√ºr schnelle Prototypen\n",
    "- üìä Weights & Biases f√ºr Experiment Tracking\n",
    "\n",
    "### üèÜ Projektideen f√ºr dein Portfolio\n",
    "\n",
    "1. **üìà Stock Price Predictor** - LSTM Neural Network f√ºr Zeitreihen\n",
    "2. **üéµ Music Genre Classifier** - Audio Features + Deep Learning\n",
    "3. **üìù Sentiment Analysis** - Text Classification mit Neural Networks\n",
    "4. **üè† House Price Predictor** - Regression mit Feature Engineering\n",
    "5. **üéÆ Game AI** - Reinforcement Learning Grundlagen\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quiz: Neural Networks Check\n",
    "\n",
    "**Teste dein Wissen:**\n",
    "\n",
    "1. Was ist die Ausgabe von ReLU(-5)?\n",
    "2. Warum brauchen wir Hidden Layers f√ºr XOR?\n",
    "3. Was passiert bei zu hoher Learning Rate?\n",
    "4. Wof√ºr steht \"Softmax\"?\n",
    "5. Wie lautet die Formel f√ºr ein einfaches Neuron?\n",
    "\n",
    "**Antworten:**\n",
    "1. 0 (ReLU = max(0, x))\n",
    "2. XOR ist nicht linear separierbar\n",
    "3. Gradient Descent springt √ºber das Minimum\n",
    "4. \"Soft\" Maximum - gibt Wahrscheinlichkeiten zur√ºck\n",
    "5. f(x) = w*x + b (ohne Aktivierung)\n",
    "\n",
    "---\n",
    "\n",
    "### üéâ Gl√ºckwunsch!\n",
    "\n",
    "Du hast Neural Networks von Grund auf verstanden und kannst jetzt:\n",
    "- ‚úÖ Eigene Neural Networks designen\n",
    "- ‚úÖ Hyperparameter intelligent w√§hlen\n",
    "- ‚úÖ Training-Probleme debuggen\n",
    "- ‚úÖ Interaktive Streamlit-Apps erstellen\n",
    "- ‚úÖ Performance richtig evaluieren\n",
    "\n",
    "**Du bist bereit f√ºr fortgeschrittene Deep Learning Themen!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
