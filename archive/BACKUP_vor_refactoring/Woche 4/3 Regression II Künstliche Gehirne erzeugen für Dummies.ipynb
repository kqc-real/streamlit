{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Regression II: K\u00fcnstliche Gehirne erzeugen f\u00fcr Dummies "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Neuronale Netze mit Deep Learning Bibliotheken\n", "In diesem Notebook wollen wir herausfinden, wie wir neuronale Netze mithilfe von Deep Learning Bibliotheken trainieren k\u00f6nnen. Konkret werden wir dazu die drei Open Source Bibliotheken Keras, Tensorflow und Pytorch verwenden. Opensource ML-Bibliotheken und Frameworks erleichtern die Implementierung und das Training von (tiefen) neuronalen Netzen enorm. In diesem Notebook wollen wir Ihnen einen generellen \u00dcberblick \u00fcber die Funktionsweise dieser drei Bibliotheken/Frameworks geben. F\u00fcr die n\u00e4chsten Notebooks von besonderer Relevanz ist die Keras Library, da wir diese noch h\u00e4ufiger verwenden werden. Wenn Sie sich f\u00fcr die Funktionsweise der beiden anderen Bibliotheken interessieren, k\u00f6nnen Sie sich zus\u00e4tzlich die beiden darauf folgenden Code-Beispiele ansehen."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Datensatz f\u00fcr die Beispiele\n", "Der Einfachheit halber werden wir f\u00fcr das Training aller drei Beispiel ML-Modelle den gleichen Datensatz verwenden, den wir im Folgenden in Form zweier Numpy Arrays definieren. Wie Sie vielleicht bereits bemerkt haben, handelt es sich dabei um das Ausgangsverhalten eines [XOR-Gates](https://de.wikipedia.org/wiki/Exklusiv-Oder-Gatter) (zu Deutsch: Exklusives-Oder-Gatter), das wir im Folgenden mit unseren Modellen erlenen m\u00f6chten.\n", "\n", "<img src=\"./images/xor_gate_aufbau.svg\"\n", "     style=\"width:400px;\"\n", "    />\n", "<p style=\"text-align: center;\">\n", "    Von 30px MovGP0 - selbst erstellt mit Inkscape, CC BY-SA 2.0 de, https://commons.wikimedia.org/w/index.php?curid=22912763\n", "</p>"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["x_train = np.array([[0, 0],\n", "                    [0, 1],\n", "                    [1, 0],\n", "                    [1, 1]], dtype = 'float64')\n", "\n", "y_train = np.array([[0],\n", "                    [1],\n", "                    [1],\n", "                    [0]], dtype = 'float64')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Keras Beispiel "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### \u00dcber Keras\n", "\n", "Keras ist eine Python Open-Source-Bibliothek f\u00fcr neuronale Netze. Sie fungiert als Schnittstelle f\u00fcr die TensorFlow-Bibliothek. Aufgrund ihres benutzerfreundlichen und modularen Aufbaus eignet sich Keras unter anderem besonders gut f\u00fcr das schnelle Experimentieren mit tiefen neuronalen Netzen. Mit nur wenigen Zeilen Code lassen sich die Schichten des neuronalen Netzes definieren, zu optimierende Verlustmetrik sowie Optimierungsverfahren festlegen und das Training starten.\n", "\n", "<figure>\n", "<img  src=\"./images/keras.png\" width=\"100\" align=\"center\"/> \n", "</figure>\n", "\n", "#### Funktionsweise\n", "Der einfachste Weg, ein neuronales Netz in Keras zu definieren ist \u00fcber die Sequential API. Dabei wird im ersten Schritt ein Sequential Modell definiert, das als eine Art Basis funktioniert. Diesem Sequential Modell k\u00f6nnen wir dann Schritt f\u00fcr Schritt Ebenen mit `add()` hinzuf\u00fcgen. In unserem Beispiel erstellen wir ein neuronales Netz bestehend aus einem Denselayer (in Keras bezeichnet Denselayer einen \"fully connected Layer\") mit 3 Neuronen, gefolgt von einer ReLU Aktivierungsschicht und einem weiteren Ausgangs-Denselayer mit der Sigmoid Aktivierungsfunktion. Die Sigmoid Funktion im Ausgang liefert uns einen Wertebereich zwischen null und eins.\n", "\n", "Es sei angemerkt, dass das Modell wissen muss, welche Form der Eingabe (wie viele Dimensionen der Eingang hat) es erwarten soll. Aus diesem Grund muss die erste Schicht in einem Sequential Modell (und nur die erste, denn die folgenden Schichten k\u00f6nnen automatisch auf die Form schlie\u00dfen) Informationen \u00fcber die Eingabeform erhalten. Dies erfolgt beispielsweise \u00fcber das Festlegen des `input_dim` Arguments der ersten Schicht. Dies wird im Folgenden Beispiel umgesetzt.\n", "\n", "Ist das Keras-Modell fertig definiert, muss es vor dem Start des Trainings noch mit der `compile()`-Methode kompiliert werden. Dazu sollten wir die zu verwendene Verlustmetrik mit `loss` sowie den Optimizer mit `optimizer` festlegen. Keras unterst\u00fctzt eine Vielzahl an Optimizern, die je nach Art verschiedene Einstellungsm\u00f6glichkeiten aufweisen.\n", "Anschlie\u00dfend l\u00e4sst sich das Training mit der `fit()`-Methode starten."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Load Library and modules\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Activation\n", "from tensorflow.keras import optimizers\n", "\n", "# Init the model\n", "model = Sequential()\n", "model.add(Dense(3, input_dim=2))\n", "model.add(Activation('relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "# Show optimizer\n", "rmsprop = optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n", "\n", "# Compile\n", "model.compile(loss='binary_crossentropy',\n", "              optimizer=rmsprop,\n", "              metrics=['accuracy'])\n", "# Train\n", "model.fit(x_train, y_train, batch_size = 4,\n", "          epochs=200)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### (Optional) Tensorflow Beispiel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### \u00dcber Tensorflow\n", "\n", "TensorFlow ist eine Open-Source-Softwarebibliothek f\u00fcr die Datenflussprogrammierung und kann f\u00fcr eine Reihe von Aufgaben eingesetzt werden. Sie ist eine symbolische mathematische Bibliothek, die f\u00fcr maschinelle Lernanwendungen wie neuronale Netze verwendet wird.\n", "\n", "\n", "<figure>\n", "<img  src=\"./images/tensor-flow.png\" width=\"100\" align=\"center\"/> \n", "</figure>\n", "\n", "\n", "#### Funktionsweise\n", "Das Definieren eines Modells in Tensorflow ist etwas umst\u00e4ndlicher als in Keras, erlaubt es uns aber, genauere Einstellungen vorzunehmen. Im ersten Schritt definieren wir die Struktur unsere Daten mit sogenannte Placeholder mit der `tf.placeholder()` Funktion. Die Variable `phX` steht dabei f\u00fcr den Eingang und `phY` f\u00fcr den Ausgang. Beim Initialisieren dieser Placeholder k\u00f6nnen wir sowohl den Datentyp, in unserem Fall float32, als auch die Form der Daten (engl. shape) festlegen. In diesem Beispiel ist unser Eingang zweidimensional und unser Ausgang eindimensional, weshalb wir hier f\u00fcr `phX` 2 und `phY` 1 w\u00e4hlen. Das `None` an erster Stelle der Shape bedeutet dabei, dass die erste Dimension, die der Batch-Gr\u00f6\u00dfe entspricht, eine beliebige Gr\u00f6\u00dfe haben kann.\n", "\n", "Als n\u00e4chstes initialisieren wir die Gewichte und Bias unseres Netzes mit zuf\u00e4lligen Startwerten. Beachten Sie, dass `w1` und `w2` bzw. `b1` und `b2` Matrizen sind, also mehrere Gewichte/Bias f\u00fcr einen Layer in einer Variablen vereinen.\n", "\n", "Diese initialisierten Gewichte und Bias k\u00f6nnen wir dann an die Feed-Forward Funktion `forward()` \u00fcbergeben, in der wir den mathematische Zusammenhang zwischen Ein- und Ausgang in unserem Tensorflow-Netz definieren. In diesem Fall definieren wir ein Modell mit zwei Schichten. Der Ausgang beider Schichten ergibt sich jeweils durch Matrixmultiplikation (`tf.matmul()`) von Schichteingang und Gewichten der jeweiligen Schicht und anschlie\u00dfender Addition mit den Bias. In der ersten Schicht wird au\u00dferdem wieder die Sigmoid Funktion auf den Ausgang angewendet, w\u00e4hrend f\u00fcr die Ausgangsschicht die Softmax Funktion verwendet wird (hier direkt beim Berechnen der Kosten `cost`).\n", "\n", "\u00c4hnlich wie beim Training mit Keras m\u00fcssen wir auch bei Tensorflow Lernrate, die Anzahl an Epochen, die zu minimierende Kostenfunktion sowie das Optimierungsverfahren spezifizieren. Das Training kann anschlie\u00dfend durch das Erstellen einer neuen Session begonnen werden. In der anschlie\u00dfenden For-Loop f\u00fchren wir die Session mit dem `run()` Befehl so h\u00e4ufig aus, wie durch die `nb_epochs` Variable spezifiziert, sodass wir das Training f\u00fcr die gew\u00fcnschte Epochenanzahl durchf\u00fchren.\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Import tensorflow\n", "import tensorflow as tf\n", "\n", "# Feedforward function\n", "def forward(x, w1, b1, w2, b2):\n", "    \n", "    Z = tf.nn.relu(tf.matmul(x,w1) + b1)\n", "    Z2 = tf.matmul(Z,w2) + b2\n", "    \n", "    return Z2\n", "    \n", "def init_weights(shape):\n", "    return tf.Variable(tf.random_normal(shape, stddev = 0.1))\n", "\n", "\n", "# Define PlaceHolder for input x_train and output y_train\n", "try:\n", "    phX = tf.placeholder(tf.float32, [None, 2])\n", "    phY = tf.placeholder(tf.float32, [None, 1])\n", "except AttributeError:\n", "    import tensorflow.compat.v1 as tf\n", "    tf.disable_v2_behavior() \n", "    phX = tf.placeholder(tf.float32, [None, 2])\n", "    phY = tf.placeholder(tf.float32, [None, 1])\n", "    \n", "# Init weights\n", "w1 = init_weights([2, 3])\n", "b1 = init_weights([3])\n", "w2 = init_weights([3, 1])\n", "b2 = init_weights([1])\n", "\n", "y_pred = forward(phX, w1, b1, w2, b2)\n", "\n", "lr = 0.1\n", "nb_epochs = 201\n", "\n", "# Init cost function\n", "cost = tf.reduce_mean(\n", "    tf.nn.sigmoid_cross_entropy_with_logits(\n", "        logits=y_pred, labels=phY))\n", "\n", "# Init optimizer\n", "train = tf.train.AdamOptimizer(lr).minimize(cost)\n", "\n", "# Create session and init variables\n", "init = tf.global_variables_initializer()\n", "sess = tf.Session()\n", "sess.run(init)\n", "\n", "# Start training\n", "for i in range(nb_epochs):\n", "    sess.run(train, feed_dict={phX: x_train, phY: y_train})\n", "    c = sess.run(cost, feed_dict={phX: x_train, phY: y_train})\n", "    if i%50 == 0:\n", "        print(\"Iteration {: >8}  |  Cost: {}\".format(i,c))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### (Optional) Pytorch Beispiel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### \u00dcber Pytorch\n", "PyTorch ist eine Open-Source-Bibliothek f\u00fcr maschinelles Lernen, die auf der Torch-Bibliothek basiert. Sie wird f\u00fcr Anwendungen wie Computer Vision und Verarbeitung nat\u00fcrlicher Sprache verwendet und haupts\u00e4chlich von Facebooks AI Research Lab entwickelt. Auch Pytorch ist ein kostenloses Opensource Projekt. Unterscheidungsmerkmal zu den vorherigen Bibliotheken ist die durchg\u00e4ngige Anwendung der Objektorientierung.\n", "<figure>\n", "<img  src=\"./images/pytorch.png\" width=\"100\" align=\"center\"/> \n", "</figure>\n", "\n", "#### Funktionsweise\n", "Zun\u00e4chst definieren wir eine neue Klasse `Net` die unser neuronales Netz repr\u00e4sentiert. In der Init-Methode (Konstruktor) dieser Klasse initialisieren wir die beiden Schichten unseres Netzes. Anders als im vorherigen Tensorflow Beispiel m\u00fcssen wir dabei die Shapes der beiden Schichten explizit angeben. In diesem Beispiel verwenden wir die [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) Klasse, die eine lineare Transformation der Form  $y=xA^T+b$ durchf\u00fchrt. So k\u00f6nnen wir direkt unsere beiden Schichten `fc1` und `fc2` definieren. Wie auch bei der Tensorflow Implementation ben\u00f6tigen wir eine `forward` Methode, die uns den Ausgang des Netzes berechnet. Wir versehen den Ausgang der ersten Schicht `fc1` erneut mit der ReLU Funktion und geben diesen auf die zweite Schicht `fc2`. Auf den Ausgang dieser zweiten Schicht wenden wir dann die Sigmoid Aktivierungsfunktion an.\n", "\n", "Auch anders als bei der Implementation mit Tensorflow ist, dass wir beim Verwenden von Pytorch Torch Tensoren als Eing\u00e4nge ben\u00f6tigen. Daher konvertieren wir unsere Numpy Trainingsdaten entsprechend. \n", "\n", "Auch hier k\u00f6nnen wir wichtige Einstellungen wie die Festlegung des Optimizers und der Verlustmetrik vornehmen, ehe wir mit dem Training beginnen.\n", "\n", "W\u00e4hrend des Trainings innerhalb der For-Loop erfolgt jede Aktualisierung des Modells nach dem gleichen Muster:\n", "\n", "- L\u00f6schen des letzten Fehlergradienten: `optimizer.zero_grad()`\n", "- Ein Vorw\u00e4rtsdurchlauf (feedforward) der Eingabe durch das Modell: `net(input)`\n", "- Berechnung des Verlusts f\u00fcr die Modellausgabe: `criterion(output, target)`\n", "- Backpropagation des Fehlers durch das Modell: `loss.backward()`\n", "- Aktualisieren des Modells, um den Verlust zu reduzieren: `optimizer.step()`\n", "\n", "Anders als bei der Tensorflow Implementation verschachteln wir dabei zwei For-Loops ineinander. Die innere Schleife f\u00fcttert das Netz dabei jeweils mit einem einzelnen Eintrag aus unserem Datensatz (`input` und `target`), w\u00e4hrend die \u00e4u\u00dfere Schleife die aktuelle Epoche steuert (`idx`).\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-info\">\n", "<b>Hinweis:</b> Um den unten stehenden Code auszuf\u00fchren, installieren Sie bitte Pytorch, indem Sie das Suchwerkzeug (grafisch) in anaconda navigator verwenden. Bitte ver\u00e4ndern Sie die Cuda Einstellungen nicht, wenn Sie Pytorch auf einem anderen Weg installieren! \n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "\n", "class Net(nn.Module):\n", "\n", "    def __init__(self):\n", "        super(Net, self).__init__()\n", "        self.fc1 = nn.Linear(2, 3, True)\n", "        self.fc2 = nn.Linear(3, 1, True)\n", "\n", "    def forward(self, x:float) -> float:\n", "        x = F.relu(self.fc1(x))\n", "        x = torch.sigmoid(self.fc2(x))\n", "        return x\n", "\n", "net = Net()\n", "\n", "inputs = torch.from_numpy(x_train).type(torch.FloatTensor)\n", "targets = torch.from_numpy(y_train).type(torch.FloatTensor)\n", "\n", "criterion = nn.BCELoss()\n", "optimizer = optim.Adam(net.parameters(), lr=0.01)\n", "\n", "print(\"Training loop:\")\n", "for idx in range(0, 201):\n", "    for input, target in zip(inputs, targets):\n", "        optimizer.zero_grad()   # zero the gradient buffers\n", "        output = net(input)\n", "        loss = criterion(output, target)\n", "        loss.backward()\n", "        optimizer.step()    # Does the update\n", "    if idx % 50 == 0:\n", "        print(\"Epoch: {: >8}  |  Loss: {}\".format(idx, loss.data.numpy()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Praktische Aufgabe - Nonlinear Climate Control\n", "\n", "Im Aufgabenteil zur Regression mit neuronalen Netzen haben Sie bereits mit Daten eines K\u00fchlsystems gearbeitet, die an dieser Stelle nochmals aufgegriffen werden.\n", "Der Verlauf der Daten ist im folgenden nochmals dargestellt."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "points_climate = dict(x=[25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0], y=[0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0])\n", "plt.figure()\n", "plt.scatter(points_climate['x'], points_climate['y'])\n", "plt.xlabel('Temperatur in \u00b0C')\n", "plt.ylabel('Optimale Arbeitslast in %')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "mean_train_data = np.mean(points_climate['x'])\n", "mean_train_labels = np.mean(points_climate['y'])\n", "std_train_data = np.std(points_climate['x'])\n", "std_train_labels = np.std(points_climate['y'])\n", "train_data = [(x-mean_train_data) / std_train_data for x in points_climate['x']]\n", "train_labels = [(x-mean_train_labels) / std_train_labels for x in points_climate['y']]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"alert alert-block alert-success\">\n", "<b>Aufgabe 4.3.1:</b> Implementieren Sie ein approximierendes Neuronales Netz mittels Keras. Das Netz soll die optimale Arbeitslast anhand der Temperatur vorhersagen. Speichern Sie das trainierte Modell in einer Variablen mit dem Namen <code>climate_model</code>.\n", "    \n", "Hinweise:\n", "* Verwenden Sie die normierten Daten. \n", "* Verwenden Sie <code>losses.MeanSquaredError</code> als Verlustfunktion. \n", "* Passen Sie den Aufruf von <code>compile</code> an, sodass <code>metrics=[metrics.MeanSquaredError()]</code> \u00fcbergeben wird. \n", "* Verwenden Sie <code>RMSprop</code> mit einer Lernrate von 0.0001. \n", "* Trainieren Sie 2000 Epochen mit einer Batch Size von 2. \n", "* Das Neuronale Netz besitzt eine verdeckte Schicht mit 2 Neuronen. \n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["from tensorflow.keras import metrics\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Activation\n", "from tensorflow.keras import optimizers, losses\n", "\n", "# STUDENT CODE HERE\n", "\n", "# STUDENT CODE until HERE"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["from tensorflow.data import Dataset\n", "from tensorflow.keras import metrics\n", "x_values = [x for x in range(int(min(points_climate['x'])), int(max(points_climate['x']))+1)]\n", "x_values_norm = [ (x - mean_train_data) / std_train_data for x in x_values]\n", "test_data = Dataset.from_tensor_slices(x_values).batch(2)\n", "y_pred = climate_model.predict(x_values_norm)\n", "y_pred_denorm = [(x*std_train_labels)+mean_train_labels for x in y_pred]\n", "\n", "plt.figure()\n", "plt.plot(points_climate['x'], points_climate['y'], marker='o', linestyle='None', label='Train data')\n", "plt.plot(x_values, y_pred_denorm, label='Ausgabe Model', marker='o', linestyle='dashed')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python-amalea"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}}, "nbformat": 4, "nbformat_minor": 4}
