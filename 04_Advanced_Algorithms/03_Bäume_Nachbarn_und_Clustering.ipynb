{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e16a4d",
   "metadata": {},
   "source": [
    "# 🌳 Woche 3: Bäume, Nachbarn und Clustering – AMALEA Kernkonzepte\n",
    "\n",
    "> 🚀 **Motivation:**\n",
    " >\n",
    "> In dieser Woche tauchst du in die drei wichtigsten Algorithmen des Machine Learning ein – verständlich, praxisnah und direkt anwendbar für dein Portfolio!\n",
    "\n",
    "> 💡 **Warum lohnt sich das?**\n",
    "- Wer Decision Trees, KNN und K-Means versteht, kann 80% aller ML-Projekte meistern.\n",
    "- Du kannst eigene ML-Apps bauen und erklären – ein echter Pluspunkt für Bewerbungen.\n",
    "- Du sammelst praktische Erfahrung mit Tools, die in der Data-Science-Praxis Standard sind.\n",
    "\n",
    "> 📚 **Glossar-Tipp:** Unklare Begriffe? Schau ins [Glossar](../../01_Python_Grundlagen/02_Glossar_Alle_Begriffe_erklärt.ipynb) – dort findest du alle wichtigen Erklärungen!\n",
    "\n",
    "**Was erwartet dich in dieser Datei?**\n",
    "- Integration der ursprünglichen AMALEA-Notebooks:\n",
    "  - \"Willkommen in der Baumschule!\" → Decision Trees\n",
    "  - \"Schöne Nachbarschaft\" → K-Nearest Neighbors\n",
    "  - \"K-Means-Clustering\" → Unsupervised Learning\n",
    "- Theoretische Grundlagen und praktische Umsetzung\n",
    "- Streamlit-Apps und Visualisierungen für alle drei Algorithmen\n",
    "- Portfolio-Tipps und weiterführende Lernpfade\n",
    "\n",
    "## 📚 Was du heute lernst\n",
    "\n",
    "- **Decision Trees** 🌳 – Wie Computer Entscheidungen treffen\n",
    "- **K-Nearest Neighbors (KNN)** 👥 – Lernen von den Nachbarn\n",
    "- **K-Means Clustering** 🎯 – Gruppen in Daten finden\n",
    "- **Supervised vs. Unsupervised Learning** unterscheiden\n",
    "- **Streamlit-Apps** für alle drei Algorithmen erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## 🎬 Ergänzende Videos: Advanced Algorithms\n",
    "\n",
    "**📼 Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "- **Video 1:** `../Kurs-Videos/amalea-kit2021-w3v2 (1080p).mp4` – Willkommen in der Baumschule! (Decision Trees)\n",
    "- **Video 2:** `../Kurs-Videos/amalea-kit2021-w3v3 (1080p).mp4` – Schöne Nachbarschaft (K-Nearest Neighbors)\n",
    "- **Video 3:** `../Kurs-Videos/amalea-kit2021-w3v4 (1080p).mp4` – K-Means Clustering\n",
    "\n",
    "💡 **Tipp:** Diese drei Algorithmen sind die \"Big 3\" des Machine Learning – wenn du sie verstehst, bist du für 80% aller ML-Projekte gerüstet!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Kernkonzepte aus dem ursprünglichen AMALEA-Kurs\n",
    "\n",
    "### Decision Trees 🌳\n",
    "> **Idee:** Wie Menschen Entscheidungen treffen – durch eine Serie von Ja/Nein-Fragen\n",
    "\n",
    "**Beispiel aus dem ursprünglichen Kurs:**\n",
    "```\n",
    "Ist es sonnig?\n",
    "├─ JA → Gehe spazieren\n",
    "└─ NEIN → Ist es regnerisch?\n",
    "    ├─ JA → Bleibe zu Hause\n",
    "    └─ NEIN → Gehe joggen\n",
    "```\n",
    "\n",
    "**Vorteile:**\n",
    "- ✅ Leicht interpretierbar\n",
    "- ✅ Keine Daten-Normalisierung nötig\n",
    "- ✅ Arbeitet mit kategorialen und numerischen Daten\n",
    "\n",
    "**Nachteile:**\n",
    "- ❌ Kann zu Overfitting neigen\n",
    "- ❌ Instabil bei kleinen Datenänderungen\n",
    "\n",
    "### K-Nearest Neighbors (KNN) 👥\n",
    "> **Idee:** \"Sage mir, wer deine Nachbarn sind, und ich sage dir, wer du bist.\"\n",
    "\n",
    "**Funktionsweise:**\n",
    "1. Finde die k nächsten Nachbarn\n",
    "2. Schaue, welche Klasse am häufigsten ist\n",
    "3. Treffe Vorhersage basierend auf Mehrheit\n",
    "\n",
    "**Parameter k:**\n",
    "- k=1: Sehr flexibel, aber anfällig für Ausreißer\n",
    "- k=groß: Glatter, aber weniger Details\n",
    "- k=ungerade: Vermeidet Unentschieden\n",
    "\n",
    "### K-Means Clustering 🎯\n",
    "> **Idee:** Finde natürliche Gruppen in den Daten (ohne Labels!)\n",
    "\n",
    "**Unterschied zu Supervised Learning:**\n",
    "- **Supervised** (Decision Trees, KNN): Haben Labels/Targets\n",
    "- **Unsupervised** (K-Means): Keine Labels, finde Muster selbst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Installation und Imports\n",
    "!pip install scikit-learn matplotlib seaborn plotly streamlit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score, adjusted_rand_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Alle Pakete erfolgreich installiert!\")\n",
    "print(\"🎯 Bereit für die 'Big 3' des Machine Learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌳 Decision Trees - \"Willkommen in der Baumschule!\" (Praktische Umsetzung)\n",
    "print(\"🌳 Decision Trees - Wie Computer Entscheidungen treffen\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Iris-Datensatz laden (der Klassiker!)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"📊 Datensatz-Info:\")\n",
    "print(f\"- Features: {feature_names}\")\n",
    "print(f\"- Targets: {target_names}\")\n",
    "print(f\"- Samples: {X.shape[0]}\")\n",
    "\n",
    "# Datenaufteilung\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree trainieren\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n✅ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"📋 Testsamples: {len(y_test)}\")\n",
    "print(f\"🎯 Richtige Vorhersagen: {sum(y_test == y_pred)}\")\n",
    "\n",
    "# Visualisierung des Entscheidungsbaums\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_classifier, feature_names=feature_names, class_names=target_names, \n",
    "          filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"🌳 Decision Tree Visualisierung - Iris Klassifikation\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"- Jeder Knoten zeigt eine Ja/Nein-Frage\")\n",
    "print(\"- Blätter zeigen die finale Klassifikation\")\n",
    "print(\"- Der Baum 'lernt' optimale Fragen automatisch!\")\n",
    "print(\"- So treffen Computer 'menschenähnliche' Entscheidungen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ca9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👥 K-Nearest Neighbors - \"Schöne Nachbarschaft\" (Praktische Umsetzung)\n",
    "print(\"👥 K-Nearest Neighbors - Lernen von den Nachbarn\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten skalieren (wichtig für KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verschiedene k-Werte testen\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15]\n",
    "accuracies = []\n",
    "\n",
    "print(\"🔍 Testing verschiedene k-Werte:\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_knn)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"k={k:2d}: Accuracy = {acc:.2%}\")\n",
    "\n",
    "# Bestes k finden\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\n🏆 Bestes k: {best_k} mit {max(accuracies):.2%} Accuracy\")\n",
    "\n",
    "# Visualisierung der k-Werte Performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('👥 KNN Performance für verschiedene k-Werte')\n",
    "plt.xlabel('k (Anzahl Nachbarn)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Bestes k={best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 KNN Learnings:\")\n",
    "print(\"- k=1: Sehr flexibel, aber anfällig für Noise\")\n",
    "print(\"- k=groß: Glatter, aber weniger Details\")\n",
    "print(\"- Skalierung ist WICHTIG bei KNN!\")\n",
    "print(\"- 'Lazy Learning': Kein Training, nur Speichern der Daten\")\n",
    "print(\"- Funktioniert wie menschliche Intuition: 'Ähnliche Dinge sind ähnlich'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 K-Means Clustering - Unsupervised Learning in Aktion\n",
    "print(\"🎯 K-Means Clustering - Unsupervised Learning in Aktion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clustering-Daten erstellen (wir simulieren \"unbekannte\" Gruppen)\n",
    "X_cluster, y_true = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42, cluster_std=1.2)\n",
    "\n",
    "print(\"📊 Clustering-Datensatz:\")\n",
    "print(f\"- Samples: {X_cluster.shape[0]}\")\n",
    "print(f\"- Features: {X_cluster.shape[1]} (für 2D-Visualisierung)\")\n",
    "print(f\"- Wahre Cluster: 4 (aber wir tun so, als wüssten wir das nicht!)\")\n",
    "\n",
    "# Elbow-Method: Finde optimales k\n",
    "print(\"\\n🔍 Elbow-Method: Suche optimales k...\")\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Elbow-Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('🎯 Elbow-Method für optimales k')\n",
    "plt.xlabel('Anzahl Cluster (k)')\n",
    "plt.ylabel('Inertia (Within-cluster sum of squares)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(4, color='red', linestyle='--', alpha=0.7, label='Erwartetes k=4')\n",
    "plt.legend()\n",
    "\n",
    "# K-Means mit k=4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "\n",
    "# Cluster-Visualisierung\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(optimal_k):\n",
    "    cluster_points = X_cluster[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "                c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', \n",
    "            s=200, linewidths=3, label='Zentroide')\n",
    "plt.title('🎯 K-Means Clustering Ergebnis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Qualitätsbewertung\n",
    "silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "ari_score = adjusted_rand_score(y_true, cluster_labels)\n",
    "\n",
    "print(f\"\\n📊 Clustering-Qualität:\")\n",
    "print(f\"✅ Silhouette Score: {silhouette_avg:.3f} (höher = besser, max=1)\")\n",
    "print(f\"✅ Adjusted Rand Index: {ari_score:.3f} (1.0 = perfekte Übereinstimmung)\")\n",
    "\n",
    "print(\"\\n💡 K-Means Learnings:\")\n",
    "print(\"- Unsupervised: Keine Labels/Targets benötigt!\")\n",
    "print(\"- Elbow-Method hilft bei k-Wahl\")\n",
    "print(\"- Zentroide = Mittelpunkt jedes Clusters\")\n",
    "print(\"- Anwendung: Customer Segmentation, Datenexploration\")\n",
    "print(\"- Findet 'versteckte' Muster in den Daten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3efc6",
   "metadata": {},
   "source": [
    "## 🎓 Zusammenfassung: Die \"Big 3\" des Machine Learning\n",
    "\n",
    "Du hast jetzt die drei wichtigsten Algorithmen des Machine Learning kennengelernt - sowohl theoretisch als auch praktisch!\n",
    "\n",
    "### 🌳 Decision Trees\n",
    "- **Stärken**: Interpretierbar, keine Skalierung nötig, arbeitet mit allen Datentypen\n",
    "- **Schwächen**: Overfitting, instabil bei kleinen Datenänderungen\n",
    "- **Einsatz**: Wenn Interpretierbarkeit wichtig ist (Medizin, Finanzen)\n",
    "- **AMALEA-Weisheit**: \"Wie Menschen denken - durch Ja/Nein-Fragen\"\n",
    "\n",
    "### 👥 K-Nearest Neighbors\n",
    "- **Stärken**: Einfach zu verstehen, keine Annahmen über Datenverteilung\n",
    "- **Schwächen**: Braucht Skalierung, langsam bei großen Daten\n",
    "- **Einsatz**: Baseline-Algorithmus, lokale Muster, Empfehlungssysteme\n",
    "- **AMALEA-Weisheit**: \"Sage mir, wer deine Nachbarn sind...\"\n",
    "\n",
    "### 🎯 K-Means Clustering\n",
    "- **Stärken**: Unsupervised, findet versteckte Muster, skalierbar\n",
    "- **Schwächen**: k muss vorgegeben werden, nur runde Cluster\n",
    "- **Einsatz**: Datenexploration, Customer Segmentation, Feature Engineering\n",
    "- **AMALEA-Weisheit**: \"Daten sprechen lassen - ohne Labels!\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Nächste Schritte & Vertiefung\n",
    "\n",
    "### 📚 Empfohlene Reihenfolge:\n",
    "1. **🎬 Videos schauen**: Die Original AMALEA-Videos ergänzen perfekt diese praktische Erfahrung\n",
    "2. **🔬 Eigene Daten testen**: Lade deine eigenen Datasets und experimentiere\n",
    "3. **📊 Streamlit-Apps erstellen**: Baue interaktive Apps für alle drei Algorithmen\n",
    "4. **🧠 Neural Networks**: Weiter zu Woche 4 - Deep Learning!\n",
    "\n",
    "### 💡 Pro-Tipps aus den AMALEA-Videos:\n",
    "- **Immer mehrere Algorithmen testen** - jeder hat seine Stärken\n",
    "- **Cross-Validation verwenden** für robuste Evaluation\n",
    "- **Feature Engineering** kann wichtiger sein als der Algorithmus\n",
    "- **Domain Knowledge** schlägt oft komplexe Algorithmen\n",
    "- **Start simple, then complexify** - beginne mit einfachen Modellen\n",
    "\n",
    "### 🔬 Experimentier-Ideen:\n",
    "- Teste verschiedene `max_depth` Werte bei Decision Trees\n",
    "- Probiere verschiedene Distance Metrics bei KNN\n",
    "- Verwende PCA vor K-Means für hochdimensionale Daten\n",
    "- Kombiniere alle drei: Clustering → Feature Engineering → Classification\n",
    "\n",
    "---\n",
    "\n",
    "> 🎓 **Original AMALEA-Weisheit**: \"Verstehst du Decision Trees, KNN und K-Means, verstehst du 80% aller ML-Projekte!\"\n",
    "\n",
    "**Herzlichen Glückwunsch! Du bist jetzt bereit für Advanced Machine Learning! 🚀**\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Was kommt als nächstes?\n",
    "- **Woche 4**: Neural Networks & Deep Learning\n",
    "- **Woche 5**: Computer Vision & CNNs  \n",
    "- **Woche 6**: Natural Language Processing & Transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
