{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌳 Woche 3: Bäume, Nachbarn und Clustering - AMALEA Kernkonzepte\n",
    "\n",
    "**Integration der ursprünglichen AMALEA-Notebooks:**\n",
    "- \"Willkommen in der Baumschule!\" → Decision Trees\n",
    "- \"Schöne Nachbarschaft\" → K-Nearest Neighbors  \n",
    "- \"K-Means-Clustering\" → Unsupervised Learning\n",
    "\n",
    "## 📚 Was du heute lernst\n",
    "\n",
    "- **Decision Trees** 🌳 - Wie Computer Entscheidungen treffen\n",
    "- **K-Nearest Neighbors (KNN)** 👥 - Lernen von den Nachbarn\n",
    "- **K-Means Clustering** 🎯 - Gruppen in Daten finden\n",
    "- **Supervised vs. Unsupervised Learning** unterscheiden\n",
    "- **Streamlit-Apps** für alle drei Algorithmen erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## 🎬 Ergänzende Videos: Advanced Algorithms\n",
    "\n",
    "**📼 Original AMALEA Video-Serie (KIT 2021):**\n",
    "\n",
    "- **Video 1:** `../Kurs-Videos/amalea-kit2021-w3v2 (1080p).mp4` - Willkommen in der Baumschule! (Decision Trees)\n",
    "- **Video 2:** `../Kurs-Videos/amalea-kit2021-w3v3 (1080p).mp4` - Schöne Nachbarschaft (K-Nearest Neighbors)  \n",
    "- **Video 3:** `../Kurs-Videos/amalea-kit2021-w3v4 (1080p).mp4` - K-Means Clustering\n",
    "\n",
    "💡 **Tipp:** Diese 3 Algorithmen sind die \"Big 3\" des Machine Learning - verstehst du sie, verstehst du 80% aller ML-Projekte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Installation und Imports\n",
    "!pip install scikit-learn matplotlib seaborn plotly streamlit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score, adjusted_rand_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Alle Pakete erfolgreich installiert!\")\n",
    "print(\"🎯 Bereit für die 'Big 3' des Machine Learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌳 Decision Trees - \"Willkommen in der Baumschule!\"\n",
    "print(\"🌳 Decision Trees - Wie Computer Entscheidungen treffen\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Iris-Datensatz laden\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"📊 Datensatz-Info:\")\n",
    "print(f\"- Features: {feature_names}\")\n",
    "print(f\"- Targets: {target_names}\")\n",
    "print(f\"- Samples: {X.shape[0]}\")\n",
    "\n",
    "# Datenaufteilung\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree trainieren\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2, random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n✅ Accuracy: {accuracy:.2%}\")\n",
    "print(f\"📋 Testsamples: {len(y_test)}\")\n",
    "print(f\"🎯 Richtige Vorhersagen: {sum(y_test == y_pred)}\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_classifier, feature_names=feature_names, class_names=target_names, filled=True, rounded=True, fontsize=10)\n",
    "plt.title(\"🌳 Decision Tree Visualisierung - Iris Klassifikation\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"- Jeder Knoten zeigt eine Ja/Nein-Frage\")\n",
    "print(\"- Blätter zeigen die finale Klassifikation\")\n",
    "print(\"- Der Baum 'lernt' optimale Fragen automatisch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👥 K-Nearest Neighbors - \"Schöne Nachbarschaft\"\n",
    "print(\"👥 K-Nearest Neighbors - Lernen von den Nachbarn\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Daten skalieren (wichtig für KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verschiedene k-Werte testen\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15]\n",
    "accuracies = []\n",
    "\n",
    "print(\"🔍 Testing verschiedene k-Werte:\")\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_knn)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"k={k:2d}: Accuracy = {acc:.2%}\")\n",
    "\n",
    "# Bestes k finden\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\n🏆 Bestes k: {best_k} mit {max(accuracies):.2%} Accuracy\")\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('👥 KNN Performance für verschiedene k-Werte')\n",
    "plt.xlabel('k (Anzahl Nachbarn)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(best_k, color='red', linestyle='--', alpha=0.7, label=f'Bestes k={best_k}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n�� KNN Learnings:\")\n",
    "print(\"- k=1: Sehr flexibel, aber anfällig für Noise\")\n",
    "print(\"- k=groß: Glatter, aber weniger Details\")\n",
    "print(\"- Skalierung ist WICHTIG bei KNN!\")\n",
    "print(\"- 'Lazy Learning': Kein Training, nur Speichern der Daten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 K-Means Clustering - Unsupervised Learning\n",
    "print(\"�� K-Means Clustering - Unsupervised Learning in Aktion\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clustering-Daten erstellen\n",
    "X_cluster, y_true = make_blobs(n_samples=300, centers=4, n_features=2, random_state=42, cluster_std=1.2)\n",
    "\n",
    "print(\"📊 Clustering-Datensatz:\")\n",
    "print(f\"- Samples: {X_cluster.shape[0]}\")\n",
    "print(f\"- Features: {X_cluster.shape[1]} (für 2D-Visualisierung)\")\n",
    "print(f\"- Wahre Cluster: 4 (aber wir tun so, als wüssten wir das nicht!)\")\n",
    "\n",
    "# Elbow-Method\n",
    "print(\"\\n🔍 Elbow-Method: Suche optimales k...\")\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_cluster)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.title('🎯 Elbow-Method für optimales k')\n",
    "plt.xlabel('Anzahl Cluster (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(4, color='red', linestyle='--', alpha=0.7, label='Erwartetes k=4')\n",
    "plt.legend()\n",
    "\n",
    "# K-Means mit k=4\n",
    "optimal_k = 4\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_cluster)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "\n",
    "# Cluster-Visualisierung\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(optimal_k):\n",
    "    cluster_points = X_cluster[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], alpha=0.6, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=200, linewidths=3, label='Zentroide')\n",
    "plt.title('🎯 K-Means Clustering Ergebnis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Qualitätsbewertung\n",
    "silhouette_avg = silhouette_score(X_cluster, cluster_labels)\n",
    "ari_score = adjusted_rand_score(y_true, cluster_labels)\n",
    "\n",
    "print(f\"\\n📊 Clustering-Qualität:\")\n",
    "print(f\"✅ Silhouette Score: {silhouette_avg:.3f} (höher = besser)\")\n",
    "print(f\"✅ Adjusted Rand Index: {ari_score:.3f} (1.0 = perfekt)\")\n",
    "\n",
    "print(\"\\n💡 K-Means Learnings:\")\n",
    "print(\"- Unsupervised: Keine Labels/Targets benötigt!\")\n",
    "print(\"- Elbow-Method hilft bei k-Wahl\")\n",
    "print(\"- Zentroide = Mittelpunkt jedes Clusters\")\n",
    "print(\"- Anwendung: Customer Segmentation, Datenexploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Zusammenfassung: Die \"Big 3\" des Machine Learning\n",
    "\n",
    "Du hast jetzt die drei wichtigsten Algorithmen des Machine Learning kennengelernt!\n",
    "\n",
    "### 🌳 Decision Trees\n",
    "- **Stärken**: Interpretierbar, keine Skalierung nötig\n",
    "- **Schwächen**: Overfitting, instabil\n",
    "- **Einsatz**: Wenn Interpretierbarkeit wichtig ist\n",
    "\n",
    "### 👥 K-Nearest Neighbors\n",
    "- **Stärken**: Einfach zu verstehen, no assumptions\n",
    "- **Schwächen**: Braucht Skalierung, langsam bei großen Daten\n",
    "- **Einsatz**: Baseline-Algorithmus, lokale Muster\n",
    "\n",
    "### 🎯 K-Means Clustering\n",
    "- **Stärken**: Unsupervised, findet versteckte Muster\n",
    "- **Schwächen**: k muss vorgegeben werden, nur runde Cluster\n",
    "- **Einsatz**: Datenexploration, Customer Segmentation\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Nächste Schritte\n",
    "\n",
    "1. **🎬 Videos schauen**: Die Original AMALEA-Videos ergänzen perfekt diese praktische Erfahrung\n",
    "2. **🔬 Eigene Daten testen**: Lade deine eigenen Datasets und experimentiere\n",
    "3. **📚 Weiterlesen**: Neural Networks & Deep Learning in Woche 4!\n",
    "\n",
    "### 💡 Pro-Tipps aus den AMALEA-Videos:\n",
    "- **Immer mehrere Algorithmen testen** - jeder hat seine Stärken\n",
    "- **Cross-Validation verwenden** für robuste Evaluation\n",
    "- **Feature Engineering** kann wichtiger sein als der Algorithmus\n",
    "- **Domain Knowledge** schlägt oft komplexe Algorithmen\n",
    "\n",
    "---\n",
    "\n",
    "> 🎓 **Original AMALEA-Weisheit**: \"Verstehst du Decision Trees, KNN und K-Means, verstehst du 80% aller ML-Projekte!\"\n",
    "\n",
    "**Herzlichen Glückwunsch! Du bist jetzt bereit für Advanced Machine Learning! 🚀**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
