{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n",
    "!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n",
    "!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie skaliert eigentlich das ganze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Daten skalieren – Bringen wir alles auf ein Level!\n",
    "\n",
    "Super, nachdem du jetzt weißt, wie du Rohdaten laden und analysieren kannst, kommt der nächste wichtige Schritt in der Datenvorverarbeitung: das **Skalieren von Daten**. Viele Machine-Learning-Algorithmen performen deutlich besser oder setzen sogar skalierte Eingangsdaten voraus. Stell dir vor, du hast ein Merkmal \"Alter\" (z.B. 20-80 Jahre) und ein anderes \"Anzahl Kinder\" (z.B. 0-5). Ohne Skalierung würde das Alter den Algorithmus viel stärker beeinflussen, nur weil die Zahlenwerte größer sind! Das wollen wir vermeiden.\n",
    "\n",
    "In diesem Kapitel zeige ich dir, wie du Daten **normalisieren** (auf einen Bereich von 0 bis 1 bringen) und **standardisieren** (Mittelwert 0, Standardabweichung 1) kannst. Außerdem klären wir, wann welche Methode sinnvoll ist. Das ist besonders wichtig, wenn du später mit künstlichen Neuronalen Netzen und Deep Learning arbeitest.\n",
    "\n",
    "### Daten normalisieren: Alles zwischen 0 und 1\n",
    "\n",
    "Den Begriff \"Normalisieren\" kennst du vielleicht schon aus der Mathematik. Hier bedeutet es, dass wir die Werte unserer Merkmale so anpassen, dass sie alle in einem einheitlichen Bereich liegen, typischerweise zwischen 0 und 1. Um das zu schaffen, brauchen wir für jedes Merkmal (also jede Spalte in unserem Datensatz) den kleinsten (Minimum) und den größten (Maximum) Wert. Diese können wir einfach finden, indem wir uns alle Werte in einer Spalte anschauen.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.1:</b> Ran an den Code! Schreibe eine Python-Funktion <code>dataset_minmax(dataset)</code>, die für jede Spalte in einem gegebenen Pandas DataFrame das Minimum und Maximum findet.\n",
    "<ul>\n",
    "    <li> <b>Hinweis:</b> Deine Funktion soll den DataFrame als Eingabe bekommen und eine Liste von Tupeln zurückgeben. Jedes Tupel soll das Format <code>(Minimum, Maximum)</code> für die jeweilige Spalte haben.\n",
    "    Stell dir vor, dein DataFrame hat 3 Spalten, dann sollte deine Funktion eine Liste mit 3 solchen Tupeln zurückgeben.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DO NOT change this cell\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_csv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/pima-indians-diabetes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# DO NOT change this cell\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = read_csv(filename, names=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset: pd.DataFrame):\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "    return minmax\n",
    "\n",
    "# small test dataset\n",
    "dataset = pd.DataFrame({'First' : [50, 30], 'Second' : [20, 90]})\n",
    "print(dataset)\n",
    "\n",
    "# Calculate min and max for each column\n",
    "minmax = dataset_minmax(dataset)\n",
    "print(minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt, wo du für jede Spalte das Minimum und Maximum bestimmen kannst, ist es Zeit, die eigentliche Normalisierung durchzuführen. Mit diesen Werten und der folgenden Formel können wir jeden einzelnen Wert in unserem Datensatz auf den Bereich zwischen 0 und 1 skalieren:\n",
    "\n",
    "$$ \\text{skalierter Wert} = \\frac{\\text{aktueller Wert} - \\text{Minimum der Spalte}}{\\text{Maximum der Spalte} - \\text{Minimum der Spalte}} $$\n",
    "\n",
    "Schau dir die Formel genau an:\n",
    "<ul>\n",
    "    <li>Wenn der aktuelle Wert das Minimum der Spalte ist, wird der Zähler zu 0, und der skalierte Wert ist 0.</li>\n",
    "    <li>Wenn der aktuelle Wert das Maximum der Spalte ist, wird der Zähler gleich dem Nenner, und der skalierte Wert ist 1.</li>\n",
    "    <li>Alle anderen Werte landen dazwischen. Clever, oder?</li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.2:</b> Implementiere diese Formel in der Funktion <code>normalize_dataset(dataset, minmax)</code>.\n",
    "<ul>\n",
    "    <li><b>Hinweis:</b> Die Funktion bekommt den ursprünglichen DataFrame und die Liste mit den Min-Max-Werten (die du in Aufgabe 1.4.1 berechnet hast) als Eingabe. Zurückgeben soll sie den normalisierten DataFrame. Achte darauf, dass du den ursprünglichen DataFrame nicht direkt veränderst, sondern eine Kopie bearbeitest (z.B. mit <code>.copy(deep=True)</code>).\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset: pd.DataFrame, minmax: list):\n",
    "    \n",
    "    return_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "            \n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn du nun die beiden Funktionen kombinierst, die du gerade implementiert hast – eine zum Finden der Min-Max-Werte und eine zur Anwendung der Normalisierungsformel – kannst du deinen Datensatz kinderleicht normalisieren. Probier's mal aus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again a small test dataset\n",
    "dataset =  pd.DataFrame({'First' : [50, 30], 'Second' : [20, 90], 'Third' : [30, 40]})\n",
    "print(dataset)\n",
    "\n",
    "# Calculate min and max for each column\n",
    "minmax = dataset_minmax(dataset)\n",
    "print(minmax)\n",
    "\n",
    "# Normalize columns\n",
    "dataset = normalize_dataset(dataset, minmax)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut! Jetzt setzen wir alles zusammen, um eine CSV-Datei direkt einzulesen und zu normalisieren.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.3:</b> Implementiere die Funktion <code>load_normalized_csv(filename)</code>. Diese Funktion soll:\n",
    "<ol>\n",
    "    <li>Eine CSV-Datei (deren Pfad über <code>filename</code> angegeben wird) einlesen. Du kannst davon ausgehen, dass die CSV-Datei keine Header-Zeile (Spaltennamen) enthält. Verwende die Spaltennamen, die wir bereits kennen: <code>names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']</code>.</li>\n",
    "    <li>Die eingelesenen Daten mithilfe deiner zuvor erstellten Funktionen <code>dataset_minmax()</code> und <code>normalize_dataset()</code> normalisieren.</li>\n",
    "    <li>Den normalisierten DataFrame zurückgeben.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normalized_csv(filename: str):\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "\n",
    "## Test:\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "\n",
    "dataset = load_normalized_csv(filename)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten standardisieren: Die Glockenkurve ruft!\n",
    "\n",
    "Neben dem Normalisieren gibt es eine weitere sehr verbreitete Methode zur Skalierung von Daten: das **Standardisieren**. Beim Standardisieren transformieren wir unsere Daten so, dass jede Spalte (jedes Merkmal) einen Mittelwert von 0 und eine Standardabweichung von 1 hat. Das Ergebnis ist eine Verteilung, die oft als Gaußsche Normalverteilung oder \"Glockenkurve\" bezeichnet wird.\n",
    "\n",
    "Warum ist das nützlich? Viele Algorithmen, insbesondere solche, die auf Distanzmessungen basieren (wie z.B. k-Nearest Neighbors oder Support Vector Machines) oder Annahmen über die Verteilung der Daten machen, profitieren von standardisierten Daten.\n",
    "\n",
    "Um unsere Daten standardisieren zu können, brauchen wir zwei Kennzahlen für jede Spalte unseres Datensatzes:\n",
    "1.  Den **Mittelwert** (Durchschnitt)\n",
    "2.  Die **Standardabweichung** (ein Maß für die Streuung der Werte um den Mittelwert)\n",
    "\n",
    "Fangen wir mit dem Mittelwert an. Die Formel dafür kennst du sicher schon:\n",
    "\n",
    "$$ \\bar{x} = \\frac{\\sum_{n=0}^{N-1} x_n}{N} $$\n",
    "\n",
    "Das bedeutet einfach: Summiere alle Werte ($x_n$) in einer Spalte auf und teile die Summe durch die Anzahl der Werte ($N$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.4:</b> Schreibe eine Python-Funktion <code>column_means(dataset)</code>, die für jede Spalte in einem gegebenen Pandas DataFrame den Mittelwert berechnet und eine Liste dieser Mittelwerte zurückgibt.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_means(dataset: pd.DataFrame):\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "    \n",
    "    return means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super! Als Nächstes brauchen wir die Standardabweichung. Wie du wahrscheinlich schon aus der Wahrscheinlichkeitsrechnung oder Statistik weißt, beschreibt die empirische Standardabweichung, wie stark die einzelnen Werte durchschnittlich vom Mittelwert abweichen – also wie sehr die Daten \"streuen\".\n",
    "\n",
    "Die Formel dafür sieht so aus:\n",
    "\n",
    "$$ s = \\sqrt{\\frac{\\sum_{n=0}^{N-1}(x_n - \\bar{x})^2}{N}} $$\n",
    "\n",
    "Schritt für Schritt bedeutet das für jede Spalte:\n",
    "1. Berechne die Differenz jedes Wertes ($x_n$) zum Mittelwert ($\bar{x$) der Spalte.\n",
    "2. Quadriere jede dieser Differenzen.\n",
    "3. Summiere all diese quadrierten Differenzen auf.\n",
    "4. Teile diese Summe durch die Anzahl der Werte ($N$). Das Ergebnis ist die sogenannte Varianz.\n",
    "5. Ziehe die Quadratwurzel aus der Varianz, um die Standardabweichung ($s$) zu erhalten.\n",
    "\n",
    "Das hilft uns später, eine Funktion zu schreiben, die die Standardabweichung für jede Spalte berechnet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.5:</b> Schreibe eine Funktion <code>column_stdevs(dataset, means)</code>, die für jede Spalte die Standardabweichung anhand der oben genannten Formel berechnet. Die Funktion soll den DataFrame und die zuvor berechneten Mittelwerte (als Liste) als Eingabe erhalten und eine Liste der Standardabweichungen zurückgeben.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def column_stdevs(dataset: pd.DataFrame, means: list):\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "        \n",
    "    return stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfekt! Jetzt können wir unsere Funktionen für Mittelwert und Standardabweichung mit einem kleinen Beispiel testen, um zu sehen, ob alles klappt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'First' : [50, 30], 'Second' : [20, 90], 'Third' : [30, 50]})\n",
    "print(dataset)\n",
    "      \n",
    "means = column_means(dataset)\n",
    "print(means)\n",
    "      \n",
    "stdevs = column_stdevs(dataset, means)\n",
    "print(stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut! Wir haben jetzt alle Werkzeuge zusammen, um unsere Daten tatsächlich zu standardisieren. Die Formel für die Standardisierung eines einzelnen Wertes ist:\n",
    "\n",
    "$$ \\text{standardisierter Wert} = \\frac{\\text{aktueller Wert} - \\text{Mittelwert der Spalte}}{\\text{Standardabweichung der Spalte}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.6:</b> Implementiere die Funktion <code>standardize_dataset(dataset, means, stdevs)</code>, die die Werte im DataFrame standardisiert. Sie bekommt den DataFrame, die Liste der Mittelwerte und die Liste der Standardabweichungen als Eingabe und gibt den standardisierten DataFrame zurück. Denk auch hier daran, eine Kopie des DataFrames zu bearbeiten.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(dataset: pd.DataFrame, means: list, stdevs: list):\n",
    "    return_dataset = dataset.copy(deep=True)\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "    \n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt ist es an der Zeit, alles zusammenzusetzen und unsere Standardisierungsfunktion zu testen.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.7:</b>\n",
    "<ol>\n",
    "    <li>Nimm den kleinen Test-DataFrame von oben.</li>\n",
    "    <li>Berechne die Mittelwerte und Standardabweichungen für jede Spalte mit deinen Funktionen <code>column_means()</code> und <code>column_stdevs()</code>.</li>\n",
    "    <li>Standardisiere den DataFrame mit deiner Funktion <code>standardize_dataset()</code>.</li>\n",
    "    <li>Gib den standardisierten DataFrame aus.</li>\n",
    "    <li>Erstelle für jede Spalte des standardisierten DataFrames ein Histogramm, um die Verteilung der Werte zu visualisieren. Du wirst sehen, dass die Werte jetzt um 0 zentriert sind!</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame({'First' : [50, 10, 30], 'Second' : [20, 25, 90], 'Third' : [30, 40, 50]})\n",
    "print(dataset)\n",
    "\n",
    "### STUDENT CODE HERE\n",
    "\n",
    "### STUDENT CODE until HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast geschafft mit der manuellen Implementierung! Lass uns nun eine Funktion schreiben, die eine CSV-Datei einliest und die Daten direkt standardisiert.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.8:</b> Vervollständige die Funktion <code>load_standardized_csv(filename)</code>. Diese Funktion soll:\n",
    "<ol>\n",
    "    <li>Eine CSV-Datei (deren Pfad über <code>filename</code> angegeben wird) einlesen. Auch hier gehen wir davon aus, dass keine Header-Zeile vorhanden ist und verwenden die bekannten Spaltennamen: <code>names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']</code>.</li>\n",
    "    <li>Die eingelesenen Daten mithilfe deiner zuvor erstellten Funktionen <code>column_means()</code>, <code>column_stdevs()</code> und <code>standardize_dataset()</code> standardisieren.</li>\n",
    "    <li>Den standardisierten DataFrame zurückgeben.</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standardized_csv(filename: str):\n",
    "    \n",
    "    ### STUDENT CODE HERE\n",
    "\n",
    "    ### STUDENT CODE until HERE\n",
    "    return standardized_dataset\n",
    "\n",
    "## Test:\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "\n",
    "dataset = load_standardized_csv(filename)\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wann normalisieren und wann standardisieren? Die Qual der Wahl!\n",
    "\n",
    "Du hast jetzt zwei mächtige Werkzeuge zur Datenskalierung kennengelernt: Normalisierung und Standardisierung. Aber wann solltest du welche Methode verwenden? Das ist eine häufige Frage, und die Antwort ist nicht immer eindeutig, aber hier sind ein paar Richtlinien:\n",
    "\n",
    "**Normalisierung (Min-Max-Skalierung auf 0-1):**\n",
    "*   **Gut für:**\n",
    "    *   Algorithmen, die keine Annahmen über die Verteilung der Daten machen, wie z.B. k-Nearest Neighbors (KNN) oder künstliche Neuronale Netze (insbesondere bei Bilddaten, wo Pixelwerte oft zwischen 0-255 liegen und auf 0-1 normalisiert werden).\n",
    "    *   Wenn deine Daten klare Minimal- und Maximalwerte haben und du die Beziehung zwischen den Werten innerhalb dieses Bereichs erhalten möchtest.\n",
    "    *   Wenn die Daten nicht normalverteilt sind (oder du es nicht weißt) und du keine Normalverteilung erzwingen möchtest.\n",
    "*   **Nachteile:**\n",
    "    *   Sehr anfällig für Ausreißer. Ein einziger extremer Wert kann den Min-Max-Bereich stark verzerren und die anderen Werte in ein sehr kleines Intervall quetschen.\n",
    "\n",
    "**Standardisierung (Mittelwert 0, Standardabweichung 1):**\n",
    "*   **Gut für:**\n",
    "    *   Algorithmen, die Annahmen über die Verteilung der Daten machen oder von normalverteilten Daten profitieren, z.B. lineare Regression, logistische Regression, Support Vector Machines (SVMs), Principal Component Analysis (PCA).\n",
    "    *   Wenn deine Daten Ausreißer enthalten, da die Standardisierung weniger empfindlich darauf reagiert als die Normalisierung (obwohl extreme Ausreißer immer noch einen Einfluss haben können).\n",
    "    *   Wenn die Merkmale unterschiedliche Einheiten und Skalen haben.\n",
    "*   **Nachteile:**\n",
    "    *   Die ursprünglichen Minimal- und Maximalwerte gehen verloren, da die Skalierung nicht auf einen festen Bereich beschränkt ist.\n",
    "\n",
    "**Faustregeln:**\n",
    "*   **Standardisierung ist oft die sicherere Wahl**, wenn du dir unsicher bist, da sie bei vielen Algorithmen gut funktioniert.\n",
    "*   **Probiere beides aus!** Manchmal ist es nicht offensichtlich, welche Transformationsmethode die beste ist. In solchen Fällen ist es legitim und oft notwendig, verschiedene Methoden auszuprobieren und zu evaluieren, welche zu besseren Ergebnissen für deinen spezifischen Anwendungsfall und Algorithmus führt (\"Trial and Error\").\n",
    "*   **Berücksichtige den Algorithmus:** Einige Algorithmen sind skaleninvariant (z.B. Entscheidungsbäume, Random Forests), was bedeutet, dass sie von Skalierung nicht direkt profitieren. Es schadet aber meist auch nicht.\n",
    "\n",
    "Neben den beiden vorgestellten Methoden gibt es viele weitere Arten der Datentransformation. Allgemein zielen diese Transformationen immer darauf ab, die Strukturen der Daten bestmöglich für den Lernalgorithmus offenzulegen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten für Machine Learning vorbereiten mit Scikit-learn: Die Profi-Werkzeuge!\n",
    "\n",
    "Das war eine Menge Handarbeit, oder? Du hast jetzt ein tiefes Verständnis dafür entwickelt, wie Normalisierung und Standardisierung funktionieren. In der Praxis musst du das Rad aber nicht jedes Mal neu erfinden. Die fantastische Python-Bibliothek **Scikit-learn** (oft als `sklearn` importiert) nimmt uns diese Arbeit ab und bietet optimierte, einfach zu bedienende Werkzeuge für die Datenvorverarbeitung.\n",
    "\n",
    "Dies ist das letzte Kapitel zur Datenvorverarbeitung für diese Woche. Wie du im vorangegangenen Kapitel gelernt hast, ist es super wichtig, deine Daten so aufzubereiten, dass ihre innere Struktur für Machine-Learning-Algorithmen optimal zugänglich wird. Mit Scikit-learn gehen wir jetzt noch einen Schritt weiter. Nach diesem Abschnitt wirst du in der Lage sein, Daten nicht nur zu normalisieren und standardisieren, sondern auch zu **binarisieren** – und das alles mit wenigen Zeilen Code!\n",
    "\n",
    "Scikit-learn bietet verschiedene \"Transformer\"-Klassen für diese Aufgaben. Die allgemeine Vorgehensweise ist dabei meist ähnlich:\n",
    "1.  **Initialisiere** ein Transformer-Objekt (z.B. einen `MinMaxScaler` für die Normalisierung).\n",
    "2.  **Passe** den Transformer an deine Trainingsdaten an (mit der `.fit()`-Methode). Dabei lernt der Transformer die notwendigen Parameter aus den Daten (z.B. Min/Max-Werte für die Normalisierung oder Mittelwert/Standardabweichung für die Standardisierung).\n",
    "3.  **Transformiere** deine Daten (sowohl Trainings- als auch Testdaten!) mit dem angepassten Transformer (mit der `.transform()`-Methode). Alternativ kannst du auch `.fit_transform()` verwenden, um beide Schritte in einem zu erledigen (meist auf den Trainingsdaten).\n",
    "\n",
    "Die Scikit-learn Library enthält zwei verschiedene Standardwege, um Daten zu transformieren. Schauen wir uns das genauer an!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage 1.4.9:</b> Scikit-learn bietet verschiedene Transformer-Klassen. Zwei sehr häufig verwendete sind der <code>MinMaxScaler</code> und der <code>StandardScaler</code>. Recherchiere und beschreibe:\n",
    "<ul>\n",
    "    <li>Was macht der <code>MinMaxScaler</code> genau und welche Parameter kann man ihm übergeben (z.B. <code>feature_range</code>)?</li>\n",
    "    <li>Was macht der <code>StandardScaler</code> genau und welche Optionen hat er (z.B. <code>with_mean</code>, <code>with_std</code>)?</li>\n",
    "    <li>Was ist der Unterschied zwischen den Methoden <code>fit()</code>, <code>transform()</code> und <code>fit_transform()</code> bei diesen Transformern? Wann würdest du welche Methode verwenden, insbesondere im Kontext von Trainings- und Testdaten?</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Deine Antwort:</b>\n",
    "\n",
    "*   <code>MinMaxScaler</code>: ...\n",
    "*   <code>StandardScaler</code>: ...\n",
    "*   <code>fit()</code>, <code>transform()</code>, <code>fit_transform()</code>: ...\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Normalisieren mit <code>MinMaxScaler</code>\n",
    "\n",
    "Beim Normalisieren eines Datensatzes wird, wie du weißt, jeder Wert auf einen bestimmten Bereich skaliert, standardmäßig zwischen 0 und 1. Das ist besonders hilfreich bei Datensätzen, die viele Nullen enthalten oder bei denen die Merkmale sehr unterschiedliche Wertebereiche (Skalen) aufweisen.\n",
    "\n",
    "Mit Scikit-learn können wir Daten ganz einfach mit der Klasse `MinMaxScaler` aus dem Modul `sklearn.preprocessing` normalisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.10:</b> Jetzt bist du dran! Normalisiere die Daten aus unserem Pima-Indians-Diabetes-Beispieldatensatz mit dem <code>MinMaxScaler</code> von Scikit-learn.\n",
    "<ol>\n",
    "    <li>Lade den Datensatz (<code>data/pima-indians-diabetes.csv</code>) wie gewohnt in einen Pandas DataFrame. Denk an die Spaltennamen: <code>names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']</code>.</li>\n",
    "    <li>Trenne die Features (Merkmale, also alle Spalten außer 'class') von der Zielvariablen ('class'). Die Skalierung wird üblicherweise nur auf die Features angewendet.</li>\n",
    "    <li>Initialisiere einen <code>MinMaxScaler</code>.</li>\n",
    "    <li>Passe den Scaler an deine Feature-Daten an und transformiere sie. Speichere das Ergebnis (ein NumPy-Array) in einer neuen Variable.</li>\n",
    "    <li>Optional, aber empfohlen: Wandle das NumPy-Array zurück in einen Pandas DataFrame mit den ursprünglichen Spaltennamen der Features, um die Lesbarkeit zu verbessern. Speichere diesen DataFrame in der Variable <code>dataframe_normalized</code>.</li>\n",
    "    <li>Gib die ersten paar Zeilen des normalisierten DataFrames aus (z.B. mit <code>.head()</code>), um das Ergebnis zu überprüfen.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale data (between 0 and 1)\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "### STUDENT CODE HERE\n",
    "\n",
    "### STUDENT CODE until HERE\n",
    "\n",
    "dataframe_normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Standardisieren mit <code>StandardScaler</code>\n",
    "\n",
    "Eine weitere sehr gängige Methode ist die Standardisierung deiner Daten, oft auch als z-Transformation bezeichnet. Das Ergebnis einer Standardisierung ist, wie du gelernt hast, eine Verteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1 für jedes Merkmal.\n",
    "\n",
    "Du kannst Daten in Scikit-learn ganz einfach standardisieren, indem du die Klasse `StandardScaler` aus `sklearn.preprocessing` verwendest.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.11:</b> Standardisiere nun die Daten des Pima-Indians-Diabetes-Beispiels mit dem <code>StandardScaler</code>.\n",
    "<ol>\n",
    "    <li>Lade den Datensatz erneut (oder verwende den bereits geladenen DataFrame).</li>\n",
    "    <li>Trenne wieder die Features von der Zielvariablen.</li>\n",
    "    <li>Initialisiere einen <code>StandardScaler</code>.</li>\n",
    "    <li>Passe den Scaler an deine Feature-Daten an und transformiere sie.</li>\n",
    "    <li>Optional: Wandle das Ergebnis zurück in einen Pandas DataFrame und speichere ihn in der Variable <code>dataframe_standardized</code>.</li>\n",
    "    <li>Gib die ersten paar Zeilen des standardisierten DataFrames aus.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "### STUDENT CODE HERE\n",
    "\n",
    "### STUDENT CODE until HERE\n",
    "dataframe_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Binarisieren mit <code>Binarizer</code>\n",
    "\n",
    "Eine etwas andere Methode der Datenvorverarbeitung ist die **Binarisierung**. Hierbei legst du einen Schwellenwert (Threshold) fest. Alle Werte in deinen Daten, die über diesem Schwellenwert liegen, werden auf 1 gesetzt, und alle Werte, die darunter oder gleich dem Schwellenwert sind, werden auf 0 gesetzt. Das Ergebnis ist also ein Datensatz, der nur noch aus Nullen und Einsen besteht.\n",
    "\n",
    "Das kann nützlich sein, um kontinuierliche oder kategoriale Merkmale in binäre (Ja/Nein, Anwesend/Abwesend) Merkmale umzuwandeln. Zum Beispiel könntest du ein Merkmal \"Temperatur\" binarisieren, um daraus \"Ist Heiß\" (1) oder \"Ist Nicht Heiß\" (0) zu machen.\n",
    "\n",
    "In Scikit-learn kann dies mit Hilfe der Klasse `Binarizer` aus `sklearn.preprocessing` erreicht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.12:</b> Binarisiere die Feature-Daten des Pima-Indians-Diabetes-Beispiels.\n",
    "<ol>\n",
    "    <li>Lade den Datensatz und trenne Features von der Zielvariablen.</li>\n",
    "    <li>Initialisiere einen <code>Binarizer</code>. Setze den Parameter <code>threshold</code> auf <code>0.0</code>. Das bedeutet, alle positiven Werte werden zu 1, und alle Werte &lt;= 0 werden zu 0.</li>\n",
    "    <li>Da der <code>Binarizer</code> (im Gegensatz zu <code>MinMaxScaler</code> und <code>StandardScaler</code>) keine Parameter aus den Daten lernen muss (der Schwellenwert ist ja fest vorgegeben), reicht es hier, die <code>.fit_transform()</code> Methode direkt auf die Feature-Daten anzuwenden (oder nur <code>.transform()</code>, wenn du ihn vorher mit <code>.fit()</code> auf Dummy-Daten angepasst hättest – aber das ist hier nicht nötig).</li>\n",
    "    <li>Optional: Wandle das Ergebnis zurück in einen Pandas DataFrame und speichere ihn in der Variable <code>dataframe_binarized</code>.</li>\n",
    "    <li>Gib die ersten paar Zeilen des binarisierten DataFrames aus. Was fällt dir bei den Werten auf?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "from pandas import read_csv\n",
    "\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "### STUDENT CODE HERE\n",
    "\n",
    "### STUDENT CODE until HERE\n",
    "dataframe_binarized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung der Woche: Daten sind das A und O!\n",
    "\n",
    "Herzlichen Glückwunsch! Du hast dich diese Woche intensiv mit den Grundlagen der Datenvorverarbeitung beschäftigt. Du hast gelernt, wie man Daten richtig einliest, sie mit deskriptiver Statistik und Visualisierungen versteht und sie schließlich für Machine-Learning-Algorithmen aufbereitet durch Normalisierung, Standardisierung und Binarisierung – sowohl manuell als auch mit den Profi-Werkzeugen von Scikit-learn.\n",
    "\n",
    "Denk immer daran: Die Qualität deiner Daten und deine Vorverarbeitungsschritte haben einen **massiven** Einfluss auf die Performance deiner Machine-Learning-Modelle. \"Garbage in, garbage out\" ist hier ein oft zitiertes Sprichwort. Gute Datenvorverarbeitung ist oft zeitaufwendig, aber sie ist eine der wichtigsten Fähigkeiten eines jeden Data Scientists!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe 1.4.13: Dein Wochenrückblick</b>\n",
    "    <ul>\n",
    "        <li>Fasse in drei bis fünf Sätzen zusammen, welche wichtigen Methoden und Konzepte zur Datenvorverarbeitung du in dieser Woche (insbesondere in diesem Notebook zu Skalierung und Scikit-learn) gelernt hast. Was war für dich neu oder besonders interessant?</li>\n",
    "        <li>Wirf nochmal einen Blick auf den Pima-Indians-Diabetes-Datensatz. Basierend auf den Analysen, die du in den vorherigen Notebooks und vielleicht auch mit den skalierten Daten gemacht hast: Treffe zwei interessante Aussagen oder Beobachtungen über den Datensatz. Zum Beispiel: \"Das Merkmal X scheint stark mit dem Auftreten von Diabetes zu korrelieren, auch nach der Skalierung.\" oder \"Die Verteilung von Merkmal Y sieht nach der Standardisierung viel symmetrischer aus.\" Du kannst auch Plots oder Ausgaben aus deinen Notebooks verwenden, um deine Aussagen zu untermauern.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Deine Antwort:</b>\n",
    "\n",
    "*   **Zusammenfassung der Woche:** ...\n",
    "\n",
    "*   **Aussagen zum Datensatz:**\n",
    "    1.  ...\n",
    "    2.  ...\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
