{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9befe4d2",
   "metadata": {},
   "source": [
    "# 🤖 Woche 2: Machine Learning in Streamlit - AMALEA modernisiert\n",
    "\n",
    "**Aufbauend auf dem ursprünglichen AMALEA-Kurs: \"Maschinelles Lernen und seine Anwendungen\"**\n",
    "\n",
    "## 📚 Was du heute lernst\n",
    "\n",
    "- **ML-Grundlagen** aus dem ursprünglichen AMALEA-Kurs verstehen\n",
    "- **Deskriptive vs. Prädiktive Statistik** (AMALEA-Konzept)\n",
    "- **Training/Test/Validation** Datenaufteilung (AMALEA-Standard)\n",
    "- **ML in Streamlit** integrieren\n",
    "- **Iris-Datensatz** klassifizieren (AMALEA-Klassiker)\n",
    "- **Interaktive ML-Apps** erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 ML-Grundlagen aus dem ursprünglichen AMALEA-Kurs\n",
    "\n",
    "### Definition aus dem ursprünglichen Kurs:\n",
    "\n",
    "> *\"Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world.\"* -- Nvidia\n",
    "\n",
    "### Wichtige Konzepte:\n",
    "\n",
    "**📊 Deskriptive Statistik** (Vergangenheit verstehen)\n",
    "- Daten aus vergangenen Ereignissen analysieren\n",
    "- Grundlegende Techniken: Anzahl, Summe, Durchschnitt, Min/Max\n",
    "- **Ziel**: Was ist passiert?\n",
    "\n",
    "**🔮 Prädiktive Statistik** (Zukunft vorhersagen)\n",
    "- Basierend auf historischen Daten Vorhersagen treffen\n",
    "- Verwendet Modelle und Algorithmen\n",
    "- **Ziel**: Was wird passieren?\n",
    "\n",
    "**🤖 Machine Learning**\n",
    "- Automatisierter Ansatz für prädiktive Statistik\n",
    "- Lernt Muster aus Daten statt explizite Regeln\n",
    "- **Ziel**: Bessere Vorhersagen als regelbasierte Systeme\n",
    "\n",
    "### Datenaufteilung (AMALEA-Standard):\n",
    "\n",
    "| Datensatz | Zweck | Anteil |\n",
    "|-----------|-------|--------|\n",
    "| **Training** | Modell-Parameter optimieren | ~60-80% |\n",
    "| **Validation** | Hyperparameter tuning | ~10-20% |\n",
    "| **Test** | Finale Bewertung (nur einmal!) | ~10-20% |\n",
    "\n",
    "> **Wichtig**: Teste nie mit den Trainingsdaten! Das wäre Betrug!\n",
    "\n",
    "---\n",
    "\n",
    "## Lernziele dieser Woche\n",
    "- Einfache ML-Modelle in Streamlit integrieren\n",
    "- Benutzer-Eingaben für Vorhersagen verwenden\n",
    "- Model Training und Evaluation in der App\n",
    "- Interaktive ML-Demos erstellen\n",
    "\n",
    "## Von Datenanalyse zu Vorhersagen\n",
    "Letzte Woche haben wir gelernt, Daten zu visualisieren. Diese Woche machen wir den nächsten Schritt: Wir lassen die App Vorhersagen treffen!\n",
    "\n",
    "### 🧠 Was ist Machine Learning?\n",
    "**Machine Learning (ML)** = Computer lernen Muster aus Daten und machen Vorhersagen\n",
    "\n",
    "**Einfaches Beispiel:**\n",
    "- **Daten:** Größe und Gewicht von 1000 Menschen\n",
    "- **Muster:** Größere Menschen wiegen meist mehr\n",
    "- **Vorhersage:** Bei neuer Person mit Größe 180cm → schätze Gewicht\n",
    "\n",
    "**Haupttypen:**\n",
    "- **Klassifikation:** Kategorie vorhersagen (z.B. Spam/Nicht-Spam)\n",
    "- **Regression:** Zahlenwert vorhersagen (z.B. Hauspreise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3f5de",
   "metadata": {},
   "source": [
    "## 📦 Zusätzliche Pakete für ML\n",
    "\n",
    "**Scikit-learn** = Die wichtigste ML-Bibliothek für Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ef39a",
   "metadata": {},
   "source": [
    "## 🎬 Video-Serie: Original AMALEA Machine Learning Grundlagen\n",
    "\n",
    "**📼 Diese Video-Serie stammt aus dem Original AMALEA-Kurs (KIT 2021) und ist GOLDSTANDARD für ML-Grundlagen!**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎥 Video 1: Maschinelles Lernen und seine Anwendungen\n",
    "**📁 Datei:** `../Kurs-Videos/amalea-kit2021-w2v1 (1080p).mp4`  \n",
    "**⏱️ Dauer:** ~25 Minuten  \n",
    "**📚 Inhalte:**\n",
    "- Was ist Machine Learning?\n",
    "- Supervised vs. Unsupervised Learning\n",
    "- Regression vs. Classification\n",
    "- Real-World Anwendungen und Beispiele\n",
    "\n",
    "**💡 Warum du dieses Video schauen solltest:** Perfekte Einführung in ML-Konzepte, die auch 2025 noch 100% relevant sind!\n",
    "\n",
    "---\n",
    "\n",
    "### 🎥 Video 2: 100% Genauigkeit - Das muss doch gut sein, oder?\n",
    "**📁 Datei:** `../Kurs-Videos/amalea-kit2021-w2v2 (1080p).mp4`  \n",
    "**⏱️ Dauer:** ~20 Minuten  \n",
    "**📚 Inhalte:**\n",
    "- Overfitting vs. Underfitting\n",
    "- Training vs. Validation vs. Test Sets\n",
    "- Warum 100% Accuracy oft schlecht ist\n",
    "- Bias-Variance Tradeoff\n",
    "\n",
    "**🎯 Learning Goal:** Verstehe, warum perfekte Modelle oft die schlechtesten sind!\n",
    "\n",
    "---\n",
    "\n",
    "### 🎥 Video 3: Oh sorry, das war ein Falsch-Positiv\n",
    "**📁 Datei:** `../Kurs-Videos/amalea-kit2021-w2v3 (1080p).mp4`  \n",
    "**⏱️ Dauer:** ~30 Minuten  \n",
    "**📚 Inhalte:**\n",
    "- Confusion Matrix verstehen\n",
    "- Precision, Recall, F1-Score\n",
    "- False Positives vs. False Negatives\n",
    "- ROC Curves und AUC\n",
    "- Wann welche Metrik verwenden?\n",
    "\n",
    "**⚠️ Praxis-Relevanz:** Diese Konzepte sind ENTSCHEIDEND für echte ML-Projekte!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Empfohlene Reihenfolge:\n",
    "\n",
    "1. **📹 Erst alle 3 Videos schauen** (ca. 1,5 Stunden)\n",
    "2. **💻 Dann dieses Notebook durcharbeiten** \n",
    "3. **🔄 Videos bei Bedarf nochmal anschauen**\n",
    "\n",
    "---\n",
    "\n",
    "> 🏆 **Pro-Tipp:** Diese Original AMALEA Videos sind so gut, dass sie auch an der Stanford University verwendet werden könnten! 😉\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3e674",
   "metadata": {},
   "source": [
    "## 🎬 Original AMALEA Videos: Machine Learning Fundamentals\n",
    "\n",
    "**Diese drei Videos aus dem ursprünglichen AMALEA-Kurs sind GOLD! 💰 Schaut sie euch unbedingt an:**\n",
    "\n",
    "### 📹 **Video 1: \"Maschinelles Lernen und seine Anwendungen\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v1 (1080p).mp4`\n",
    "- **Dauer:** ~20 Minuten\n",
    "- **Inhalt:** Was ist ML? Supervised vs. Unsupervised Learning, Anwendungsbeispiele\n",
    "- **Warum wichtig:** Perfekte Einführung in ML-Konzepte\n",
    "\n",
    "### 📹 **Video 2: \"100% Genauigkeit, das muss doch gut sein, oder?\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v2 (1080p).mp4`\n",
    "- **Dauer:** ~15 Minuten  \n",
    "- **Inhalt:** Overfitting, Underfitting, Bias-Variance Trade-off\n",
    "- **Warum wichtig:** Versteht, warum perfekte Genauigkeit oft schlecht ist!\n",
    "\n",
    "### 📹 **Video 3: \"Oh sorry, das war ein Falsch-Positiv\"**\n",
    "- **Datei:** `../Kurs-Videos/amalea-kit2021-w2v3 (1080p).mp4`\n",
    "- **Dauer:** ~18 Minuten\n",
    "- **Inhalt:** Precision, Recall, F1-Score, Confusion Matrix\n",
    "- **Warum wichtig:** ML-Evaluation wie ein Profi!\n",
    "\n",
    "> **💡 Lerntipp:** Schaut die Videos → dann macht die praktischen Übungen unten. Die Theorie ist zeitlos und perfekt erklärt!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installiere ML-Bibliotheken\n",
    "!pip install scikit-learn joblib\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import plotly.express as px\n",
    "import joblib\n",
    "\n",
    "print(\"✅ ML-Pakete erfolgreich installiert!\")\n",
    "\n",
    "# 🎯 ML-Grundlagen aus dem ursprünglichen AMALEA-Kurs\n",
    "# Basiert auf \"Maschinelles Lernen und seine Anwendungen\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"🤖 Machine Learning Grundlagen - AMALEA modernisiert\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1️⃣ Iris-Datensatz laden (AMALEA-Klassiker)\n",
    "print(\"1️⃣ Iris-Datensatz laden (bekannt aus dem ursprünglichen AMALEA-Kurs):\")\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "print(f\"📊 Datensatz-Info:\")\n",
    "print(f\"- Samples (Zeilen): {df.shape[0]}\")\n",
    "print(f\"- Features (Spalten): {df.shape[1]-1}\")  # -1 für target\n",
    "print(f\"- Klassen: {list(iris.target_names)}\")\n",
    "print(f\"\\n📋 Erste 5 Zeilen:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2️⃣ Features vs. Target (AMALEA-Konzept)\n",
    "print(f\"\\n2️⃣ Features vs. Target identifizieren (AMALEA-Konzept):\")\n",
    "X = iris.data  # Features (Input)\n",
    "y = iris.target  # Target (Output)\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Features (Input): {feature_names}\")\n",
    "print(f\"Target (Output): Species {target_names}\")\n",
    "\n",
    "# 3️⃣ Datenaufteilung (AMALEA-Standard)\n",
    "print(f\"\\n3️⃣ Datenaufteilung nach AMALEA-Standard:\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training: {X_train.shape[0]} Samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} Samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
    "\n",
    "# 4️⃣ ML-Algorithmus (Random Forest - war im ursprünglichen AMALEA erwähnt)\n",
    "print(f\"\\n4️⃣ Random Forest Algorithmus (aus AMALEA-Kurs):\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5️⃣ Vorhersagen treffen\n",
    "print(f\"\\n5️⃣ Vorhersagen auf Testdaten:\")\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")\n",
    "\n",
    "# 6️⃣ Feature Importance (wichtig für Interpretierbarkeit)\n",
    "print(f\"\\n6️⃣ Feature Importance (Welche Merkmale sind wichtig?):\")\n",
    "importance = model.feature_importances_\n",
    "for name, imp in zip(feature_names, importance):\n",
    "    print(f\"- {name}: {imp:.3f}\")\n",
    "\n",
    "# 7️⃣ Verwirrungsmatrix verstehen (wichtig für Evaluation)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\n7️⃣ Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 8️⃣ Demo: Neue Vorhersage\n",
    "print(f\"\\n8️⃣ Demo - Neue Vorhersage:\")\n",
    "new_flower = [[5.1, 3.5, 1.4, 0.2]]  # Beispiel-Werte\n",
    "prediction = model.predict(new_flower)\n",
    "probability = model.predict_proba(new_flower)\n",
    "\n",
    "print(f\"Eingabe: {new_flower[0]}\")\n",
    "print(f\"Vorhersage: {target_names[prediction[0]]}\")\n",
    "print(f\"Wahrscheinlichkeiten:\")\n",
    "for name, prob in zip(target_names, probability[0]):\n",
    "    print(f\"  - {name}: {prob:.2%}\")\n",
    "\n",
    "print(f\"\\n✅ Das sind die ML-Grundlagen aus dem AMALEA-Kurs!\")\n",
    "print(f\"🚀 Jetzt bauen wir daraus eine interaktive Streamlit-App...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046e3d6",
   "metadata": {},
   "source": [
    "## 🎯 Demo 2.1: Iris Klassifikation mit ML\n",
    "\n",
    "Wir erweitern unsere Iris-App um ML-Funktionalität:\n",
    "\n",
    "### Was machen wir?\n",
    "1. **Daten laden** (Iris-Dataset)\n",
    "2. **Modell trainieren** (Random Forest)\n",
    "3. **Benutzer-Eingaben** für neue Vorhersagen\n",
    "4. **Performance anzeigen** (Genauigkeit, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f06b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_ml_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "st.set_page_config(page_title=\"Iris ML Predictor\", page_icon=\"🌸\")\n",
    "\n",
    "st.title(\"🌸🤖 Iris ML Vorhersage-App\")\n",
    "st.write(\"Trainiere ein ML-Modell und mache Vorhersagen!\")\n",
    "\n",
    "# === DATEN LADEN UND VORBEREITEN ===\n",
    "@st.cache_data\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Lädt Iris-Daten und bereitet sie für ML vor\"\"\"\n",
    "    iris = sns.load_dataset('iris')\n",
    "    \n",
    "    # Features (X) und Target (y) trennen\n",
    "    X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "    y = iris['species']\n",
    "    \n",
    "    return X, y, iris\n",
    "\n",
    "X, y, iris_data = load_and_prepare_data()\n",
    "\n",
    "# === SIDEBAR FÜR MODELL-EINSTELLUNGEN ===\n",
    "st.sidebar.header(\"🔧 Modell-Einstellungen\")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "random_state = st.sidebar.number_input(\"Random State\", 0, 100, 42)\n",
    "\n",
    "# === DATEN AUFTEILEN ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_model(test_size, random_state):\n",
    "    \"\"\"Trainiert das ML-Modell mit gegebenen Parametern\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Random Forest Classifier erstellen und trainieren\n",
    "    model = RandomForestClassifier(random_state=random_state, n_estimators=100)\n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Performance berechnen\n",
    "    train_accuracy = accuracy_score(y_train_cached, model.predict(X_train_cached))\n",
    "    test_accuracy = accuracy_score(y_test_cached, model.predict(X_test_cached))\n",
    "    \n",
    "    return model, train_accuracy, test_accuracy\n",
    "\n",
    "model, train_acc, test_acc = train_model(test_size, random_state)\n",
    "\n",
    "# === HAUPTBEREICH IN TABS ===\n",
    "tab1, tab2, tab3 = st.tabs([\"🎯 Vorhersage\", \"📊 Modell-Performance\", \"📋 Daten-Explorer\"])\n",
    "\n",
    "# === TAB 1: VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"🎯 Mache eine Vorhersage\")\n",
    "    st.write(\"Gib die Merkmale einer Iris-Blume ein und lass das Modell die Art vorhersagen:\")\n",
    "    \n",
    "    # Eingabe-Widgets für Features\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        sepal_length = st.number_input(\n",
    "            \"Kelchblatt Länge (cm)\", \n",
    "            min_value=4.0, max_value=8.0, value=5.8, step=0.1\n",
    "        )\n",
    "        sepal_width = st.number_input(\n",
    "            \"Kelchblatt Breite (cm)\", \n",
    "            min_value=2.0, max_value=4.5, value=3.0, step=0.1\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        petal_length = st.number_input(\n",
    "            \"Blütenblatt Länge (cm)\", \n",
    "            min_value=1.0, max_value=7.0, value=4.3, step=0.1\n",
    "        )\n",
    "        petal_width = st.number_input(\n",
    "            \"Blütenblatt Breite (cm)\", \n",
    "            min_value=0.1, max_value=2.5, value=1.3, step=0.1\n",
    "        )\n",
    "    \n",
    "    # Vorhersage machen\n",
    "    user_input = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    prediction = model.predict(user_input)[0]\n",
    "    prediction_proba = model.predict_proba(user_input)[0]\n",
    "    \n",
    "    # Ergebnis anzeigen\n",
    "    st.subheader(\"🔮 Vorhersage-Ergebnis:\")\n",
    "    st.success(f\"Die Iris-Art ist wahrscheinlich: **{prediction}**\")\n",
    "    \n",
    "    # Wahrscheinlichkeiten visualisieren\n",
    "    prob_df = pd.DataFrame({\n",
    "        'Art': model.classes_,\n",
    "        'Wahrscheinlichkeit': prediction_proba\n",
    "    })\n",
    "    \n",
    "    fig = px.bar(prob_df, x='Art', y='Wahrscheinlichkeit', \n",
    "                title=\"Vorhersage-Wahrscheinlichkeiten\",\n",
    "                color='Wahrscheinlichkeit',\n",
    "                color_continuous_scale='viridis')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Interpretation\n",
    "    confidence = max(prediction_proba)\n",
    "    if confidence > 0.8:\n",
    "        st.success(f\"🎯 Sehr sicher! Confidence: {confidence:.1%}\")\n",
    "    elif confidence > 0.6:\n",
    "        st.warning(f\"⚠️ Mäßig sicher. Confidence: {confidence:.1%}\")\n",
    "    else:\n",
    "        st.error(f\"❌ Unsicher. Confidence: {confidence:.1%}\")\n",
    "\n",
    "# === TAB 2: MODELL-PERFORMANCE ===\n",
    "with tab2:\n",
    "    st.header(\"📊 Modell-Performance\")\n",
    "    \n",
    "    # Key Metrics\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.metric(\"Training Genauigkeit\", f\"{train_acc:.1%}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test Genauigkeit\", f\"{test_acc:.1%}\")\n",
    "    with col3:\n",
    "        overfitting = train_acc - test_acc\n",
    "        st.metric(\"Overfitting\", f\"{overfitting:.1%}\")\n",
    "    \n",
    "    # Overfitting-Warnung\n",
    "    if overfitting > 0.1:\n",
    "        st.warning(\"⚠️ Mögliches Overfitting! Modell könnte auf neuen Daten schlechter sein.\")\n",
    "    else:\n",
    "        st.success(\"✅ Gute Generalisierung!\")\n",
    "    \n",
    "    # Feature Importance\n",
    "    st.subheader(\"🎯 Feature Wichtigkeit\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': ['Kelchblatt Länge', 'Kelchblatt Breite', 'Blütenblatt Länge', 'Blütenblatt Breite'],\n",
    "        'Wichtigkeit': model.feature_importances_\n",
    "    }).sort_values('Wichtigkeit', ascending=True)\n",
    "    \n",
    "    fig_importance = px.bar(importance_df, x='Wichtigkeit', y='Feature', \n",
    "                           orientation='h', title=\"Welche Features sind am wichtigsten?\",\n",
    "                           color='Wichtigkeit', color_continuous_scale='blues')\n",
    "    st.plotly_chart(fig_importance, use_container_width=True)\n",
    "    \n",
    "    # Erklärung\n",
    "    most_important = importance_df.iloc[-1]['Feature']\n",
    "    st.write(f\"💡 **{most_important}** ist das wichtigste Merkmal für die Klassifikation!\")\n",
    "    \n",
    "    # Confusion Matrix (vereinfacht)\n",
    "    y_pred = model.predict(X_test)\n",
    "    st.subheader(\"🔍 Detaillierte Performance\")\n",
    "    \n",
    "    # Classification Report als DataFrame\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    st.dataframe(report_df.round(3))\n",
    "\n",
    "# === TAB 3: DATEN-EXPLORER ===\n",
    "with tab3:\n",
    "    st.header(\"📋 Daten-Explorer\")\n",
    "    \n",
    "    # Trainings- vs Test-Daten\n",
    "    st.subheader(\"📊 Datenaufteilung\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.metric(\"Trainings-Samples\", len(X_train))\n",
    "    with col2:\n",
    "        st.metric(\"Test-Samples\", len(X_test))\n",
    "    \n",
    "    # Originaledaten anzeigen\n",
    "    st.subheader(\"🔍 Original-Dataset\")\n",
    "    st.dataframe(iris_data)\n",
    "    \n",
    "    # Korrelations-Heatmap\n",
    "    st.subheader(\"🔗 Feature-Korrelationen\")\n",
    "    correlation_matrix = X.corr()\n",
    "    fig_corr = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
    "                        title=\"Wie hängen die Features zusammen?\",\n",
    "                        color_continuous_scale='RdBu')\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "    \n",
    "    # Scatter Plot Matrix\n",
    "    st.subheader(\"📈 Feature-Beziehungen\")\n",
    "    fig_scatter = px.scatter_matrix(iris_data, \n",
    "                                   dimensions=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "                                   color='species')\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"💡 **ML-Tipp:** Je mehr gute Daten, desto besser das Modell!\")\n",
    "st.sidebar.write(\"🎯 **Nächster Schritt:** Probiere verschiedene Modell-Parameter aus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8890f",
   "metadata": {},
   "source": [
    "## 💡 ML-Konzepte erklärt\n",
    "\n",
    "### Train-Test Split\n",
    "**Warum teilen wir Daten auf?**\n",
    "- **Training:** Modell lernt aus diesen Daten\n",
    "- **Test:** Prüfen wie gut es bei unbekannten Daten ist\n",
    "- **80/20 oder 70/30** sind typische Aufteilungen\n",
    "\n",
    "### Random Forest\n",
    "**Was ist das?**\n",
    "- **Ensemble-Methode:** Kombiniert viele Entscheidungsbäume\n",
    "- **Robust:** Funktioniert gut bei vielen Problemen\n",
    "- **Feature Importance:** Zeigt, welche Variablen wichtig sind\n",
    "\n",
    "### Overfitting\n",
    "**Problem:** Modell lernt Trainingsdaten auswendig, kann aber nicht generalisieren\n",
    "**Erkennung:** Training-Performance >> Test-Performance\n",
    "**Lösung:** Mehr Daten, einfachere Modelle, Regularisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e7c66",
   "metadata": {},
   "source": [
    "## 🎯 Aufgabe 2.1: Regression-App erstellen\n",
    "\n",
    "Jetzt erstellen wir eine App für ein **Regressions-Problem** (Zahlenwerte vorhersagen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b070cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile housing_regression_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import plotly.express as px\n",
    "\n",
    "st.set_page_config(page_title=\"Housing Price Predictor\", page_icon=\"🏠\")\n",
    "\n",
    "st.title(\"🏠💰 Immobilienpreis-Vorhersage\")\n",
    "st.write(\"Schätze Hauspreise basierend auf verschiedenen Merkmalen\")\n",
    "\n",
    "# === SIMULIERTE HOUSING-DATEN ERSTELLEN ===\n",
    "@st.cache_data\n",
    "def create_housing_data():\n",
    "    \"\"\"Erstellt realistische Beispiel-Immobiliendaten\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 500\n",
    "    \n",
    "    # Features erstellen\n",
    "    rooms = np.random.normal(6, 1.5, n_samples)\n",
    "    rooms = np.clip(rooms, 3, 10)  # Zwischen 3 und 10 Zimmer\n",
    "    \n",
    "    age = np.random.uniform(1, 100, n_samples)\n",
    "    distance_to_city = np.random.uniform(1, 12, n_samples)\n",
    "    crime_rate = np.random.exponential(3, n_samples)\n",
    "    crime_rate = np.clip(crime_rate, 0, 15)  # Max 15\n",
    "    \n",
    "    # Target erstellen (mit realistischen Zusammenhängen)\n",
    "    price = (\n",
    "        rooms * 50000 +                    # Mehr Zimmer = teurer\n",
    "        (100 - age) * 1000 +              # Neuer = teurer\n",
    "        (12 - distance_to_city) * 5000 +  # Näher zur Stadt = teurer\n",
    "        (-crime_rate * 2000) +            # Mehr Kriminalität = billiger\n",
    "        np.random.normal(0, 20000, n_samples) +  # Zufälliges Rauschen\n",
    "        200000                            # Basis-Preis\n",
    "    )\n",
    "    \n",
    "    # Negative Preise vermeiden\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'rooms': rooms,\n",
    "        'age': age,\n",
    "        'distance_to_city': distance_to_city,\n",
    "        'crime_rate': crime_rate,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# === DATEN LADEN ===\n",
    "housing_data = create_housing_data()\n",
    "\n",
    "# Features und Target trennen\n",
    "feature_columns = ['rooms', 'age', 'distance_to_city', 'crime_rate']\n",
    "X = housing_data[feature_columns]\n",
    "y = housing_data['price']\n",
    "\n",
    "# === SIDEBAR FÜR MODELL-AUSWAHL ===\n",
    "st.sidebar.header(\"🔧 Modell-Konfiguration\")\n",
    "model_type = st.sidebar.selectbox(\n",
    "    \"Wähle Algorithmus:\", \n",
    "    [\"Linear Regression\", \"Random Forest\"]\n",
    ")\n",
    "test_size = st.sidebar.slider(\"Test-Datenanteil\", 0.1, 0.5, 0.2)\n",
    "\n",
    "# === MODELL TRAINIEREN ===\n",
    "@st.cache_data\n",
    "def train_regression_model(model_type, test_size):\n",
    "    \"\"\"Trainiert Regressions-Modell\"\"\"\n",
    "    X_train_cached, X_test_cached, y_train_cached, y_test_cached = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    if model_type == \"Linear Regression\":\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "    \n",
    "    model.fit(X_train_cached, y_train_cached)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    train_pred = model.predict(X_train_cached)\n",
    "    test_pred = model.predict(X_test_cached)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_r2 = r2_score(y_train_cached, train_pred)\n",
    "    test_r2 = r2_score(y_test_cached, test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_cached, train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_cached, test_pred))\n",
    "    \n",
    "    return model, train_r2, test_r2, train_rmse, test_rmse, X_test_cached, y_test_cached\n",
    "\n",
    "model, train_r2, test_r2, train_rmse, test_rmse, X_test, y_test = train_regression_model(model_type, test_size)\n",
    "\n",
    "# === APP-LAYOUT ===\n",
    "tab1, tab2, tab3 = st.tabs([\"🏠 Preis-Vorhersage\", \"📈 Modell-Analyse\", \"📊 Daten-Übersicht\"])\n",
    "\n",
    "# === TAB 1: PREIS-VORHERSAGE ===\n",
    "with tab1:\n",
    "    st.header(\"🏠 Immobilienpreis schätzen\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        rooms = st.slider(\"Anzahl Zimmer\", 3.0, 10.0, 6.0, 0.1)\n",
    "        age = st.slider(\"Alter des Hauses (Jahre)\", 1, 100, 30)\n",
    "    \n",
    "    with col2:\n",
    "        distance = st.slider(\"Entfernung zur Stadt (km)\", 1.0, 12.0, 5.0, 0.1)\n",
    "        crime_rate = st.slider(\"Kriminalitätsrate\", 0.0, 15.0, 3.0, 0.1)\n",
    "    \n",
    "    # Vorhersage\n",
    "    user_input = np.array([[rooms, age, distance, crime_rate]])\n",
    "    predicted_price = model.predict(user_input)[0]\n",
    "    \n",
    "    st.subheader(\"💰 Geschätzter Preis:\")\n",
    "    st.success(f\"**${predicted_price:,.0f}**\")\n",
    "    \n",
    "    # Preis-Kategorisierung\n",
    "    if predicted_price < 300000:\n",
    "        st.info(\"🏠 Günstiges Segment\")\n",
    "    elif predicted_price < 600000:\n",
    "        st.warning(\"🏘️ Mittleres Segment\")\n",
    "    else:\n",
    "        st.error(\"🏰 Luxus-Segment\")\n",
    "    \n",
    "    # Vergleich mit ähnlichen Häusern\n",
    "    st.subheader(\"📊 Vergleich mit ähnlichen Häusern\")\n",
    "    similar_houses = housing_data[\n",
    "        (abs(housing_data['rooms'] - rooms) < 1) &\n",
    "        (abs(housing_data['age'] - age) < 10)\n",
    "    ]\n",
    "    \n",
    "    if len(similar_houses) > 0:\n",
    "        avg_similar_price = similar_houses['price'].mean()\n",
    "        difference = predicted_price - avg_similar_price\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.metric(\"Durchschnitt ähnlicher Häuser\", f\"${avg_similar_price:,.0f}\")\n",
    "        with col2:\n",
    "            st.metric(\"Unterschied\", f\"${difference:,.0f}\")\n",
    "\n",
    "# === TAB 2: MODELL-ANALYSE ===\n",
    "with tab2:\n",
    "    st.header(\"📈 Modell-Performance\")\n",
    "    \n",
    "    # Performance Metriken\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Train R²\", f\"{train_r2:.3f}\")\n",
    "    with col2:\n",
    "        st.metric(\"Test R²\", f\"{test_r2:.3f}\")\n",
    "    with col3:\n",
    "        st.metric(\"Train RMSE\", f\"${train_rmse:,.0f}\")\n",
    "    with col4:\n",
    "        st.metric(\"Test RMSE\", f\"${test_rmse:,.0f}\")\n",
    "    \n",
    "    # R² Erklärung\n",
    "    st.write(\"**R² (R-Squared) Interpretation:**\")\n",
    "    if test_r2 > 0.8:\n",
    "        st.success(\"🎯 Sehr gutes Modell!\")\n",
    "    elif test_r2 > 0.6:\n",
    "        st.warning(\"⚠️ OK-es Modell\")\n",
    "    else:\n",
    "        st.error(\"❌ Schwaches Modell\")\n",
    "    \n",
    "    st.write(f\"Das Modell erklärt {test_r2:.1%} der Preisvarianz.\")\n",
    "    \n",
    "    # Vorhersage vs. Realität Plot\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Echte Preise': y_test, \n",
    "        'Vorhergesagte Preise': y_pred\n",
    "    })\n",
    "    \n",
    "    fig_pred = px.scatter(\n",
    "        pred_df, \n",
    "        x='Echte Preise', \n",
    "        y='Vorhergesagte Preise',\n",
    "        title=\"Vorhersage vs. Realität\"\n",
    "    )\n",
    "    # Perfekte Linie hinzufügen\n",
    "    min_price = min(pred_df['Echte Preise'].min(), pred_df['Vorhergesagte Preise'].min())\n",
    "    max_price = max(pred_df['Echte Preise'].max(), pred_df['Vorhergesagte Preise'].max())\n",
    "    fig_pred.add_shape(\n",
    "        type=\"line\", \n",
    "        x0=min_price, y0=min_price,\n",
    "        x1=max_price, y1=max_price,\n",
    "        line=dict(color=\"red\", dash=\"dash\")\n",
    "    )\n",
    "    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "    \n",
    "    # Feature Importance (nur für Random Forest)\n",
    "    if model_type == \"Random Forest\":\n",
    "        st.subheader(\"🎯 Feature Wichtigkeit\")\n",
    "        feature_names = ['Zimmer', 'Alter', 'Entfernung zur Stadt', 'Kriminalitätsrate']\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Wichtigkeit': model.feature_importances_\n",
    "        }).sort_values('Wichtigkeit', ascending=True)\n",
    "        \n",
    "        fig_imp = px.bar(\n",
    "            importance_df, \n",
    "            x='Wichtigkeit', \n",
    "            y='Feature',\n",
    "            orientation='h',\n",
    "            title=\"Welche Faktoren beeinflussen den Preis am meisten?\"\n",
    "        )\n",
    "        st.plotly_chart(fig_imp, use_container_width=True)\n",
    "\n",
    "# === TAB 3: DATEN-ÜBERSICHT ===\n",
    "with tab3:\n",
    "    st.header(\"📊 Datensatz-Übersicht\")\n",
    "    \n",
    "    # Basis-Statistiken\n",
    "    st.subheader(\"📋 Grundlegende Statistiken\")\n",
    "    st.dataframe(housing_data.describe(), use_container_width=True)\n",
    "    \n",
    "    # Rohdaten\n",
    "    st.subheader(\"🔍 Rohdaten (Erste 20 Zeilen)\")\n",
    "    st.dataframe(housing_data.head(20))\n",
    "    \n",
    "    # Verteilungen der Features\n",
    "    st.subheader(\"📈 Feature-Verteilungen\")\n",
    "    feature_to_plot = st.selectbox(\n",
    "        \"Feature für Histogramm:\", \n",
    "        ['rooms', 'age', 'distance_to_city', 'crime_rate', 'price']\n",
    "    )\n",
    "    \n",
    "    fig_hist = px.histogram(\n",
    "        housing_data, \n",
    "        x=feature_to_plot, \n",
    "        title=f\"Verteilung von {feature_to_plot}\",\n",
    "        nbins=30\n",
    "    )\n",
    "    st.plotly_chart(fig_hist, use_container_width=True)\n",
    "    \n",
    "    # Korrelations-Matrix\n",
    "    st.subheader(\"🔗 Korrelationen\")\n",
    "    corr_matrix = housing_data.corr()\n",
    "    fig_corr = px.imshow(\n",
    "        corr_matrix,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Korrelations-Matrix\"\n",
    "    )\n",
    "    st.plotly_chart(fig_corr, use_container_width=True)\n",
    "\n",
    "# === FOOTER ===\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"💡 **Regression-Tipp:** R² näher bei 1 = besseres Modell\")\n",
    "st.sidebar.write(\"🎯 **Ausprobieren:** Verändere die Eingaben und beobachte die Auswirkungen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb514866",
   "metadata": {},
   "source": [
    "## 🏆 Lernziel-Check Woche 2\n",
    "\n",
    "Am Ende dieser Woche solltest du:\n",
    "\n",
    "- [ ] **ML-Grundlagen verstehen:** Klassifikation vs. Regression, Training vs. Test\n",
    "- [ ] **ML-Modelle in Streamlit integrieren:** Scikit-learn + Streamlit\n",
    "- [ ] **Benutzer-Eingaben für Vorhersagen nutzen:** Input-Widgets → Predictions\n",
    "- [ ] **Modell-Performance bewerten:** Accuracy, R², Overfitting erkennen\n",
    "- [ ] **Feature Importance interpretieren:** Welche Variablen sind wichtig?\n",
    "- [ ] **Eine funktionsfähige ML-App entwickelt haben:** Vollständig und benutzerfreundlich\n",
    "\n",
    "## 📝 Vorbereitung für die Fallstudie\n",
    "\n",
    "**Denke bereits jetzt über dein Fallstudie-Projekt nach:**\n",
    "\n",
    "### Projektideen für deine Fallstudie:\n",
    "1. **Kunden-Segmentierung** (E-Commerce, Bank)\n",
    "2. **Preis-Vorhersage** (Immobilien, Autos, Aktien)\n",
    "3. **Klassifikation** (Spam-Erkennung, Sentiment-Analyse)\n",
    "4. **Gesundheitsdaten** (Krankheits-Vorhersage, Fitness-Tracking)\n",
    "5. **Sport-Analytics** (Spieler-Performance, Ergebnis-Vorhersagen)\n",
    "\n",
    "### Fragen für dein Projekt:\n",
    "1. **Welches Problem möchtest du lösen?**\n",
    "2. **Welche Daten brauchst du?** (CSV, API, Web-Scraping?)\n",
    "3. **Wer ist deine Zielgruppe?** (Manager, Endkunden, Experten?)\n",
    "4. **Was ist das gewünschte Ergebnis?** (Klassifikation, Regression, Clustering?)\n",
    "\n",
    "## 🔮 Ausblick auf Woche 3\n",
    "\n",
    "Nächste Woche werden wir:\n",
    "- **Apps online deployen** (Streamlit Cloud, Heroku)\n",
    "- **Externe Datenquellen** einbinden (APIs, Datenbanken)\n",
    "- **App-Performance optimieren** (Caching, Lazy Loading)\n",
    "- **Professional Design** (Styling, UX/UI Verbesserungen)\n",
    "\n",
    "**💡 Tipp:** Beginne bereits jetzt mit der Sammlung von Daten für deine Fallstudie!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
