{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48417768",
   "metadata": {},
   "source": [
    "# üß† Woche 4: Neural Networks in Streamlit - AMALEA Deep Learning\n",
    "\n",
    "**Integration der urspr√ºnglichen AMALEA-Notebooks:**\n",
    "- \"Jetzt geht's in die Tiefe\" ‚Üí Neural Network Grundlagen\n",
    "- \"Wir trainieren nur bergab\" ‚Üí Backpropagation & Optimierung\n",
    "- \"Regression II\" ‚Üí Neural Networks f√ºr Regression\n",
    "- \"Classification Softmax\" ‚Üí Neural Networks f√ºr Klassifikation\n",
    "\n",
    "## üìö Was du heute lernst\n",
    "\n",
    "- **K√ºnstliche Neuronen** üß† - Grundbausteine des Deep Learning\n",
    "- **Neural Networks** üï∏Ô∏è - Mehrere Neuronen vernetzen\n",
    "- **Backpropagation** ‚ö° - Wie Neuronen \"lernen\"\n",
    "- **Activation Functions** üìà - ReLU, Sigmoid, Softmax verstehen\n",
    "- **Streamlit Neural Network Apps** üöÄ - Interaktive Demos erstellen\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Neural Network Grundlagen aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "\n",
    "### Das k√ºnstliche Neuron (aus \"Jetzt geht's in die Tiefe\")\n",
    "\n",
    "Ein **k√ºnstliches Neuron** ist eine mathematische Funktion, inspiriert von biologischen Nervenzellen:\n",
    "\n",
    "$$f_{neuron}(x) = œÜ(‚àë_{n=1}^m x_n ¬∑ w_n + b)$$\n",
    "\n",
    "**Komponenten:**\n",
    "- **Eingaben** (x_n): Numerische Werte (Daten oder andere Neuronen)\n",
    "- **Gewichte** (w_n): Werden mit Eingaben multipliziert\n",
    "- **Bias** (b): Konstanter Wert, wird zur Summe addiert\n",
    "- **Aktivierungsfunktion** (œÜ): Transformiert die Summe\n",
    "- **Ausgabe** (y): Aktivierung des Neurons\n",
    "\n",
    "### Einfachstes Neuron: Lineare Regression\n",
    "\n",
    "Ohne Aktivierungsfunktion: `f(x) = w * x + b`\n",
    "\n",
    "Das ist bereits **lineare Regression**! ü§Ø\n",
    "\n",
    "### Aktivierungsfunktionen (entscheidend f√ºr Deep Learning)\n",
    "\n",
    "| Funktion | Formel | Verwendung |\n",
    "|----------|--------|------------|\n",
    "| **Linear** | f(x) = x | Regression (Ausgabeschicht) |\n",
    "| **ReLU** | f(x) = max(0, x) | **Meistverwendet** in Hidden Layers |\n",
    "| **Sigmoid** | f(x) = 1/(1+e^(-x)) | Bin√§re Klassifikation |\n",
    "| **Softmax** | f(x_i) = e^(x_i)/Œ£e^(x_j) | Multi-Class Klassifikation |\n",
    "\n",
    "### Warum Neural Networks so m√§chtig sind\n",
    "\n",
    "> **Universal Approximation Theorem**: Ein ausreichend gro√ües neuronales Netz kann jede kontinuierliche Funktion beliebig genau approximieren!\n",
    "\n",
    "**Aber**: Mehr Parameter ‚â† automatisch bessere Performance (Overfitting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70013964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Neural Network Demo - Basierend auf urspr√ºnglichen AMALEA-Konzepten\n",
    "# Integration von \"Jetzt geht's in die Tiefe\" + \"Wir trainieren nur bergab\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "print(\"üß† Neural Networks - AMALEA Konzepte modernisiert\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1Ô∏è‚É£ Einfachstes Neuron implementieren (aus urspr√ºnglichem AMALEA)\n",
    "print(\"1Ô∏è‚É£ Einfachstes Neuron: f(x) = w*x + b\")\n",
    "\n",
    "def simple_neuron(x, w, b):\n",
    "    \"\"\"\n",
    "    Implementierung aus dem urspr√ºnglichen AMALEA-Kurs\n",
    "    Aufgabe 4.1.1: Einfachstes Neuron ohne Aktivierungsfunktion\n",
    "    \"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "# Demo mit AMALEA-Beispiel (w=-1, b=1)\n",
    "w, b = -1, 1\n",
    "x_values = range(10)\n",
    "y_values = [simple_neuron(x, w, b) for x in x_values]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x_values, y_values, 'o-', linewidth=2, markersize=8)\n",
    "plt.title(\"Einfachstes Neuron (AMALEA Original: w=-1, b=1)\")\n",
    "plt.xlabel(\"x (Input)\")\n",
    "plt.ylabel(\"y (Output)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mit w={w}, b={b}: f(5) = {simple_neuron(5, w, b)}\")\n",
    "\n",
    "# 2Ô∏è‚É£ Aktivierungsfunktionen visualisieren (aus AMALEA erweitert)\n",
    "print(f\"\\n2Ô∏è‚É£ Aktivierungsfunktionen (zentral f√ºr Deep Learning):\")\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, relu(x), 'b-', linewidth=2)\n",
    "plt.title(\"ReLU: max(0, x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, sigmoid(x), 'r-', linewidth=2)\n",
    "plt.title(\"Sigmoid: 1/(1+e^(-x))\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, tanh(x), 'g-', linewidth=2)\n",
    "plt.title(\"Tanh: tanh(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3Ô∏è‚É£ Regression mit Neural Network (aus \"Regression II\")\n",
    "print(f\"\\n3Ô∏è‚É£ Neural Network f√ºr Regression (aus AMALEA 'Regression II'):\")\n",
    "\n",
    "# K√ºnstliche Daten erstellen\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Neural Network f√ºr Regression\n",
    "nn_regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 5),  # 2 Hidden Layers\n",
    "    activation='relu',           # ReLU Aktivierung (AMALEA-Standard)\n",
    "    solver='adam',              # Adam Optimizer (aus \"Wir trainieren nur bergab\")\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_regressor.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg = nn_regressor.predict(X_test_reg)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train_reg, y_train_reg, alpha=0.6, label='Training')\n",
    "plt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='Test', color='orange')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regressionsdaten\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
    "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')\n",
    "plt.xlabel(\"Tats√§chliche Werte\")\n",
    "plt.ylabel(\"Vorhergesagte Werte\")\n",
    "plt.title(\"Vorhersage vs. Realit√§t\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# 4Ô∏è‚É£ Klassifikation mit Neural Network (aus \"Classification Softmax\")\n",
    "print(f\"\\n4Ô∏è‚É£ Neural Network f√ºr Klassifikation (aus AMALEA 'Softmax'):\")\n",
    "\n",
    "# K√ºnstliche Klassifikationsdaten\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=300, n_features=2, n_redundant=0, \n",
    "    n_informative=2, n_clusters_per_class=1, \n",
    "    random_state=42\n",
    ")\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Neural Network f√ºr Klassifikation\n",
    "nn_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 5),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_classifier.fit(X_train_clf, y_train_clf)\n",
    "y_pred_clf = nn_classifier.predict(X_test_clf)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue']\n",
    "for i, color in enumerate(colors):\n",
    "    mask = y_train_clf == i\n",
    "    plt.scatter(X_train_clf[mask, 0], X_train_clf[mask, 1], \n",
    "               c=color, alpha=0.6, label=f'Klasse {i}')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Klassifikationsdaten\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Vorhergesagte Klasse\")\n",
    "plt.ylabel(\"Tats√§chliche Klasse\")\n",
    "\n",
    "# Zahlen in der Matrix anzeigen\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Genauigkeit: {accuracy:.2%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Neural Network Grundlagen aus dem AMALEA-Kurs erfolgreich modernisiert!\")\n",
    "print(f\"üöÄ Jetzt erstellen wir daraus interaktive Streamlit-Apps...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
