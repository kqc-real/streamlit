{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# [Nur Colab] Diese Zellen müssen nur auf *Google Colab* ausgeführt werden und installieren Packete und Daten\n","!wget -q https://raw.githubusercontent.com/KI-Campus/AMALEA/master/requirements.txt && pip install --quiet -r requirements.txt\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/data/data.zip\" && unzip -q data.zip\n","!wget --quiet \"https://github.com/KI-Campus/AMALEA/releases/download/images/images.zip\" && unzip -q images.zip"]},{"cell_type":"markdown","metadata":{},"source":["# Falten, Ausschneiden und fertig ist das CNN"]},{"cell_type":"markdown","metadata":{},"source":["## Convolutional Neural Networks (CNNs)\n","\n","### Grundlagen\n","\n","Neuronale Netze können auf verschiedene Daten angewendet werden, wobei sich für visuelle Informationen Convolutional Neural Networks (CNNs) als effizienter und effektiver erwiesen haben. Tatsächlich schneiden diese Modelle besser ab als reguläre Strukturen mit mehreren Hidden Layers, sofern die betrachteten Daten räumliche Abhängigkeiten enthalten. \n","Diese Voraussetzung ist in Bildern als Pixelwerte gegeben, die sich räumlich aufeinander beziehen. Inspiriert von Filtern, die in der Ära der harten Featureextraktion in der Computer Vision verwendet wurden, passen diese Netze ihre Gewichte in Form von `lernbaren Filtern` an, um das Bild in höhere räumliche Dimensionen zu projizieren. Features sind definiert als eine Charakteristik der zu beobachteten Daten, beginnend in den ersten Faltungsschichten (engl. convolutional layers), indem sie die Kanten eines Bildes oder eine `niedrigere semantische` Repräsentation darstellen. Durch das weitere Durchlaufen eines CNN, werden diese in gröbere \"abstrakte Attribute\" umgewandelt, die es dem Netzwerk ermöglichen, einzelne oder mehrere Objekte in einem Bild zu unterscheiden oder zu klassifizieren. Als Beispiel wird unten die Klassifizierung eines Autos gezeigt. Beachten Sie, dass `CONV` eine Faltungsoperation ist, `RELU` sich auf die gleichgerichtete lineare Einheit bezieht und `POOL` eine Pooling-Schicht zur Reduzierung der Bilddimensionen beschreibt. Diese Operationen werden in Kürze erklärt. \n","\n","![CNN feedforward](images/stanford_cnn.jpg \"CNN_feedforward\")\n","\n","\n","                                    Quelle: http://cs231n.github.io/convolutional-networks/"]},{"cell_type":"markdown","metadata":{},"source":["In der Computer Vision oder Bildverarbeitung gibt es gängige Filtermatrizen, die verwendet werden, um Informationen aus einem Bild zu extrahieren oder die Darstellung der Features zu verändern. Lernbare Filter in der Faltungsschicht (engl. convolutional layer) verändern ebenfalls die Darstellung eines Bildes. Diese werden später im Abschnitt über die Faltungsschichten besprochen.\n","\n","Im Moment konzentrieren wir uns auf die bekannten (nicht erlernbaren) Bildfilter. Um die Auswirkungen dieser Filter zu verdeutlichen, werden wir sie auf ein Beispielbild anwenden:"]},{"cell_type":"markdown","metadata":{},"source":["### Importe"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from ipywidgets import interact, interactive, fixed, interact_manual\n","import ipywidgets as widgets\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from matplotlib import figure\n","import numpy as np\n","from scipy import misc, signal"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#Load Ascent image from scipy\n","ascent = misc.ascent()\n","\n","figure_inches = 10\n","fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n","ax.set_title('Original-Bild', fontsize = 15)\n","ax.imshow(ascent, interpolation='nearest', cmap='gray')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["#### Mean-Filter\n","\n","Der erste Filter, den wir anwenden werden, heißt \"Mean-Filter\". Der Mean-Filter ersetzt einen Pixelwert durch den Mittelwert der Werte, die sich in der Nachbarschaft des Pixels $9\\times 9$ befinden. Das heißt, wir verwenden alle benachbarten Pixel sowie die Werte des zu ersetzenden Pixels, dann berechnen wir den Mittelwert dieser neun Werte und verwenden das Ergebnis als neuen Pixelwert."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Some 3x3 filter matrices used in computer vision\n","\n","# Mean-filter\n","mean = 1/9 * np.ones([9,9])\n","\n","# Approximation of the gradient\n","prewitt_x =  np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n","prewitt_y =  np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n","\n","sobel_x =  np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n","sobel_y =  np.array([[-1, -2, 1], [0, 0, 0], [1, 2, 1]])\n","\n","# Second-order derivation \n","laplace_var = np.array([[0, 1, 0], [1, -4, 1],[0, 1, 0]])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["ascent_mean = signal.convolve2d(ascent, mean, boundary='symm', mode='same')\n","fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n","ax.set_title('Ergebnis nach Anwendung des Mean-Filters', fontsize = 15)\n","ax.imshow(ascent_mean, interpolation='nearest', cmap='gray')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["Wie Sie sehen können, wird dadurch das Bild unscharf."]},{"cell_type":"markdown","metadata":{},"source":["#### Prewitt-Filter\n","\n","Das Ziel des Prewitt-Filters ist, den Pixel-Wert durch seine Ableitung, d.h. die Änderung im Farbwert, zu ersetzen. Auch hier kommt eine $9\\times 9$-Nachbarschaft zum Einsatz. Da Bilder i.d.R. 2-dimensional sind (x- und y-Achse), kann die Ableitung sowohl der einen als auch der anderen Dimension berechnet werden. Praktisch bedeutet dies, dass es zwei Prewitt-Filter gibt, einen in x- und einen in y-Richtung. Außerdem korrespondieren größere Änderungen im Pixelwert (d.h. der Wert der Ableitung ist größer) mit Kanten im Bild, weshalb die Prewitt-Filter auch als Kantendetektoren bezeichnet werden."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["ascent_x_prew = signal.convolve2d(ascent, prewitt_x, boundary='symm', mode='same')\n","ascent_x_prew = np.absolute(ascent_x_prew)\n","fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n","ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x-Richtung', fontsize = 15)\n","ax.imshow(ascent_x_prew, interpolation='nearest', cmap='gray')\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["ascent_y_prew = signal.convolve2d(ascent, prewitt_y, boundary='symm', mode='same')\n","ascent_y_prew = np.absolute(ascent_y_prew)\n","fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n","ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in y-Richtung', fontsize = 15)\n","ax.imshow(ascent_y_prew, interpolation='nearest', cmap='gray')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["Auf den beiden Bildern sieht man, dass die entsprechenden Kanten in entweder x- oder y-Richtung extrahiert wurden. In der praktischen Anwendung, möchte man aber meist alle Kanten extrahieren - unabhängig von der Richtung. In diesem Fall werden schlichtweg die beiden Bilder addiert und anschließend durch 2 geteilt. Das Teilen durch 2 ist notwendig, damit die Pixelwert im Bereich 0 bis 255 verbleiben. "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["x_y_prew = (ascent_y_prew + ascent_x_prew) / 2\n","\n","fig, ax = plt.subplots(figsize=(figure_inches, figure_inches))\n","ax.set_title('Ergebnis nach Anwendung des Prewitt-Filters in x- & y-Richtung', fontsize = 15)\n","ax.imshow(x_y_prew, interpolation='nearest', cmap='gray')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-success\">\n","<b>Frage 5.1.1:</b> Welcher lineare Prewitt-Filter wird benötigt, um horizontale Kanten hervorzuheben? \n","</div>\n","\n","<div class=\"alert alert-block alert-success\">\n","<b>Ihre Antwort:</b></div>\n"]},{"cell_type":"markdown","metadata":{},"source":["### Gesichtserkennung\n","\n","Im folgenden wird die im Video erwähnte Gesichtserkennung skizziert.\n","\n","Zu Beginn erstmal ein Bild mit Gesichtern:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","img = Image.open('images/Gesichter.jpg')\n","\n","plt.figure(figsize=(15,10))\n","plt.imshow(img)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Nun kann mithilfe einer Gesichtserkennung, die ensprechenden Bounding Boxen extrahiert werden."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["img = Image.open('images/Gesichtserkennung_1_Bounding_Boxes.png')\n","\n","plt.figure(figsize=(15,10))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Und abschließend die Landmarks:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["img = Image.open('images/Gesichtserkennung_2_Landmarks.png')\n","\n","plt.figure(figsize=(15,10))\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Nun können die Gesichter extrahiert werden"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os\n","\n","path = './images/landmarks/face/'\n","for file in os.listdir(path):\n","    if \".png\" in file:\n","        img = Image.open(os.path.join(path, file))\n","        plt.figure()\n","        plt.imshow(img)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Nun können die Landmarken extrahiert werden:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import os\n","\n","path = './images/landmarks/landmarks/'\n","for file in os.listdir(path):\n","    if \".png\" in file:\n","        img = Image.open(os.path.join(path, file))\n","        plt.figure()\n","        plt.imshow(img)\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Und zuletzt werden die Gesichter transformiert, sodass alle gleich große (96x96 Pixel) sind und mittig liegen. Somit können die Landmarken (z.B. Augen) immer an der gleichen Stelle im Bild auftauchen. "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import os\n","\n","path = './images/landmarks/transformed/'\n","for file in os.listdir(path):\n","    if \".png\" in file:\n","        img = Image.open(os.path.join(path, file))\n","        plt.figure()\n","        plt.imshow(img)\n","        plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python-amalea"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
