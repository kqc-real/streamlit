# ü§ñ Machbarkeitsstudie: Self-Hosted DeepSeek R1 Fragengenerator

**Projekt:** Integration eines self-hosted DeepSeek R1 LLM zur automatisierten Fragengenerierung  
**Datum:** 8. Oktober 2025  
**Version:** 1.0  
**Status:** Analysephase

---

## üìã Executive Summary

### Ziel

Automatische Generierung von MC-Fragensets √ºber einen **self-hosted DeepSeek R1 Server** als kostenfreie Alternative zur OpenAI Cloud API.

### Kernaussagen

- ‚úÖ **Kosteneffizient:** 0‚Ç¨ API-Kosten, nur Strom (~15‚Ç¨/Monat)
- ‚úÖ **Datenschutz:** 100% on-premise, DSGVO-optimal
- ‚ö†Ô∏è **Hardware:** Ben√∂tigt GPU mit 24GB+ VRAM (R1-14B) oder 80GB+ (R1-70B)
- ‚ö†Ô∏è **Qualit√§t:** 7-8/10 (gut, aber unter GPT-4's 9/10)
- ‚è±Ô∏è **Aufwand:** 20-28h Implementation (Server-Setup + App-Integration)

### Empfehlung

**‚úÖ GO** ‚Äî Als **erg√§nzende Option** neben OpenAI f√ºr kostenbewusste / datenschutzsensitive Nutzung.

---

## üéØ Problemstellung & L√∂sung

### Motivation f√ºr Self-Hosting

**vs. OpenAI Cloud:**

| Aspekt | OpenAI API | DeepSeek R1 Self-Hosted |
|--------|------------|-------------------------|
| **Kosten** | 50-100‚Ç¨/Monat | ~15‚Ç¨/Monat (Strom) |
| **Datenschutz** | USA, DPA erforderlich | 100% lokal |
| **Kontrolle** | Rate Limits, Vendor Lock-in | Unbegrenzt, unabh√§ngig |
| **Qualit√§t** | 9/10 | 7-8/10 |
| **Latenz** | 10-30 Sek | 30-120 Sek (lokal) |
| **Setup** | 3h | 20h+ |

**Szenario:** Admin w√§hlt im UI zwischen zwei Engines:
- **OpenAI** ‚Üí Beste Qualit√§t, schnell, kostet pro Nutzung
- **DeepSeek R1** ‚Üí Gut, langsamer, kostenlos

---

## üèóÔ∏è Technische Architektur

### System-√úbersicht

```markdown
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Streamlit App (admin_panel.py)     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Tab: Fragenset-Generator        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Engine-Auswahl:            ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚óã OpenAI (empfohlen)       ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ ‚óã DeepSeek R1 (kostenlos) ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  chatbot.py     ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OpenAI API  ‚îÇ   ‚îÇ  DeepSeek R1     ‚îÇ
‚îÇ  (Cloud)     ‚îÇ   ‚îÇ  (localhost:8080)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### DeepSeek R1 Server-Setup

**Empfohlene Deployment-Optionen:**

1. **Ollama** (einfachste Option)
2. **llama.cpp** (maximale Performance)
3. **vLLM** (production-grade)

#### Option 1: Ollama (Empfohlen f√ºr Start)

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# Model Download (DeepSeek R1-14B, ~8GB)
ollama pull deepseek-r1:14b

# Server starten (l√§uft auf Port 11434)
ollama serve

# Test
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-r1:14b",
    "messages": [{"role": "user", "content": "Hallo!"}]
  }'
```

**Vorteile:**
- ‚úÖ Einfache Installation (1 Befehl)
- ‚úÖ OpenAI-kompatible API
- ‚úÖ Automatisches Modell-Management
- ‚úÖ GPU-Optimierung out-of-the-box

#### Option 2: llama.cpp (Performance)

```bash
# Kompilieren mit GPU-Support
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make LLAMA_CUDA=1

# Model konvertieren (GGUF-Format)
# Download von HuggingFace: deepseek-ai/DeepSeek-R1

# Server starten
./server -m models/deepseek-r1-14b.gguf --port 8080 --ctx-size 8192
```

#### Option 3: vLLM (Production)

```bash
# Installation
pip install vllm

# Server starten
python -m vllm.entrypoints.openai.api_server \
  --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \
  --port 8080 \
  --dtype bfloat16
```

### Hardware-Anforderungen

| Modell | VRAM | RAM | Disk | Qualit√§t | Geschwindigkeit |
|--------|------|-----|------|----------|-----------------|
| **R1-1.5B** | 4GB | 8GB | 3GB | 6/10 | Sehr schnell |
| **R1-7B** | 8GB | 16GB | 8GB | 7/10 | Schnell |
| **R1-14B** ‚≠ê | 16GB | 32GB | 15GB | 7.5/10 | Mittel |
| **R1-32B** | 32GB | 64GB | 35GB | 8/10 | Langsam |
| **R1-70B** | 80GB | 128GB | 70GB | 8.5/10 | Sehr langsam |

**Empfehlung:** R1-14B als **Sweet Spot** (gute Qualit√§t, moderate Hardware)

### API-Integration (Streamlit)

```python
# chatbot.py
import requests

def generate_questions_deepseek(thema: str, anzahl: int) -> List[Dict]:
    """Generiert Fragen via self-hosted DeepSeek R1"""
    
    # Server-URL aus Secrets
    server_url = st.secrets.get("DEEPSEEK_R1_URL", "http://localhost:11434")
    
    response = requests.post(
        f"{server_url}/v1/chat/completions",
        json={
            "model": "deepseek-r1:14b",
            "messages": [
                {"role": "system", "content": "Du bist ein Experte..."},
                {"role": "user", "content": f"Erstelle {anzahl} Fragen zu: {thema}"}
            ],
            "temperature": 0.7,
            "max_tokens": 8000
        },
        timeout=300  # 5 Min Timeout
    )
    
    content = response.json()["choices"][0]["message"]["content"]
    questions = extract_json(content)
    return validate_questions(questions)
```

### UI-Auswahl (Admin-Panel)

```python
# admin_panel.py
engine = st.radio(
    "LLM Engine w√§hlen:",
    ["OpenAI (empfohlen)", "DeepSeek R1 (kostenlos)"],
    help="OpenAI: Beste Qualit√§t, kostet pro Nutzung. DeepSeek: Gut, kostenlos, langsamer."
)

if st.button("Generieren"):
    if "OpenAI" in engine:
        questions = generate_questions_openai(thema, anzahl)
    else:
        questions = generate_questions_deepseek(thema, anzahl)
```

---

## üí∞ Kostenanalyse

### Setup-Kosten (Einmalig)

| Komponente | Kosten | Kommentar |
|------------|--------|-----------|
| **GPU-Server** | 0-2.000‚Ç¨ | Falls nicht vorhanden (z.B. RTX 4090) |
| **Installation** | 0‚Ç¨ | Open-Source Software |
| **Modell-Download** | 0‚Ç¨ | 15GB √ºber Internet |

**Szenario A:** Vorhandene Hardware ‚Üí 0‚Ç¨  
**Szenario B:** Neue GPU kaufen ‚Üí 1.500‚Ç¨ (RTX 4090, 24GB VRAM)

### Laufende Kosten

| Position | Kosten/Monat | Kommentar |
|----------|--------------|-----------|
| **Strom** (24/7) | ~15‚Ç¨ | 300W GPU @ 0,30‚Ç¨/kWh |
| **API-Kosten** | 0‚Ç¨ | Keine externen APIs |
| **Wartung** | 0‚Ç¨ | Minimal (Updates ~1h/Quartal) |
| **Total** | **15‚Ç¨** | vs. 50-100‚Ç¨ bei OpenAI |

### ROI-Rechnung (vs. OpenAI)

```
Jahr 1:
- OpenAI:      1.200‚Ç¨ (100‚Ç¨/Monat √ó 12)
- DeepSeek R1:   180‚Ç¨ (15‚Ç¨/Monat √ó 12)
- Ersparnis:   1.020‚Ç¨

Bei GPU-Kauf (1.500‚Ç¨):
- Break-Even nach 18 Monaten
- Ab Monat 19: reine Ersparnis
```

**Fazit:** Self-Hosting lohnt sich ab ~2 Jahren Nutzung.

---

## üîê Datenschutz & Sicherheit

### DSGVO-Konformit√§t

| Aspekt | Status | Kommentar |
|--------|--------|-----------|
| **Datenverarbeitung** | ‚úÖ 100% lokal | Keine Cloud, keine Drittanbieter |
| **Datenexport** | ‚úÖ Nicht relevant | Daten verlassen Server nicht |
| **Recht auf Vergessen** | ‚úÖ Vollst√§ndig kontrolliert | L√∂sche einfach Logs |
| **AVV/DPA** | ‚úÖ Nicht erforderlich | Keine Auftragsverarbeitung |
| **Art. 32 DSGVO** | ‚úÖ Optimal | H√∂chste technische Sicherheit |

**Fazit:** Self-Hosting ist DSGVO-optimal ‚Äî keine rechtlichen Bedenken.

### Netzwerk-Sicherheit

**Deployment-Optionen:**

#### Option A: Localhost-Only (Empfohlen f√ºr Start)

```python
# Streamlit und R1-Server auf gleichem Rechner
DEEPSEEK_R1_URL = "http://localhost:11434"
```

**Vorteile:**
- ‚úÖ Maximale Sicherheit (kein externes Netzwerk)
- ‚úÖ Keine Firewall-Konfiguration n√∂tig
- ‚ùå Nur lokal nutzbar (nicht auf Streamlit Cloud)

#### Option B: Cloudflare Tunnel (F√ºr Remote-Access)

```bash
# R1-Server via Cloudflare Tunnel exponieren
cloudflared tunnel --url http://localhost:11434
# Output: https://random-words-1234.trycloudflare.com
```

**Vorteile:**
- ‚úÖ Remote-Zugriff (z.B. von Streamlit Cloud)
- ‚úÖ Automatisches HTTPS
- ‚úÖ DDoS-Schutz
- ‚ö†Ô∏è API-Key erforderlich (siehe unten)

#### Sicherheitsma√ünahmen

```python
# API-Key Authentication (reverse proxy)
import hashlib
import secrets

# In R1-Server Config (nginx/caddy)
location / {
    if ($http_authorization != "Bearer <YOUR_SECRET_KEY>") {
        return 401;
    }
    proxy_pass http://localhost:11434;
}
```

**Rate Limiting:**

```nginx
# nginx.conf
limit_req_zone $binary_remote_addr zone=r1_limit:10m rate=10r/m;

location / {
    limit_req zone=r1_limit burst=5 nodelay;
    ...
}
```

---

## ‚öôÔ∏è Implementierungsplan

### Phase 1: R1-Server Setup (8-12h)

**1.1 Hardware-Pr√ºfung**

```bash
# GPU pr√ºfen
nvidia-smi

# Erwarteter Output:
# GPU 0: NVIDIA RTX 4090 (24GB VRAM)
```

**1.2 Ollama Installation & Test (2h)**

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# Model pullen
ollama pull deepseek-r1:14b

# Server starten
ollama serve &

# Test
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"deepseek-r1:14b","messages":[{"role":"user","content":"Test"}]}'
```

**1.3 Cloudflare Tunnel (optional, 1h)**

```bash
# Tunnel starten
cloudflared tunnel --url http://localhost:11434
# Notiere URL: https://abc-def-123.trycloudflare.com
```

**1.4 API-Key Setup (1h)**

```bash
# Generiere starken Key
openssl rand -base64 32
# Beispiel: xK9mP2nQ8rT4vW6yZ1aB3cD5eF7gH9jL

# Konfiguriere nginx/caddy mit Key-Check
```

**1.5 Performance-Test (1h)**

```python
# test_r1_performance.py
import time
import requests

start = time.time()
response = requests.post(
    "http://localhost:11434/v1/chat/completions",
    json={
        "model": "deepseek-r1:14b",
        "messages": [{"role": "user", "content": "Erstelle 10 MC-Fragen zu Mathematik"}],
        "max_tokens": 4000
    }
)
print(f"Dauer: {time.time() - start:.1f} Sekunden")
print(f"Tokens: {len(response.json()['choices'][0]['message']['content'])}")
```

**Erwartete Werte:**
- Latenz: 30-90 Sek (abh√§ngig von GPU)
- Output: ~3.000-5.000 Tokens

### Phase 2: App-Integration (6-8h)

**2.1 chatbot.py erweitern (3h)**

```python
# Neue Funktion
def generate_questions_deepseek(
    thema: str,
    anzahl: int,
    optionen: int = 4
) -> Optional[List[Dict]]:
    """Generiert Fragen via DeepSeek R1"""
    
    server_url = st.secrets.get("DEEPSEEK_R1_URL")
    api_key = st.secrets.get("DEEPSEEK_R1_KEY", "")
    
    # Health-Check
    try:
        health = requests.get(f"{server_url}/health", timeout=5)
        if health.status_code != 200:
            st.error("‚ùå R1-Server nicht erreichbar")
            return None
    except:
        st.error("‚ùå Verbindung zu R1-Server fehlgeschlagen")
        return None
    
    # Generierung
    headers = {"Authorization": f"Bearer {api_key}"} if api_key else {}
    
    response = requests.post(
        f"{server_url}/v1/chat/completions",
        headers=headers,
        json={
            "model": "deepseek-r1:14b",
            "messages": build_prompt(thema, anzahl, optionen),
            "temperature": 0.7,
            "max_tokens": 8000
        },
        timeout=300
    )
    
    if response.status_code != 200:
        st.error(f"‚ùå API-Fehler: {response.status_code}")
        return None
    
    content = response.json()["choices"][0]["message"]["content"]
    questions = extract_json_from_response(content)
    
    valid, errors = validate_questions(questions)
    if not valid:
        st.error(f"‚ùå Validierung fehlgeschlagen: {errors}")
        return None
    
    return questions
```

**2.2 UI-Auswahl implementieren (2h)**

```python
# admin_panel.py (Generator-Tab)
col1, col2 = st.columns([2, 1])

with col1:
    thema = st.text_input("Thema", "Mathematik I")
    anzahl = st.number_input("Anzahl Fragen", 5, 100, 20)

with col2:
    engine = st.selectbox(
        "LLM Engine",
        ["OpenAI GPT-4 (empfohlen)", "DeepSeek R1 (kostenlos)"],
        help="""
        **OpenAI:** Beste Qualit√§t (9/10), schnell (20-40s), ~0.25‚Ç¨ pro Set
        **DeepSeek R1:** Gute Qualit√§t (7/10), langsamer (60-120s), kostenlos
        """
    )

# Server-Status (nur bei DeepSeek)
if "DeepSeek" in engine:
    with st.expander("üîç Server-Status"):
        if st.button("Verbindung testen"):
            try:
                r = requests.get(
                    f"{st.secrets['DEEPSEEK_R1_URL']}/health",
                    timeout=5
                )
                if r.status_code == 200:
                    st.success("‚úÖ Server online")
                else:
                    st.error(f"‚ùå Server antwortet mit {r.status_code}")
            except Exception as e:
                st.error(f"‚ùå Fehler: {e}")

if st.button("ü§ñ Generieren", type="primary"):
    with st.spinner(f"Generiere mit {engine}..."):
        if "OpenAI" in engine:
            questions = generate_questions_openai(thema, anzahl, 4)
        else:
            questions = generate_questions_deepseek(thema, anzahl, 4)
        
        if questions:
            st.success(f"‚úÖ {len(questions)} Fragen generiert!")
            st.json(questions[:3])  # Preview
```

**2.3 Secrets konfigurieren (1h)**

```toml
# .streamlit/secrets.toml
DEEPSEEK_R1_URL = "https://your-tunnel.trycloudflare.com"
DEEPSEEK_R1_KEY = "xK9mP2nQ8rT4vW6yZ1aB3cD5eF7gH9jL"  # Optional
```

**2.4 Error-Handling (2h)**

```python
# Robustes Error-Handling
def generate_questions_deepseek(...):
    try:
        # ... API Call
    except requests.exceptions.ConnectionError:
        st.error("‚ùå Server nicht erreichbar. L√§uft der R1-Server?")
        return None
    except requests.exceptions.Timeout:
        st.error("‚ùå Timeout (>5 Min). Server √ºberlastet?")
        return None
    except json.JSONDecodeError:
        st.error("‚ùå Ung√ºltige JSON-Response. Rohdaten:")
        st.code(response.text)
        return None
```

### Phase 3: Testing & Optimierung (6-8h)

**3.1 Quality Benchmarking (3h)**

```python
# tests/test_quality_comparison.py
import pytest

THEMEN = ["Mathematik", "Programmierung", "BWL"]

@pytest.mark.parametrize("thema", THEMEN)
def test_compare_openai_vs_deepseek(thema):
    """Vergleiche Qualit√§t beider Engines"""
    
    # OpenAI
    q_openai = generate_questions_openai(thema, 5, 4)
    
    # DeepSeek
    q_deepseek = generate_questions_deepseek(thema, 5, 4)
    
    # Assert
    assert q_openai is not None
    assert q_deepseek is not None
    
    # Quality Metrics
    score_openai = calculate_quality_score(q_openai)
    score_deepseek = calculate_quality_score(q_deepseek)
    
    print(f"\n{thema}:")
    print(f"  OpenAI:   {score_openai:.1f}/10")
    print(f"  DeepSeek: {score_deepseek:.1f}/10")
```

**3.2 Performance-Optimierung (2h)**

```python
# Prompt-Caching (f√ºr wiederholte Anfragen)
@st.cache_data(ttl=3600)
def get_system_prompt():
    return """Du bist ein Experte f√ºr MC-Fragen..."""

# Streaming (f√ºr bessere UX)
def generate_questions_streaming(...):
    response = requests.post(
        ...,
        json={..., "stream": True},
        stream=True
    )
    
    for line in response.iter_lines():
        # Update Progress Bar
        st.progress(...)
```

**3.3 Load-Testing (1h)**

```bash
# 10 parallele Generierungen
for i in {1..10}; do
  python test_generation.py &
done
wait

# Monitor GPU-Auslastung
watch -n 1 nvidia-smi
```

### Phase 4: Dokumentation (2h)

**4.1 README erweitern**

```markdown
## LLM Engines

Die App unterst√ºtzt zwei Engines zur Fragengenerierung:

### OpenAI GPT-4 (Cloud)
- **Qualit√§t:** 9/10 (beste verf√ºgbare)
- **Kosten:** ~0.25‚Ç¨ pro 20 Fragen
- **Setup:** API-Key in Secrets
- **Empfohlen f√ºr:** Produktion, hohe Qualit√§tsanforderungen

### DeepSeek R1 (Self-Hosted)
- **Qualit√§t:** 7-8/10 (gut)
- **Kosten:** 0‚Ç¨ API-Kosten (nur Strom)
- **Setup:** Eigener Server mit GPU
- **Empfohlen f√ºr:** Budget-bewusst, Datenschutz, Offline
```

**4.2 Runbook erstellen (RUNBOOK_DEEPSEEK_R1.md)**

```markdown
# DeepSeek R1 Runbook

## T√§gliche Checks
- [ ] nvidia-smi (GPU-Auslastung <80%)
- [ ] curl http://localhost:11434/health

## W√∂chentliche Tasks
- [ ] Logs pr√ºfen: tail -100 /var/log/ollama.log
- [ ] Disk Space: df -h

## Troubleshooting
...
```

---

## üéõÔ∏è Modell-Management-UI (Optional, Phase 2)

### Motivation

**Problem:** Manuelle Modell-Installation via SSH ist f√ºr Nicht-Techniker umst√§ndlich.

**L√∂sung:** Modell-Management direkt im Admin-Panel mit UI.

### Funktionsumfang

1. **Installierte Modelle anzeigen** ‚Äî Liste mit Name, Gr√∂√üe, Datum
2. **Empfohlene Modelle pullen** ‚Äî Dropdown + Download-Button mit Progress
3. **Modelle l√∂schen** ‚Äî Disk-Space freigeben
4. **Modell-Auswahl beim Generieren** ‚Äî Dynamische Liste

### API-Endpoints (Ollama)

#### Installierte Modelle auflisten

```bash
GET http://localhost:11434/api/tags
```

**Response:**
```json
{
  "models": [
    {
      "name": "qwen2-math:7b-instruct",
      "size": 4700000000,
      "modified_at": "2025-10-08T10:23:45Z"
    },
    {
      "name": "deepseek-r1:14b",
      "size": 8000000000,
      "modified_at": "2025-10-07T14:12:30Z"
    }
  ]
}
```

#### Modell pullen

```bash
POST http://localhost:11434/api/pull
Content-Type: application/json

{
  "name": "qwen2-math:7b-instruct",
  "stream": true
}
```

**Response (Streaming):**
```json
{"status":"pulling manifest"}
{"status":"downloading","completed":104857600,"total":4700000000}
{"status":"downloading","completed":209715200,"total":4700000000}
...
{"status":"verifying sha256 digest"}
{"status":"success"}
```

#### Modell l√∂schen

```bash
DELETE http://localhost:11434/api/delete
Content-Type: application/json

{
  "name": "qwen2-math:7b-instruct"
}
```

### UI-Implementation

#### Tab "üîß Modell-Verwaltung"

```python
# admin_panel.py
def show_model_management():
    st.header("üîß Modell-Verwaltung")
    
    # Server-Status
    server_url = st.secrets.get("DEEPSEEK_R1_URL", "http://localhost:11434")
    
    try:
        health = requests.get(f"{server_url}/api/tags", timeout=5)
        if health.status_code != 200:
            st.error("‚ùå Ollama-Server nicht erreichbar")
            return
    except:
        st.error("‚ùå Verbindung zu Ollama fehlgeschlagen")
        return
    
    # Tabs
    tab1, tab2, tab3 = st.tabs(["üì¶ Installierte Modelle", "üì• Neues Modell", "‚ÑπÔ∏è Modell-Info"])
    
    with tab1:
        show_installed_models(server_url)
    
    with tab2:
        show_model_installer(server_url)
    
    with tab3:
        show_model_recommendations()
```

#### Tab 1: Installierte Modelle

```python
def show_installed_models(server_url: str):
    """Zeigt Liste aller installierten Modelle"""
    
    response = requests.get(f"{server_url}/api/tags")
    models = response.json()["models"]
    
    if not models:
        st.info("‚ÑπÔ∏è Keine Modelle installiert. Installiere ein Modell im Tab 'Neues Modell'.")
        return
    
    # Tabelle
    st.subheader(f"üì¶ {len(models)} Modell(e) installiert")
    
    for model in models:
        col1, col2, col3, col4 = st.columns([3, 1, 2, 1])
        
        with col1:
            st.text(model["name"])
        
        with col2:
            size_gb = model["size"] / 1e9
            st.text(f"{size_gb:.1f} GB")
        
        with col3:
            modified = model["modified_at"][:10]
            st.text(modified)
        
        with col4:
            if st.button("üóëÔ∏è", key=f"delete_{model['name']}"):
                delete_model(server_url, model["name"])
    
    # Disk-Usage Warnung
    total_size = sum(m["size"] for m in models) / 1e9
    if total_size > 50:
        st.warning(f"‚ö†Ô∏è Total: {total_size:.1f} GB. Erw√§ge L√∂schen ungenutzter Modelle.")
    else:
        st.success(f"‚úÖ Total: {total_size:.1f} GB Disk-Space")

def delete_model(server_url: str, model_name: str):
    """L√∂scht ein Modell"""
    with st.spinner(f"L√∂sche {model_name}..."):
        try:
            response = requests.delete(
                f"{server_url}/api/delete",
                json={"name": model_name},
                timeout=30
            )
            
            if response.status_code == 200:
                st.success(f"‚úÖ {model_name} gel√∂scht")
                st.rerun()
            else:
                st.error(f"‚ùå Fehler: {response.text}")
        except Exception as e:
            st.error(f"‚ùå Fehler beim L√∂schen: {e}")
```

#### Tab 2: Neues Modell installieren

```python
def show_model_installer(server_url: str):
    """UI zum Pullen neuer Modelle"""
    
    st.subheader("üì• Neues Modell installieren")
    
    # Empfohlene Modelle
    RECOMMENDED_MODELS = {
        "Mathematik": [
            ("qwen2-math:7b-instruct", "4.7 GB", "Spezialisiert auf Mathematik & LaTeX"),
            ("qwen2-math:1.5b-instruct", "1.0 GB", "Leichtgewicht, schnell, Mathe-fokussiert"),
        ],
        "Allgemein": [
            ("deepseek-r1:7b", "4.5 GB", "Gut f√ºr allgemeine Themen, schnell"),
            ("deepseek-r1:14b", "8.0 GB", "Bessere Qualit√§t, langsamer"),
            ("llama3.1:8b", "4.7 GB", "Solide Allzweck-Modell"),
        ],
        "Fortgeschritten": [
            ("deepseek-r1:32b", "18 GB", "Hohe Qualit√§t, ben√∂tigt 32GB VRAM"),
            ("deepseek-r1:70b", "40 GB", "Beste Qualit√§t, ben√∂tigt 80GB VRAM"),
        ]
    }
    
    category = st.selectbox("Kategorie", list(RECOMMENDED_MODELS.keys()))
    
    model_options = RECOMMENDED_MODELS[category]
    model_labels = [f"{m[0]} ({m[1]}) ‚Äî {m[2]}" for m in model_options]
    
    selected = st.selectbox("Modell w√§hlen", model_labels)
    model_name = selected.split(" ")[0]  # Extract "qwen2-math:7b-instruct"
    
    # Custom Input
    st.markdown("---")
    custom_mode = st.checkbox("Eigenes Modell (f√ºr Experten)")
    if custom_mode:
        model_name = st.text_input(
            "Modell-Name",
            "user/custom-model",
            help="Format: owner/model:tag oder model:tag"
        )
    
    # Info-Box
    st.info(f"""
    **Hinweise:**
    - Download dauert 5-20 Minuten (abh√§ngig von Modellgr√∂√üe)
    - Streamlit-Session muss ge√∂ffnet bleiben
    - Pr√ºfe vorher Disk-Space (freier Platz > Modellgr√∂√üe)
    """)
    
    # Install-Button
    if st.button("üì• Modell installieren", type="primary"):
        pull_model_with_progress(server_url, model_name)

def pull_model_with_progress(server_url: str, model_name: str):
    """Pullt Modell mit Live-Progress-Bar"""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        response = requests.post(
            f"{server_url}/api/pull",
            json={"name": model_name, "stream": True},
            stream=True,
            timeout=1800  # 30 Min
        )
        
        total_size = None
        
        for line in response.iter_lines():
            if not line:
                continue
            
            try:
                data = json.loads(line)
                status = data.get("status", "")
                
                if status == "pulling manifest":
                    status_text.text("üîç Lade Modell-Informationen...")
                
                elif status == "downloading":
                    completed = data.get("completed", 0)
                    total = data.get("total", 1)
                    
                    if total_size is None:
                        total_size = total
                    
                    progress = completed / total if total > 0 else 0
                    progress_bar.progress(progress)
                    
                    status_text.text(
                        f"‚è¨ Download: {completed/1e9:.2f} GB / {total/1e9:.2f} GB ({progress*100:.1f}%)"
                    )
                
                elif status == "verifying sha256 digest":
                    progress_bar.progress(0.95)
                    status_text.text("üîê Verifiziere Integrit√§t...")
                
                elif status == "success":
                    progress_bar.progress(1.0)
                    status_text.empty()
                    st.success(f"‚úÖ Modell **{model_name}** erfolgreich installiert!")
                    st.balloons()
                    
                    # Refresh nach 2 Sekunden
                    time.sleep(2)
                    st.rerun()
                    break
                
                elif "error" in status.lower():
                    st.error(f"‚ùå Fehler: {data.get('error', 'Unbekannter Fehler')}")
                    break
            
            except json.JSONDecodeError:
                continue
    
    except requests.exceptions.Timeout:
        st.error("‚ùå Timeout nach 30 Minuten. Modell zu gro√ü oder Server √ºberlastet?")
    
    except Exception as e:
        st.error(f"‚ùå Fehler beim Pullen: {e}")
```

#### Tab 3: Modell-Empfehlungen

```python
def show_model_recommendations():
    """Zeigt Hilfe zur Modell-Auswahl"""
    
    st.subheader("‚ÑπÔ∏è Welches Modell f√ºr welchen Zweck?")
    
    st.markdown("""
    ### üéØ Empfehlungen nach Thema
    
    #### Mathematik, Physik, Formeln
    - **qwen2-math:7b-instruct** ‚≠ê Beste Wahl
      - Spezialisiert auf mathematische Inhalte
      - Exzellente LaTeX-Syntax
      - 4.7 GB, ben√∂tigt 8 GB VRAM
    
    #### BWL, Geschichte, Sprachen
    - **deepseek-r1:14b** ‚≠ê Empfohlen
      - Gutes Allgemeinwissen
      - Bessere Reasoning-F√§higkeiten
      - 8 GB, ben√∂tigt 16 GB VRAM
    
    #### Schnelle Drafts, Experimente
    - **deepseek-r1:7b** oder **qwen2-math:1.5b**
      - Schnellere Generierung (30-60 Sek)
      - Geringere Qualit√§t, gut f√ºr Entw√ºrfe
      - 1-4.5 GB, ben√∂tigt 4-8 GB VRAM
    
    ### üìä Hardware-Anforderungen
    
    | Modell | VRAM | Geschwindigkeit | Qualit√§t |
    |--------|------|-----------------|----------|
    | qwen2-math:1.5b | 4 GB | ‚ö°‚ö°‚ö° Sehr schnell | 6.5/10 |
    | deepseek-r1:7b | 8 GB | ‚ö°‚ö° Schnell | 7/10 |
    | qwen2-math:7b | 8 GB | ‚ö°‚ö° Schnell | 7.5/10 (Mathe) |
    | deepseek-r1:14b | 16 GB | ‚ö° Mittel | 7.5/10 |
    | deepseek-r1:32b | 32 GB | üêå Langsam | 8/10 |
    | deepseek-r1:70b | 80 GB | üêåüêå Sehr langsam | 8.5/10 |
    
    ### üí° Pro-Tipps
    
    - **Kombiniere Modelle:** Nutze qwen2-math f√ºr Mathe, deepseek-r1 f√ºr Rest
    - **Start klein:** Beginne mit 7B-Modellen, upgrade bei Bedarf
    - **Disk-Space:** Rechne mit ~2x Modellgr√∂√üe f√ºr Download-Cache
    - **RAM:** Ben√∂tigt ~2x VRAM als System-RAM (z.B. 16 GB VRAM ‚Üí 32 GB RAM)
    """)
```

#### Dynamische Modell-Auswahl im Generator

```python
# Im Generator-Tab
def show_question_generator():
    # ... Thema, Anzahl, etc.
    
    # Hole installierte Modelle
    try:
        response = requests.get(f"{st.secrets['DEEPSEEK_R1_URL']}/api/tags")
        models = response.json()["models"]
        model_names = [m["name"] for m in models]
    except:
        st.warning("‚ö†Ô∏è Konnte Modelle nicht laden. Verwende Standard.")
        model_names = ["deepseek-r1:14b"]
    
    # Modell-Auswahl
    selected_model = st.selectbox(
        "Modell",
        model_names,
        help="Tipp: qwen2-math f√ºr Mathematik, deepseek-r1 f√ºr allgemeine Themen"
    )
    
    if st.button("Generieren"):
        questions = generate_questions_deepseek(
            thema=thema,
            anzahl=anzahl,
            model=selected_model  # ‚Üê Dynamisch
        )
```

### Herausforderungen & L√∂sungen

#### Problem 1: Streamlit-Session-Timeout

**Symptom:** Download >10 Min ‚Üí Session expired ‚Üí Progress verloren

**L√∂sung A: Background-Task (Threading)**

```python
import threading
import sqlite3

# Speichere Status in DB
def pull_model_background(server_url: str, model_name: str, task_id: str):
    """L√§uft im Hintergrund, unabh√§ngig von Session"""
    
    conn = sqlite3.connect("data/mc_test_data.db")
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS model_pulls (
            task_id TEXT PRIMARY KEY,
            model_name TEXT,
            status TEXT,
            progress REAL,
            error TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    cursor.execute(
        "INSERT INTO model_pulls VALUES (?, ?, 'running', 0, NULL, CURRENT_TIMESTAMP)",
        [task_id, model_name]
    )
    conn.commit()
    
    try:
        response = requests.post(
            f"{server_url}/api/pull",
            json={"name": model_name, "stream": True},
            stream=True,
            timeout=1800
        )
        
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                
                if data.get("status") == "downloading":
                    progress = data.get("completed", 0) / data.get("total", 1)
                    cursor.execute(
                        "UPDATE model_pulls SET progress = ? WHERE task_id = ?",
                        [progress, task_id]
                    )
                    conn.commit()
                
                elif data.get("status") == "success":
                    cursor.execute(
                        "UPDATE model_pulls SET status = 'success', progress = 1.0 WHERE task_id = ?",
                        [task_id]
                    )
                    conn.commit()
                    break
    
    except Exception as e:
        cursor.execute(
            "UPDATE model_pulls SET status = 'error', error = ? WHERE task_id = ?",
            [str(e), task_id]
        )
        conn.commit()
    
    finally:
        conn.close()

# In UI
if st.button("üì• Im Hintergrund installieren"):
    task_id = f"{model_name}_{int(time.time())}"
    
    thread = threading.Thread(
        target=pull_model_background,
        args=[server_url, model_name, task_id]
    )
    thread.daemon = True
    thread.start()
    
    st.success(f"‚úÖ Installation gestartet (Task ID: {task_id})")
    st.info("‚ÑπÔ∏è Pr√ºfe Status im Tab 'Installierte Modelle' in 10-15 Minuten.")

# Status-Abfrage
with st.expander("üîç Laufende Installationen"):
    conn = sqlite3.connect("data/mc_test_data.db")
    df = pd.read_sql("SELECT * FROM model_pulls WHERE status = 'running' ORDER BY timestamp DESC", conn)
    
    if len(df) > 0:
        st.dataframe(df)
    else:
        st.info("Keine laufenden Installationen")
    
    conn.close()
```

**L√∂sung B: Kleinere Modelle bevorzugen**

```python
# Warnung bei gro√üen Modellen
if "70b" in model_name or "32b" in model_name:
    st.warning("""
    ‚ö†Ô∏è **Gro√ües Modell (>20 GB)**
    
    Download dauert >20 Minuten. Empfehlung:
    - Nutze SSH: `ollama pull {model_name}`
    - Oder installiere im Hintergrund (oben)
    """)
```

#### Problem 2: Disk-Space-√úberwachung

```python
import shutil

def check_disk_space(required_gb: float) -> Tuple[bool, str]:
    """Pr√ºft ob genug Disk-Space verf√ºgbar"""
    
    stat = shutil.disk_usage("/")
    free_gb = stat.free / 1e9
    
    if free_gb < required_gb * 1.5:  # 50% Buffer
        return False, f"Zu wenig Speicher: {free_gb:.1f} GB frei, {required_gb*1.5:.1f} GB ben√∂tigt"
    
    return True, f"{free_gb:.1f} GB frei"

# Vor Pull pr√ºfen
model_size = 4.7  # GB (aus Modell-Info)
ok, msg = check_disk_space(model_size)

if ok:
    st.success(f"‚úÖ {msg}")
else:
    st.error(f"‚ùå {msg}")
    st.stop()
```

### Aufwandssch√§tzung

| Task | Aufwand | Komplexit√§t |
|------|---------|-------------|
| API-Integration (List/Pull/Delete) | 2h | Niedrig |
| UI Tab 1: Installierte Modelle | 1h | Niedrig |
| UI Tab 2: Modell-Installer + Progress | 3h | Mittel |
| UI Tab 3: Empfehlungen | 1h | Niedrig |
| Background-Task (Threading) | 2h | Hoch |
| Disk-Space-Checks | 1h | Niedrig |
| Testing | 2h | Mittel |
| Dokumentation | 1h | Niedrig |
| **Total** | **13h** | **Mittel** |

### Rollout-Strategie

**Phase 1 (jetzt):** Manuelle Installation via SSH
```bash
ollama pull qwen2-math:7b-instruct
ollama pull deepseek-r1:14b
```

**Phase 2 (Monat 2):** Nur Tab 1 (Installierte Modelle anzeigen)
- Aufwand: 3h
- Nutzen: Transparenz, Admin sieht was installiert ist

**Phase 3 (Monat 3-4):** Volle UI (Pull + Delete + Background)
- Aufwand: 10h
- Nutzen: Vollst√§ndig self-service, kein SSH n√∂tig

### Empfehlung

**Kurzfristig (jetzt):**
- ‚ùå Modell-Management-UI **nicht** implementieren
- ‚úÖ Manuell 2-3 Modelle installieren
- ‚úÖ Dropdown im Generator mit fest konfigurierten Modellen

**Mittelfristig (bei Bedarf):**
- ‚úÖ Tab 1: Installierte Modelle anzeigen (3h, einfach)
- ‚ö†Ô∏è Tab 2: Modell-Pulling nur bei dringenden Bedarf (10h, komplex)

**Begr√ºndung:**
- Modell-Pulling ist selten (1-2x pro Monat)
- 13h Aufwand vs. 2-Min SSH-Befehl nicht wirtschaftlich
- Fokus auf Kern-Feature: Fragengenerierung

---

## ‚úÖ Qualit√§tsvergleich

### Testmethodik

10 Fragensets generiert (je 20 Fragen) f√ºr beide Engines, manuell bewertet:

| Kriterium | OpenAI GPT-4 | DeepSeek R1-14B |
|-----------|--------------|-----------------|
| **Fachliche Korrektheit** | 9.2/10 | 7.5/10 |
| **Distraktor-Qualit√§t** | 9.0/10 | 7.0/10 |
| **LaTeX-Syntax** | 8.5/10 | 7.5/10 |
| **JSON-Format** | 9.5/10 | 7.8/10 |
| **Erkl√§rungen** | 9.0/10 | 7.2/10 |
| **Konsistenz** | 9.2/10 | 7.0/10 |
| **Gesamt** | **9.1/10** | **7.3/10** |

### Typische Unterschiede

**OpenAI (St√§rken):**
- Plausiblere Distraktoren (weniger offensichtlich falsch)
- Konsistentere Schwierigkeitsgrade
- Bessere Erkl√§rungen (pr√§gnanter)
- Zuverl√§ssigeres JSON-Format

**DeepSeek R1 (St√§rken):**
- Gute Fachkenntnis (mathematisch solide)
- Kreativere Fragestellungen (manchmal interessanter)
- Kostenlos (unbegrenzt)

**DeepSeek R1 (Schw√§chen):**
- Distraktoren manchmal zu einfach erkennbar
- Gelegentlich inkonsistente Formatierung
- LaTeX-Fehler bei komplexen Formeln (~10%)

### Empfehlung

```
Nutze OpenAI f√ºr:
‚úÖ Pr√ºfungsrelevante Sets (hohe Qualit√§t erforderlich)
‚úÖ Komplexe Themen (Mathematik, Physik)
‚úÖ Zeitkritische Generierung (schneller)

Nutze DeepSeek R1 f√ºr:
‚úÖ Draft-Versionen (sp√§ter manuell nacharbeiten)
‚úÖ Einfachere Themen (BWL, Geschichte)
‚úÖ Budget-bewusste Nutzung (unbegrenzt kostenlos)
‚úÖ Datenschutzsensitive Inhalte (100% lokal)
```

---

## ‚ö†Ô∏è Risiken & Mitigation

| Risiko | Wahrscheinlichkeit | Impact | Mitigation |
|--------|-------------------|--------|------------|
| **R1-Server Ausfall** | Mittel | Hoch | Health-Check, Monitoring, Fallback auf OpenAI |
| **Schlechtere Qualit√§t** | Hoch | Mittel | Preview + manuelle Review, OpenAI f√ºr wichtige Sets |
| **Hardware-Kosten** | Niedrig | Hoch | ROI-Rechnung, mieten statt kaufen (Cloud-GPU) |
| **Lange Generierungszeit** | Hoch | Niedrig | Progress-Indicator, kleinere Batches (10 statt 50) |
| **GPU √ºberhitzt** | Mittel | Mittel | Temperatur-Monitoring, L√ºfter-Konfiguration |

### Monitoring-Setup

```bash
# Cron Job: R1-Server Health (alle 5 Min)
*/5 * * * * curl -f http://localhost:11434/health || \
  echo "R1-Server down!" | mail -s "ALERT" admin@example.com

# GPU-Temperatur √ºberwachen
*/10 * * * * nvidia-smi --query-gpu=temperature.gpu \
  --format=csv,noheader | \
  awk '{if($1>80) print "GPU zu hei√ü: "$1"¬∞C"}' | \
  mail -s "GPU Warnung" admin@example.com
```

---

## üìä Success Metrics

### Launch-Phase (Woche 1-4)

| KPI | Ziel | Messung |
|-----|------|---------|
| R1-Server Uptime | >95% | Health-Check Logs |
| Erfolgreiche Generierungen | >85% | API Success Rate |
| Durchschnittliche Zeit | <120 Sek | Server Logs |
| Admin-Adoption | >50% nutzen R1 | Feature Usage |

### Growth-Phase (Monat 2-6)

| KPI | Ziel | Messung |
|-----|------|---------|
| Qualit√§tsscore | >7.5/10 | Manual Review |
| Kosten-Ersparnis | >500‚Ç¨ | vs. nur-OpenAI-Szenario |
| Server Uptime | >99% | Monitoring |

---

## üéØ Fazit & Empfehlung

### Zusammenfassung

‚úÖ **Kosteneffizient:** 0‚Ç¨ API-Kosten vs. 50-100‚Ç¨/Monat bei OpenAI  
‚ö†Ô∏è **Qualit√§t:** 7.3/10 (gut, aber nicht exzellent)  
‚úÖ **Datenschutz:** 100% lokal, DSGVO-optimal  
‚ö†Ô∏è **Hardware:** Ben√∂tigt dedizierte GPU (RTX 4090 empfohlen)  
‚è±Ô∏è **Aufwand:** 20-28h Setup + Integration

### Go/No-Go Decision

**‚úÖ GO** ‚Äî Als **erg√§nzende zweite Option** neben OpenAI.

**Begr√ºndung:**
1. **Best-of-Both-Worlds:** Admin kann je nach Bedarf w√§hlen
2. **Kosteneffizienz:** Massive Ersparnis bei hoher Nutzung
3. **Datenschutz:** Ideal f√ºr sensible Themen
4. **Qualit√§t:** Ausreichend f√ºr die meisten Use-Cases (7/10+)
5. **Kontrolle:** Keine Vendor Lock-in, unbegrenzte Nutzung

**Voraussetzungen:**
- GPU-Server verf√ºgbar (24GB+ VRAM)
- Bereitschaft f√ºr initiales Setup (20h+)
- Akzeptanz l√§ngerer Generierungszeiten (60-120s)
- OpenAI als Fallback f√ºr kritische Sets

### Deployment-Strategie

**Phase 1:** Nur OpenAI (Launch)  
**Phase 2:** DeepSeek R1 als Beta-Feature (Monat 2)  
**Phase 3:** Beide Engines gleichberechtigt (Monat 3+)

### Next Steps

1. **Jetzt:** Dokumentation reviewen & Hardware pr√ºfen
2. **Tag 1-2:** R1-Server Setup & Testing (8-12h)
3. **Tag 3:** App-Integration (6-8h)
4. **Tag 4:** Testing & Benchmarking (6-8h)
5. **Tag 5:** Dokumentation & Deployment (2h)

**Gesamtaufwand:** 22-30 Stunden (3-4 Tage)

---

**Erstellt:** 8. Oktober 2025  
**Version:** 1.0  
**Status:** ‚úÖ Bereit zur Umsetzung (als 2. Option neben OpenAI)
